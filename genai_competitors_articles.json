[
  {
    "source": "凤凰网（凤凰新媒体）",
    "company": "OpenAI",
    "title": "OpenAI 再不上市，财务窟窿就要把巨头们拖垮了",
    "date": "2026-01-30T05:11:54Z",
    "url": "https://tech.ifeng.com/c/8qKlbtnKkGd",
    "content": "OpenAI 已经走过了十个年头，但现在人们讨论的焦点，早已从 ChatGPT 的产品奇迹，转移到这家公司的生存能力上：权威机构和业内人士预测，OpenAI 最早可能在 2027 年就会耗尽资金。\n\n前不久「美国外交关系委员会」高级研究员、知名经济历史学家塞巴斯蒂安·马拉比 (Sebastian Mallaby) 在《纽约时报》刊登专栏文章，预测 OpenAI 可能在未来 18 个月内，也即 2027 年中左右烧光现金储备。\n\n根据微软之前财报里「无意间泄露」的信息，原 Fidelity 明星基金经理乔治·诺贝尔 (George Noble) 也发文指出，OpenAI 2025 年下半年的季度亏损可能高达 120 亿美元 -- -- 几乎和全年实收相同。而对于 OpenAI 来说，低垂的果实早就摘完了，创新越来越艰难，算力需求达到了 5 倍，模型的性能增长却只有 2 倍训练模型获得性能翻倍需要足足 5 倍的算力输入。\n\n《大空头》的原型人物迈克尔·布瑞 (Michale Burry) 转发了帖子，表示自己认为泡沫将会破裂，而由于 AI 已经和整个经济强绑定，政府将不得不出面救市，就像他在 2008 年经历过的那次一样。\n\n预测市场 Polymarket 上各种各样关于 OpenAI 的押注的关注度很高。\n\n有人在为 OpenAI 上市时的市值下注，估值各不相同；也有人在赌政府是否会在今年 7 月之前出手救助 OpenAI；还有人押注 OpenAI 会在 2027 年前被其他公司收购。\n\n今天最新报道，来自《华尔街日报》：OpenAI 可能计划在 26 年结束前上市。\n\n综合整个业界的预测，回顾 OpenAI 成立至今的发展轨迹，有下面几种可能的情境：\n\nOpenAI 在耗尽投资者资金后破产\n\nOpenAI 在一年到两年内，通过 IPO 继续募集资金\n\nAI 泡沫破裂，OpenAI 得到政府救市\n\nOpenAI 被其它巨头公司收/并购，或与其他 AI 公司合并\n\n以上几种情况的组合\n\n当然大家可以看出来，上述情境中大部分是偏负面的预测，没有覆盖所有可能出现的情况。不过站在 2026 年初的时间点，AI 泡沫论甚嚣尘上，我们还是要围绕事实 -- -- OpenAI 并不乐观的财务境况，来看看数字背后的故事。\n\n订阅补贴带来沉重负担\n\n在 OpenAI 官方发布的《2025 年企业 AI 状况报告》里，CEO 山姆·奥特曼透露了 ChatGPT 周活跃用户超过 8 亿。\n\n这是一个非常优秀的里程碑，毕竟 8 亿约等于全球人口的 10%。但尴尬的是，在这些用户中付费订阅用户只占约 5%，即 4000 万人左右。\n\n然而，去年汇丰对 OpenAI 的收入模型进行了分析，指出由于 AI 基础设施建设将在未来 5 年内对 OpenAI 造成至少 7920 亿美元的成本，OpenAI 需要在 2030 年初实现 30 亿周活跃用户，并且将付费用户比例提高到 10%，才能够避免「入不敷出」，甚至资金链断裂。\n\n换句话说，OpenAI 的周活用户量只需要再翻两倍多一点，但是付费用户数量却需要增长 6.5 倍。\n\n这份报告来自于汇丰环球投资研究 (HSBC Global Investment Research)，去年 11 月 24 日发布，名为《OpenAI：重新分析承诺与现金流》(OpenAI: Reassessing Promises and Cash Flow)，作者是欧洲 TMT 和全球科技解决方案的首席 Nicolas Cote-Colisson。\n\nOpenAI 要完成几乎不可能的用户增长，与此同时还要和 Google、Anthropic 等美国本土对手，以及大洋彼岸纷纷重金入局 AI 的互联网巨头公司和 AI 小龙展开直接的竞争。\n\n即便到了 2030 年 OpenAI 能实现 2-3 亿不等的付费用户，OpenAI 仍然需要支付巨额的算力成本去补贴其余 27-28 亿使用 ChatGPT 的免费用户。\n\nOpenAI 最近宣布为免费和 Go 档用户加入广告，毫无意外也是因为在 8 亿周活用户的量级上做补贴的成本太高了，越来越难以承受。\n\n而另一边，最新消息 Google 开始给 Gemini 用户发放 Google Cloud 额度了（Pro 每月 10 美元，Ultra 100 美元） -- -- 谁也不好说，没准这就是 OpenAI 被拖死的开始呢？\n\nOpenAI 的营收渠道\n\n即使 OpenAI 实现了 2-3 亿付费用户的目标，仅靠订阅收入和广告可能仍然无法回本。\n\n综合 OpenAI 自己的预测以及第三方研报，ChatGPT 本身到 2030 年的累积订阅收入将达到 2700 亿美元，而整个公司的年收入目标更是要达到 1700 亿美元左右，并且努力冲击 2000 亿美元。\n\n而 ChatGPT 订阅只能提供一部分，能够提供的年化收入只有 480-720 亿美元左右。粗略估算，到 2030 年，OpenAI 仍会有 1000-1500 亿美元的年收入缺口，需要开拓其它领域，增加收入来源。\n\n潜在的收入来源包括：\n\n广告：OpenAI 已经有庞大的用户基础，且用户粘性还算不错，广告展示和转化确实可能成为重要的收入支柱。但问在大语言模型产品中如何自然地植入广告，不只是 OpenAI，也是所有类似公司待解的难题。\n\n消费者硬件：传闻 OpenAI 可能在 2026 年下半年推出自有硬件设备，APPSO 和爱范儿此前报道该设备可能是一个「笔」形的产品。综合各权威信源，OpenAI 对富士康等 OEM 提出了高达 4000-5000 万台的备货要求。不过上述消息尚未得到官方确认。\n\nAPI和 Agent 服务：随着 AI agent 应用程序和变成 API 使用案例的增长，这块业务确实有潜力带来可观收入。\n\n但问题是，和具有记忆能力的大语言模型终端产品不同，API 是相对商品化的，只要性能相似、价格合理，用户就敢于比较轻松地切换不同 API 提供商。这意味着 OpenAI 很难在 API 业务上形成持续的壁垒。\n\n以及还有一个地缘问题：中国的最广大用户是很难转化为 OpenAI 的用户的（无论是 ChatGPT 还是 API）。而此时十年前的硅谷增长命题「下一个十亿」又回来了，这也是为什么你会看到 OpenAI 在疯狂地培育印度市场。\n\n企业许可：像 Perplexity 这样的公司使用 OpenAI 的模型来构建独特的产品，这部分 B 端业务相对稳定，但增长空间有限。另外就在前两天，苹果为 AI Siri 计划引入了 Gemini 作为底层模型供应商，虽然没有完全取代 ChatGPT（在苹果计划中 Siri 的部分功能可能分别由不同模型驱动），也为 OpenAI 的大客户增长带来冲击。\n\nOpenAI 在过去一年里与英伟达、甲骨文、微软、AMD、CoreWeave、Google 等公司，达成了极其复杂的连锁交易。在这些交易当中，OpenAI 或许能够获得一些周转资金，或是基础设施费用的减免。\n\n但问题是，这个巨大的「命运共同体」的本质，是拿钱去搞基建 -- -- OpenAI 只是过手，不是钱最终流去的方向。\n\n金钱窟窿\n\n根据今年 1 月公布的细节，OpenAI 整个 2025 年的实际收入达到 130 亿美元（去年 OpenAI CFO 曾经预测 25 年 ARR 200 亿美元，仅供参考） -- -- 与 2024 年的 37 亿美元收入相比，实现了惊人的增长。\n\n然而，这种增长伴随着巨大的成本。据 The Information 报道，OpenAI 在 2025 年上半年的实收为 43 亿美元，但现金消耗达 25 亿美元，研发支出高达 67 亿美元。\n\n前不久有消息称，软银为 OpenAI F 轮注入了 410 亿美元的现金（其中软银 Vision Fund 2 300亿，共同投资者 110 亿）。据报道，软银为了完成这笔交易不得不提前清算其它投资头寸。\n\n昨天，The Information 再爆料 OpenAI 新一轮融资的总额可能高达 100 0 亿美元。其中除了软银的 300 亿之外，还有英伟达拟投资的 300 亿，以及亚马逊的 200 亿美元左右。\n\n显然，市面上能扶得起 OpenAI 的资方已经不多了。OpenAI 的资方早已不再是 VC、PE，而是整个产业。算力提供商既是 OpenAI 最大的债主，现在又要变成最大的股东。左手倒右手的资本循环，更加彰显这个命运共同体的本质。\n\n根据德意志银行今年 1 月的最新预测，OpenAI 在 2026 年的全年现金消耗将飙升至 170 亿美元左右。也就是说，以现在的烧钱增速来看，如果 OpenAI 谈不下来这轮 1000 亿的钱，可能不到两年 F 轮融资就可能烧完。\n\n与此同时，前面提到的汇丰首席分析师 Nicolas Cote-Collison 在研报中预测，OpenAI 到 2030 年仍然无法实现盈利，累计自由现金流仍然为负。他认为 OpenAI 处在一个史无前例般的财务黑洞当中：在扣除已知的融资和预计收入后，汇丰计算 OpenAI 将面临高达 2070 亿美元的资金短缺。\n\n2070 亿美元是什么概念？OpenAI 必须在未来 5 年内，平均每天额外筹集 1 亿美元，才能填补窟窿。金融时报 Alphaville 博客做了一个著名的讽刺：OpenAI 本质上只是一个「盖着网页的金钱窟窿」(a money pit with a website on top.)\n\n问题是，填补窟窿的方法还没有找到，但 OpenAI 继续花钱的承诺已经给出去了：在接下来的 5 年里，日常产品服务的后端造成的数据中心租赁费用可能高达 6200 亿美元 -- -- OpenAI 的用户增长速度早已超过了数据中心修建的速度，所以不得不从微软、亚马逊、甲骨文等租用数据中心提供算力。\n\n还有电费，OpenAI 对于 Stargate 数据中心的目标，是到 2030 年底拥有超过 36吉瓦的 AI 算力，相当于比美国佛罗里达州还要大的用电需求。\n\n根据汇丰的估算，算力租赁、基建、电力相加起来，OpenAI 未来 5 年内的基础设施总成本将高达 7920 亿美元，其中算力账单将高达 4300 亿美元。基建的投资增速只增不减：CEO 奥特曼还提出未来 8 年总计 1.4 万亿美元的总算力承诺。\n\nOpenAI 需要更多的钱，但已经投进去的巨头当中开始有人坐不住了。以微软为例，其最新财报显示云计算 45% 的积压订单来自 OpenAI。但与此同时，微软的资本支出增幅高达 89%，完全突破了最近 10 年以来的 CAPEX 曲线。\n\n财报发出后，微软股价一度暴跌 10%。\n\nOpenAI 这种巨大的、永无止境的资本需求，也是市场上出现关于政府救市、2027 年 IPO 或被收购等各种猜测的原因。\n\n再回头看刚才提到的 OpenAI 千亿美元融资：该轮融资的投前估值，可能在 7300-7500 亿美元左右。再结合 OpenAI 1 月披露的初步财务摘要，2025 年全年实际营收 130 亿美元 -- -- 意味着 OpenAI 的市销率高达 56 倍。\n\n对于一个还在巨额亏损的公司来说，这个估值让人不得不担忧。对比其它科技巨头，英伟达、微软均为高额盈利，市销率分别为 24.5 和 12 倍；成熟盈利模式的科技巨头市销率一般在 10-15 倍左右。\n\n假设 OpenAI 上市并进入标普 500，指数里唯一比 OpenAI 市销率高的是 Palantir，超过 100 倍。但 Palantir 不仅处于盈利状态，还和全球混乱局势正相关。如果 AI 泡沫破裂，世界经济遭到重创， Palantir 只会赚得更多，但 OpenAI 可能会完蛋。\n\nOpenAI 的估值，赌的其实是它能够从 AI 产品、服务商和基模供应商，进化为一个全球级别的 AI「基础设施」。但如果 2027 年之前它无法通过各种方法实现前面提到的付费用户增长，并且获得规模更离谱的新融资的话，7500 亿美元估值将面临巨大的回调压力。\n\n事实上，整个 AI 行业都被卷入了这场前所未有的烧钱竞赛。\n\n根据 IDC 等多家研究机构的数据，2025 年 AI 基础设施支出达到了惊人的规模：四大科技巨头（亚马逊、微软、Alphabet 和 Meta）去年在 AI 基础设施上的资本支出超过 3000 亿美元；瑞银估计，全球企业在 AI 基础设施上的支出，在未来几年内将上升到每年 5000 亿美元。\n\n这种前所未有的基础设施投资浪潮，是由 AI 模型的巨大计算需求，以及主要技术公司之间 AI 能力和产品的激烈竞争所驱动的。\n\n传统软件实现规模化的成本更低，但 AI 能力的规模化需要天量的前期投资。随着模型越来越大，训练和推理的成本只会继续上升。每个模型推理的高昂成本，包括并不限于显卡算力、电力、运营成本、人力成本等等，都进一步加剧各公司财务的紧张。\n\n市场份额刺刀战\n\n虽然 ChatGPT 在 2025 年曾一度凭借网页端超过 80% 的流量（数据源自 Visual Capitalist）建立起接近垄断的优势，但到了 2026 年初，这种优势正在被迅速蚕食。\n\n根据 Similarweb 2026 年 1 月的最新监测（和 Visual Capitalist 统计相比，增加了移动端的统计口径），随着 Google Gemini 在 Android 生态的深度发力，ChatGPT 的总体流量份额在 68% 左右，而 Gemini 则成功突破了 18% 的关键门槛。\n\n这意味着，AI 聊天市场已从「一家独大」开始进入双雄争霸，甚至群雄并起的新阶段。\n\n根据 Google 官方博客，Gemini 在 2025 年 11 月突破了 6.5 亿月活跃用户。这是一个重要的里程碑，虽然仍低于 ChatGPT 的 8 亿周活跃用户，但差距正在逐步缩小。\n\n与此同时，Anthropic 的 Claude 在特定领域特别是编程和代码相关任务中表现优异，正在蚕食 OpenAI 的专业用户基础。根据 The Information 最新报道，Anthropic 预测其营收可能会在 2029 年超过 OpenAI。\n\n另外，埃隆·马斯克的 xAI 也在虎视眈眈，依托着 X（原 Twitter）的庞大用户群体。\n\n与竞争对手不同，OpenAI 缺乏一个「杀手级应用」生态系统来深度绑定用户：\n\nGoogle 拥有整个工作空间（Gmail、Docs、Sheets 等）和 Android 生态系统的加持\n\nAnthropic 在专业的 Claude 代码使用方面建立了良好口碑\n\nxAI 拥有 X 平台的分发渠道和用户基础\n\n在 OpenAI 能够推出自己的杀手级应用或物理设备之前，通往 26 亿用户的道路可能会异常艰难。\n\n资本的退路上充满荆棘\n\n随着公司估值突破 7500 亿美元，自由市场的接盘能力已经接近极限。在 2026-2027 年的融资环境中，IPO 已经是 OpenAI 不得不考虑的突围战。《华尔街日报》今天报道，援引知情人士称 OpenAI 计划在 26 年第四季度前完成上市。\n\n尽早上市，OpenAI 尚可以利用公众目前尚且高涨的乐观情绪，将基建融资的风险分散给二级市场的全球资本。这符合股东套现的需求，同时也是维持 OpenAI 烧钱速度的可持续融资方案。\n\n然而，IPO 的时间选择至关重要。汇丰研报指出，OpenAI 目前仍然处在一个相对「轻资产」的阶段。但如果 Stargate 项目真的推进下去，到了 2030 年，OpenAI 将转变为一家背负上千亿美元硬件折旧压力的重资产公司 -- -- IPO 的风险将成倍增加。更别提届时投资者对 AI 的热情可能已经冷却。\n\n不仅如此，通往 IPO 的道路上，荆棘重重。除了前面提到的财务窟窿之外，OpenAI 还需要解决公司架构、财务和法务上的一些问题。\n\n比如，OpenAI 当前的「非盈利+封顶盈利」(nonprofit + capped profit) 架构，本身就是一个非常扭捏的发明，与当前二级市场的规则格格不入。如果要 IPO，OpenAI 势必要再经历一次争议性拉满的架构重组（更别提马斯克也已经发出了诉讼，要求千亿美元的天价赔偿，也会进一步让事态复杂化）。\n\n此外，OpenAI 与微软、英伟达、甲骨文等公司之间复杂且令人费解的连环资本局，在上市的时候也必须公之于众。并且这些交易的细节，也会对 OpenAI 去年所谓的 130 亿实收的真实情况带来修正。\n\n政府接盘的可能性\n\n考虑到 AI 的战略重要性以及 OpenAI 的关键作用，政府干预 -- -- 无论是崩塌的时候出面救市，还是直接参与投资，甚至像 Palantir 那样将其「国有化」都是完全有可能的。\n\n硅谷公司曾经崇尚海盗精神，但现如今 AI 圈子里已经全然沉浸在贸易保护主义当中。DeepSeek、Qwen、Kimi、GLM 等中国模型早已迎头赶上，本土的 OpenAI、Antropic 等公司也在借用地缘政治为自己溢价。\n\nOpenAI 作为美国 AI 霸权的排头兵之一，事实上已经获得了某种类似于「军工复合体」的地位。而这种地位，也有可能为其提供一层隐形的国家背书和保障。\n\n一种可能性是，政府通过低息贷款、税收抵免等方式来减轻 OpenAI 的财务负担。以及，政府还可能大规模采购 OpenAI 服务，为期提供流动性。\n\n这个事情的底线在于，美国政府可能不会允许领先的美国 AI 企业因为资金链断裂而崩溃，所以可能会届时出面救市。\n\n然而，政府救助也会带来独特的挑战，可能会严重影响 OpenAI 的运营自由度和战略方向。而且，在一个政治极化的时代，是否救助一家 AI 私企，必然会成为充满争议的政治话题。\n\n大而不倒，泡沫何时破？\n\nOpenAI 无疑在技术方面取得了巨大成就。GPT-4、GPT-4o 以及后续的模型都代表了人工智能的最前沿水平。ChatGPT 已经成为有史以来增长最快的消费者应用之一，8 亿周活跃用户就是最好的证明。\n\n但将技术成功转化为财务可持续性，是下一个更加困难的挑战。接下来，每个新模型都需要更强大的计算资源，更多的训练数据，更长的训练时间。这意味着 OpenAI 不能停止投入，必须持续增大投资才能保持技术领先。\n\n但是，AI 技术的进步现在似乎也撞上了所谓的「规模法则墙」。模型训练花费的成本更高，收益却更低。GPT-5 就是这个边际收益递减的最糟糕案例......。\n\n这就形成了一个恶性循环：需要更多资金来保持技术领先，但技术领先又需要更多资金来维持，而盈利的目标却似乎越来越遥远。\n\n从很多角度来看，OpenAI 确实「大而不倒」(too big to fail)。有太多的 VC、PE、大银行、产业巨头和经济利益与它紧密相关。放任它资金链断裂将产生巨大的连锁反应，各方绝对不可能视而不管。\n\n综上所述，事态发展的最佳方向，是 OpenAI 一边追赶模型性能，一边推出新服务来增长收入，同时获得过桥融资来维持运营，最终在 2027 年左右 IPO。\n\n整个 AI 行业多少都面临着和 OpenAI 类似的困境。成百上千亿美元资金涌入，推高了估值，创造了一个看似繁荣的生态系统。但仔细观察就会发现，很多 AI 公司的亏损增速远比收入增长更快。AI 投资已经变成了一个信仰行为，而非务实的财务投资行为。\n\n金钱不是万能的，但没有钱是万万不能的。\n\nOpenAI 能否在资金耗尽之前，找到活路？"
  },
  {
    "source": "凤凰网（凤凰新媒体）",
    "company": "OpenAI",
    "title": "OpenAI 再不上市，财务窟窿就要把巨头们拖垮了",
    "date": "2026-01-30T05:47:20Z",
    "url": "https://tech.ifeng.com/c/8qKna1t7NeB",
    "content": "OpenAI 已经走过了十个年头，但现在人们讨论的焦点，早已从 GPT 的技术突破，转移到这家公司的生存能力上：权威机构和业内人士预测，OpenAI 最早可能在 2027 年就会耗尽资金。\n\n前不久「美国外交关系委员会」高级研究员、知名经济历史学家塞巴斯蒂安·马拉比 (Sebastian Mallaby) 在《纽约时报》刊登专栏文章，预测 OpenAI 可能在未来 18 个月内，也即 2027 年中左右烧光现金。\n\n根据微软之前财报里「无意间泄露」的信息，原 Fidelity 明星基金经理乔治·诺贝尔 (George Noble) 也发文指出，OpenAI 2025 年下半年的季度亏损可能高达 120 亿美元。而对于 OpenAI 来说，低垂的果实早就摘完了，创新越来越艰难，算力需求达到了 5 倍，模型的性能增长却只有 2 倍训练模型获得性能翻倍需要足足 5 倍的算力输入。\n\n《大空头》的原型人物迈克尔·布瑞 (Michale Burry) 转发了帖子，表示自己认为泡沫将会破裂，而由于 AI 已经和整个经济强绑定，政府将不得不出面救市，就像他在 2008 年经历过的那次一样。\n\n预测市场 Polymarket 上各种各样关于 OpenAI 的押注的关注度很高。\n\n有人在为 OpenAI 上市时的市值下注，估值各不相同；也有人在赌政府是否会在今年 7 月之前出手救助 OpenAI；还有人押注 OpenAI 会在 2027 年前被其他公司收购。\n\n综合整个业界的预测，回顾 OpenAI 成立至今的发展轨迹，有下面几种可能的情境：\n\nOpenAI 在耗尽投资者后破产\n\nOpenAI 在一年到两年内，通过 IPO 继续募集资金\n\nAI 泡沫破裂，OpenAI 得到政府救市\n\nOpenAI 被其它巨头公司收/并购，或与其他 AI 公司合并\n\n以上几种情况的组合\n\n当然大家可以看出来，上述情境中大部分是偏负面的预测，没有覆盖所有可能出现的情况。不过站在 2026 年初的时间点，AI 泡沫论甚嚣尘上，我们还是要围绕事实 -- -- OpenAI 并不乐观的财务境况，来看看数字背后的故事。\n\n订阅补贴带来沉重负担\n\n在 OpenAI 官方发布的《2025 年企业 AI 状况报告》里，CEO 山姆·奥特曼透露了 ChatGPT 周活跃用户超过 8 亿。\n\n这是一个非常优秀的里程碑，毕竟 8 亿约等于全球人口的 10%。但尴尬的是，在这些用户中付费订阅用户只占约 5%，即 4000 万人左右。\n\n然而，去年汇丰对 OpenAI 的收入模型进行了分析，指出由于 AI 基础设施建设将在未来 5 年内对 OpenAI 造成至少 7920 亿美元的成本，OpenAI 需要在 2030 年初实现 30 亿周活跃用户，并且将付费用户比例提高到 10%，才能够避免「入不敷出」，甚至资金链断裂。\n\n换句话说，OpenAI 的周活用户量只需要再翻两倍多一点，但是付费用户数量却需要增长 6.5 倍。\n\n这份报告来自于汇丰环球投资研究 (HSBC Global Investment Research)，去年 11 月 24 日发布，名为《OpenAI：重新分析承诺与现金流》(OpenAI: Reassessing Promises and Cash Flow)，作者是欧洲 TMT 和全球科技解决方案的首席 Nicolas Cote-Colisson。\n\nOpenAI 要完成几乎不可能的用户增长，与此同时还要和 Google、Anthropic 等美国本土对手，以及大洋彼岸纷纷重金入局 AI 的互联网巨头公司和 AI 小龙展开直接的竞争。\n\n即便到了 2030 年 OpenAI 能实现 2-3 亿不等的付费用户，OpenAI 仍然需要支付巨额的算力成本去补贴其余 27-28 亿使用 ChatGPT 的免费用户。\n\nOpenAI 最近宣布为免费和 Go 档用户加入广告，毫无意外也是因为在 7 亿周活用户的量级上做补贴的成本太高了，越来越难以承受。\n\n而另一边，最新消息 Google 开始给 Gemini 用户发放 Google Cloud 额度了（Pro 每月 10 美元，Ultra 100 美元） -- -- 谁也不好说，没准这就是 OpenAI 被拖死的开始呢？\n\n即使 OpenAI 实现了 2-3 亿付费用户的目标，仅靠订阅收入和广告可能仍然无法回本。\n\n综合 OpenAI 自己的预测以及第三方研报，ChatGPT 本身到 2030 年的累积订阅收入将达到 2700 亿美元，而整个公司的年收入目标更是要达到 1700 亿美元左右，并且努力冲击 2000 亿美元。\n\n而 ChatGPT 订阅只能提供一部分，能够提供的年化收入只有 480-720 亿美元左右。粗略估算，到 2030 年，OpenAI 仍会有 1000-1500 亿美元的年收入缺口，需要开拓其它领域，增加收入来源。\n\n潜在的收入来源包括：\n\n广告：OpenAI 已经有庞大的用户基础，且用户粘性还算不错，广告展示和转化确实可能成为重要的收入支柱。但问在大语言模型产品中如何自然地植入广告，不只是 OpenAI，也是所有类似公司待解的难题。\n\n消费者硬件：传闻 OpenAI 可能在 2026 年下半年推出自有硬件设备，APPSO 和爱范儿此前报道该设备可能是一个「笔」形的产品。综合各权威信源，OpenAI 对富士康等 OEM 提出了高达 4000-5000 万台的备货要求。不过上述消息尚未得到官方确认。\n\nAPI 和 Agent 服务：随着 AI agent 应用程序和变成 API 使用案例的增长，这块业务确实有潜力带来可观收入。\n\n但问题是，和具有记忆能力的大语言模型终端产品不同，API 是相对商品化的，只要性能相似、价格合理，用户就敢于比较轻松地切换不同 API 提供商。这意味着 OpenAI 很难在 API 业务上形成持续的壁垒。\n\n以及还有一个地缘问题：中国的最广大用户是很难转化为 OpenAI 的用户的（无论是 ChatGPT 还是 API）。而此时十年前的硅谷增长命题「下一个十亿」又回来了，这也是为什么你会看到 OpenAI 在疯狂地培育印度市场。\n\n企业许可：像 Perplexity 这样的公司使用 OpenAI 的模型来构建独特的产品，这部分 B 端业务相对稳定，但增长空间有限。另外就在前两天，苹果为 AI Siri 计划引入了 Gemini 作为底层模型供应商，虽然没有完全取代 ChatGPT（在苹果计划中 Siri 的部分功能可能分别由不同模型驱动），也为 OpenAI 的大客户增长带来冲击。\n\nOpenAI 在过去一年里与英伟达、甲骨文、微软、AMD、CoreWeave、Google 等公司，达成了极其复杂的连锁交易。在这些交易当中，OpenAI 或许能够获得一些周转资金，或是基础设施费用的减免。\n\n但问题是，这个巨大的「命运共同体」的本质，是拿钱去搞基建 -- -- OpenAI 只是过手，不是钱最终流去的方向。\n\n根据今年 1 月公布的细节，OpenAI 整个 2025 年的实际收入达到 130 亿美元（去年 OpenAI CFO 曾经预测 25 年 ARR 200 亿美元，仅供参考） -- -- 与 2024 年的 37 亿美元收入相比，实现了惊人的增长。\n\n然而，这种增长伴随着巨大的成本。据 The Information 报道，OpenAI 在 2025 年上半年的实收为 43 亿美元，但现金消耗达 25 亿美元，研发支出高达 67 亿美元。\n\n前不久，软银为 OpenAI F 轮注入了 410 亿美元的现金（软银 Vision Fund 2 300亿，共同投资者 110 亿）。据报道，软银为了完成这笔交易不得不提前清算其它投资头寸 -- -- 市面上能扶得起 OpenAI 的资方，已经不多了。\n\n而根据德意志银行今年 1 月的最新预测，OpenAI 在 2026 年的全年现金消耗将飙升至 170 亿美元左右。也就是说，以现在的烧钱增速来看，最多两年，甚至可能不到两年，F 轮融资就可能烧完。\n\n与此同时，前面提到的汇丰首席分析师 Nicolas Cote-Collison 在研报中预测，OpenAI 到 2030 年仍然无法实现盈利，累计自由现金流仍然为负。他认为 OpenAI 处在一个史无前例般的财务黑洞当中：在扣除已知的融资和预计收入后，汇丰计算 OpenAI 将面临高达 2070 亿美元的资金短缺。\n\n2070 亿美元是什么概念？OpenAI 必须在未来 5 年内，平均每天额外筹集 1 亿美元，才能填补窟窿。金融时报 Alphaville 博客做了一个著名的讽刺：OpenAI 本质上只是一个「盖着网页的金钱窟窿」(a money pit with a website on top.)\n\n问题是，填补窟窿的方法还没有找到，但 OpenAI 继续花钱的承诺已经给出去了：在接下来的 5 年里，日常产品服务的后端造成的数据中心租赁费用可能高达 6200 亿美元 -- -- OpenAI 的用户增长速度早已超过了数据中心修建的速度，所以不得不从微软、亚马逊、甲骨文等租用数据中心提供算力。\n\n还有电费，OpenAI 对于 Stargate 数据中心的目标，是到 2030 年底拥有超过 36吉瓦的 AI 算力，相当于比美国佛罗里达州还要大的用电需求。\n\n根据汇丰的估算，算力租赁、基建、电力相加起来，OpenAI 未来 5 年内的基础设施总成本将高达 7920 亿美元。基建的投资增速只增不减：CEO 奥特曼还提出未来 8 年总计 1.4 万亿美元的总算力承诺。\n\n这种巨大的、永无止境的资本需求，也是市场上出现关于政府救市、2027 年 IPO 或被收购等各种猜测的原因。\n\n根据《华尔街日报》、彭博社的报道，OpenAI 已经进入新一轮规模至少在 500 亿美元，可能高达 1000 一亿美元的巨额融资谈判。投资方中可能包括来自阿联酋等主权财富基金的资金。\n\n该轮融资的投前估值高达 7500 亿美元。再结合 OpenAI 1 月披露的初步财务摘要，2025 年全年实际营收 130 亿美元 -- -- 意味着 OpenAI 的市销率高达 57.7 倍。\n\n对于一个还在巨额亏损的公司来说，这个估值让人不得不担忧。对比其它科技巨头，英伟达、微软均为高额盈利，市销率分别为 24.5 和 12 倍；成熟盈利模式的科技巨头市销率一般在 10-15 倍左右。\n\n假设 OpenAI 上市并进入标普 500，指数里唯一比 OpenAI 市销率高的是 Palantir，超过 100 倍。但 Palantir 不仅处于盈利状态，还和全球混乱局势正相关。如果 AI 泡沫破裂，世界经济遭到重创， Palantir 只会赚得更多，但 OpenAI 可能会完蛋。\n\nOpenAI 的估值，赌的其实是它能够从 AI 产品、服务商和基模供应商，进化为一个全球级别的 AI「基础设施」。但如果 2027 年之前它无法通过各种方法实现前面提到的付费用户增长，并且获得规模更离谱的新融资的话，7500 亿美元估值将面临巨大的回调压力。\n\n事实上，整个 AI 行业都被卷入了这场前所未有的烧钱竞赛。\n\n根据 IDC 等多家研究机构的数据，2025 年 AI 基础设施支出达到了惊人的规模：四大科技巨头（亚马逊、微软、Alphabet 和 Meta）去年在 AI 基础设施上的资本支出超过 3000 亿美元；瑞银估计，全球企业在 AI 基础设施上的支出，在未来几年内将上升到每年 5000 亿美元。\n\n这种前所未有的基础设施投资浪潮，是由 AI 模型的巨大计算需求，以及主要技术公司之间 AI 能力和产品的激烈竞争所驱动的。\n\n传统软件实现规模化的成本更低，但 AI 能力的规模化需要天量的前期投资。随着模型越来越大，训练和推理的成本只会继续上升。每个模型推理的高昂成本，包括并不限于显卡算力、电力、运营成本、人力成本等等，都进一步加剧各公司财务的紧张。\n\n市场份额刺刀战\n\n虽然 ChatGPT 在 2025 年曾一度凭借网页端超过 80% 的流量（数据源自 Visual Capitalist）建立起接近垄断的优势，但到了 2026 年初，这种优势正在被迅速蚕食。\n\n根据 Similarweb 2026 年 1 月的最新监测（和 Visual Capitalist 统计相比，增加了移动端的统计口径），随着 Google Gemini 在 Android 生态的深度发力，ChatGPT 的总体流量份额在 68% 左右，而 Gemini 则成功突破了 18% 的关键门槛。\n\n这意味着，AI 聊天市场已从「一家独大」开始进入双雄争霸，甚至群雄并起的新阶段。\n\n根据 Google 官方博客，Gemini 在 2025 年 11 月突破了 6.5 亿月活跃用户。这是一个重要的里程碑，虽然仍低于 ChatGPT 的 8 亿周活跃用户，但差距正在逐步缩小。\n\n与此同时，Anthropic 的 Claude 在特定领域特别是编程和代码相关任务中表现优异，正在蚕食 OpenAI 的专业用户基础。埃隆·马斯克的 xAI 也在虎视眈眈，依托着 X（原 Twitter）的庞大用户群体。\n\n与竞争对手不同，OpenAI 缺乏一个「杀手级应用」生态系统来深度绑定用户：\n\nGoogle 拥有整个工作空间（Gmail、Docs、Sheets 等）和 Android 生态系统的加持\n\nAnthropic 在专业的 Claude 代码使用方面建立了良好口碑\n\nxAI 拥有 X 平台的分发渠道和用户基础\n\n在 OpenAI 能够推出自己的杀手级应用或物理设备之前，通往 26 亿用户的道路可能会异常艰难。\n\n资本的退路上充满荆棘\n\n随着公司估值突破 7500 亿美元，自由市场的接盘能力已经接近极限。在 2026-2027 年的融资环境中，IPO 已经是 OpenAI 不得不考虑的突围战。\n\n2027 年左右 IPO，OpenAI 可以利用公众目前尚且高涨的乐观情绪，将基础设施融资的风险分散给二级市场的全球资本。这符合股东套现的需求，同时也是维持 OpenAI 烧钱速度的可持续融资方案。\n\n然而，IPO 的时间选择至关重要。汇丰研报指出，OpenAI 目前仍然处在一个相对「轻资产」的阶段。但如果 Stargate 项目真的推进下去，到了 2030 年，OpenAI 将转变为一家背负上千亿美元硬件折旧压力的重资产公司 -- -- IPO 的风险将成倍增加。更别提届时投资者对 AI 的热情可能已经冷却。\n\n不仅如此，通往 IPO 的道路上，荆棘重重。除了前面提到的财务窟窿之外，OpenAI 还需要解决公司架构、财务和法务上的一些问题。\n\n比如，OpenAI 当前的「非盈利+封顶盈利」(nonprofit + capped profit) 架构，本身就是一个非常扭捏的发明，与当前二级市场的规则格格不入。如果要 IPO，OpenAI 势必要再经历一次争议性拉满的架构重组（更别提马斯克也已经发出了诉讼，要求千亿美元的天价赔偿，也会进一步让事态复杂化）。\n\n此外，OpenAI 与微软、英伟达、甲骨文等公司之间复杂且令人费解的连环资本局，在上市的时候也必须公之于众。并且这些交易的细节，也会对 OpenAI 去年所谓的 130 亿实收的真实情况带来修正。\n\n政府接盘的可能性\n\n考虑到 AI 的战略重要性以及 OpenAI 的关键作用，政府干预 -- -- 无论是崩塌的时候出面救市，还是直接参与投资，甚至像 Palantir 那样将其「国有化」都是完全有可能的。\n\n硅谷公司曾经崇尚海盗精神，但现如今 AI 圈子里已经全然沉浸在贸易保护主义当中。DeepSeek、Qwen、Kimi、GLM 等中国模型早已迎头赶上，本土的 OpenAI、Antropic 等公司也在借用地缘政治为自己溢价。\n\nOpenAI 作为美国 AI 霸权的排头兵之一，事实上已经获得了某种类似于「军工复合体」的地位。而这种地位，也有可能为其提供一层隐形的国家背书和保障。\n\n一种可能性是，政府通过低息贷款、税收抵免等方式来减轻 OpenAI 的财务负担。以及，政府还可能大规模采购 OpenAI 服务，为期提供流动性。\n\n这个事情的底线在于，美国政府可能不会允许领先的美国 AI 企业因为资金链断裂而崩溃，所以可能会届时出面救市。\n\n然而，政府救助也会带来独特的挑战，可能会严重影响 OpenAI 的运营自由度和战略方向。而且，在一个政治极化的时代，是否救助一家 AI 私企，必然会成为充满争议的政治话题。\n\n大而不倒，泡沫何时破？\n\nOpenAI 无疑在技术方面取得了巨大成就。GPT-4、GPT-4o 以及后续的模型都代表了人工智能的最前沿水平。ChatGPT 已经成为有史以来增长最快的消费者应用之一，8 亿周活跃用户就是最好的证明。\n\n但将技术成功转化为财务可持续性，是下一个更加困难的挑战。接下来，每个新模型都需要更强大的计算资源，更多的训练数据，更长的训练时间。这意味着 OpenAI 不能停止投入，必须持续增大投资才能保持技术领先。\n\n但是，AI 技术的进步现在似乎也撞上了所谓的「规模法则墙」。模型训练花费的成本更高，收益却更低。GPT-5 就是这个边际收益递减的最糟糕案例......。\n\n这就形成了一个恶性循环：需要更多资金来保持技术领先，但技术领先又需要更多资金来维持，而盈利的目标却似乎越来越遥远。\n\n从很多角度来看，OpenAI 确实「大而不倒」(too big to fail)。有太多的 VC、PE、大银行、产业巨头和经济利益与它紧密相关。放任它资金链断裂将产生巨大的连锁反应，各方绝对不可能视而不管。\n\n综上所述，事态发展的最佳方向，是 OpenAI 一边追赶模型性能，一边推出新服务来增长收入，同时获得过桥融资来维持运营，最终在 2027 年左右 IPO。\n\n整个 AI 行业多少都面临着和 OpenAI 类似的困境。成百上千亿美元资金涌入，推高了估值，创造了一个看似繁荣的生态系统。但仔细观察就会发现，很多 AI 公司的亏损增速远比收入增长更快。AI 投资已经变成了一个信仰行为，而非务实的财务投资行为。\n\n金钱不是万能的，但没有钱是万万不能的。\n\nOpenAI 能否在资金耗尽之前，找到活路？"
  },
  {
    "source": "爱范儿",
    "company": "OpenAI",
    "title": "OpenAI 再不上市，财务窟窿就要把巨头们拖垮了",
    "date": "2026-01-30T05:07:03Z",
    "url": "https://www.ifanr.com/1653377",
    "content": "OpenAI 已经走过了十个年头，但现在人们讨论的焦点，早已从 GPT 的技术突破，转移到这家公司的生存能力上：权威机构和业内人士预测，OpenAI 最早可能在 2027 年就会耗尽资金。\n\n前不久「美国外交关系委员会」高级研究员、知名经济历史学家塞巴斯蒂安·马拉比 (Sebastian Mallaby) 在《纽约时报》刊登专栏文章，预测 OpenAI 可能在未来 18 个月内，也即 2027 年中左右烧光现金。\n\n根据微软之前财报里「无意间泄露」的信息，原 Fidelity 明星基金经理乔治·诺贝尔 (George Noble) 也发文指出，OpenAI 2025 年下半年的季度亏损可能高达 120 亿美元。而对于 OpenAI 来说，低垂的果实早就摘完了，创新越来越艰难，算力需求达到了 5 倍，模型的性能增长却只有 2 倍训练模型获得性能翻倍需要足足 5 倍的算力输入。\n\n《大空头》的原型人物迈克尔·布瑞 (Michale Burry) 转发了帖子，表示自己认为泡沫将会破裂，而由于 AI 已经和整个经济强绑定，政府将不得不出面救市，就像他在 2008 年经历过的那次一样。\n\n预测市场 Polymarket 上各种各样关于 OpenAI 的押注的关注度很高。\n\n有人在为 OpenAI 上市时的市值下注，估值各不相同；也有人在赌政府是否会在今年 7 月之前出手救助 OpenAI；还有人押注 OpenAI 会在 2027 年前被其他公司收购。\n\n综合整个业界的预测，回顾 OpenAI 成立至今的发展轨迹，有下面几种可能的情境：\n\n当然大家可以看出来，上述情境中大部分是偏负面的预测，没有覆盖所有可能出现的情况。不过站在 2026 年初的时间点，AI 泡沫论甚嚣尘上，我们还是要围绕事实 -- -- OpenAI 并不乐观的财务境况，来看看数字背后的故事。\n\n在 OpenAI 官方发布的《2025 年企业 AI 状况报告》里，CEO 山姆·奥特曼透露了 ChatGPT 周活跃用户超过 8 亿。\n\n这是一个非常优秀的里程碑，毕竟 8 亿约等于全球人口的 10%。但尴尬的是，在这些用户中付费订阅用户只占约 5%，即 4000 万人左右。\n\n然而，去年汇丰对 OpenAI 的收入模型进行了分析，指出由于 AI 基础设施建设将在未来 5 年内对 OpenAI 造成至少 7920 亿美元的成本，OpenAI 需要在 2030 年初实现 30 亿周活跃用户，并且将付费用户比例提高到 10%，才能够避免「入不敷出」，甚至资金链断裂。\n\n换句话说，OpenAI 的周活用户量只需要再翻两倍多一点，但是付费用户数量却需要增长 6.5 倍。\n\n这份报告来自于汇丰环球投资研究 (HSBC Global Investment Research)，去年 11 月 24 日发布，名为《OpenAI：重新分析承诺与现金流》(OpenAI: Reassessing Promises and Cash Flow)，作者是欧洲 TMT 和全球科技解决方案的首席 Nicolas Cote-Colisson。\n\nOpenAI 要完成几乎不可能的用户增长，与此同时还要和 Google、Anthropic 等美国本土对手，以及大洋彼岸纷纷重金入局 AI 的互联网巨头公司和 AI 小龙展开直接的竞争。\n\n即便到了 2030 年 OpenAI 能实现 2-3 亿不等的付费用户，OpenAI 仍然需要支付巨额的算力成本去补贴其余 27-28 亿使用 ChatGPT 的免费用户。\n\nOpenAI 最近宣布为免费和 Go 档用户加入广告，毫无意外也是因为在 7 亿周活用户的量级上做补贴的成本太高了，越来越难以承受。\n\n而另一边，最新消息 Google 开始给 Gemini 用户发放 Google Cloud 额度了（Pro 每月 10 美元，Ultra 100 美元） -- -- 谁也不好说，没准这就是 OpenAI 被拖死的开始呢？\n\n即使 OpenAI 实现了 2-3 亿付费用户的目标，仅靠订阅收入和广告可能仍然无法回本。\n\n综合 OpenAI 自己的预测以及第三方研报，ChatGPT 本身到 2030 年的累积订阅收入将达到 2700 亿美元，而整个公司的年收入目标更是要达到 1700 亿美元左右，并且努力冲击 2000 亿美元。\n\n而 ChatGPT 订阅只能提供一部分，能够提供的年化收入只有 480-720 亿美元左右。粗略估算，到 2030 年，OpenAI 仍会有 1000-1500 亿美元的年收入缺口，需要开拓其它领域，增加收入来源。\n\n潜在的收入来源包括：\n\n广告：OpenAI 已经有庞大的用户基础，且用户粘性还算不错，广告展示和转化确实可能成为重要的收入支柱。但问在大语言模型产品中如何自然地植入广告，不只是 OpenAI，也是所有类似公司待解的难题。\n\n消费者硬件：传闻 OpenAI 可能在 2026 年下半年推出自有硬件设备，APPSO 和爱范儿此前报道该设备可能是一个「笔」形的产品。综合各权威信源，OpenAI 对富士康等 OEM 提出了高达 4000-5000 万台的备货要求。不过上述消息尚未得到官方确认。\n\nAPI 和 Agent 服务：随着 AI agent 应用程序和变成 API 使用案例的增长，这块业务确实有潜力带来可观收入。\n\n但问题是，和具有记忆能力的大语言模型终端产品不同，API 是相对商品化的，只要性能相似、价格合理，用户就敢于比较轻松地切换不同 API 提供商。这意味着 OpenAI 很难在 API 业务上形成持续的壁垒。\n\n以及还有一个地缘问题：中国的最广大用户是很难转化为 OpenAI 的用户的（无论是 ChatGPT 还是 API）。而此时十年前的硅谷增长命题「下一个十亿」又回来了，这也是为什么你会看到 OpenAI 在疯狂地培育印度市场。\n\n企业许可：像 Perplexity 这样的公司使用 OpenAI 的模型来构建独特的产品，这部分 B 端业务相对稳定，但增长空间有限。另外就在前两天，苹果为 AI Siri 计划引入了 Gemini 作为底层模型供应商，虽然没有完全取代 ChatGPT（在苹果计划中 Siri 的部分功能可能分别由不同模型驱动），也为 OpenAI 的大客户增长带来冲击。\n\nOpenAI 在过去一年里与英伟达、甲骨文、微软、AMD、CoreWeave、Google 等公司，达成了极其复杂的连锁交易。在这些交易当中，OpenAI 或许能够获得一些周转资金，或是基础设施费用的减免。\n\n但问题是，这个巨大的「命运共同体」的本质，是拿钱去搞基建 -- -- OpenAI 只是过手，不是钱最终流去的方向。\n\n根据今年 1 月公布的细节，OpenAI 整个 2025 年的实际收入达到 130 亿美元（去年 OpenAI CFO 曾经预测 25 年 ARR 200 亿美元，仅供参考） -- -- 与 2024 年的 37 亿美元收入相比，实现了惊人的增长。\n\n然而，这种增长伴随着巨大的成本。据 The Information 报道，OpenAI 在 2025 年上半年的实收为 43 亿美元，但现金消耗达 25 亿美元，研发支出高达 67 亿美元。\n\n前不久，软银为 OpenAI F 轮注入了 410 亿美元的现金（软银 Vision Fund 2 300亿，共同投资者 110 亿）。据报道，软银为了完成这笔交易不得不提前清算其它投资头寸 -- -- 市面上能扶得起 OpenAI 的资方，已经不多了。\n\n而根据德意志银行今年 1 月的最新预测，OpenAI 在 2026 年的全年现金消耗将飙升至 170 亿美元左右。也就是说，以现在的烧钱增速来看，最多两年，甚至可能不到两年，F 轮融资就可能烧完。\n\n与此同时，前面提到的汇丰首席分析师 Nicolas Cote-Collison 在研报中预测，OpenAI 到 2030 年仍然无法实现盈利，累计自由现金流仍然为负。他认为 OpenAI 处在一个史无前例般的财务黑洞当中：在扣除已知的融资和预计收入后，汇丰计算 OpenAI 将面临高达 2070 亿美元的资金短缺。\n\n2070 亿美元是什么概念？OpenAI 必须在未来 5 年内，平均每天额外筹集 1 亿美元，才能填补窟窿。金融时报 Alphaville 博客做了一个著名的讽刺：OpenAI 本质上只是一个「盖着网页的金钱窟窿」(a money pit with a website on top.)\n\n问题是，填补窟窿的方法还没有找到，但 OpenAI 继续花钱的承诺已经给出去了：在接下来的 5 年里，日常产品服务的后端造成的数据中心租赁费用可能高达 6200 亿美元 -- -- OpenAI 的用户增长速度早已超过了数据中心修建的速度，所以不得不从微软、亚马逊、甲骨文等租用数据中心提供算力。\n\n还有电费，OpenAI 对于 Stargate 数据中心的目标，是到 2030 年底拥有超过 36吉瓦的 AI 算力，相当于比美国佛罗里达州还要大的用电需求。\n\n根据汇丰的估算，算力租赁、基建、电力相加起来，OpenAI 未来 5 年内的基础设施总成本将高达 7920 亿美元。基建的投资增速只增不减：CEO 奥特曼还提出未来 8 年总计 1.4 万亿美元的总算力承诺。\n\n这种巨大的、永无止境的资本需求，也是市场上出现关于政府救市、2027 年 IPO 或被收购等各种猜测的原因。\n\n根据《华尔街日报》、彭博社的报道，OpenAI 已经进入新一轮规模至少在 500 亿美元，可能高达 1000 一亿美元的巨额融资谈判。投资方中可能包括来自阿联酋等主权财富基金的资金。\n\n该轮融资的投前估值高达 7500 亿美元。再结合 OpenAI 1 月披露的初步财务摘要，2025 年全年实际营收 130 亿美元 -- -- 意味着 OpenAI 的市销率高达 57.7 倍。\n\n对于一个还在巨额亏损的公司来说，这个估值让人不得不担忧。对比其它科技巨头，英伟达、微软均为高额盈利，市销率分别为 24.5 和 12 倍；成熟盈利模式的科技巨头市销率一般在 10-15 倍左右。\n\n假设 OpenAI 上市并进入标普 500，指数里唯一比 OpenAI 市销率高的是 Palantir，超过 100 倍。但 Palantir 不仅处于盈利状态，还和全球混乱局势正相关。如果 AI 泡沫破裂，世界经济遭到重创， Palantir 只会赚得更多，但 OpenAI 可能会完蛋。\n\nOpenAI 的估值，赌的其实是它能够从 AI 产品、服务商和基模供应商，进化为一个全球级别的 AI「基础设施」。但如果 2027 年之前它无法通过各种方法实现前面提到的付费用户增长，并且获得规模更离谱的新融资的话，7500 亿美元估值将面临巨大的回调压力。\n\n事实上，整个 AI 行业都被卷入了这场前所未有的烧钱竞赛。\n\n根据 IDC 等多家研究机构的数据，2025 年 AI 基础设施支出达到了惊人的规模：四大科技巨头（亚马逊、微软、Alphabet 和 Meta）去年在 AI 基础设施上的资本支出超过 3000 亿美元；瑞银估计，全球企业在 AI 基础设施上的支出，在未来几年内将上升到每年 5000 亿美元。\n\n这种前所未有的基础设施投资浪潮，是由 AI 模型的巨大计算需求，以及主要技术公司之间 AI 能力和产品的激烈竞争所驱动的。\n\n传统软件实现规模化的成本更低，但 AI 能力的规模化需要天量的前期投资。随着模型越来越大，训练和推理的成本只会继续上升。每个模型推理的高昂成本，包括并不限于显卡算力、电力、运营成本、人力成本等等，都进一步加剧各公司财务的紧张。\n\n虽然 ChatGPT 在 2025 年曾一度凭借网页端超过 80% 的流量（数据源自 Visual Capitalist）建立起接近垄断的优势，但到了 2026 年初，这种优势正在被迅速蚕食。\n\n根据 Similarweb 2026 年 1 月的最新监测（和 Visual Capitalist 统计相比，增加了移动端的统计口径），随着 Google Gemini 在 Android 生态的深度发力，ChatGPT 的总体流量份额在 68% 左右，而 Gemini 则成功突破了 18% 的关键门槛。\n\n这意味着，AI 聊天市场已从「一家独大」开始进入双雄争霸，甚至群雄并起的新阶段。\n\n根据 Google 官方博客，Gemini 在 2025 年 11 月突破了 6.5 亿月活跃用户。这是一个重要的里程碑，虽然仍低于 ChatGPT 的 8 亿周活跃用户，但差距正在逐步缩小。\n\n与此同时，Anthropic 的 Claude 在特定领域特别是编程和代码相关任务中表现优异，正在蚕食 OpenAI 的专业用户基础。埃隆·马斯克的 xAI 也在虎视眈眈，依托着 X（原 Twitter）的庞大用户群体。\n\n与竞争对手不同，OpenAI 缺乏一个「杀手级应用」生态系统来深度绑定用户：\n\n在 OpenAI 能够推出自己的杀手级应用或物理设备之前，通往 26 亿用户的道路可能会异常艰难。\n\n随着公司估值突破 7500 亿美元，自由市场的接盘能力已经接近极限。在 2026-2027 年的融资环境中，IPO 已经是 OpenAI 不得不考虑的突围战。\n\n2027 年左右 IPO，OpenAI 可以利用公众目前尚且高涨的乐观情绪，将基础设施融资的风险分散给二级市场的全球资本。这符合股东套现的需求，同时也是维持 OpenAI 烧钱速度的可持续融资方案。\n\n然而，IPO 的时间选择至关重要。汇丰研报指出，OpenAI 目前仍然处在一个相对「轻资产」的阶段。但如果 Stargate 项目真的推进下去，到了 2030 年，OpenAI 将转变为一家背负上千亿美元硬件折旧压力的重资产公司 -- -- IPO 的风险将成倍增加。更别提届时投资者对 AI 的热情可能已经冷却。\n\n不仅如此，通往 IPO 的道路上，荆棘重重。除了前面提到的财务窟窿之外，OpenAI 还需要解决公司架构、财务和法务上的一些问题。\n\n比如，OpenAI 当前的「非盈利+封顶盈利」(nonprofit + capped profit) 架构，本身就是一个非常扭捏的发明，与当前二级市场的规则格格不入。如果要 IPO，OpenAI 势必要再经历一次争议性拉满的架构重组（更别提马斯克也已经发出了诉讼，要求千亿美元的天价赔偿，也会进一步让事态复杂化）。\n\n此外，OpenAI 与微软、英伟达、甲骨文等公司之间复杂且令人费解的连环资本局，在上市的时候也必须公之于众。并且这些交易的细节，也会对 OpenAI 去年所谓的 130 亿实收的真实情况带来修正。\n\n考虑到 AI 的战略重要性以及 OpenAI 的关键作用，政府干预 -- -- 无论是崩塌的时候出面救市，还是直接参与投资，甚至像 Palantir 那样将其「国有化」都是完全有可能的。\n\n硅谷公司曾经崇尚海盗精神，但现如今 AI 圈子里已经全然沉浸在贸易保护主义当中。DeepSeek、Qwen、Kimi、GLM 等中国模型早已迎头赶上，本土的 OpenAI、Antropic 等公司也在借用地缘政治为自己溢价。\n\nOpenAI 作为美国 AI 霸权的排头兵之一，事实上已经获得了某种类似于「军工复合体」的地位。而这种地位，也有可能为其提供一层隐形的国家背书和保障。\n\n一种可能性是，政府通过低息贷款、税收抵免等方式来减轻 OpenAI 的财务负担。以及，政府还可能大规模采购 OpenAI 服务，为期提供流动性。\n\n这个事情的底线在于，美国政府可能不会允许领先的美国 AI 企业因为资金链断裂而崩溃，所以可能会届时出面救市。\n\n然而，政府救助也会带来独特的挑战，可能会严重影响 OpenAI 的运营自由度和战略方向。而且，在一个政治极化的时代，是否救助一家 AI 私企，必然会成为充满争议的政治话题。\n\nOpenAI 无疑在技术方面取得了巨大成就。GPT-4、GPT-4o 以及后续的模型都代表了人工智能的最前沿水平。ChatGPT 已经成为有史以来增长最快的消费者应用之一，8 亿周活跃用户就是最好的证明。\n\n但将技术成功转化为财务可持续性，是下一个更加困难的挑战。接下来，每个新模型都需要更强大的计算资源，更多的训练数据，更长的训练时间。这意味着 OpenAI 不能停止投入，必须持续增大投资才能保持技术领先。\n\n但是，AI 技术的进步现在似乎也撞上了所谓的「规模法则墙」。模型训练花费的成本更高，收益却更低。GPT-5 就是这个边际收益递减的最糟糕案例......。\n\n这就形成了一个恶性循环：需要更多资金来保持技术领先，但技术领先又需要更多资金来维持，而盈利的目标却似乎越来越遥远。\n\n从很多角度来看，OpenAI 确实「大而不倒」(too big to fail)。有太多的 VC、PE、大银行、产业巨头和经济利益与它紧密相关。放任它资金链断裂将产生巨大的连锁反应，各方绝对不可能视而不管。\n\n综上所述，事态发展的最佳方向，是 OpenAI 一边追赶模型性能，一边推出新服务来增长收入，同时获得过桥融资来维持运营，最终在 2027 年左右 IPO。\n\n整个 AI 行业多少都面临着和 OpenAI 类似的困境。成百上千亿美元资金涌入，推高了估值，创造了一个看似繁荣的生态系统。但仔细观察就会发现，很多 AI 公司的亏损增速远比收入增长更快。AI 投资已经变成了一个信仰行为，而非务实的财务投资行为。\n\n金钱不是万能的，但没有钱是万万不能的。\n\nOpenAI 能否在资金耗尽之前，找到活路？"
  },
  {
    "source": "凤凰网（凤凰新媒体）",
    "company": "OpenAI",
    "title": "在OpenAI\"创新已经变得困难\"！离职高管深喉爆料",
    "date": "2026-01-23T23:48:01Z",
    "url": "https://tech.ifeng.com/c/8qASLIImSry",
    "content": "刚离职的副总裁，把OpenAI的创新困境一股脑全曝光了。\n\n编译 | 陈骏达\n\n编辑 | Panken\n\n智东西1月23日消息，昨天，由知名媒体人和作家Ashlee Vance主办的Core Memory播客，发布了对OpenAI前研究副总裁Jerry Tworek的深度专访。今年1月初决定离开的OpenAI的Tworek分享了一个关键洞察：随着竞争加剧与组织急速膨胀，OpenAI正逐渐陷入一种难以再承担真正高风险研究的结构性困境，一些前沿创新的研究方向，已经难以在OpenAI内部推进。\n\n在正式进入访谈内容前，我们有必要了解下Tworek的传奇履历。Tworek是OpenAI元老级成员，2019年便加入该公司。他是OpenAI推理模型o1、o3背后的关键人物，将强化学习做到了极致，也让强化学习、推理模型真正进入了主流视野。此外，Tworek还在编程和Agent领域颇有建树。\n\n今年1月7日，Tworek在X平台上分享了自己离职的消息，评论区涌入了诸多OpenAI大佬，言语间满是不舍之情。\n\n这场访谈长达70分钟，实录近2万字，智东西梳理出Tworek分享的八大关键洞察：\n\n1、OpenAI的创新困境：成本、增长压力等多重因素影响了OpenAI对风险的\"胃口\"，同时该公司尚未找到良好的跨团队的研究协作模式。\n\n2、谷歌崛起：与其说是谷歌\"回归\"，不如说是OpenAI自己犯了错误，没能充分把握住自己的领先优势。OpenAI本应该持续领先。\n\n3、行业弊病：5家头部AI公司路径完全趋同，研究员想在主流机器学习范式之外做点不同的事情，几乎找不到合适的地方，这令人沮丧。\n\n4、人才争夺战：人才争夺战已演变成一场肥皂剧，有些人频繁地更换工作，而真正投入到工作的时间不多。\n\n5、创新引擎：明星AI研究员并不是驱动创新的核心因素，公司本身能否打造个人责任感强、允许探索和做大事的环境，可能更为关键。\n\n6、什么阻碍了创新：阻碍AI Lab研究的因素不是算力短缺，而是缺乏专注。对OpenAI来说，\"集中力量办大事\"已经变得有些困难。\n\n7、AGI时间表：目前AGI仍然缺失关键拼图，架构创新与持续学习是两大重要方向，但AGI将会在2029年左右实现。\n\n8、强化学习的回归：科学史已经反复证明，好的想法往往会卷土重来，判断一个想法是否重要并不难，难的是判断它什么时候会变得重要。\n\n以下是访谈内容的完整编译：\n\n01.\n\n竞争激烈、组织膨胀\n\nOpenAI的创新困境\n\n主持人：你的离职声明写得很好，充满感情。你在OpenAI经历了非常重要的一段时间，见证了巨大的变化。那种感觉怎么样？\n\nJerry Tworek：在OpenAI的每一年，都是一家完全不同的公司。公司本身的高速成长，以及整个AI世界的变化。\n\n这种经历我觉得在人类历史上都很少见。我很庆幸自己能亲身经历这一切。正如我之前说的，每一个阶段都完全不同。\n\n主持人：OpenAI在2019年时大概只有30个人？现在已经是几千人了吧？\n\nJerry Tworek：说实话，很难统计清楚。全球多地办公室，遍布世界各地。现在几乎找不到没听说过OpenAI的人了。而我刚加入的时候，只是几个小团队，各自做着自己的研究项目。\n\n但有一件事始终没变 -- -- OpenAI的野心。从一开始就瞄准AGI，想真正改变世界，并且带来正面的影响。而通过ChatGPT，把智能和实用性真正分发给了全球用户，我觉得这是一件非常了不起的事情。\n\n主持人：所以你发了那条推文之后，是不是全球所有基础模型实验室都来找你了？\n\nJerry Tworek：确实有很多。我现在也在思考下一步该做什么。在这个行业这么多年，我已经认识了很多人。我并不急着做决定。\n\n我已经连续高强度工作很多年了，甚至没太多时间好好和人聊天。现在正好可以慢下来，想一想：接下来的七年，我想怎么度过。但确实，我正在和很多人交流。\n\n主持人：你在推文里提到，你想做一些在OpenAI没法做的研究。能具体说说吗？\n\nJerry Tworek：当前，在全球范围内争夺\"最佳AI模型\"的竞争异常激烈且严苛。想要保持竞争力，公司在运营的多个层面都面临着极大的挑战。\n\n其中一个核心问题在于风险承担的意愿：从避免落后的角度出发，公司自然会被迫思考，究竟愿意承担多大的风险。无论是用户增长指标，还是持续支付高昂的GPU成本，现实都极其残酷。\n\n也正因如此，持续展示实力、不断推出最强模型，对所有人而言都变得至关重要。这是当下几乎所有主要AI公司共同面临的处境，而这种压力无疑会影响一家机构对风险的\"胃口\"。\n\n另一组同样难以权衡的因素来自组织结构。公司有其组织架构图，而组织架构往往在很大程度上决定了你能够开展什么样的研究：每个团队都需要明确的身份认同、研究边界以及其专注解决的问题集合。\n\n跨组织的研究往往异常困难，而如何在大规模条件下高效地组织研究，这个问题可能还没有被真正解决。\n\n研究本身偏好活力，甚至可以说偏好某种程度的混乱；而大型组织却需要秩序、结构与清晰的分工。这正是为什么\"你最终交付的是你的组织架构图\"这一说法如此流行：研究工作往往会演变成那些最适合现有人员配置的项目。\n\n我也正是在这种背景下意识到，有一些我真正想做的研究方向，并不是OpenAI当前的组织架构所能支持的。\n\n02.\n\nTransformer肯定不是最终形态\n\n有很多路径尚未得到系统性实践\n\n主持人：我曾在播客里和Mark Chen（OpenAI首席研究官）聊过这个问题 -- -- 几乎所有人都在向他（以及Jakub，OpenAI首席科学家）提出自己的想法。OpenAI的确有一个优良传统：愿意承担风险，愿意去做一些其他实验室不敢做的事情。\n\n但现实是，无论聚集了多少聪明人，资源已相当可观，这终究是一家资源有限的公司。它必须做出重大的取舍：哪些方向值得投入，哪些现在还无法承担成本。\n\n而真正足够新颖的路径，往往恰恰是那种让人犹豫的方向 -- -- 我们不知道现在该不该走，也不知道钱包是否负担得起。\n\nJerry Tworek：关于Ilya提出的\"研究时代\"这一概念，我不确定它是否像他所描述的那样非此即彼，但我确信，在AI和机器学习领域，仍然存在大量尚未被充分探索的可能性。\n\n六年前，我们选定了Transformer架构，此后人们不断对其进行Scaling，并且效果显著。路径非常清晰：每个季度训练更大的模型，使用更多计算资源和数据，而进步似乎从未真正停滞。\n\n但问题在于：这就是全部了吗？这是最终形态吗？我相当确定不是。模型仍然可以通过多种方式改进，而其中许多路径至今尚未被系统性地实践。\n\n正如你提到的，我曾在推理和强化学习扩展方面投入大量工作。在那之前，整个领域几乎把所有赌注都押在了Transformer预训练的Scaling上。\n\n这种方式确实有效：每一次预训练都可以打造出更强的模型，其能力都会全面提升，各项评测指标也随之改善。因此，人们很容易得出结论：只要不断扩展预训练，模型就会持续变好。\n\n但后来，一些研究者开始相信，我们能做的不止于此。他们尝试证明：如果在语言模型之上，以与预训练相当的计算规模去扩展强化学习，就能教会模型一些仅靠预训练永远无法获得的能力。\n\n正是由于这种探索，我们今天才拥有了这些能够自动化复杂任务、显著降低计算与数据需求的智能体系统。一旦发现新的扩展路径，就能解锁全新的能力，而如果只沿着预训练的扩展定律前进，这些能力可能需要极其漫长的时间才能出现。\n\n在我看来，自GPT-4发布以来，推理模型代表了一次真正重大的能力跃迁。而我也坚信，类似这样的突破并非孤例。研究者不应只满足于渐进式改进，更应持续思考如何从根本上改变游戏规则。\n\n03.\n\n头部AI玩家路径趋同\n\n这是件令人遗憾的事儿\n\n主持人：去年在NeurIPS上，Ilya提到\"我们正在耗尽数据\"，暗示预训练终将触及瓶颈。\n\nJerry Tworek：我并不认为这意味着预训练即将终结，它仍然在持续改进，也依然有许多优化空间。但预训练并不是提升模型能力的唯一方式，而且在很多情况下，它提升得非常缓慢。其他方法，或许能更快地推动能力跃迁。\n\n主持人：硅谷长期存在一种有趣的现象：科技公司往往会提出一些在外界看来怪异、甚至离经叛道的想法，而正是这些想法催生了真正颠覆性的创新。\n\n但一旦某条路径被证明是成功的，局面就会迅速反转，形成强烈的共识，所有人开始沿着同一方向竞赛。\n\n这正是我们当前所处的阶段。模型竞赛已经持续了两三年，几乎所有主要实验室都在做同一件事。你认为这是个问题吗？\n\nJerry Tworek：我对此感到相当、相当遗憾，几乎所有公司都在做和OpenAI一样的事儿。OpenAI无疑取得了巨大成功，做对了更多事儿，引领了扩展Transformer的范式，也证明了大规模模型能够为世界创造真实而广泛的价值。\n\n但如今，有多少公司在做着几乎一模一样的事情？竞争当然有其价值，但我们现在大概有五家严肃的AI公司，使用几乎相同的技术配方，在同一技术基础上构建略有差异的产品。\n\n也许这是正确的路径，但我仍然希望看到更多多样性，模型之间真正的差异，而不仅是微小的调优。\n\n如果你观察当下最顶级的模型，很少有人能真正分辨它们之间的不同。或许我们应该进行更多盲测：让用户与不同模型对话，看看他们是否能分辨出差异。\n\n我怀疑99.9%的用户做不到。这些模型在体验上极其相似，即便它们来自不同团队、采用了略有不同的技术选择。在这样的环境中，真正的探索在哪里？真正的创新，以及与他人区分开来的能力，又在哪里？\n\n04.\n\n与OpenAI已出现实质性分歧\n\n分开比勉强合作更健康\n\n主持人：我问一个有些尖锐的问题：你在OpenAI内外都被视为传奇人物，参与的项目成功率极高。如果连你这样的人，都觉得自己真正想做的事情在公司内部难以推进 -- -- 无论公司是否明确反对，这种阻力本身就已经存在。\n\n对于一家最初以研究实验室起家的公司而言，这是否是一个值得警惕的信号？\n\nJerry Tworek：我的看法是，有时候人们会成长到某个阶段，需要与过去分道扬镳。对一家公司及其成员来说，就目标和前进方向达成一致极其重要。\n\n而在某个时刻，我意识到自己对未来研究路线的看法，与OpenAI所选择的方向在某种实质性层面上出现了分歧。在这种情况下，分开也许比勉强合作要更健康。\n\n正因如此，我也认为，如果不同公司能够真正专注于不同的事情，行业会因此变得更好。专注对一家公司而言至关重要，而OpenAI很可能正在做所有正确的事情。\n\n也许只是我怀抱了一些不切实际的梦想。我是一个相对乐观的人，我相信世界上始终还有很多不同的事情可以去做，这在原则上完全是可能的。\n\n关键在于专注，把真正核心的事情做到极致。事实上，很多事情、很多公司，只有做到这一点，才能生存下来并进入下一个阶段。\n\n在一个理想的世界里，应该存在大量做着不同事情的公司。尤其是对研究人员而言，他们很难在一个自己并不真正相信的研究方向上长期投入。他们理应能够找到一个地方，在那里从事自己最笃信的研究，并让时间来检验其价值。\n\n也正因为如此，我对如今几乎所有公司都在做同样的事情感到有些难过。现实是，如果你想在主流机器学习范式之外做点不同的事情，几乎找不到合适的地方。这可能是目前让我最沮丧的一点。\n\n主持人：当你开始认真思考\"下一步要做什么\"时，这种同质化的问题会变得尤为明显。如果所有实验室都在做同样的事，你自然也不会觉得自己只是换一家大实验室就能获得真正不同的空间。\n\nJerry Tworek：我确实在思考人生的下一个阶段，但如果世界上能有更多选择，让人可以稍微偏离主流，去做一些不那么热门、但可能同样重要的事情，那会让我更开心，也更容易做出决定。\n\n主持人：这就引出了一个问题：我们到底需要什么，才能真正偏离主流？\n\n一个投入了如此多资金和资源、又处在聚光灯下的公司，会本能地害怕承担风险。但问题在于，这些风险也许恰恰是必要的。那么，究竟需要改变什么？这种状况未来会不会发生改变？\n\nJerry Tworek：有趣的是，我个人其实非常喜欢冒险，别人也常这样形容我。冒险本身是一件好事。\n\n但当风险和巨额资金绑定在一起时，愿意、也有能力承担这种风险的人就会变得极其稀少。风险承受能力是一种高度个人化、极其独特的特质。我与很多人共事过，深切体会到这一点。\n\n我真心认为，人们本该更愿意承担风险，去尝试更多不同的事情。尤其是研究人员这一群体 -- -- 如今AI领域的薪酬水平已经相当夸张了，而这反而可能带来一种副作用：人们变得不愿意失去工作，不愿意经历糟糕的绩效周期。于是，他们更倾向于追逐短期回报。\n\n很多研究人员本身非常聪明、也很有想法，只是整个系统的激励机制过于短视。可恰恰是研究人员，才最应该被鼓励去冒险、去做大胆的尝试 -- -- 因为真正的进步，正是这样产生的。\n\n05.\n\n算力门槛正阻碍创新\n\n\"探索与利用\"的权衡是关键问题\n\n主持人：当然，我们也看到了一些例子。比如游戏教父John Carmack，他去了达拉斯的\"洞穴\"，一度几乎是独自工作，现在可能也只有极少数员工。Carmack说过：\"也许我未必能做出真正不同的东西，但至少应该有人在认真尝试一条完全不同的路径。\"\n\n我也和Ilya聊过，不过我并不清楚他具体在研究什么。所以我无法判断，他的工作是在延续过去的方向，还是某种更加激进的尝试。但可以肯定的是，如果他不认为那是一条不同的道路，就不会去筹集那么多资金来做这件事。杨立昆显然也有与主流不同的理念。\n\n这正是让我觉得这个领域非常有趣的地方。AI在某种意义上是一个非常古老的领域，可以追溯到几十年前；但当下这套主流范式，其实是相对较新的。当我和研究人员交流时，他们仍然会说：\"只要把主要论文都读一遍，很快就能跟上进度。\"\n\n可我时常会想，会不会有某个人，突然带着一个极其激进、全新的想法出现，彻底推动整个领域向前？如今这件事似乎变得更难了，因为你可能需要一个州那么大的数据中心来支撑实验。\n\nJerry Tworek：这是一个巨大的资源门槛，也确实让问题变得更加棘手。但这同样是一个值得认真思考、试图解决的问题。\n\n世界上有大量学术研究在进行，许多学生在做各种各样的探索，但其中绝大多数都严重缺乏资源。结果是，许多原本可能很有潜力的研究，最终不了了之，因为真正关键的研究往往需要大规模实验。\n\n也正因如此，我对当下的一个趋势感到非常欣慰：确实有相当多的资金开始流向那些支持新颖、激进想法的尝试。像Carmack、Ilya、杨立昆这样的人，正是当下应该存在、也应该被资助的对象。\n\n显然，并不是所有尝试都会成功，但其中一些一定会 -- -- 世界上的创新正是以这种方式发生的。\n\n在强化学习领域，\"探索与利用\"的权衡早已是一个经典概念。即便是我们在优化智能体时，也始终面临这个问题：是选择那些已被验证有效、成功路径明确的策略，还是尝试全新的方法，用不同的方式解决旧问题？\n\n这是一个艰难但无法回避的权衡。当我们思考智能体该如何行动时，也许同样应该反思我们自己是如何做选择的。\n\n主持人：至于那个由顶尖AI研究者组成的小圈子，人们是否真的清楚Carmack在做什么？\n\nJerry Tworek：说实话，我并不完全清楚。我的印象是，他正在大力押注于通过鼠标和键盘，在电子游戏中进行端到端的强化学习。如果我没记错的话，大致是这样。\n\n而这恰恰让我觉得非常有意思。长期以来，我一直认为电子游戏是训练智能的绝佳环境之一。游戏是为人类大脑设计的，要让人类觉得有趣，它们融合了故事、权力幻想、解谜和问题解决，必须持续保持新鲜感，不能变得重复。\n\n从某种意义上说，电子游戏是为人类认知量身定制的学习环境，而问题解决这样的能力，正是我们希望智能体具备的能力。\n\n但我们至今还没有真正聪明的模型，能够在这种高频、多模态的环境中稳定运行。这也许暴露了某些架构层面的限制。但我依然认为，在电子游戏上训练AI，是一件非常有前景的事情。\n\n强化学习之父Richard Sutton过去做过大量相关工作，不仅是电子游戏，还有扑克等复杂博弈。我曾去过他的实验室。当然，他当年的游戏环境，比我们后来在OpenAI让模型玩Dota时要简单得多。DeepMind CEO Demis Hassabis也一直在坚持类似的想法。\n\n06.\n\n好的想法\n\n往往会卷土重来\n\n主持人：有趣的是，这些思路曾一度被认为\"过时\"。在ChatGPT时代，它们看起来不像是主流方向。\n\nJerry Tworek：科学史反复告诉我们：好的想法往往会卷土重来。判断一个想法是否重要并不难，难的是判断它什么时候会变得重要。\n\n七年前我刚加入OpenAI时，基于游戏的强化学习是绝对的热点。我们解决了Dota、《星际争霸》。当时DeepMind的AlphaGo更是一个里程碑。\n\n但这些模型有一个非常明显的问题：它们几乎没有世界知识。它们只是在从零开始学习如何玩某一个游戏，而并不真正理解我们的世界。\n\n显然，这不是正确的路径。模型首先需要对现实世界形成高层次的理解，而不仅仅是对像素作出反应。从零开始的强化学习，更像是一种\"蜥蜴脑\"或\"猴脑\"的学习方式。我们真正希望的是让模型具备更抽象的概念结构。\n\n而经过多年大规模预训练，我们终于获得了对世界极其丰富、稳固的表征。现在，是时候在此基础上重新引入强化学习了。推理模型的真正魔力，正是在于：它们在一个强大的世界表征之上，通过强化学习构建能力层级。这才是未来的方向。\n\n主持人：至于世界模型，谷歌做过相关探索，杨立昆和李飞飞的研究在某种程度上也指向这一方向。我们作为婴儿并不是生活在黑箱中，而是通过不断试探来理解世界。所以，将世界模型与强化学习结合，在我看来是非常合理的。\n\nJerry Tworek：这个想法显然是正确的。真正有趣的地方在于，我们如何将世界模型的表征构建与强化学习结合起来。强化学习用于教会模型各种技能，而这些技能正是模型在现实世界中运作所必需的 -- -- 它赋予模型实现自身目标的能力。\n\n然而，要想实现目标，模型首先必须理解自己所处的世界；只有在具备这种理解之后，它才能形成有效的计划与策略。这正是为什么世界模型与强化学习必须协同发展的原因。一旦有人能够在一个训练良好的世界模型之上成功地进行强化学习，那将会是一个极其令人振奋、具有里程碑意义的时刻。\n\n07.\n\n架构创新与持续学习是两大方向\n\nAGI仍然缺失关键拼图\n\n主持人：你现在对什么最感兴趣？\n\nJerry Tworek：总体上，我认为简单地去重复实验室里已经在做的事情，其实意义不大。在现有的范式和设置中，仍然有很多可以调整、可以改进的地方，但有两个方向，我觉得要么被明显低估了，要么至少没有得到足够的资源去真正推进。\n\n第一个方向是架构层面的创新。我们在Transformer架构上多少有些过于固步自封了。它无疑是一个伟大的架构，也已经被极其深入地探索过。\n\n人们在对Transformer进行局部改进、试图通过一些小的结构调整来进一步提升它时，确实遇到了不少困难，当然，也有一些相当成功的尝试 -- -- 比如稀疏性显然就非常成功，各种降低注意力机制计算成本的方法也取得了不错的效果。\n\n但问题是：Transformer会是机器学习的终极架构吗？显然不会。尽管Transformer的提出者们做了极其出色的工作，几乎定义了接下来十年机器学习的发展格局，但事情远不止于此。\n\n一定还存在其他训练大模型的方法 -- -- 它们可能看起来有点像Transformer，也可能完全不像。这是一个非常值得投入精力去探索的问题。如果没有人去做这件事，那我会很乐意自己试一试。\n\n第二个方向是一个更热门的话题，但我并不认为目前有人真正把它做好了，那就是持续学习，以及如何真正地、彻底地将test time与train time融合在一起。\n\n对人类而言，这种方式再自然不过了：我们并不存在一个明确分离的\"学习模式\"和\"回答模式\"，一切都是在持续不断地同时发生的。我们的模型也应该更接近这种运作方式。\n\n这很可能是我们在实现AGI之前，仍然缺失的几个关键能力要素之一。如果模型无法从它们所接触到的数据中持续学习，那么无论它们在其他方面多么强大，依然会给人一种受限、甚至有些\"愚钝\"的感觉。\n\n主持人：说到AGI，我们上次聊天时我提到过，相比一两年前，现在我已经不太常听到关于时间线的讨论了。甚至连关于AGI本身的讨论似乎也减少了。所以我其实挺好奇的。\n\n你称自己对AI持谨慎乐观的态度。那么在你看来，我们现在处在AGI时间线的哪个位置？\n\nJerry Tworek：是的，我个人的看法其实也略有更新。我一直认为，扩大强化学习的规模是实现AGI的必要组成部分。大约在一年前或一年半前，我几乎坚信，只要我们把模型的强化学习规模做大，它就会成为AGI。\n\n而现在，我不得不稍微修正这一观点。不过有些东西，只有在真正进入下一个阶段之后你才能看清。我们也必须承认，今天的模型在非常非常多的方面已经做得相当出色了。\n\n它们在编程方面所能做到的事情，对我来说尤其震撼 -- -- 因为写代码本身就是我最喜欢的事情之一。你现在可以非常、非常快地完成大量工作。\n\n对十年前的一些人来说，如果你向他们展示我们今天所拥有的能力，他们可能已经会把这称作AGI了。所以，谈论AGI已经不再像过去那样离谱或疯狂。\n\n但至少按照我自己的定义，当前的模型仍然不能算是AGI，因为持续学习还没有以任何实质性的方式与我们的模型真正整合在一起。同时，从模型目前的状态来看，甚至在多模态感知这样的能力上也仍有明显缺失。\n\n如果模型看不到外部世界，或者无法观看视频并对其进行良好的理解，那么即便它们在文本理解和编程方面非常出色，我们真的能称它们为AGI吗？\n\n因此，要真正实现构建AGI这一文明级别的里程碑，还有许多我称之为\"必要步骤\"的问题需要解决。\n\n一段时间我曾想过，如果我们真的非常努力，如果所有事情都做得非常好，也许2026年至少会成为我们在真正优秀的持续学习和真正通用的强化学习方面取得突破的一年。\n\n我的时间线判断依然是有些浮动的。但与此同时，AI领域的发展速度确实非常快。投资每年都在不断增长，越来越多的人进入AI领域，这扩大了人才储备，也增加了我们能够探索的想法数量。\n\n所以我并不认为这个想法完全荒诞或不切实际。也许会更早一些，也许会稍晚一些 -- -- 可能是26年，也可能是27年、28年，甚至29年。我不认为会比这再晚太多。\n\n当然，还有大量工作要做，但确实有很多人正在为实现AGI而努力。\n\n08.\n\n我们正处在变革时代\n\n保持担忧和谨慎有必要\n\n主持人：如果我的记忆没错的话，在\"Strawberry\"项目出现之前，你是不是在研究Q*项目？那时候有很多风声，大家都在谈论Ilya看到了Q*，知道AGI已经来了，这把所有人都吓坏了。\n\n我的意思是，听你刚才这么说，反而让人觉得有点好笑。因为这确实是一件非常棘手的事情：这些系统能做到一些极其令人印象深刻的事，于是我们就会变得异常兴奋。然后时间过去，\n\n你知道，就像现在的\"Strawberry\"项目一样 -- -- 它确实令人难以置信，几乎改变了整个领域，但我并不觉得我第一次使用它的时候被\"吓到了\"。\n\nJerry Tworek：我明白你的意思。这是人类心理中非常有趣的一部分，在某种程度上也反映了我们与技术互动的方式。\n\n对我来说，强化学习Scale up的效果仍然非常显著，而且随着时间推移，我们会看到更多这样的成果。尤其是在编程领域，这将以许多不同的方式影响我们的生活。\n\n今天，进行任何大规模编程项目的体验，与一年前相比几乎是天壤之别。我们会在各种各样的事情中看到这些变化。当我和我的团队，以及OpenAI的许多人，在两年前第一次看到Q*开始显现出有效迹象时，\n\n你坐在一个房间里，目睹一项具有实质性意义的新技术。如果你在那一刻没有感到哪怕一点点害怕、一点点担忧，没有对\"我们正在做这件事会带来什么后果\"产生一丝疑虑，那么我会认为你对自己的工作不够负责。\n\n我觉得每一位AI研究人员都应该问自己：如果我正在做的事情是全新的，具备前所未有的能力，那么它会给世界带来什么影响？事实上，很多研究人员确实在这样思考。有时候，人们也确实会不小心走得快了一两步。\n\n到目前为止，AI还没有对世界造成任何真正的伤害。尽管像\"阿谀奉承\"这样的问题或许可以争论一下，其他问题至少据我们所知还没有。\n\n但即便如此，我仍然认为，在向世界发布任何新技术时，保持担忧和谨慎是一种非常好的、也非常健康的反应。\n\n我们正处在一个变革的时代，一个许多新事物不断向世界扩散的时代。它们会产生许多影响，影响人们如何度过一生，影响人们如何看待自己、看待他人，影响人际关系，也影响国际关系，还会影响GDP和生产力。\n\n有时候，有人写下一行代码，所引发的连锁反应却会像瀑布一样，贯穿这一切。而其中所承载的责任，是相当沉重的。\n\n主持人：这些想法确实都很有道理，其实我之前也一直在反复思考这些问题。我们此前大概也零星讨论过一些。只是那段时间里，随着所谓的\"OpenAI政变\"事件逐渐浮出水面，我总会下意识地试着设身处地为你着想。\n\n但在这样一个关键的时刻，一个本应被认真理解的创造物，却率先成为人们迷恋、投射与争夺的对象，这本身难道不会让人感到一种微妙的怪异吗？\n\n而与此同时，我看到你所创造的东西被推到聚光灯下，在尚未真正被理解之前，就被媒体反复谈论，又被卷入一场近乎肥皂剧式的纷争之中。我一时间甚至不知道该用什么词来形容这种感觉 -- -- 说\"好笑\"似乎并不完全贴切。\n\nJerry Tworek：很难将科技世界、概念世界、人类情感、人类生活、人类之间的共同点以及分歧彼此分离开来。我们生活在这样一个世界：AI领域的重要参与者之间，存在着极其复杂、跨越多个层面的关系网络。\n\n要真正理清这一切，历史学家恐怕需要花费很多年，甚至几十年，才能弄清这里究竟发生了什么，真实情况到底是什么。\n\n说实话，即便是我自己，现在对\"OpenAI政变\"期间发生的一切，也只保留着非常零碎的记忆。每当有新的证词出现，每当新的文件被披露，我们都会了解到一些此前未知的事实。将来肯定会有人把所有真相拼凑出来，但世界本身就是复杂的。\n\n或许我们确实需要一种更健康的方式来讨论技术，找到一个合适的讨论平台，让这些分歧在某种程度上得到解决。但我们生活在一个没有完美解决方案的世界，也没有完美的讨论方式。\n\n09.\n\n分歧不可避免\n\n只能依靠想法、信念与梦想\n\n主持人：你也不认为X平台是一个理想的媒介？\n\nJerry Tworek：我个人其实很喜欢在X上发帖，喜欢和研究社区、和身边所有人分享想法，但X平台也并不是一个完全严肃的地方。所以很多时候，讨论总是介于玩笑和认真之间。\n\n那么，什么才是正确的解决方案呢？当一个人担心某项技术过于危险，主张应当停止研究，而另一个人却认为它或许应当继续推进，因为它能够扩展人类的能力。第一个人又进一步认为，这甚至不是一条正确的研究路径，我们理应转向完全不同的方向。\n\n在技术进步与科研探索的领域中，这样的分歧几乎不可避免，而一切又都笼罩在未知之中。没有人真正知道未来会走向何方。我们所能依靠的，只有想法、信念与梦想。在这种根本性的不可确定性里，我们仍然必须继续生活、继续选择，并且往往不得不在许多关键问题上，以某种方式学会求同存异。\n\n主持人：是的，考虑到当时媒体对Q*的高度关注，诸如\"伊利亚看到了什么\"之类的叙事，相关的炒作确实过于密集了，而且几乎是一月接着一月不断升级。我对此并非没有意识到，只是仍然感到有些困惑。\n\n我之所以好奇，是因为我们中的许多人在推特上都非常活跃，也都在不同程度上参与、放大，甚至推动了这种讨论和想象。那么，从你的角度来看，你如何看待这种持续升温的炒作？你是否也觉得，它或许需要稍微降温一些了？我个人认为，我们确实应该大幅降温。\n\nJerry Tworek：但与此同时，如果有人在七年前告诉你，OpenAI会成为一家万亿美元级别的公司，会建设史上最大规模的数据中心，拥有全球最大的网络产品之一，所有人都会时刻谈论AI。你当时一定会觉得那些人疯了。这听起来本身就像是炒作。\n\n我其实认为，在很多方面，炒作背后是有实质内容的。有时它会过头，有时又不够，但AI确实很重要，也确实需要被讨论。我想现在已经没有人会认为AI是一个不重要的话题了。\n\n几年前的情况肯定不同，当时确实有很多人认为AI不重要。但现在已经很清楚了，AI可能是这个世界上最重要的话题之一，值得我们持续讨论和深入思考。\n\n进展会有多快？哪些路径是正确的？它到底有多安全，或者多危险？这些问题当然可以存在分歧和争论，但AI已经深度地融入了这个世界，而且只会变得越来越强。\n\n10.\n\n有些人频繁跳槽\n\n做的实事并不多\n\n主持人：完全同意。但如果暂时把技术本身放在一边，我的意思是，我报道过Meta的挖角狂潮。这件事已经变成了一场肥皂剧、一档真人秀，而不再只是关于硬核科学的问题。你已经在这个领域工作了这么久。我只是好奇，我们是不是已经越界，进入了真人秀的范畴？\n\nJerry Tworek：但问题是，究竟是谁在制造这场肥皂剧呢？肯定不是我。\n\n主持人：我的年龄足以让我亲历互联网泡沫，以及更早几个技术周期。而这一次的感觉，确实更像一场肥皂剧。即便回想当年的生产力软件大战，事情也并非如此。\n\n很大一部分原因在于，今天的利害关系实在过于巨大。牵涉的资金规模、研究人员在各个实验室之间的流动，再加上一连串高度戏剧化的事件，这些因素叠加在一起，让整个局势长期处于紧绷状态。\n\n从一开始我就有一种强烈的感觉：旧金山仿佛为自己创造了一个独立的世界。与其说这是泡沫，不如说是我们彼此不断说服自己，这就是终局，赌注巨大，这是一场竞赛，既可能极其精彩，也可能极其糟糕。一切都高度紧张，也因此带来了额外的心理负担。\n\n所以我确实觉得，这一次很不一样。互联网泡沫时期，一切源于一个简单而天真的念头：这太酷了，世界上所有的信息都触手可及，人可以彼此连接。公司是后来才出现的，金钱竞争更是逐渐浮现的结果。而现在却仿佛从一开始，整个世界的重量就压在了这件事情上。\n\n老实说，我不知道你们是怎么撑过来的。我看到无论是OpenAI、Anthropic还是其他实验室，都在拼命工作、彼此竞争，而赌注又如此之高。连续七八年处在这样的状态里，任何人都会被消耗。我完全理解，为什么你会想停下来休息一段时间。\n\n这不仅是体力上的消耗，更是心理上的磨损。因为一旦你真正接受了这种设定，它本身就会不断侵蚀你。\n\nJerry Tworek：确实，这一切都会带来心理上的损耗。不过我可以告诉你，曾经有一位在应对压力方面比我经验丰富得多的人对我说过一句话：每经历一次高压时刻，就像是做了一次俯卧撑，你对压力的承受能力都会稍微增强一点。\n\n坦率地说这七年的工作确实锻炼了我极强的心理与情感韧性。至少我真切地感觉到，自己能够屏蔽掉大量噪音和无谓的干扰，在无论发生什么情况时，都尽量保持稳定和坚定，不管是公司濒临崩溃、研究人员频繁流动，还是项目被不断重新分配。\n\n总会有一些事情发生。我也听到有人把人才挖角比作体育队的转会。体育联盟之所以能相对有序地运作，在于它们有清晰的角色分工，以及明确的转会规则，何时可以流动，何时不能流动。遗憾的是，加州法律在这方面几乎没有真正的限制。\n\n我确实认为，如果能在这方面建立一些规则，或许会是一件好事。因为在这个行业里，确实存在这样一种现象：有些人频繁地更换工作，而真正投入到工作的时间，反而显得更少。这种情况正在发生，而且并不罕见。\n\n主持人：那么，给AI领域加上工资帽怎么样？\n\nJerry Tworek：确实有些人在频繁跳槽，也有些人仍然在坚持工作，努力把前沿继续向前推进。不过，AI 毫无疑问已经是一门大生意了。\n\n主持人：前两天我还在和同事聊，我们需要列一份名单，上面包括所有在前沿AI机构工作过的人，还要标注他们在每一个地方待了多久。肯定至少有不少人完成了\"湾区大满贯\"，每家都呆过。\n\n11.\n\n揭秘OpenAI内部\"波兰黑手党\"：\n\n勤奋是项重要品质\n\n主持人：我们能聊聊\"波兰黑手党\"吗？当我刚开始写这本关于OpenAI的书的时候，大概是2018年左右，那时整个公司里大约只有三十个人。这个最初的群体中，有相当一部分来自波兰，数量多得出人意料。他们几乎都是数学天才，有些人从小就彼此认识，有些则并非如此。\n\n不过，这确实在某种程度上反映了苏联教育体系在数学人才培养方面的卓越之处，或者也可能只是因为，只要有一个人先去了OpenAI，大家彼此认识，就陆续跟着去了。\n\nJerry Tworek：就我个人而言，在最终加入OpenAI之前，我完全不认识那里任何一个人，来到OpenAI纯属机缘巧合。\n\n但在OpenAI的早期阶段，波兰人的比例确实非常高。我并不认为这种趋势能够长期持续。现在，波兰裔员工的绝对人数比早期更多了，但考虑到公司规模扩大了上百倍，这个比例其实已经不算高了。\n\n不过，我们的教育系统确实有点东西。不过我没有亲身经历过其他教育体系，所以也无法真正判断波兰教育体系是否真的如此出色。\n\n波兰确实拥有很多杰出的人才。而我非常欣赏波兰的一点，就是波兰人非常勤奋。其实随着时间推移，尤其是在许多发达国家，勤奋工作似乎越来越不被重视。生活变得更安逸了，人们有更多其他事情可以关注和优先考虑，这本身也很正常。但波兰人确实非常看重勤奋。\n\n在我出生之前，波兰还是一个共产主义国家。就在我出生的那一年，国家转型为自由市场经济。这个过程相当残酷，但社会拥抱了这种变化，努力摸索如何变得更具创业精神，如何为自己的未来奋斗，如何实现经济繁荣。而事实证明，这是成功的。\n\n我是一个移居海外的人，如今并不住在波兰。但每次回去，大概一年一两次吧，我都能清楚地看到国家在持续建设和发展。我看到它变得更好、更美丽、更繁荣。这真的是一个了不起的故事。\n\n主持人：你在当地算是个名人吗？我总觉得，波兰政府可能在想：该死，我们本来可以把这件事做成的。我们当初应该把这些人都留下来。我去年去了波兰，我知道他们已经意识到这一点了。几乎每个人都会问：你认识Wojciech（OpenAI联合创始人之一，也是少数仍在OpenAI工作的早期OpenAI成员）吗？\n\nJerry Tworek：Wojciech真的是一个了不起的人，非常友善。不过硅谷也是完全独一无二的，雄心、规模以及活力，这并不是在世界任何地方都能轻易实现的。但我可以向你保证，波兰人非常勤奋，而且能够识破\"忽悠\"。这一点，在生活中真的能让你走得很远。\n\n12.\n\n谷歌的回归背后\n\n是OpenAI在犯错\n\n主持人：你对谷歌的回归，或者说重新崛起感到惊讶吗？看起来他们做对了很多事情，你们一直都认为他们最终能理清思路，然后迎头赶上吗？还是说，这其实是个意外？\n\nJerry Tworek：我个人认为，与其说是谷歌的\"回归\"，不如说是OpenAI自己犯了一些错误。尽管OpenAI做对了很多事情，但即便在理想环境下，它也犯过几次错，执行速度比本可以做到的要慢。\n\n如果你是一家领先的公司，并且拥有OpenAI所具备的全部优势，那么你理应始终保持领先。但如果你在这个过程中做出了错误决策，而别人做出了正确决策，那么别人就会赶上来。\n\n谷歌确实做对了很多事情，他们在硬件、人才等方面拥有巨大的结构性优势。当OpenAI刚起步时，谷歌在几乎所有机器学习与研究方向上都是明显的第一名。\n\nOpenAI能够脱颖而出，主要源于对某一特定方向、特定路径的坚定研究信念。而世界花了极其漫长的时间，才意识到这是一个很好的信念、一个很好的方向。\n\n即便在GPT-2、GPT-3、GPT-3.5被训练出来的时候，也并没有太多人真正放在心上。你去NeurIPS和研究人员交流，大家会觉得OpenAI挺酷，但其他实验室往往会说：嗯，我们迟早也能复现。那些大语言模型挺有意思，但也就那样。\n\n只有当OpenAI开始通过ChatGPT真正赚钱时，其他公司才突然意识到：哦，这东西现在能盈利了，我们真的需要做这件事了。\n\n这给了OpenAI一个极其漫长的时间窗口，从构建技术到实现商业化，而其他人直到后来才意识到\"我们真的、真的需要做了\"。谷歌也是从那时起才开始认真对待大语言模型的训练。\n\n而由于OpenAI没能充分把握住自己的领先优势，谷歌如今在模型能力和训练方面已经非常、非常接近了。对谷歌来说，这是件好事，我会给他们送上祝贺，因为他们扭转了局面、并且执行得非常出色。\n\n主持人：有哪些失误？我记得当时我报道你们推出搜索功能时，外界的说法是：OpenAI推出搜索，谷歌要完了。我当时就想，我并不确定会是这样。那么，具体的失误是什么呢？\n\nJerry Tworek：我不太想深入讨论内部决策的细节，哪些是对的，哪些是错的。但我再强调一次：在理想的执行情况下，如果你一开始就领先，你本应保持领先。\n\n13.\n\nOpenAI需要加快进度\n\nAnthropic令人钦佩\n\n主持人：看起来你认为OpenAI存在一些技术层面的失误，同时公司内部的一些戏剧性事件在某些阶段拖慢了进度。我和足够多的OpenAI内部人士聊过，他们一直在思考公司该如何继续向前。然后在某个阶段，一批关键人物离开了。但听起来，你刚才更多是在谈技术层面的问题。\n\nJerry Tworek：这些事情有时是相关的。从技术上讲，我并不认为人员流动本身是一个严重问题。在任何公司，人来人往都应该是正常的现象。但有时，人员离开确实是问题的征兆。\n\n但如果公司有人说：\"有人在做错误的事情，我们不再相信这家公司了，我们应该离开\"，那可能确实说明存在更深层次的问题。不过，正如我之前所说，有些事情的进展速度显然是可以更快的。\n\n主持人：正如你所说，各大实验室在总体方向上做的是类似的事情。那么Meta在某种程度上算是后来者。虽然他们早就涉足AI，但现在看起来，他们是想用不同方式来做这件事，同时从其他公司挖人。\n\n我不太清楚Meta具体在做什么，但给我的感觉是，他们并不是要走出一条真正不同的道路，而是想走同一条路。这在我看来是一个根本性的问题。你来得晚了一点，却在做和别人一样的事情，结果可能不会太好。你觉得他们真的有不同的方法吗？\n\nJerry Tworek：我对他们的策略并不是特别熟悉，所以无法确定。但从外部来看，我觉得他们意识到了一点：在当前的AI世界里，你可以用两种方式来思考你想做什么。\n\n一种是，我们想打造一个在某些方面明显优于他人的模型；另一种是，我想打造一个和别人同样优秀的模型，但以不同的方式去使用它，或者围绕它构建不同的产品。\n\n就我对Meta的理解而言，这家公司关注的是连接人、建立关系、打造体验，无论是元宇宙、社交网络，还是其他形式的体验。我再强调一次，这只是我的推测，但我认为他们的思路是，利用行业已经理解并掌握的AI技术和Transformer，来尝试构建这些体验。\n\n从一家极其盈利、拥有全球最大社交网络的公司角度来看，这可能是一种相当不错的策略。\n\n主持人：我们刚刚谈到了谷歌的回归。在OpenAI与其他公司的持续竞争中，有没有某个AI Lab给留下了特别深刻的印象？\n\nJerry Tworek：我得说，这是最近才发生的变化，但在过去一年里，我对Anthropic的钦佩程度确实大幅上升。我从来都不是特别关注模型\"个性\"的那种人。虽然我听说Claude的个性不错，也许吧。\n\n但他们在编程模型和编程智能体方面所做的事情，他们围绕这些成果建立的品牌以及他们所拥有的大量开发者，这些绝对是令人震惊的成就。\n\nAnthropic起步更晚，计算资源受限，团队规模也更小，在获取优质算力和硬件方面遇到了许多困难，但他们依然成功构建了卓越的产品。这些产品正在改变人们开发软件的方式，并且据我所知，显著提升了企业生产力。祝贺他们。\n\n主持人：他们似乎正处在一个高光时刻。我认识的每一个人都在谈论Claude Code，但我确实不知道，他们是如何做出一个如此出色、像ChatGPT一样被广泛喜爱的Claude Code的。似乎很多实验室确实在借鉴这个工具，还有些实验室被断供了。\n\nJerry Tworek：是的。在OpenAI，我们也在开发Codex，这是我们自己的编程工具，它也挺不错的。有意思的是，我自己其实并没有怎么用过Claude Code。毕竟我当时受雇于OpenAI，所以没怎么用过。\n\n所以我真的说不太准。但我觉得Codex不是一个坏产品。只不过，从Twitter上的情绪来看，Claude确实深受全球开发者的喜爱。\n\n14.\n\nAI圈缺乏专注度已成普遍问题\n\nOpenAI很难\"集中力量办大事\"\n\n主持人：根据我们之前的对话，你似乎在智识层面上对科学怀有浓厚兴趣。你关于推理的研究，源自你想创造\"AI科学家\"的长期愿景。当我看到你宣布离开的那条推文时，我就在想，你究竟是会继续留在这场以基础模型为中心的竞赛中，还是会走一条不同的道路。我感觉你可能会进入生物技术领域，或类似的方向，以一种相当不同的方式去追求这个目标。\n\nJerry Tworek：如果我能克隆自己，去做多种不同的事情，我真的很想那样做。但长话短说，在某些时刻我醒来，会意识到自己对一生中所取得的成就感到相当满足，也感到自豪。\n\n但我现在真正想做的，是押注一两个重大的研究方向，并竭尽全力让它们成功。我认为人们应该愿意承担风险。我是那种愿意尝试疯狂想法、拥有极高风险承受能力的人之一。我觉得我应该把这种能力用在一些有益的事情上。\n\n主持人：把你脑海中的想法真正落地，需要多长时间？这是一个一年的项目吗？还是你所说的\"高风险\"，需要投入四五年的人生，去追逐一个可能并不比现有技术更好的东西？\n\nJerry Tworek：我绝对愿意投入大量时间。同时，我也认为人们应该快速执行，做事慢并不是值得骄傲的理由。为了在研究项目上执行得好，我希望能尽快做好。\n\n但真正重要的部分，还是我之前提到的：专注和信念。如果你同时做很多不同的事情，就会分散你的注意力，分散你的资源。尽管AI Lab经常说他们受限于计算资源，因此研究变慢了，这也确实是重要的影响因素之一。但很多时候，更常见、更普遍的问题，其实是缺乏专注力。毕竟，你每天能分配的注意力是有限的。\n\n我经常告诉我合作的研究人员：减少实验次数，但要对每一次实验思考得更深入。因为有时候，即便只是花时间，比如几个小时，不运行任何程序，仅仅更仔细地分析实验数据，相比于运行更多实验，反而更容易带来突破。\n\n主持人：像OpenAI这样拥有大量计算资源的机构，其实只是把资源分散在了太多项目上。实际上，如果把这些资源集中到更少的项目中，算力本身是完全足够的。\n\nJerry Tworek：这又回到了风险承担和信念的问题。如果你同时做三个项目，其中一个成功了，另外两个可能被放弃。如果三个都成功了，那当然非常棒，但如果你只做一个项目，会推进得快得多，因为你可以更加专注，信念也更加坚定。\n\n当然，如果项目最后失败了，麻烦就大了，但如果成功了，就可能拥有世界上最好的模型。\n\n对OpenAI来说，目前要让整个公司集中力量去做一些全新的、完全不同的事情，是有点困难的。要让我们完全不在乎Gemini下个季度会不会有更好的模型，也非常难做到。\n\n这样的事情绝对需要一种特定类型的人，只有这种人才愿意去承担风险。这正是关键所在。\n\n主持人：我知道你不能谈论那些所谓的\"秘密配方\"。但我还是很好奇，OpenAI正在朝哪个方向发展？或者至少，从宏观上看，他们把资源投向了哪里？最近OpenAI给ChatGPT加广告的消息刷爆了全网。\n\nJerry Tworek：我不应该、也不能谈论OpenAI的任何计划。\n\n主持人：你觉得，在这些模型公司中，会不会有哪一家有勇气像OpenAI一样加入广告？也许\"勇气\"这个词并不准确，因为不放广告可能本身就是一个糟糕的决定。广告变现是不是不可避免的？\n\nJerry Tworek：这是一个商业策略问题，而我的工作是训练模型。\n\n15.\n\nOpenAI真正擅长的是\"1到100\"\n\n驱动创新的是\"运作方式\"\n\n主持人：我并不是想为难你，只是在进行了这次完整的对话之后，我仍然在试图理清一些想法。当你谈到你想要追求的新方向时，你确实需要一定的\"马力\"。你会自己进行尝试，还是必须身处一个拥有足够\"能量\"的地方，才能进行你想做的研究？\n\nJerry Tworek：这是我目前正在努力理解的首要问题。每一项AI研究仍然需要GPU，需要算力，我需要考虑什么才是最好的方式。\n\n主持人：这是波兰的机会。他们需要给你一个国家级数据中心。\n\nJerry Tworek：这个主意或许不错。我还在逐渐理清自己的速录，我知道自己想做哪些类型的研究，也在不断尝试弄清楚，什么才是实现它们的最佳路径。\n\n我不止一次听别人说，你离职后比以前快乐多了。我从一个现在自己创业的人那里听说，在OpenAI工作比创业压力还要更大，这让我非常震惊。OpenAI确实是一个相当有压力的地方。\n\n主持人：最后一个问题，除了大家追逐的东西过于相似之外，你有没有观察到AI领域内其他的重大错误？\n\nJerry Tworek：我不认为存在什么巨大的错误。因为要让所有人都犯下同一个巨大错误，其实很难。我觉得这里只有一个真正的问题：如何在探索和延续原有技术路线之间取得平衡？\n\n主持人：我刚才那个问题可能问得不太好，我更想问的是，在研究界中，是否存在一些你认为被低估了、没有得到世界足够关注的想法？\n\nJerry Tworek：说实话，这样的想法有很多，但它们最需要的，其实只是多一点关注、多一点计算资源，以及多一点为之奋斗的精神。\n\n我觉得有一点比较独特：很多研究人员喜欢做从0到1的工作。很多学术研究正是如此，创造出一些全新的想法，证明它在某种程度上是可行的，然后就把它发表出来。\n\n而我认为，我和我在OpenAI的团队真正擅长的，以及我觉得我们做得非常出色的一点，是把研究从1推进到100，也就是采纳那些不同的、我们以前没有做过、但已经初步被验证的想法，并找出如何让它们在大规模训练前沿模型时，可靠地工作，同时还要整合许多其他相关因素。\n\n这正是大量学术研究所欠缺的东西。概念验证当然很酷，但要用某种特定技术训练出世界上最有能力的模型之一，需要做大量非常具体、细致的工作。如果方法不对，可能需要数年时间，但如果你有合适的算法，知道如何引入这些东西，可能只需要几个月。这正是我未来想多多尝试的事情。\n\n主持人：当我们谈到OpenAI的一些人员离职时，你曾说，公司应该能够承受这些损失。但AI领域在某种程度上似乎一直是由\"明星\"驱动的，比如Alec Radford这样的明星人物。挖人的行为也是持续不断。\n\n从这些实验室的行为来看，显然这些公司认为AI一个由研究明星驱动的领域。我很好奇你的看法。你刚才似乎对这个问题有些犹豫。行业中既有整个学界、整个领域长期积累的工作，也有一些关键时刻和重大的突破来自极少数个人。\n\nJerry Tworek：这是一个相当复杂的话题，但我觉得两件事可以同时成立。很多时候，就像你在OpenAI看到的那样，确实是极少数个人产生了超乎寻常的影响，推动了一系列完全开创性的成果，并将其扩散到整个行业。我一次又一次地看到这种情况发生。\n\n但与此同时，每当我看到人们换公司时，我很少看到这对原公司造成真正重大的影响。公司本身的特质，或者说一种近乎\"运作方式\"的东西，才是真正的研究引擎，而不是某一个特定研究员是否还在这里。\n\n我也观察到，那些在公司之间跳槽的研究员，往往在新环境中并没有那么高效。即使他们过去常常做出伟大的工作，来到新地方后，也可能变得有些分心，需要时间适应环境，或者暂时没有特别新鲜的想法。\n\n当然，在这个领域的经验肯定能带来一些优势，但更重要的是，创造一种个人责任感强、允许探索、能够赋能人们去做大事的氛围。\n\n而且，无论是这批人，还是另一批人，都完全有可能组建出许多能够做出伟大成果的团队。我并不认为某个特定的人是不可替代的。在我看来，良好的研究结构、良好的研究文化、良好的协作方式，远比某个具体的人是否在你的团队中重要得多。"
  },
  {
    "source": "k.sina.com.cn",
    "company": "OpenAI",
    "title": "在OpenAI\"创新已经变得困难\"！离职高管深喉爆料",
    "date": "2026-01-23T14:42:22Z",
    "url": "https://k.sina.com.cn/article_5953190046_162d6789e06702ln1s.html",
    "content": "来源：智东西\n\n刚离职的副总裁，把OpenAI的创新困境一股脑全曝光了。\n\n编译 | 陈骏达\n\n编辑 | Panken\n\n智东西1月23日消息，昨天，由知名媒体人和作家Ashlee Vance主办的Core Memory播客，发布了对OpenAI前研究副总裁Jerry Tworek的深度专访。今年1月初决定离开的OpenAI的Tworek分享了一个关键洞察：随着竞争加剧与组织急速膨胀，OpenAI正逐渐陷入一种难以再承担真正高风险研究的结构性困境，一些前沿创新的研究方向，已经难以在OpenAI内部推进。\n\n在正式进入访谈内容前，我们有必要了解下Tworek的传奇履历。Tworek是OpenAI元老级成员，2019年便加入该公司。他是OpenAI推理模型o1、o3背后的关键人物，将强化学习做到了极致，也让强化学习、推理模型真正进入了主流视野。此外，Tworek还在编程和Agent领域颇有建树。\n\n今年1月7日，Tworek在X平台上分享了自己离职的消息，评论区涌入了诸多OpenAI大佬，言语间满是不舍之情。\n\n这场访谈长达70分钟，实录近2万字，智东西梳理出Tworek分享的八大关键洞察：\n\n1、OpenAI的创新困境：成本、增长压力等多重因素影响了OpenAI对风险的\"胃口\"，同时该公司尚未找到良好的跨团队的研究协作模式。\n\n2、谷歌崛起：与其说是谷歌\"回归\"，不如说是OpenAI自己犯了错误，没能充分把握住自己的领先优势。OpenAI本应该持续领先。\n\n3、行业弊病：5家头部AI公司路径完全趋同，研究员想在主流机器学习范式之外做点不同的事情，几乎找不到合适的地方，这令人沮丧。\n\n4、人才争夺战：人才争夺战已演变成一场肥皂剧，有些人频繁地更换工作，而真正投入到工作的时间不多。\n\n5、创新引擎：明星AI研究员并不是驱动创新的核心因素，公司本身能否打造个人责任感强、允许探索和做大事的环境，可能更为关键。\n\n6、什么阻碍了创新：阻碍AI Lab研究的因素不是算力短缺，而是缺乏专注。对OpenAI来说，\"集中力量办大事\"已经变得有些困难。\n\n7、AGI时间表：目前AGI仍然缺失关键拼图，架构创新与持续学习是两大重要方向，但AGI将会在2029年左右实现。\n\n8、强化学习的回归：科学史已经反复证明，好的想法往往会卷土重来，判断一个想法是否重要并不难，难的是判断它什么时候会变得重要。\n\n以下是访谈内容的完整编译：\n\n01．\n\n竞争激烈、组织膨胀\n\nOpenAI的创新困境\n\n主持人：你的离职声明写得很好，充满感情。你在OpenAI经历了非常重要的一段时间，见证了巨大的变化。那种感觉怎么样？\n\nJerry Tworek：在OpenAI的每一年，都是一家完全不同的公司。公司本身的高速成长，以及整个AI世界的变化。\n\n这种经历我觉得在人类历史上都很少见。我很庆幸自己能亲身经历这一切。正如我之前说的，每一个阶段都完全不同。\n\n主持人：OpenAI在2019年时大概只有30个人？现在已经是几千人了吧？\n\nJerry Tworek：说实话，很难统计清楚。全球多地办公室，遍布世界各地。现在几乎找不到没听说过OpenAI的人了。而我刚加入的时候，只是几个小团队，各自做着自己的研究项目。\n\n但有一件事始终没变 -- -- OpenAI的野心。从一开始就瞄准AGI，想真正改变世界，并且带来正面的影响。而通过ChatGPT，把智能和实用性真正分发给了全球用户，我觉得这是一件非常了不起的事情。\n\n主持人：所以你发了那条推文之后，是不是全球所有基础模型实验室都来找你了？\n\nJerry Tworek：确实有很多。我现在也在思考下一步该做什么。在这个行业这么多年，我已经认识了很多人。我并不急着做决定。\n\n我已经连续高强度工作很多年了，甚至没太多时间好好和人聊天。现在正好可以慢下来，想一想：接下来的七年，我想怎么度过。但确实，我正在和很多人交流。\n\n主持人：你在推文里提到，你想做一些在OpenAI没法做的研究。能具体说说吗？\n\nJerry Tworek：当前，在全球范围内争夺\"最佳AI模型\"的竞争异常激烈且严苛。想要保持竞争力，公司在运营的多个层面都面临着极大的挑战。\n\n其中一个核心问题在于风险承担的意愿：从避免落后的角度出发，公司自然会被迫思考，究竟愿意承担多大的风险。无论是用户增长指标，还是持续支付高昂的GPU成本，现实都极其残酷。\n\n也正因如此，持续展示实力、不断推出最强模型，对所有人而言都变得至关重要。这是当下几乎所有主要AI公司共同面临的处境，而这种压力无疑会影响一家机构对风险的\"胃口\"。\n\n另一组同样难以权衡的因素来自组织结构。公司有其组织架构图，而组织架构往往在很大程度上决定了你能够开展什么样的研究：每个团队都需要明确的身份认同、研究边界以及其专注解决的问题集合。\n\n跨组织的研究往往异常困难，而如何在大规模条件下高效地组织研究，这个问题可能还没有被真正解决。\n\n研究本身偏好活力，甚至可以说偏好某种程度的混乱；而大型组织却需要秩序、结构与清晰的分工。这正是为什么\"你最终交付的是你的组织架构图\"这一说法如此流行：研究工作往往会演变成那些最适合现有人员配置的项目。\n\n我也正是在这种背景下意识到，有一些我真正想做的研究方向，并不是OpenAI当前的组织架构所能支持的。\n\n02．\n\nTransformer肯定不是最终形态\n\n有很多路径尚未得到系统性实践\n\n主持人：我曾在播客里和Mark Chen（OpenAI首席研究官）聊过这个问题 -- -- 几乎所有人都在向他（以及Jakub，OpenAI首席科学家）提出自己的想法。OpenAI的确有一个优良传统：愿意承担风险，愿意去做一些其他实验室不敢做的事情。\n\n但现实是，无论聚集了多少聪明人，资源已相当可观，这终究是一家资源有限的公司。它必须做出重大的取舍：哪些方向值得投入，哪些现在还无法承担成本。\n\n而真正足够新颖的路径，往往恰恰是那种让人犹豫的方向 -- -- 我们不知道现在该不该走，也不知道钱包是否负担得起。\n\nJerry Tworek：关于Ilya提出的\"研究时代\"这一概念，我不确定它是否像他所描述的那样非此即彼，但我确信，在AI和机器学习领域，仍然存在大量尚未被充分探索的可能性。\n\n六年前，我们选定了Transformer架构，此后人们不断对其进行Scaling，并且效果显著。路径非常清晰：每个季度训练更大的模型，使用更多计算资源和数据，而进步似乎从未真正停滞。\n\n但问题在于：这就是全部了吗？这是最终形态吗？我相当确定不是。模型仍然可以通过多种方式改进，而其中许多路径至今尚未被系统性地实践。\n\n正如你提到的，我曾在推理和强化学习扩展方面投入大量工作。在那之前，整个领域几乎把所有赌注都押在了Transformer预训练的Scaling上。\n\n这种方式确实有效：每一次预训练都可以打造出更强的模型，其能力都会全面提升，各项评测指标也随之改善。因此，人们很容易得出结论：只要不断扩展预训练，模型就会持续变好。\n\n但后来，一些研究者开始相信，我们能做的不止于此。他们尝试证明：如果在语言模型之上，以与预训练相当的计算规模去扩展强化学习，就能教会模型一些仅靠预训练永远无法获得的能力。\n\n正是由于这种探索，我们今天才拥有了这些能够自动化复杂任务、显著降低计算与数据需求的智能体系统。一旦发现新的扩展路径，就能解锁全新的能力，而如果只沿着预训练的扩展定律前进，这些能力可能需要极其漫长的时间才能出现。\n\n在我看来，自GPT-4发布以来，推理模型代表了一次真正重大的能力跃迁。而我也坚信，类似这样的突破并非孤例。研究者不应只满足于渐进式改进，更应持续思考如何从根本上改变游戏规则。\n\n03．\n\n头部AI玩家路径趋同\n\n这是件令人遗憾的事儿\n\n主持人：去年在NeurIPS上，Ilya提到\"我们正在耗尽数据\"，暗示预训练终将触及瓶颈。\n\nJerry Tworek：我并不认为这意味着预训练即将终结，它仍然在持续改进，也依然有许多优化空间。但预训练并不是提升模型能力的唯一方式，而且在很多情况下，它提升得非常缓慢。其他方法，或许能更快地推动能力跃迁。\n\n主持人：硅谷长期存在一种有趣的现象：科技公司往往会提出一些在外界看来怪异、甚至离经叛道的想法，而正是这些想法催生了真正颠覆性的创新。\n\n但一旦某条路径被证明是成功的，局面就会迅速反转，形成强烈的共识，所有人开始沿着同一方向竞赛。\n\n这正是我们当前所处的阶段。模型竞赛已经持续了两三年，几乎所有主要实验室都在做同一件事。你认为这是个问题吗？\n\nJerry Tworek：我对此感到相当、相当遗憾，几乎所有公司都在做和OpenAI一样的事儿。OpenAI无疑取得了巨大成功，做对了更多事儿，引领了扩展Transformer的范式，也证明了大规模模型能够为世界创造真实而广泛的价值。\n\n但如今，有多少公司在做着几乎一模一样的事情？竞争当然有其价值，但我们现在大概有五家严肃的AI公司，使用几乎相同的技术配方，在同一技术基础上构建略有差异的产品。\n\n也许这是正确的路径，但我仍然希望看到更多多样性，模型之间真正的差异，而不仅是微小的调优。\n\n如果你观察当下最顶级的模型，很少有人能真正分辨它们之间的不同。或许我们应该进行更多盲测：让用户与不同模型对话，看看他们是否能分辨出差异。\n\n我怀疑99.9%的用户做不到。这些模型在体验上极其相似，即便它们来自不同团队、采用了略有不同的技术选择。在这样的环境中，真正的探索在哪里？真正的创新，以及与他人区分开来的能力，又在哪里？\n\n04．\n\n与OpenAI已出现实质性分歧\n\n分开比勉强合作更健康\n\n主持人：我问一个有些尖锐的问题：你在OpenAI内外都被视为传奇人物，参与的项目成功率极高。如果连你这样的人，都觉得自己真正想做的事情在公司内部难以推进 -- -- 无论公司是否明确反对，这种阻力本身就已经存在。\n\n对于一家最初以研究实验室起家的公司而言，这是否是一个值得警惕的信号？\n\nJerry Tworek：我的看法是，有时候人们会成长到某个阶段，需要与过去分道扬镳。对一家公司及其成员来说，就目标和前进方向达成一致极其重要。\n\n而在某个时刻，我意识到自己对未来研究路线的看法，与OpenAI所选择的方向在某种实质性层面上出现了分歧。在这种情况下，分开也许比勉强合作要更健康。\n\n正因如此，我也认为，如果不同公司能够真正专注于不同的事情，行业会因此变得更好。专注对一家公司而言至关重要，而OpenAI很可能正在做所有正确的事情。\n\n也许只是我怀抱了一些不切实际的梦想。我是一个相对乐观的人，我相信世界上始终还有很多不同的事情可以去做，这在原则上完全是可能的。\n\n关键在于专注，把真正核心的事情做到极致。事实上，很多事情、很多公司，只有做到这一点，才能生存下来并进入下一个阶段。\n\n在一个理想的世界里，应该存在大量做着不同事情的公司。尤其是对研究人员而言，他们很难在一个自己并不真正相信的研究方向上长期投入。他们理应能够找到一个地方，在那里从事自己最笃信的研究，并让时间来检验其价值。\n\n也正因为如此，我对如今几乎所有公司都在做同样的事情感到有些难过。现实是，如果你想在主流机器学习范式之外做点不同的事情，几乎找不到合适的地方。这可能是目前让我最沮丧的一点。\n\n主持人：当你开始认真思考\"下一步要做什么\"时，这种同质化的问题会变得尤为明显。如果所有实验室都在做同样的事，你自然也不会觉得自己只是换一家大实验室就能获得真正不同的空间。\n\nJerry Tworek：我确实在思考人生的下一个阶段，但如果世界上能有更多选择，让人可以稍微偏离主流，去做一些不那么热门、但可能同样重要的事情，那会让我更开心，也更容易做出决定。\n\n主持人：这就引出了一个问题：我们到底需要什么，才能真正偏离主流？\n\n一个投入了如此多资金和资源、又处在聚光灯下的公司，会本能地害怕承担风险。但问题在于，这些风险也许恰恰是必要的。那么，究竟需要改变什么？这种状况未来会不会发生改变？\n\nJerry Tworek：有趣的是，我个人其实非常喜欢冒险，别人也常这样形容我。冒险本身是一件好事。\n\n但当风险和巨额资金绑定在一起时，愿意、也有能力承担这种风险的人就会变得极其稀少。风险承受能力是一种高度个人化、极其独特的特质。我与很多人共事过，深切体会到这一点。\n\n我真心认为，人们本该更愿意承担风险，去尝试更多不同的事情。尤其是研究人员这一群体 -- -- 如今AI领域的薪酬水平已经相当夸张了，而这反而可能带来一种副作用：人们变得不愿意失去工作，不愿意经历糟糕的绩效周期。于是，他们更倾向于追逐短期回报。\n\n很多研究人员本身非常聪明、也很有想法，只是整个系统的激励机制过于短视。可恰恰是研究人员，才最应该被鼓励去冒险、去做大胆的尝试 -- -- 因为真正的进步，正是这样产生的。\n\n05．\n\n算力门槛正阻碍创新\n\n\"探索与利用\"的权衡是关键问题\n\n主持人：当然，我们也看到了一些例子。比如游戏教父John Carmack，他去了达拉斯的\"洞穴\"，一度几乎是独自工作，现在可能也只有极少数员工。Carmack说过：\"也许我未必能做出真正不同的东西，但至少应该有人在认真尝试一条完全不同的路径。\"\n\n我也和Ilya聊过，不过我并不清楚他具体在研究什么。所以我无法判断，他的工作是在延续过去的方向，还是某种更加激进的尝试。但可以肯定的是，如果他不认为那是一条不同的道路，就不会去筹集那么多资金来做这件事。杨立昆显然也有与主流不同的理念。\n\n这正是让我觉得这个领域非常有趣的地方。AI在某种意义上是一个非常古老的领域，可以追溯到几十年前；但当下这套主流范式，其实是相对较新的。当我和研究人员交流时，他们仍然会说：\"只要把主要论文都读一遍，很快就能跟上进度。\"\n\n可我时常会想，会不会有某个人，突然带着一个极其激进、全新的想法出现，彻底推动整个领域向前？如今这件事似乎变得更难了，因为你可能需要一个州那么大的数据中心来支撑实验。\n\nJerry Tworek：这是一个巨大的资源门槛，也确实让问题变得更加棘手。但这同样是一个值得认真思考、试图解决的问题。\n\n世界上有大量学术研究在进行，许多学生在做各种各样的探索，但其中绝大多数都严重缺乏资源。结果是，许多原本可能很有潜力的研究，最终不了了之，因为真正关键的研究往往需要大规模实验。\n\n也正因如此，我对当下的一个趋势感到非常欣慰：确实有相当多的资金开始流向那些支持新颖、激进想法的尝试。像Carmack、Ilya、杨立昆这样的人，正是当下应该存在、也应该被资助的对象。\n\n显然，并不是所有尝试都会成功，但其中一些一定会 -- -- 世界上的创新正是以这种方式发生的。\n\n在强化学习领域，\"探索与利用\"的权衡早已是一个经典概念。即便是我们在优化智能体时，也始终面临这个问题：是选择那些已被验证有效、成功路径明确的策略，还是尝试全新的方法，用不同的方式解决旧问题？\n\n这是一个艰难但无法回避的权衡。当我们思考智能体该如何行动时，也许同样应该反思我们自己是如何做选择的。\n\n主持人：至于那个由顶尖AI研究者组成的小圈子，人们是否真的清楚Carmack在做什么？\n\nJerry Tworek：说实话，我并不完全清楚。我的印象是，他正在大力押注于通过鼠标和键盘，在电子游戏中进行端到端的强化学习。如果我没记错的话，大致是这样。\n\n而这恰恰让我觉得非常有意思。长期以来，我一直认为电子游戏是训练智能的绝佳环境之一。游戏是为人类大脑设计的，要让人类觉得有趣，它们融合了故事、权力幻想、解谜和问题解决，必须持续保持新鲜感，不能变得重复。\n\n从某种意义上说，电子游戏是为人类认知量身定制的学习环境，而问题解决这样的能力，正是我们希望智能体具备的能力。\n\n但我们至今还没有真正聪明的模型，能够在这种高频、多模态的环境中稳定运行。这也许暴露了某些架构层面的限制。但我依然认为，在电子游戏上训练AI，是一件非常有前景的事情。\n\n强化学习之父Richard Sutton过去做过大量相关工作，不仅是电子游戏，还有扑克等复杂博弈。我曾去过他的实验室。当然，他当年的游戏环境，比我们后来在OpenAI让模型玩Dota时要简单得多。DeepMind CEO Demis Hassabis也一直在坚持类似的想法。\n\n06．\n\n好的想法\n\n往往会卷土重来\n\n主持人：有趣的是，这些思路曾一度被认为\"过时\"。在ChatGPT时代，它们看起来不像是主流方向。\n\nJerry Tworek：科学史反复告诉我们：好的想法往往会卷土重来。判断一个想法是否重要并不难，难的是判断它什么时候会变得重要。\n\n七年前我刚加入OpenAI时，基于游戏的强化学习是绝对的热点。我们解决了Dota、《星际争霸》。当时DeepMind的AlphaGo更是一个里程碑。\n\n但这些模型有一个非常明显的问题：它们几乎没有世界知识。它们只是在从零开始学习如何玩某一个游戏，而并不真正理解我们的世界。\n\n显然，这不是正确的路径。模型首先需要对现实世界形成高层次的理解，而不仅仅是对像素作出反应。从零开始的强化学习，更像是一种\"蜥蜴脑\"或\"猴脑\"的学习方式。我们真正希望的是让模型具备更抽象的概念结构。\n\n而经过多年大规模预训练，我们终于获得了对世界极其丰富、稳固的表征。现在，是时候在此基础上重新引入强化学习了。推理模型的真正魔力，正是在于：它们在一个强大的世界表征之上，通过强化学习构建能力层级。这才是未来的方向。\n\n主持人：至于世界模型，谷歌做过相关探索，杨立昆和李飞飞的研究在某种程度上也指向这一方向。我们作为婴儿并不是生活在黑箱中，而是通过不断试探来理解世界。所以，将世界模型与强化学习结合，在我看来是非常合理的。\n\nJerry Tworek：这个想法显然是正确的。真正有趣的地方在于，我们如何将世界模型的表征构建与强化学习结合起来。强化学习用于教会模型各种技能，而这些技能正是模型在现实世界中运作所必需的 -- -- 它赋予模型实现自身目标的能力。\n\n然而，要想实现目标，模型首先必须理解自己所处的世界；只有在具备这种理解之后，它才能形成有效的计划与策略。这正是为什么世界模型与强化学习必须协同发展的原因。一旦有人能够在一个训练良好的世界模型之上成功地进行强化学习，那将会是一个极其令人振奋、具有里程碑意义的时刻。\n\n07．\n\n架构创新与持续学习是两大方向\n\nAGI仍然缺失关键拼图\n\n主持人：你现在对什么最感兴趣？\n\nJerry Tworek：总体上，我认为简单地去重复实验室里已经在做的事情，其实意义不大。在现有的范式和设置中，仍然有很多可以调整、可以改进的地方，但有两个方向，我觉得要么被明显低估了，要么至少没有得到足够的资源去真正推进。\n\n第一个方向是架构层面的创新。我们在Transformer架构上多少有些过于固步自封了。它无疑是一个伟大的架构，也已经被极其深入地探索过。\n\n人们在对Transformer进行局部改进、试图通过一些小的结构调整来进一步提升它时，确实遇到了不少困难，当然，也有一些相当成功的尝试 -- -- 比如稀疏性显然就非常成功，各种降低注意力机制计算成本的方法也取得了不错的效果。\n\n但问题是：Transformer会是机器学习的终极架构吗？显然不会。尽管Transformer的提出者们做了极其出色的工作，几乎定义了接下来十年机器学习的发展格局，但事情远不止于此。\n\n一定还存在其他训练大模型的方法 -- -- 它们可能看起来有点像Transformer，也可能完全不像。这是一个非常值得投入精力去探索的问题。如果没有人去做这件事，那我会很乐意自己试一试。\n\n第二个方向是一个更热门的话题，但我并不认为目前有人真正把它做好了，那就是持续学习，以及如何真正地、彻底地将test time与train time融合在一起。\n\n对人类而言，这种方式再自然不过了：我们并不存在一个明确分离的\"学习模式\"和\"回答模式\"，一切都是在持续不断地同时发生的。我们的模型也应该更接近这种运作方式。\n\n这很可能是我们在实现AGI之前，仍然缺失的几个关键能力要素之一。如果模型无法从它们所接触到的数据中持续学习，那么无论它们在其他方面多么强大，依然会给人一种受限、甚至有些\"愚钝\"的感觉。\n\n主持人：说到AGI，我们上次聊天时我提到过，相比一两年前，现在我已经不太常听到关于时间线的讨论了。甚至连关于AGI本身的讨论似乎也减少了。所以我其实挺好奇的。\n\n你称自己对AI持谨慎乐观的态度。那么在你看来，我们现在处在AGI时间线的哪个位置？\n\nJerry Tworek：是的，我个人的看法其实也略有更新。我一直认为，扩大强化学习的规模是实现AGI的必要组成部分。大约在一年前或一年半前，我几乎坚信，只要我们把模型的强化学习规模做大，它就会成为AGI。\n\n而现在，我不得不稍微修正这一观点。不过有些东西，只有在真正进入下一个阶段之后你才能看清。我们也必须承认，今天的模型在非常非常多的方面已经做得相当出色了。\n\n它们在编程方面所能做到的事情，对我来说尤其震撼 -- -- 因为写代码本身就是我最喜欢的事情之一。你现在可以非常、非常快地完成大量工作。\n\n对十年前的一些人来说，如果你向他们展示我们今天所拥有的能力，他们可能已经会把这称作AGI了。所以，谈论AGI已经不再像过去那样离谱或疯狂。\n\n但至少按照我自己的定义，当前的模型仍然不能算是AGI，因为持续学习还没有以任何实质性的方式与我们的模型真正整合在一起。同时，从模型目前的状态来看，甚至在多模态感知这样的能力上也仍有明显缺失。\n\n如果模型看不到外部世界，或者无法观看视频并对其进行良好的理解，那么即便它们在文本理解和编程方面非常出色，我们真的能称它们为AGI吗？\n\n因此，要真正实现构建AGI这一文明级别的里程碑，还有许多我称之为\"必要步骤\"的问题需要解决。\n\n一段时间我曾想过，如果我们真的非常努力，如果所有事情都做得非常好，也许2026年至少会成为我们在真正优秀的持续学习和真正通用的强化学习方面取得突破的一年。\n\n我的时间线判断依然是有些浮动的。但与此同时，AI领域的发展速度确实非常快。投资每年都在不断增长，越来越多的人进入AI领域，这扩大了人才储备，也增加了我们能够探索的想法数量。\n\n所以我并不认为这个想法完全荒诞或不切实际。也许会更早一些，也许会稍晚一些 -- -- 可能是26年，也可能是27年、28年，甚至29年。我不认为会比这再晚太多。\n\n当然，还有大量工作要做，但确实有很多人正在为实现AGI而努力。\n\n08．\n\n我们正处在变革时代\n\n保持担忧和谨慎有必要\n\n主持人：如果我的记忆没错的话，在\"Strawberry\"项目出现之前，你是不是在研究Q*项目？那时候有很多风声，大家都在谈论Ilya看到了Q*，知道AGI已经来了，这把所有人都吓坏了。\n\n我的意思是，听你刚才这么说，反而让人觉得有点好笑。因为这确实是一件非常棘手的事情：这些系统能做到一些极其令人印象深刻的事，于是我们就会变得异常兴奋。然后时间过去，\n\n你知道，就像现在的\"Strawberry\"项目一样 -- -- 它确实令人难以置信，几乎改变了整个领域，但我并不觉得我第一次使用它的时候被\"吓到了\"。\n\nJerry Tworek：我明白你的意思。这是人类心理中非常有趣的一部分，在某种程度上也反映了我们与技术互动的方式。\n\n对我来说，强化学习Scale up的效果仍然非常显著，而且随着时间推移，我们会看到更多这样的成果。尤其是在编程领域，这将以许多不同的方式影响我们的生活。\n\n今天，进行任何大规模编程项目的体验，与一年前相比几乎是天壤之别。我们会在各种各样的事情中看到这些变化。当我和我的团队，以及OpenAI的许多人，在两年前第一次看到Q*开始显现出有效迹象时，\n\n你坐在一个房间里，目睹一项具有实质性意义的新技术。如果你在那一刻没有感到哪怕一点点害怕、一点点担忧，没有对\"我们正在做这件事会带来什么后果\"产生一丝疑虑，那么我会认为你对自己的工作不够负责。\n\n我觉得每一位AI研究人员都应该问自己：如果我正在做的事情是全新的，具备前所未有的能力，那么它会给世界带来什么影响？事实上，很多研究人员确实在这样思考。有时候，人们也确实会不小心走得快了一两步。\n\n到目前为止，AI还没有对世界造成任何真正的伤害。尽管像\"阿谀奉承\"这样的问题或许可以争论一下，其他问题至少据我们所知还没有。\n\n但即便如此，我仍然认为，在向世界发布任何新技术时，保持担忧和谨慎是一种非常好的、也非常健康的反应。\n\n我们正处在一个变革的时代，一个许多新事物不断向世界扩散的时代。它们会产生许多影响，影响人们如何度过一生，影响人们如何看待自己、看待他人，影响人际关系，也影响国际关系，还会影响GDP和生产力。\n\n有时候，有人写下一行代码，所引发的连锁反应却会像瀑布一样，贯穿这一切。而其中所承载的责任，是相当沉重的。\n\n主持人：这些想法确实都很有道理，其实我之前也一直在反复思考这些问题。我们此前大概也零星讨论过一些。只是那段时间里，随着所谓的\"OpenAI政变\"事件逐渐浮出水面，我总会下意识地试着设身处地为你着想。\n\n但在这样一个关键的时刻，一个本应被认真理解的创造物，却率先成为人们迷恋、投射与争夺的对象，这本身难道不会让人感到一种微妙的怪异吗？\n\n而与此同时，我看到你所创造的东西被推到聚光灯下，在尚未真正被理解之前，就被媒体反复谈论，又被卷入一场近乎肥皂剧式的纷争之中。我一时间甚至不知道该用什么词来形容这种感觉 -- -- 说\"好笑\"似乎并不完全贴切。\n\nJerry Tworek：很难将科技世界、概念世界、人类情感、人类生活、人类之间的共同点以及分歧彼此分离开来。我们生活在这样一个世界：AI领域的重要参与者之间，存在着极其复杂、跨越多个层面的关系网络。\n\n要真正理清这一切，历史学家恐怕需要花费很多年，甚至几十年，才能弄清这里究竟发生了什么，真实情况到底是什么。\n\n说实话，即便是我自己，现在对\"OpenAI政变\"期间发生的一切，也只保留着非常零碎的记忆。每当有新的证词出现，每当新的文件被披露，我们都会了解到一些此前未知的事实。将来肯定会有人把所有真相拼凑出来，但世界本身就是复杂的。\n\n或许我们确实需要一种更健康的方式来讨论技术，找到一个合适的讨论平台，让这些分歧在某种程度上得到解决。但我们生活在一个没有完美解决方案的世界，也没有完美的讨论方式。\n\n09．\n\n分歧不可避免\n\n只能依靠想法、信念与梦想\n\n主持人：你也不认为X平台是一个理想的媒介？\n\nJerry Tworek：我个人其实很喜欢在X上发帖，喜欢和研究社区、和身边所有人分享想法，但X平台也并不是一个完全严肃的地方。所以很多时候，讨论总是介于玩笑和认真之间。\n\n那么，什么才是正确的解决方案呢？当一个人担心某项技术过于危险，主张应当停止研究，而另一个人却认为它或许应当继续推进，因为它能够扩展人类的能力。第一个人又进一步认为，这甚至不是一条正确的研究路径，我们理应转向完全不同的方向。\n\n在技术进步与科研探索的领域中，这样的分歧几乎不可避免，而一切又都笼罩在未知之中。没有人真正知道未来会走向何方。我们所能依靠的，只有想法、信念与梦想。在这种根本性的不可确定性里，我们仍然必须继续生活、继续选择，并且往往不得不在许多关键问题上，以某种方式学会求同存异。\n\n主持人：是的，考虑到当时媒体对Q*的高度关注，诸如\"伊利亚看到了什么\"之类的叙事，相关的炒作确实过于密集了，而且几乎是一月接着一月不断升级。我对此并非没有意识到，只是仍然感到有些困惑。\n\n我之所以好奇，是因为我们中的许多人在推特上都非常活跃，也都在不同程度上参与、放大，甚至推动了这种讨论和想象。那么，从你的角度来看，你如何看待这种持续升温的炒作？你是否也觉得，它或许需要稍微降温一些了？我个人认为，我们确实应该大幅降温。\n\nJerry Tworek：但与此同时，如果有人在七年前告诉你，OpenAI会成为一家万亿美元级别的公司，会建设史上最大规模的数据中心，拥有全球最大的网络产品之一，所有人都会时刻谈论AI。你当时一定会觉得那些人疯了。这听起来本身就像是炒作。\n\n我其实认为，在很多方面，炒作背后是有实质内容的。有时它会过头，有时又不够，但AI确实很重要，也确实需要被讨论。我想现在已经没有人会认为AI是一个不重要的话题了。\n\n几年前的情况肯定不同，当时确实有很多人认为AI不重要。但现在已经很清楚了，AI可能是这个世界上最重要的话题之一，值得我们持续讨论和深入思考。\n\n进展会有多快？哪些路径是正确的？它到底有多安全，或者多危险？这些问题当然可以存在分歧和争论，但AI已经深度地融入了这个世界，而且只会变得越来越强。\n\n10．\n\n有些人频繁跳槽\n\n做的实事并不多\n\n主持人：完全同意。但如果暂时把技术本身放在一边，我的意思是，我报道过Meta的挖角狂潮。这件事已经变成了一场肥皂剧、一档真人秀，而不再只是关于硬核科学的问题。你已经在这个领域工作了这么久。我只是好奇，我们是不是已经越界，进入了真人秀的范畴？\n\nJerry Tworek：但问题是，究竟是谁在制造这场肥皂剧呢？肯定不是我。\n\n主持人：我的年龄足以让我亲历互联网泡沫，以及更早几个技术周期。而这一次的感觉，确实更像一场肥皂剧。即便回想当年的生产力软件大战，事情也并非如此。\n\n很大一部分原因在于，今天的利害关系实在过于巨大。牵涉的资金规模、研究人员在各个实验室之间的流动，再加上一连串高度戏剧化的事件，这些因素叠加在一起，让整个局势长期处于紧绷状态。\n\n从一开始我就有一种强烈的感觉：旧金山仿佛为自己创造了一个独立的世界。与其说这是泡沫，不如说是我们彼此不断说服自己，这就是终局，赌注巨大，这是一场竞赛，既可能极其精彩，也可能极其糟糕。一切都高度紧张，也因此带来了额外的心理负担。\n\n所以我确实觉得，这一次很不一样。互联网泡沫时期，一切源于一个简单而天真的念头：这太酷了，世界上所有的信息都触手可及，人可以彼此连接。公司是后来才出现的，金钱竞争更是逐渐浮现的结果。而现在却仿佛从一开始，整个世界的重量就压在了这件事情上。\n\n老实说，我不知道你们是怎么撑过来的。我看到无论是OpenAI、Anthropic还是其他实验室，都在拼命工作、彼此竞争，而赌注又如此之高。连续七八年处在这样的状态里，任何人都会被消耗。我完全理解，为什么你会想停下来休息一段时间。\n\n这不仅是体力上的消耗，更是心理上的磨损。因为一旦你真正接受了这种设定，它本身就会不断侵蚀你。\n\nJerry Tworek：确实，这一切都会带来心理上的损耗。不过我可以告诉你，曾经有一位在应对压力方面比我经验丰富得多的人对我说过一句话：每经历一次高压时刻，就像是做了一次俯卧撑，你对压力的承受能力都会稍微增强一点。\n\n坦率地说这七年的工作确实锻炼了我极强的心理与情感韧性。至少我真切地感觉到，自己能够屏蔽掉大量噪音和无谓的干扰，在无论发生什么情况时，都尽量保持稳定和坚定，不管是公司濒临崩溃、研究人员频繁流动，还是项目被不断重新分配。\n\n总会有一些事情发生。我也听到有人把人才挖角比作体育队的转会。体育联盟之所以能相对有序地运作，在于它们有清晰的角色分工，以及明确的转会规则，何时可以流动，何时不能流动。遗憾的是，加州法律在这方面几乎没有真正的限制。\n\n我确实认为，如果能在这方面建立一些规则，或许会是一件好事。因为在这个行业里，确实存在这样一种现象：有些人频繁地更换工作，而真正投入到工作的时间，反而显得更少。这种情况正在发生，而且并不罕见。\n\n主持人：那么，给AI领域加上工资帽怎么样？\n\nJerry Tworek：确实有些人在频繁跳槽，也有些人仍然在坚持工作，努力把前沿继续向前推进。不过，AI 毫无疑问已经是一门大生意了。\n\n主持人：前两天我还在和同事聊，我们需要列一份名单，上面包括所有在前沿AI机构工作过的人，还要标注他们在每一个地方待了多久。肯定至少有不少人完成了\"湾区大满贯\"，每家都呆过。\n\n11．\n\n揭秘OpenAI内部\"波兰黑手党\"：\n\n勤奋是项重要品质\n\n主持人：我们能聊聊\"波兰黑手党\"吗？当我刚开始写这本关于OpenAI的书的时候，大概是2018年左右，那时整个公司里大约只有三十个人。这个最初的群体中，有相当一部分来自波兰，数量多得出人意料。他们几乎都是数学天才，有些人从小就彼此认识，有些则并非如此。\n\n不过，这确实在某种程度上反映了苏联教育体系在数学人才培养方面的卓越之处，或者也可能只是因为，只要有一个人先去了OpenAI，大家彼此认识，就陆续跟着去了。\n\nJerry Tworek：就我个人而言，在最终加入OpenAI之前，我完全不认识那里任何一个人，来到OpenAI纯属机缘巧合。\n\n但在OpenAI的早期阶段，波兰人的比例确实非常高。我并不认为这种趋势能够长期持续。现在，波兰裔员工的绝对人数比早期更多了，但考虑到公司规模扩大了上百倍，这个比例其实已经不算高了。\n\n不过，我们的教育系统确实有点东西。不过我没有亲身经历过其他教育体系，所以也无法真正判断波兰教育体系是否真的如此出色。\n\n波兰确实拥有很多杰出的人才。而我非常欣赏波兰的一点，就是波兰人非常勤奋。其实随着时间推移，尤其是在许多发达国家，勤奋工作似乎越来越不被重视。生活变得更安逸了，人们有更多其他事情可以关注和优先考虑，这本身也很正常。但波兰人确实非常看重勤奋。\n\n在我出生之前，波兰还是一个共产主义国家。就在我出生的那一年，国家转型为自由市场经济。这个过程相当残酷，但社会拥抱了这种变化，努力摸索如何变得更具创业精神，如何为自己的未来奋斗，如何实现经济繁荣。而事实证明，这是成功的。\n\n我是一个移居海外的人，如今并不住在波兰。但每次回去，大概一年一两次吧，我都能清楚地看到国家在持续建设和发展。我看到它变得更好、更美丽、更繁荣。这真的是一个了不起的故事。\n\n主持人：你在当地算是个名人吗？我总觉得，波兰政府可能在想：该死，我们本来可以把这件事做成的。我们当初应该把这些人都留下来。我去年去了波兰，我知道他们已经意识到这一点了。几乎每个人都会问：你认识Wojciech（OpenAI联合创始人之一，也是少数仍在OpenAI工作的早期OpenAI成员）吗？\n\nJerry Tworek：Wojciech真的是一个了不起的人，非常友善。不过硅谷也是完全独一无二的，雄心、规模以及活力，这并不是在世界任何地方都能轻易实现的。但我可以向你保证，波兰人非常勤奋，而且能够识破\"忽悠\"。这一点，在生活中真的能让你走得很远。\n\n12．\n\n谷歌的回归背后\n\n是OpenAI在犯错\n\n主持人：你对谷歌的回归，或者说重新崛起感到惊讶吗？看起来他们做对了很多事情，你们一直都认为他们最终能理清思路，然后迎头赶上吗？还是说，这其实是个意外？\n\nJerry Tworek：我个人认为，与其说是谷歌的\"回归\"，不如说是OpenAI自己犯了一些错误。尽管OpenAI做对了很多事情，但即便在理想环境下，它也犯过几次错，执行速度比本可以做到的要慢。\n\n如果你是一家领先的公司，并且拥有OpenAI所具备的全部优势，那么你理应始终保持领先。但如果你在这个过程中做出了错误决策，而别人做出了正确决策，那么别人就会赶上来。\n\n谷歌确实做对了很多事情，他们在硬件、人才等方面拥有巨大的结构性优势。当OpenAI刚起步时，谷歌在几乎所有机器学习与研究方向上都是明显的第一名。\n\nOpenAI能够脱颖而出，主要源于对某一特定方向、特定路径的坚定研究信念。而世界花了极其漫长的时间，才意识到这是一个很好的信念、一个很好的方向。\n\n即便在GPT-2、GPT-3、GPT-3.5被训练出来的时候，也并没有太多人真正放在心上。你去NeurIPS和研究人员交流，大家会觉得OpenAI挺酷，但其他实验室往往会说：嗯，我们迟早也能复现。那些大语言模型挺有意思，但也就那样。\n\n只有当OpenAI开始通过ChatGPT真正赚钱时，其他公司才突然意识到：哦，这东西现在能盈利了，我们真的需要做这件事了。\n\n这给了OpenAI一个极其漫长的时间窗口，从构建技术到实现商业化，而其他人直到后来才意识到\"我们真的、真的需要做了\"。谷歌也是从那时起才开始认真对待大语言模型的训练。\n\n而由于OpenAI没能充分把握住自己的领先优势，谷歌如今在模型能力和训练方面已经非常、非常接近了。对谷歌来说，这是件好事，我会给他们送上祝贺，因为他们扭转了局面、并且执行得非常出色。\n\n主持人：有哪些失误？我记得当时我报道你们推出搜索功能时，外界的说法是：OpenAI推出搜索，谷歌要完了。我当时就想，我并不确定会是这样。那么，具体的失误是什么呢？\n\nJerry Tworek：我不太想深入讨论内部决策的细节，哪些是对的，哪些是错的。但我再强调一次：在理想的执行情况下，如果你一开始就领先，你本应保持领先。\n\n13．\n\nOpenAI需要加快进度\n\nAnthropic令人钦佩\n\n主持人：看起来你认为OpenAI存在一些技术层面的失误，同时公司内部的一些戏剧性事件在某些阶段拖慢了进度。我和足够多的OpenAI内部人士聊过，他们一直在思考公司该如何继续向前。然后在某个阶段，一批关键人物离开了。但听起来，你刚才更多是在谈技术层面的问题。\n\nJerry Tworek：这些事情有时是相关的。从技术上讲，我并不认为人员流动本身是一个严重问题。在任何公司，人来人往都应该是正常的现象。但有时，人员离开确实是问题的征兆。\n\n但如果公司有人说：\"有人在做错误的事情，我们不再相信这家公司了，我们应该离开\"，那可能确实说明存在更深层次的问题。不过，正如我之前所说，有些事情的进展速度显然是可以更快的。\n\n主持人：正如你所说，各大实验室在总体方向上做的是类似的事情。那么Meta在某种程度上算是后来者。虽然他们早就涉足AI，但现在看起来，他们是想用不同方式来做这件事，同时从其他公司挖人。\n\n我不太清楚Meta具体在做什么，但给我的感觉是，他们并不是要走出一条真正不同的道路，而是想走同一条路。这在我看来是一个根本性的问题。你来得晚了一点，却在做和别人一样的事情，结果可能不会太好。你觉得他们真的有不同的方法吗？\n\nJerry Tworek：我对他们的策略并不是特别熟悉，所以无法确定。但从外部来看，我觉得他们意识到了一点：在当前的AI世界里，你可以用两种方式来思考你想做什么。\n\n一种是，我们想打造一个在某些方面明显优于他人的模型；另一种是，我想打造一个和别人同样优秀的模型，但以不同的方式去使用它，或者围绕它构建不同的产品。\n\n就我对Meta的理解而言，这家公司关注的是连接人、建立关系、打造体验，无论是元宇宙、社交网络，还是其他形式的体验。我再强调一次，这只是我的推测，但我认为他们的思路是，利用行业已经理解并掌握的AI技术和Transformer，来尝试构建这些体验。\n\n从一家极其盈利、拥有全球最大社交网络的公司角度来看，这可能是一种相当不错的策略。\n\n主持人：我们刚刚谈到了谷歌的回归。在OpenAI与其他公司的持续竞争中，有没有某个AI Lab给留下了特别深刻的印象？\n\nJerry Tworek：我得说，这是最近才发生的变化，但在过去一年里，我对Anthropic的钦佩程度确实大幅上升。我从来都不是特别关注模型\"个性\"的那种人。虽然我听说Claude的个性不错，也许吧。\n\n但他们在编程模型和编程智能体方面所做的事情，他们围绕这些成果建立的品牌以及他们所拥有的大量开发者，这些绝对是令人震惊的成就。\n\nAnthropic起步更晚，计算资源受限，团队规模也更小，在获取优质算力和硬件方面遇到了许多困难，但他们依然成功构建了卓越的产品。这些产品正在改变人们开发软件的方式，并且据我所知，显著提升了企业生产力。祝贺他们。\n\n主持人：他们似乎正处在一个高光时刻。我认识的每一个人都在谈论Claude Code，但我确实不知道，他们是如何做出一个如此出色、像ChatGPT一样被广泛喜爱的Claude Code的。似乎很多实验室确实在借鉴这个工具，还有些实验室被断供了。\n\nJerry Tworek：是的。在OpenAI，我们也在开发Codex，这是我们自己的编程工具，它也挺不错的。有意思的是，我自己其实并没有怎么用过Claude Code。毕竟我当时受雇于OpenAI，所以没怎么用过。\n\n所以我真的说不太准。但我觉得Codex不是一个坏产品。只不过，从Twitter上的情绪来看，Claude确实深受全球开发者的喜爱。\n\n14．\n\nAI圈缺乏专注度已成普遍问题\n\nOpenAI很难\"集中力量办大事\"\n\n主持人：根据我们之前的对话，你似乎在智识层面上对科学怀有浓厚兴趣。你关于推理的研究，源自你想创造\"AI科学家\"的长期愿景。当我看到你宣布离开的那条推文时，我就在想，你究竟是会继续留在这场以基础模型为中心的竞赛中，还是会走一条不同的道路。我感觉你可能会进入生物技术领域，或类似的方向，以一种相当不同的方式去追求这个目标。\n\nJerry Tworek：如果我能克隆自己，去做多种不同的事情，我真的很想那样做。但长话短说，在某些时刻我醒来，会意识到自己对一生中所取得的成就感到相当满足，也感到自豪。\n\n但我现在真正想做的，是押注一两个重大的研究方向，并竭尽全力让它们成功。我认为人们应该愿意承担风险。我是那种愿意尝试疯狂想法、拥有极高风险承受能力的人之一。我觉得我应该把这种能力用在一些有益的事情上。\n\n主持人：把你脑海中的想法真正落地，需要多长时间？这是一个一年的项目吗？还是你所说的\"高风险\"，需要投入四五年的人生，去追逐一个可能并不比现有技术更好的东西？\n\nJerry Tworek：我绝对愿意投入大量时间。同时，我也认为人们应该快速执行，做事慢并不是值得骄傲的理由。为了在研究项目上执行得好，我希望能尽快做好。\n\n但真正重要的部分，还是我之前提到的：专注和信念。如果你同时做很多不同的事情，就会分散你的注意力，分散你的资源。尽管AI Lab经常说他们受限于计算资源，因此研究变慢了，这也确实是重要的影响因素之一。但很多时候，更常见、更普遍的问题，其实是缺乏专注力。毕竟，你每天能分配的注意力是有限的。\n\n我经常告诉我合作的研究人员：减少实验次数，但要对每一次实验思考得更深入。因为有时候，即便只是花时间，比如几个小时，不运行任何程序，仅仅更仔细地分析实验数据，相比于运行更多实验，反而更容易带来突破。\n\n主持人：像OpenAI这样拥有大量计算资源的机构，其实只是把资源分散在了太多项目上。实际上，如果把这些资源集中到更少的项目中，算力本身是完全足够的。\n\nJerry Tworek：这又回到了风险承担和信念的问题。如果你同时做三个项目，其中一个成功了，另外两个可能被放弃。如果三个都成功了，那当然非常棒，但如果你只做一个项目，会推进得快得多，因为你可以更加专注，信念也更加坚定。\n\n当然，如果项目最后失败了，麻烦就大了，但如果成功了，就可能拥有世界上最好的模型。\n\n对OpenAI来说，目前要让整个公司集中力量去做一些全新的、完全不同的事情，是有点困难的。要让我们完全不在乎Gemini下个季度会不会有更好的模型，也非常难做到。\n\n这样的事情绝对需要一种特定类型的人，只有这种人才愿意去承担风险。这正是关键所在。\n\n主持人：我知道你不能谈论那些所谓的\"秘密配方\"。但我还是很好奇，OpenAI正在朝哪个方向发展？或者至少，从宏观上看，他们把资源投向了哪里？最近OpenAI给ChatGPT加广告的消息刷爆了全网。\n\nJerry Tworek：我不应该、也不能谈论OpenAI的任何计划。\n\n主持人：你觉得，在这些模型公司中，会不会有哪一家有勇气像OpenAI一样加入广告？也许\"勇气\"这个词并不准确，因为不放广告可能本身就是一个糟糕的决定。广告变现是不是不可避免的？\n\nJerry Tworek：这是一个商业策略问题，而我的工作是训练模型。\n\n15．\n\nOpenAI真正擅长的是\"1到100\"\n\n驱动创新的是\"运作方式\"\n\n主持人：我并不是想为难你，只是在进行了这次完整的对话之后，我仍然在试图理清一些想法。当你谈到你想要追求的新方向时，你确实需要一定的\"马力\"。你会自己进行尝试，还是必须身处一个拥有足够\"能量\"的地方，才能进行你想做的研究？\n\nJerry Tworek：这是我目前正在努力理解的首要问题。每一项AI研究仍然需要GPU，需要算力，我需要考虑什么才是最好的方式。\n\n主持人：这是波兰的机会。他们需要给你一个国家级数据中心。\n\nJerry Tworek：这个主意或许不错。我还在逐渐理清自己的速录，我知道自己想做哪些类型的研究，也在不断尝试弄清楚，什么才是实现它们的最佳路径。\n\n我不止一次听别人说，你离职后比以前快乐多了。我从一个现在自己创业的人那里听说，在OpenAI工作比创业压力还要更大，这让我非常震惊。OpenAI确实是一个相当有压力的地方。\n\n主持人：最后一个问题，除了大家追逐的东西过于相似之外，你有没有观察到AI领域内其他的重大错误？\n\nJerry Tworek：我不认为存在什么巨大的错误。因为要让所有人都犯下同一个巨大错误，其实很难。我觉得这里只有一个真正的问题：如何在探索和延续原有技术路线之间取得平衡？\n\n主持人：我刚才那个问题可能问得不太好，我更想问的是，在研究界中，是否存在一些你认为被低估了、没有得到世界足够关注的想法？\n\nJerry Tworek：说实话，这样的想法有很多，但它们最需要的，其实只是多一点关注、多一点计算资源，以及多一点为之奋斗的精神。\n\n我觉得有一点比较独特：很多研究人员喜欢做从0到1的工作。很多学术研究正是如此，创造出一些全新的想法，证明它在某种程度上是可行的，然后就把它发表出来。\n\n而我认为，我和我在OpenAI的团队真正擅长的，以及我觉得我们做得非常出色的一点，是把研究从1推进到100，也就是采纳那些不同的、我们以前没有做过、但已经初步被验证的想法，并找出如何让它们在大规模训练前沿模型时，可靠地工作，同时还要整合许多其他相关因素。\n\n这正是大量学术研究所欠缺的东西。概念验证当然很酷，但要用某种特定技术训练出世界上最有能力的模型之一，需要做大量非常具体、细致的工作。如果方法不对，可能需要数年时间，但如果你有合适的算法，知道如何引入这些东西，可能只需要几个月。这正是我未来想多多尝试的事情。\n\n主持人：当我们谈到OpenAI的一些人员离职时，你曾说，公司应该能够承受这些损失。但AI领域在某种程度上似乎一直是由\"明星\"驱动的，比如Alec Radford这样的明星人物。挖人的行为也是持续不断。\n\n从这些实验室的行为来看，显然这些公司认为AI一个由研究明星驱动的领域。我很好奇你的看法。你刚才似乎对这个问题有些犹豫。行业中既有整个学界、整个领域长期积累的工作，也有一些关键时刻和重大的突破来自极少数个人。\n\nJerry Tworek：这是一个相当复杂的话题，但我觉得两件事可以同时成立。很多时候，就像你在OpenAI看到的那样，确实是极少数个人产生了超乎寻常的影响，推动了一系列完全开创性的成果，并将其扩散到整个行业。我一次又一次地看到这种情况发生。\n\n但与此同时，每当我看到人们换公司时，我很少看到这对原公司造成真正重大的影响。公司本身的特质，或者说一种近乎\"运作方式\"的东西，才是真正的研究引擎，而不是某一个特定研究员是否还在这里。\n\n我也观察到，那些在公司之间跳槽的研究员，往往在新环境中并没有那么高效。即使他们过去常常做出伟大的工作，来到新地方后，也可能变得有些分心，需要时间适应环境，或者暂时没有特别新鲜的想法。\n\n当然，在这个领域的经验肯定能带来一些优势，但更重要的是，创造一种个人责任感强、允许探索、能够赋能人们去做大事的氛围。\n\n而且，无论是这批人，还是另一批人，都完全有可能组建出许多能够做出伟大成果的团队。我并不认为某个特定的人是不可替代的。在我看来，良好的研究结构、良好的研究文化、良好的协作方式，远比某个具体的人是否在你的团队中重要得多。"
  },
  {
    "source": "k.sina.com.cn",
    "company": "OpenAI",
    "title": "硅谷\"钱太多\"毁了AI ？！前OpenAI o1负责人炮轰：别吹谷歌，Q-Star 被炒成肥皂剧，7年高压被\"逼疯\"！",
    "date": "2026-01-25T01:52:56Z",
    "url": "https://k.sina.com.cn/article_5953741034_162dee0ea067033ows.html",
    "content": "来源丨AI前线\n\n编译 | Tina\n\n这不是离职八卦，而是在一个把技术做成剧情、把研究变成围观的行业里，扛了七年高压后的选择。\n\n2026 年的第一个月，Jerry Tworek 离开 OpenAI 的消息传出来时，几位 OpenAI 的员工在 X 上几乎失控地发声：\"我真的崩溃了\"\"这太难受了\"。大家的反应像是：这事来得太突然，也太重。\n\nJerry 是现代 AI 浪潮背后最有影响力、却也最少公开露面的关键人物之一。 2019 年加入 OpenAI 时，当时该公司还只有约 30 名员工。他参与了许多最重要的项目，包括后来被称为 Q-Star 和 Strawberry 的推理方法，最终发展成为 o1 推理模型。\n\n这次离职后，他在接受 Core Memory 的播客采访时解释了原因：他想从事有风险的基础研究，这种研究在像 OpenAI 这样的公司已经不可能进行了，因为像用户增长这样的指标才是优先考虑的。他对 ChatGPT 广告的看法体现了研究与商业化之间的脱节：\"这是一种商业策略，而我负责训练模型。\" 这番言论印证了有关 OpenAI 人工智能研究与产品开发之间日益加剧的分歧的传言。\n\nTworek 指出，创新不足的原因有很多。最佳模型的竞争异常激烈，公司需要不断展现实力才能留住用户并证明 GPU 成本的合理性。僵化的组织结构更是雪上加霜，组织架构图决定了哪些研究是可能的：团队各自为政，职责分明，跨团队研究难以开展，Tworek 解释道。\n\n这场采访，也是一次\"离职解读\"，Jerry 还批评了整个人工智能行业，指出所有主要的人工智能公司都在开发几乎相同的技术，产品也几乎没有区别，这迫使研究人员追求短期利益，而不是实验性突破。更重要的是，他开始认真思考：如果研究真的需要冒险、需要不同路径，那他是否还应该继续待在这场高度同质化的竞赛中。\n\n在 Tworek 看来，谷歌之所以能够在 AI 竞赛中成功追赶 OpenAI，本质上是 OpenAI 自身的失误。他表示，这家 AI 实验室犯了一些错误，行动过于缓慢，没能充分利用自己原本拥有的巨大领先优势；而与此同时，谷歌则做出了许多正确的决策。\n\n当被问及 OpenAI 的具体问题时，Tworek 并未展开细说，只是暗示：员工流失有时是更深层问题的表象。他强调说，人走人来本来很正常，但如果一波人是因为\"方向不对、决策错了\"才走，那就说明公司里确实有点事 -- -- 也难怪有些关键推进会慢得不该那么慢。\n\n与这种\"慢得不该那么慢\"的状态形成对照的，是 Tworek 对 Anthropic 的评价。在播客中，他高度评价了这家 OpenAI 最强的初创公司对手，认为它在过去一年里展现出了一种罕见的\"清晰感\"：算力更少、团队更小，却异常专注，执行力极强。他特别提到 Anthropic 在代码模型与代码 Agent 方向上的进展 -- -- 那不是靠简单堆规模取得的成果，而是一种\"非常清楚自己在做什么\"的工程与研究结合状态。\n\n随着谈话继续，话题很快从技术转向了另一件更微妙的事。\n\nJerry 说，这几年最让他感到\"不对劲\"的，并不只是研究路线，而是整个大模型行业正在发生的变化。他形容现在的状态有点像这样：你做出一个新东西，大家还没真正弄清楚它是什么，它已经被卷进了一整套剧情里。谁离职、谁跳槽、谁被挖、谁\"内部有分歧\"，每天都像连续剧更新；湾区像一个巨大的转会市场，研究者在几家前沿实验室之间流动，围观者负责情绪，媒体负责剪辑 -- -- 研究现场，被包裹进了一层娱乐业式的叙事。\n\n\"技术、概念、人类情绪、现实生活，是分不开的。\"Jerry 说。\n\n当一个行业被持续围观，每一次进展都会被强行赋予意义，每一次内部变化都会被解读成信号，整个系统就会被不断加压。你不是在安静地做研究，而是在聚光灯下跑一场没有终点的马拉松。\n\n他用一个很个人的比喻形容这七年：\"像做俯卧撑。\"每一次高压过去，你会更能扛一点。 你学会屏蔽噪音，学会在混乱中保持稳定。但代价是，你也会慢慢习惯这种状态 -- -- 把异常当成常态，把围观当成空气，把压力当成日常。\n\n我们翻译并整理了这期播客的完整对话，以飨读者。\n\n当整个大模型行业只剩\n\n下一套\"配方\"，有些人宁愿离场\n\n主持人：今天我们请来重量级嘉宾 -- -- OpenAI 的 Jerry Tworek。他在 AI 圈算是\"活传奇\"那种人，而且刚刚离开 OpenAI，所以这期信息非常新、也非常重磅。我刷到不少 OpenAI 的同事在 X 上直接说\"我崩溃了\"\"太难受了\"。这就能看出来他在内部的分量。\n\n他主导或参与了 OpenAI 很多最重要的项目。这一波\"推理模型\"的时代，在很大程度上也和 Jerry 有关。今天他会聊他的经历、他做过的事情，然后我们也看看他会不会讲得更\"辣\"一点 -- -- 希望如此。\n\nJerry，你好。你身上有一种......\"刚失业的光芒\"。\n\nJerry： 我已经失业八天了，确实是一种变化。我已经很久没有失业过了，但这件事也有很多好处。比如我现在晒太阳的时间多了很多。\n\n主持人：那这期节目就算你的\"离职访谈\"了。我们刚才已经简单介绍了你的背景，我再稍微补充一点。你大概是 2019 年加入 OpenAI 的。你来自波兰，在来 AI 领域之前，和很多 AI 从业者一样，曾经在高频交易相关的领域工作过。在 OpenAI，你参与或领导了很多大家非常熟悉的重要项目。最近，很多人听说过 Strawberry、o1，以及这波\"推理模型\"的兴起，而这是你追了相当长一段时间的方向。然后，如大家所知，你最近刚离开 OpenAI。这件事在 X（推特）上引起了不少讨论。\n\n大家好，我做了一个艰难的决定：离开 OpenAI。\n\n我在这里将近七年，经历了很多美好与疯狂的时刻 -- -- 但美好远远多于疯狂。\n\n我非常享受和这支团队共事的时光。我有机会在\"机器人上的强化学习规模化\"还没流行之前就参与其中；训练了世界上最早的一批代码模型，推动了 LLM 编程革命；在\"Chinchilla（缩放规律）\"还没被叫作 Chinchilla 之前就发现了它；参与了 GPT-4 和 ChatGPT 的工作；最近则是组建了一支团队，建立了一种训练与推理算力规模化的新范式 -- -- 我们通常把它称为\"推理模型\"。\n\n我在这里结识了许多朋友，有些夜晚也在办公室度过；我参与并见证了相当多的技术突破；也和许多我视为至亲的人一起欢笑、一起担忧。我有幸招募并壮大了 -- -- 在我看来 -- -- 世界上最强的机器学习团队。\n\n这段旅程非常精彩。虽然我将离开，去探索一些在 OpenAI 很难开展的研究方向，但这依然是一家特别的公司、一个特别的地方，它已经在全人类的历史中占据了永恒的一席之地。\n\nJerry： 某种意义上，这事挺棘手的：我如果不自己说，媒体迟早也会替我说 -- -- 要么写成\"独家\"，要么当成\"泄露\"。所以我宁愿自己把话讲清楚，省得消息一传十、十传百，越传越走样。\n\n主持人： 对，我们最怕\"越传越离谱\"。你其实可以先跟我们说。\n\nJerry：（笑）我可以随时给你们打电话，告诉你们我生活里发生的任何事 -- -- 比如我中午吃了什么。\n\n主持人：但说真的，你那条离职帖写得很好，而且挺真情实感的。你在那里待了七年，经历了巨大的变化。从你的视角看，这七年是什么感觉？\n\nJerry： 老实说，我在 OpenAI 的每一年，都像是在一家完全不同的公司里。无论是公司本身的高速增长，还是整个 AI 世界的变化速度，都非常罕见。我不觉得历史上有很多类似的例子。我很高兴自己亲身经历了这一切。几乎每一个阶段，情况都完全不同。\n\n主持人： 你 2019 年加入的时候，公司大概只有 30 人左右？\n\nJerry： 对，大概就是那个规模。\n\n主持人： 那现在呢？几千人？\n\nJerry： 已经没法数清楚了。现在是一家规模非常大的公司，有很多办公室，全球各地都有团队。现在几乎很难找到没听说过 OpenAI 的人。我加入的时候，还是几个小团队各自在做自己的小研究项目。那时唯一始终不变的，是野心 -- -- 从一开始就瞄准 AGI，想要改变世界、产生正向影响。我觉得公司在这方面做得非常成功。ChatGPT 把一种\"可用的智能\"分发给了非常多的人，这本身就是一件非常了不起的事情。\n\n主持人：你发了那条离职推文之后，是不是几乎所有基础模型实验室都立刻联系你了？\n\nJerry： 确实有很多。我现在正在慢慢梳理下一步要做什么。在这个行业待了这么多年，我本来就认识很多人，也有很多联系。从积极的角度看，我并不急着立刻做决定。过去很多年我工作得非常拼，几乎没有时间去见人、聊天。现在终于有机会停下来，认真想一想接下来的七年要怎么度过。\n\n主持人：你在推文里提到，你想做一些在 OpenAI 觉得无法进行的研究。能具体解释一下吗？\n\nJerry： 是这样：在一家必须参与当下这种极其残酷、极其高压的竞赛、必须争夺\"世界上最强 AI 模型\"的公司里，有些事情就是很难做。这背后有几个方面的原因。\n\n其中一个因素是风险偏好。公司愿意承担多大风险，会受到很多现实约束：比如不能落后于用户增长指标，比如 GPU 成本极其高昂。因此，向外界展示实力、持续拥有最强模型，对所有主要 AI 公司来说都非常重要。但这确实会影响你愿意承担风险的\"胃口\"。\n\n另一个很难的取舍是组织架构。公司有 org chart，而 org chart 往往决定了你能做什么研究。每个团队都需要一个身份、一个研究范围、一组他们要解决的问题。跨组织的研究就会变得非常困难。\n\n我也不确定这是不是一个已经被完全解决的问题：当研究规模变得很大时，究竟该如何把研究组织好？研究本身喜欢动态，甚至可以说喜欢混沌；但一大群人需要秩序、结构和组织架构。\n\n所以，\"把组织架构交付出去（shipping your org chart）\"成了一种非常普遍的现象，研究也不例外。你最终会做那些组织结构最容易支持的项目。而与此同时，我确实想做一些研究，但公司的组织结构并不容易支持我去做这些事情。\n\n主持人：这是否意味着我们将看到一项新突破？\n\nJerry： 我想，其实 AI 世界里的每一位研究者，都想参与下一次真正的突破 -- -- 我当然也包括在内。\n\n主持人：我之前在播客里跟 Mark（Mark Chen，OpenAI 的 首席研究官） 聊过这个话题：几乎所有人都会带着自己的想法去找他、找 Yakob（Jakub Pachocki，OpenAI 的核心研究负责人之一）。OpenAI 一直以来确实有一段\"押注冒险想法、去做其他实验室没做的事\"的历史，而且这种策略也确实为他们带来了回报。但我也很清楚 -- -- 你们那里一定聚集了大量非常聪明的人，所有人都会不断提出各种想法。\n\n而在某个时刻，公司终究是一家资源有限的组织 -- -- 哪怕这些资源已经非常多了 -- -- 也必须做出取舍。所以，这必然是一个非常艰难的决策过程。也正因为如此，我在思考的那些方向，大概确实属于那种\"相当新、相当不寻常\"的路径：公司需要判断，我们到底要不要往这个方向走？现在有没有能力、有没有余力去承担这种不确定性？我们是否能在当下负担得起？\n\nJerry： 关于\"研究时代\"的判断，我不确定事情是否真的像他说的那样是非黑即白的。但我非常确定的一点是：在 AI 和机器学习的世界里，还有大量东西尚未被真正探索。\n\n大约六年前，我们基本确定了以 Transformer 为核心的架构路线。此后相当长一段时间里，整个行业都在持续扩大 Transformer 的规模，而且进展确实不错。路径也非常清晰：每个季度用稍多一点算力、稍多一点数据，训练出一个更强的模型。到目前为止，这条路看起来并没有明显的\"天花板\"，进步仍在持续。\n\n但问题是：这就是终点了吗？这是最后一条路了吗？我几乎可以确定不是。\n\n我们还有很多改进模型的方式，目前根本还没真正开始做。正如你刚才提到的，我自己主要做的是\"推理\"，以及扩大强化学习的规模。在那之前，整个领域几乎所有的\"大赌注\"都押在 Transformer 的预训练规模上。\n\n扩大预训练规模，确实是一种有效的扩展方式，而且效果很好。每一次更大规模的预训练，模型能力都会整体提升，各方面都会变强。所以你当然可以说：那我们就继续扩展预训练规模，模型自然会越来越好。\n\n但后来，有那么一小撮\"做梦的人\"、研究者开始相信：事情不止这一种做法。我们不只是扩展预训练，还可以在语言模型之上，大规模扩展强化学习，而且投入的计算量可以和预训练处在同一个量级。这样做，能够教会模型一些 仅靠预训练永远学不会的东西。\n\n正因为如此，我们今天才有了这些令人惊叹的 Agent：它们可以自动化工作、解决复杂问题。而如果只靠预训练模型去完成这些任务，可能需要极其夸张的算力和数据量。\n\n也就是说，当你发明了一种新的\"扩展方式\"，你就会得到一整套全新的能力；而如果你只是沿着原有的预训练扩展路线走，那可能要花非常、非常久，才能逼近这些能力。这一次，其实是一次相当大的跃迁。\n\n在我看来，自从 GPT-4 引入以来，\"推理模型\"几乎是这几年里最重要的一次能力跃升。而我相信，类似这样的跃迁还会出现不止一次。\n\n所以我一直觉得，研究者不应该只盯着\"渐进式改进\"，而是要去思考：有没有办法把整个棋盘掀翻？\n\n主持人：去年在 NeurIPS 上，Ilya 曾说过一句话，大意是：\"我们正在耗尽数据，这条路迟早会走到尽头。\"关于\"预训练是否正在进入一个越来越艰难的阶段\"，我一直在想：那下一个真正的突破会是什么？这正是你现在想问的问题，对吧？\n\nJerry： 是的。但我并不认为这等于在说\"预训练已经结束了\"。预训练仍然在持续改进，而且还有很多方式可以继续优化它。但它已经不再是唯一的改进路径，而且其他路径，可能在很多维度上能更快地带来提升。\n\n扩大预训练规模，在很多能力上提升得其实非常慢 -- -- 它确实会让模型更好，但提升是渐进的。而与此同时，可能还存在其他方式，能带来更大的跃迁。\n\n主持人：硅谷有一个很有意思的现象：很多时候，科技公司会提出一些非常原创、甚至看起来\"怪异\"的想法，外界一开始完全不理解。但正是这样，才催生了全新的商业模式、新的科学、新的研究方向。而科学研究本身，也是如此：你需要去追逐别人还没走的方向。\n\n可一旦某个方向\"爆了\"，事情就会反过来 -- -- 会形成一种巨大的共识。突然之间，所有人都开始说：\"我们就该这么做。\"然后大家不再讨论\"该不该走这条路\"，而是开始比拼\"谁在这条路上跑得更快\"。\n\n这其实就是你刚才描述的那种状态。那么问题来了：当我们已经进入这种\"模型竞赛\"，而且已经持续了两三年之后，会不会出问题？是不是所有主要实验室都变得越来越保守？这会不会成为一个普遍性的结构问题？\n\nJerry：让我感到非常\"难过\"的事，就是现在几乎所有 AI 实验室都在试图做和 OpenAI 一模一样的事情。\n\nOpenAI 显然是一家非常成功的公司，它在很多关键问题上做对了选择，把整个世界带进了\"规模化 Transformer\"的范式之中，也证明了：通过扩展机器学习模型的规模，确实可以为世界带来大量非常有价值、非常有用的能力。\n\n但问题是：这个世界究竟需要多少家\"做完全同一件事\"的公司？我不知道。竞争当然是好事，所以肯定不止一家更好。但 现在我们大概已经有五家相当严肃、体量巨大的 AI 公司，基本上在用完全同一套\"配方\"，试图在同一套技术之上，做出一点点差异化的产品。\n\n也许这确实是对的选择，但我还是希望能看到更多多样性 -- -- 更多模型层面的差异。\n\n如果你去看现在世界上最好的那些模型，实际上很少有人真的能注意到它们之间的区别。我觉得应该做更多\"盲测\"：让人们分别和不同模型对话，看他们是否真的能分辨出哪个是哪个。我敢说，99.9% 的用户根本察觉不出来这些模型有什么不同；在他们的感受里，这些模型几乎一模一样。\n\n即便背后是不同团队，在做一些细微不同的事情，但所有实验室都觉得\"我们在这个点上做得稍微好一点\"\"对方在另一个技巧上可能更强\"，最终的结果却是：大家全都挤在一个非常接近的位置上。\n\n那真正的探索在哪里？真正的创新空间在哪里？真正能让你和别人拉开距离的差异化又在哪里？\n\n主持人：我主要用这些模型做文字工作，偶尔会在 Gemini、ChatGPT、Claude 之间切换 -- -- 差别确实有，但很细，更多是语气和\"性格\"。比如我最近更常用 Claude，因为它更直接、不啰嗦；而 ChatGPT 的语气我一直很难调到那种感觉。不过总体我也同意，大多数人其实分不清这些模型的区别。\n\n话说回来，我想问一个可能有点尖锐的问题：你在 OpenAI 待了这么久，在公司内部算是传奇人物之一，而且你的履历也证明，你参与的项目往往能做成。那从外界看，如果连你这样的人都觉得 -- -- 自己真正想做的研究在公司里推进起来足够困难，以至于最后选择离开 -- -- 这是不是一个不太好的信号？尤其对一家最初以研究实验室起家的公司来说，这意味着什么？\n\nJerry： 我觉得有时候，人和组织都会成长到一个阶段：必须意识到，彼此的道路需要分开。\n\n对一家单一公司来说，非常重要的一点是：公司内部的人，必须在某种程度上对目标、对前进路径保持一致。而在某个时刻，我对\"未来研究路径\"的判断，和 OpenAI 选择的方向，至少在一些足够重要的点上，出现了分歧 -- -- 包括接下来一年研究该是什么样子。\n\n在这种情况下，我认为分开，反而比强行在分歧中继续合作要好得多。否则，那些分歧可能会不断积累、发酵。\n\n所以我反而认为：不同公司去做同样的事情，在某种意义上是合理的。因为专注对于一家公司来说非常重要，而 OpenAI 很可能正在做所有\"正确的事\"。\n\n也许只是我自己有一些不太现实的梦想；也许我对\"还能做些什么其他事情\"过于乐观 -- -- 这完全有可能。\n\n很多公司必须专注于自己的核心路径，才能活下来，才能进入下一个阶段。所以在一个理想的世界里，应该有很多不同的公司，在做很多不同的事情。而研究者 -- -- 尤其是那些很难去做自己并不真正相信之事的研究者 -- -- 应该能找到一个地方，在那里，他们能投入到自己最相信的研究方向中。最终，历史会证明哪一条路是对的。\n\n正因为如此，我才会对\"大家都在做同一件事\"感到有点难过。因为在当下，如果你想做一些偏离主流机器学习路线的事情，真的非常难找到一个合适的地方。这大概是我目前最感到遗憾的一点。\n\n主持人：那你现在还在思考下一步要做什么，对吧？如果所有实验室都在做同一件事，那你应该不会想简单跳去另一家大实验室？\n\nJerry： 我当然还在认真思考下一阶段。但如果有更多\"稍微偏离主流、但依然具备规模\"的选择，那我会更开心，也会更容易做决定。\n\n主持人：那你觉得，要让整个行业偏离当前主流路径，需要什么条件？我可以想象，这些公司投入了巨额资金、消耗了大量资源，又处在聚光灯下，自然会害怕承担风险。但也许这些风险是必要的。那到底要改变什么？或者这种改变真的会发生吗？\n\nJerry： 这正是一个非常有意思的问题。\n\n我其实非常喜欢冒风险，也经常被人这样评价。我认为，冒风险本身是一件好事。但当你面对的是\"巨额资金在押\"的局面时，真正有能力、也愿意承担风险的人，其实非常非常少。\n\n每个人的风险偏好都是极其个人化、极其独特的。我和很多人共事过，我真心觉得：人们应该愿意多承担一些风险，多去尝试一些事情。\n\n但另一方面，现在 AI 世界里的研究者薪酬已经高得离谱了。这在某种程度上，也会让人变得非常害怕失去工作、害怕一次不好的绩效周期。结果就是：人们更倾向于追求短期、确定性的收益路径。而这些人本身往往都是非常聪明、动机也非常正直的研究者。只是整个系统在某些地方，确实更容易鼓励\"短视\"。\n\n我认为，研究者应该被更明确地鼓励去冒风险、去下大胆的赌注，因为真正的进步，正是这样发生的。\n\nYann LeCun 的世界模型，\n\n\"方向无疑是正确的\"\n\n主持人：那我们已经看到了一些\"独行侠\"式的人物。比如 John Carmack。Carmack 跑去了达拉斯，像是进了自己的洞穴里。一开始似乎是单干，现在好像有几个人在跟他一起做。他几年前说的，其实和你刚才讲的很像：也许我不知道能不能走出一条完全不同的路，但至少应该有人在一条完全不同的路径上持续折腾。\n\n我和 Ilya 聊过，但并不知道他现在具体在做什么。我不知道那是他之前工作的延续，还是某种非常激进的新路线。不过我想，如果不是完全不同的方向，他大概也不会去募那么多钱、重新开始。\n\n然后还有 Yann LeCun，他显然有一套不同的哲学。有时候我会觉得这个领域挺奇怪的：AI 从某种意义上说很\"老\"，已经发展了几十年；但当前这一波 AI 又非常新。和研究者聊天时，他们会说：现在把主要论文读完，其实很快就能跟上前沿。所以我一直在想，会不会有某个人，突然从完全意想不到的方向出现，带来一个极端激进的新想法，把整个领域往前推一大步？但与此同时，又好像越来越难 -- -- 因为现在你几乎需要一个\"国家级规模\"的数据中心，才能真正参与到这个层级的竞争中。\n\nJerry： 这正是事情变得非常困难的地方，同时也是一个非常值得解决的问题。\n\n世界上其实有大量学术研究在发生，也有很多学生在做各种各样的事情，但其中大多数都严重缺乏资源。这使得很多研究最终走不远，因为你真正想做的研究，往往必须在\"大规模\"下才能完成。\n\n但这也是让我感到非常乐观的一点：现在确实有相当多的资金，正在流向那些\"想做新东西\"的人。像 John Carmack、像 Ilya -- -- 他们做的事情，正是当下这个时代应该存在、也应该被资助的。当然，不是所有尝试都会成功，但其中一定会有一些成功，而创新正是这样发生的。对于任何一个强化学习研究者来说，\"探索（exploration）与利用（exploitation）\"之间的权衡，都是一个非常基础、非常重要的概念。\n\n即便是在优化 agent 时，你也必须不断权衡：是走已经被证明有效的路径，还是去尝试全新的方法，用完全不同的方式解决老问题？这是一个非常困难的取舍，但它本身就是一个被研究、也值得研究的问题。而正如我们在设计 agent 时会思考这个问题一样，我们也应该反过来问自己：我们自己在做研究时，是如何在探索与利用之间取舍的？\n\n主持人：在这个非常非常顶尖的小圈子里，大家都知道 Carmack 在做什么吗？你们彼此是互相了解的吗？\n\nJerry： 老实说，我并不完全清楚。但如果我没记错的话，我隐约知道一些。他可能是在押注一种非常端到端的强化学习方式 -- -- 通过鼠标和键盘，在电脑游戏中训练 agent。\n\n如果真是这样，那其实非常有意思。因为我长期以来一直在想：电子游戏，可能是训练智能体的最有趣环境之一。游戏本身就是为了\"对人类大脑有吸引力\"而设计的。它们包含故事、权力幻想，但更重要的是：大量的问题求解。游戏必须有趣、必须有挑战、不能重复。\n\n在某种意义上，电子游戏非常贴合人类智能，它们天然地在教你资源分配、解谜、如何在不同规则下取胜 -- -- 这正是我们希望 agent 能学会的事情。当然，我们现在还没有真正能在高频、多模态环境中稳定运行的超强模型，可能存在一些架构层面的限制。但我认为，用电子游戏来训练 AI，是一件非常值得做的事情。\n\n主持人：Richard Richard Sutton 过去在扑克、游戏等领域做过大量工作；我也曾在他的实验室待过。早期的那些游戏环境，比后来 OpenAI 的 Dota 要原始得多。但你可以看到，这个想法一直贯穿其中。\n\nDemis Hassabis 也长期在追逐类似的方向。所以你提到这一点很有意思 -- -- 这其实是一个\"老想法\"。一段时间里，各大实验室都在比谁能打通更复杂的游戏、谁能更好地\"秀\"成果；后来在 ChatGPT 时代，这条路线似乎被边缘化了。但也许，它仍然有潜力。\n\nJerry： 在科学史上，有一个非常常见的现象：好的想法，往往会反复出现。真正困难的，并不是提前预测\"哪个想法是重要的\"，而是判断\"什么时候是对的时机\"。即便在 OpenAI 早期，我们也常说：不能断言某种方法\"行不通\"，也许只是\"现在还行不通\"。\n\n我七年前刚加入 OpenAI 时，强化学习在游戏上是一个非常火的方向。我们解决了很多游戏问题：StarCraft、Dota，而 AlphaGo 更是一个标志性时刻。但这些模型有一个非常明显的缺陷：它们几乎没有世界知识。它们并不理解我们的世界，只是从零开始，专门为某一个游戏训练。\n\n这显然不是正确的路径。我们必须先教模型理解世界，理解更高层次的概念，而不仅仅是对像素做出反应。从零开始的强化学习，更像是\"猴脑\"或\"蜥蜴脑\"。而我们想要的，是具备更高层次抽象能力的模型。\n\n在多年大规模预训练之后，我们现在已经能够学到一套非常强的\"世界表征\"。而接下来，我们应该利用它。这正是\"推理模型\"的核心魔法：在一个对世界有深刻理解的基础之上，叠加一层强化学习。未来就应该沿着这个方向前进。\n\n主持人：那这不就和\"世界模型\"的方向一致了吗？Google 在做这个，Yann LeCun 似乎也在推动类似的想法。这在直觉上是合理的 -- -- 这也是人类学习世界的方式。我们不是在一个黑箱里长大的，而是通过不断试探、感知世界来学习的。所以你对这个方向是非常看好的。\n\nJerry：这个方向毫无疑问是正确的。真正有挑战性的，是：如何把从世界建模中学到的表征，与强化学习真正结合起来。\n\n强化学习教会模型\"技能\" -- -- 让它学会如何在世界中实现自己的目标。但在此之前，模型必须先理解世界，否则它连\"如何设定目标\"\"如何达成目标\"都无从谈起。\n\n正因为如此，这两件事情必须结合起来。\n\n如果有人能在一个高质量世界模型之上，真正把强化学习跑通，那将会是一个非常令人振奋的时刻。\n\n主持人：就你现在这些正在吸引你的研究方向来说 -- -- 你能不能稍微给我们一点提示？还是说，这样就直接暴露你下一家创业公司的方向了？\n\nJerry： 我现在最兴奋的研究方向大概有两个。主要原因也很简单：我不觉得重复去做各大实验室正在做的那套事情有什么意义。现有体系里当然还有很多可以微调、可以改进的地方，但我认为有两个方向长期被低估了投入 -- -- 或者至少没有得到足够的资源与重视。\n\n第一，是某种意义上的\"架构创新\"。我觉得我们对 Transformer 架构有点过于\"路径依赖\"了。Transformer 确实很伟大，也被非常深入地研究过。人们一直试图在本地做一些小改动，让 Transformer 更强，但这件事并不容易。虽然也有一些相当成功的改进 -- -- 比如稀疏化非常成功；还有各种让注意力计算更便宜的方法，也取得了不错的效果。\n\n但 Transformer 会是机器学习的最终架构吗？显然不会。尽管 Transformer 的发明者做出了惊人的贡献，并且几乎定义了接下来十年的机器学习格局，但我相信一定还有更多可能。\n\n一定存在一些训练大模型的方法 -- -- 它们也许有点像 Transformer，也许完全不像。我觉得这是一个值得去解决的问题。甚至如果没有别人去做，我也愿意卷起袖子自己上，试着把它做出来。\n\n第二个方向相对更\"热门\"，但我觉得几乎没有人把它做得真正好，那就是持续学习（continual learning）：如何把测试时（test time）与训练时（train time）真正打通、真正融合起来。\n\n人类显然就是这样运作的：我们没有一个\"专门学习模式\"和一个\"专门回答问题模式\"。学习与反应是连续发生的、时时刻刻都在进行。我觉得我们的模型也应该更接近这种状态。\n\n这可能是我们在把模型真正称为 AGI 之前，最后几个关键能力要素之一。如果模型不能从它看到的数据中持续学习，它就仍然显得有点受限 -- -- 甚至有点\"笨\"。\n\n新技术炒作带来的恐惧感\n\n主持人：说到 AGI，我们上次录播客时我提过：我已经不像一两年前那样经常听到\"时间线\"讨论了。那时候大家非常热衷谈什么时候会实现 AGI，甚至连\"AGI\"这个词最近都没那么火了。你自称对 AI 是\"谨慎的乐观主义者\"。那你觉得我们现在处在 AGI 时间线的哪个位置？\n\nJerry： 我个人的看法是：我对时间线做了一点更新。\n\n我一直认为，把强化学习规模化（scaling reinforcement learning）是通向 AGI 的必要部分。一年、或一年半之前，我非常坚定地认为：只要把 RL 规模化到我们的模型之上，那就是 AGI 了。但我确实不得不稍微修正这个判断。因为有些东西，只有当你真的到了\"下一阶段\"之后才看得见。\n\n我们也必须承认：今天的模型在很多方面已经非常非常强了。就拿编码来说 -- -- \"vibe coding\"是我最喜欢的爱好之一，你现在可以非常快地写出很多东西。对一些十年前的人来说，如果你把今天这些能力展示给他们，他们可能已经会把它叫做 AGI 了。\n\n所以我不觉得谈 AGI 还是一种多么离谱、多么疯狂的事。但至少按我的定义，现在的模型仍然不是 AGI -- -- 原因之一是：持续学习完全还没有以真正的方式被整合进模型体系里。\n\n除此之外，还有很多问题。比如多模态感知：如果模型文本理解很强、编程也很强，但它看不见真实世界、不能看视频并且很好地理解视频，那我们能称它为 AGI 吗？\n\n所以我认为，要真正达到那个\"文明级里程碑\" -- -- 构建 AGI -- -- 还有很多必要步骤要完成。\n\n有一段时间我曾想：如果我们真的拼命推进，并且把所有关键问题都做得足够好，也许 2026 年至少能实现非常强的持续学习，以及真正通用的强化学习。\n\n我觉得我的时间线仍在漂移。但与此同时，AI 领域移动得太快了：投资在年复一年累积增长，越来越多人进入 AI 领域，人才池变大，我们探索的想法数量也变多。\n\n所以我不觉得\"这个想法完全荒唐\"。也许会早一点，也许会晚一点：可能是 2026，也可能 2027、2028、2029。我不觉得会比这更久太多。但确实还有很多工作要做。不过人们正在非常努力地做 AGI。\n\n主持人：你刚才提到的内容 -- -- 让我想起你之前做的那些事。除非我记错：在 Strawberry 还没成为一个\"明确项目\"之前，外界不是有过所谓的 Q-Star 传闻吗？而且在那次\"内部风波\"期间，这件事被反复提起：什么\"他们知道 AGI 已经到了\"，把所有人都吓到了。但听你现在这么说又挺有意思的。因为确实，这些东西做出来以后非常惊人，我们会一度情绪很亢奋；然后时间过去，大家就习惯了。现在回头看，Strawberry 确实很不可思议，也确实改变了整个领域。\n\n但我第一次用它的时候，并没有到那种\"把我吓死\"的程度。你懂我意思吧？\n\nJerry： 我懂你意思。\n\n这其实涉及人类心理，以及我们如何与技术互动的方式。对我来说，把强化学习规模化带来的效果仍然非常显著，而且我觉得随着时间推移，我们会看到更多影响。\n\n尤其是应用在编程上，这会以很多很多方式改变我们的生活。你今天做一个大规模编程项目，和一年前相比，完全是另一种游戏。我们会在很多领域看到这种变化带来的连锁影响。\n\n但我也想说：两年前，当我和团队、以及 OpenAI 的很多人第一次看到 Q-Star 的一些早期迹象真的开始工作时 -- -- 你坐在一个房间里，看到一种\"有意义的新技术\"正在出现。\n\n如果你在那一刻不感到一点害怕、不感到一点担忧、不暂停一下想一想\"这对世界意味着什么后果\"，那我会觉得你没有在负责任地对待自己的工作。我认为每一个 AI 研究者都应该想这些问题：如果我正在做的东西是全新的、它展现出了以前从未出现过的新能力，那世界会发生什么？\n\n很多研究者确实会这么想。当然，有时候也会把担忧推得太远。一方面，到目前为止，AI 还没有给世界带来什么\"实质性的重大伤害\"；但另一方面，一些事情（比如\"某些很花哨的东西\"）是不是算有问题 -- -- 也许还可以争论。（笑）\n\n但总体来说，我认为：当你向世界释放新技术时，感到担忧与谨慎，是一种非常好、也非常健康的反应。\n\n我们正在经历一个变化的时代：大量新事物正在扩散到世界里，它们会产生影响 -- -- 影响人们如何生活，如何看待自己、看待他人；影响人际关系、国际关系；影响 GDP、影响生产力。\n\n有时候，一个人写下的一行代码，就可能引发连锁反应。经历了这一切，肩膀上的单子就相当重。\n\n为什么大模型行业叙事\n\n变成了肥皂剧、真人秀\n\n主持人：我一直在想，尤其\"政变\"那段时间：你做出来的东西被媒体炒得很热，还被卷进各种戏剧化叙事。我不知道\"滑稽\"这个词对不对，很多人其实还没弄清它到底是什么，就已经围观成现象了。你当时是什么感觉？\n\nJerry： 技术、概念、人类情绪、人类生活、人和人之间的协议与分歧 -- -- 在现实里很难被切开来看。\n\n我们确实活在一个世界里：AI 领域的重要参与者之间，有一个非常复杂的关系网络，很多层次叠在一起。要把它完全理清楚，可能得历史学家花很多年、甚至几十年，才能真正弄明白到底发生了什么、哪些因素起了关键作用。\n\n老实说，到现在为止，我对那段时间发生的一切也只剩下非常零散的记忆。我们也在不断\"补课\" -- -- 每当有新的证词出现、每当新的文件被披露，就会冒出一些新事实。未来某个时刻，肯定会有人把所有内容都挖出来、完整还原。\n\n但现实世界就是这么复杂。我也确实觉得，也许应该有一种更健康的方式来讨论技术：找到一个更合适的讨论场域，让分歧能够被更充分、更有建设性地展开。但我们生活在这样一个世界里：没有完美解，也不存在一种绝对正确的讨论机制。\n\n主持人：所以你觉得 X（推特）也不是理想媒介？\n\nJerry： 我个人其实很喜欢在 X 上发内容，分享想法，和社区交流。但它也不是一个完全严肃的地方 -- -- 很多时候都是半开玩笑、半认真。更核心的问题是：有人担心某件事太危险不该继续；有人觉得继续做是对的，因为它会增强能力；还有人认为方向本身就不对，我们应该做别的研究。\n\n在技术进步与研究的世界里，这些事情很多都是未知的。没人知道未来。我们只有想法、信念和梦想。\n\n我们必须和这种不确定性共处，也必须学会在很多问题上\"求同存异\" -- -- 很多时候只能接受：大家各自下注、各自承担后果。\n\n主持人：说到当时媒体对 Q-Star 的关注 -- -- 那阵子简直是炒作过度，几乎天天都在加码，每个月都愈演愈烈。我看着会觉得：这是不是太\"嗨\"了、太多 hype 了？而且我们俩也都在推特上，多少也参与了这股热度。你怎么看：这种 hype 该不该降一降？我个人确实觉得，强度可以往回拧一点。\n\nJerry： 我了解。反过来想，如果七年前有人告诉你：OpenAI 会成为万亿美元级别的公司；会建造规模堪比史上最大基础设施项目的数据中心；会拥有世界上最大的 Web 产品之一；全世界会无时无刻都在谈 AI -- -- 你一定会觉得那个人疯了，会说\"这就是炒作\"。\n\n可我真心觉得：这波 hype 在很多层面其实是有事实支撑的。人工智能在很多方面存在过度反应和反应不足的情况（有时候被高估，有时候也会低估），但 AI 的重要性毋庸置疑 -- -- 它值得被讨论。我不觉得现在还有谁会认为 AI 是个\"不重要、不值得讨论\"的话题。几年前确实还有人这么想，但现在已经很清楚：AI 很可能是当今世界最重要的议题之一，值得持续讨论与思考。至于进展会有多快、路径到底对不对、安全还是危险 -- -- 这些当然都可以争论。但 AI 会长期存在，而且只会越来越强。\n\n主持人：完全同意。但如果先把技术放一边 -- -- 我甚至报道过\"挖人狂潮\"。我越来越觉得，这个行业的叙事变得像肥皂剧、像真人秀，很多时候讨论的不是硬核科学，而是剧情、阵营和情绪。你会不会也觉得我们有点\"跑偏\"了？\n\nJerry： 但到底是谁在制造这场肥皂剧？这才是问题。\n\n主持人：嗯，说真的，这一轮比我经历过的任何技术周期都更\"肥皂剧\"。可能是赌注太高、钱太多，再加上挖人和各种戏剧化叙事，整个旧金山像活在一套自己的现实里。\n\n我有时都替你们累 -- -- 七八年一直在这种高压竞速里，你现在想停下来喘口气，我完全能理解。\n\nJerry：的确很消耗。\n\n但我可以跟你分享一句对我很有帮助的话：有一次，一个比我更有经验、更擅长应对压力的人跟我说 -- -- Jerry，这就像做俯卧撑。每经历一次艰难、紧张的时刻，你就更擅长应对压力一点。\n\n老实说，这七年让我练出了很强的心理和情绪韧性。我真的学会了在大量噪音、很多胡扯面前，把自己抽离出来，尽量保持稳定、保持定力。\n\n不管外部发生什么 -- -- 公司看起来要塌了也好，研究者流动也好，项目被重新分配也好 -- -- 总会有事情在推进，总会有新的变化。\n\n我听过有人把\"挖人\"这件事类比成体育队伍的转会。体育之所以还能运转，是因为有角色、有规则。我差点想说：可惜在加州的法律框架下，这类规则基本不可能出现。但我确实觉得，如果能有一些规则，可能会更健康。\n\n因为确实存在这样一种现象：有些人换工作的频率，比他们真正产出成果的频率还高。\n\n主持人： AI 薪资帽？（笑）\n\nJerry：（笑）确实有人这样。但也仍然有很多人在认真做事，推动前沿继续往前走。不过，AI 是一门大生意 -- -- 这点无论如何都没法否认。\n\n主持人： 我还跟同事说，我们真该做一张表，把那些在每一家前沿实验室都待过的人列出来，标注他们在每家待了多久。（笑）肯定至少有一小撮人，把整个湾区的\"前沿实验室巡回赛\"跑完了。说真的，这太疯狂了。\n\n主持人：2018 年前后，OpenAI 还只有三十来个人。有一件事当时让我印象特别深：最早那批成员里，波兰人的比例异常高，而且很多都是非常典型的\"数学脑\"。\n\n有些人彼此从小就认识，有些并不认识。我一直很好奇：这到底反映的是一种教育背景的集中效应 -- -- 比如偏重数学训练的体系，确实更容易培养出这类人？还是说，其实只是早期有几个人先来了，后来通过学术和个人网络，慢慢把更多同类的人吸引到了 OpenAI？\n\nJerry： 先澄清一点：我在加入 OpenAI 之前，完全不认识任何 OpenAI 的人。我是非常随机、机缘巧合地进来的。\n\n但你说得没错，在 OpenAI 非常早期，波兰人的占比确实偏高。不过我并不觉得这种情况\"经得起时间检验\"。现在公司里，波兰人的比例仍然略高于平均水平，但考虑到 OpenAI 的规模已经增长了大概一百倍，这种早期的\"高浓度\"并没有按比例延续。\n\n我觉得这里面确实有一些值得讨论的因素，但我并没有足够多对其他教育体系的亲身体验，所以不敢轻易下结论，说波兰的教育体系\"天然更强\"。我能确定的是：我们确实有很多非常聪明、数学直觉很强的人。\n\n但如果说有一件我特别认可、也特别喜欢的事情，那就是波兰人对\"努力工作\"这件事的重视从我个人经历来看，这种特质在很多地方正在变得越来越少见 -- -- 尤其是在一些生活条件已经非常优渥的社会里，人们对工作的强调确实在下降。\n\nGoogle 的\"回归\"\n\n还是 OpenAI 的\"失误\"？\n\n主持人：你怎么看 Google 最近这一轮的\"回归\"？你是觉得意外、惊讶，还是说其实早就料到了？看起来他们这段时间做对了不少事情。你们之前是不是一直都觉得：Google 迟早会把局面理顺？\n\nJerry： 我个人其实不太愿意把这件事称为\"Google 的回归\"。它应该被视为 OpenAI 的失误。\n\nOpenAI 确实在很多关键点上做对了事情，但也不可否认，在某些阶段出现过判断或执行上的失误，导致整体推进速度比它本可以达到的状态要慢。\n\n在一种理想的执行情境里，如果你是一家已经取得领先优势的公司，而且拥有 OpenAI 那样的技术、人才和资源条件，那么你理论上是可以持续保持领先的。但如果在这个过程中，你做出了一些错误决策，而你的竞争对手做出了更多正确决策 -- -- 而 Google 在最近一段时间里，确实做对了不少事情 -- -- 那对方追上来，其实并不奇怪。\n\n你也必须承认：Google 在硬件、算力和人才储备上，本身就有非常巨大的优势。事实上，在 OpenAI 刚起步的那些年里，Google 在几乎所有机器学习方向上，都是明显的行业第一。\n\nOpenAI 能真正跑出来，靠的主要不是资源优势，而是 研究方向上的强烈信念：对某一条具体技术路线、某一个具体长期赌注的坚定投入。\n\n但让整个行业、让外部世界真正意识到\"这是一个正确的赌注\"，花的时间比很多人想象的要长得多。哪怕 GPT-2 训练完成了，GPT-3 训练完成了，后来 GPT-3.5 也出来了 -- -- 在那个阶段，其实并没有太多人真正重视这件事。\n\n你去 NeurIPS 这样的会议和研究者聊天，大家会觉得 OpenAI 很酷，但很多其他实验室的态度是：\"嗯，我们迟早也能复现。\"语言模型确实挺有意思，但在他们看来，也就止步于\"有意思\"。\n\n真正的转折点，是 OpenAI 开始通过 ChatGPT 赚到钱。那一刻，其他公司才突然意识到：\"好，这不只是研究展示，而是一个已经被验证的商业方向，我们必须认真投入了。\"\n\n这里其实存在一个很关键、但常常被忽略的时间窗口：从你开始构建一项技术，到它真正被商业化，中间往往隔着一段很长的时间。\n\n这段时间，足够让其他公司观察、犹豫、评估风险，然后再决定是否下场。而在这个阶段，Google 显然开始非常认真地对待大语言模型这条路线。再叠加 OpenAI 在执行层面的一些失误，最终导致今天的结果：在模型能力和训练成果上，双方已经变得非常接近。\n\n所以，从 Google 的角度来看，这确实是一件值得祝贺的事情。能够把团队重新拉回状态、把执行节奏提起来，背后一定做了大量艰难而高质量的工作。\n\n主持人：那你说的这些\"失误\"，具体指的是什么？我在努力回忆。我记得当年你们推出 Search 的时候，外界一度在说\"Google 完了\"，但我当时就觉得未必如此。所以你提到的失误，更多是指哪些方面？\n\nJerry： 我不太想展开讨论具体的内部决策细节，哪些判断是对的，哪些是错的。\n\n但我想强调的核心其实很简单：如果一家领先公司执行得足够好，那么在大多数情况下，它是可以把领先优势持续下去的。\n\n而在现实中，很明显有一些事情的推进速度，比它本可以达到的节奏要慢。\n\n主持人：你的意思是技术层面的失误吗？因为从外界看，也确实发生了不少公司层面的戏剧性的狗血剧情，这些在某些阶段显然拖慢了整体节奏。\n\n我跟 OpenAI 的一些人聊过，关于公司要如何继续向前，确实出现过一些阶段性的混乱，比如关键人物离开等等。所以我原本以为你指的是纯技术问题，但听起来你的意思更复杂一些。\n\nJerry： 这些事情有时候确实是相互关联的。\n\n从技术角度来说，我并不认为\"有人离开\"这件事本身就一定构成问题。在任何一家公司，人来人往其实都很正常，也应该是一种常态。\n\n但如果离开变成了某种更深层问题的 症状 -- -- 比如有人觉得：\"公司在一些关键事情上做错了决定，我不再相信这家公司了，所以选择离开\" -- -- 那这往往意味着，背后确实存在一些需要被正视的问题。\n\n所以回到我最初的判断：确实有一些事情，推进得比它本可以做到的速度要慢。 这并不否认 OpenAI 的成功，但也不能忽视这些失误带来的影响。\n\n主持人：如果像你说的那样，各大实验室基本都在走同一条路，那 Meta 显然也是其中之一。他们在 AI 上投入巨大，也在从各家实验室挖人。我并不完全清楚 Meta 内部的具体策略，但从外部看，他们似乎并没有选择一条完全不同的路线，而更像是在追赶同一条主流路线。\n\n这听起来像一个根本性的问题：如果你既起步更晚，又在做和别人几乎一样的事情，这真的可能有好结果吗？还是说，你觉得 Meta 实际上走的是一条不一样的路？\n\nJerry： 我并不完全了解他们的内部策略，所以只能谈一些外部观察。\n\n我的感觉是，他们已经意识到一件非常关键的事情：\"规模化\"在当前的 AI 世界里是不可回避的。如果你放眼现在的 AI 行业，基本可以抽象出两种不同的战略选择。\n\n第一种是：我要做一种和其他人都不一样的模型 -- -- 它在某些方面会明显更强，我希望把这种差异化模型带给世界。第二种是：我也希望拥有和别人一样强、同一量级的模型，但我的重点不在模型本身，而在于我如何使用这些模型、以及我基于它们构建什么样的产品。\n\n从我对 Meta 一贯路线的理解来看，这家公司长期以来关注的核心，一直是连接人与人、构建关系、打造大规模的用户体验型产品。无论是社交网络、沉浸式体验，还是他们设想中的元宇宙，本质上都是围绕\"体验\"和\"连接\"展开的。\n\n所以我这里是基于外部推测，但我认为 Meta 的思路，很可能是：使用我们已经熟悉、已经理解得比较透彻的 AI 技术（比如 Transformer），来构建全新的产品体验，而不是在模型层面追求完全不同的路线。\n\n从一家极其成功、极其赚钱、而且已经拥有全球最大社交网络的公司视角来看，这其实完全可能是一种非常合理、甚至非常聪明的策略。\n\n主持人：我们刚才聊了 Google，也聊了 Meta。但我想换一个角度问：在你们内部讨论、或者评估其他实验室的时候，有没有哪一家，让你们真的觉得\"被震撼到了\"？哪一家是你个人印象最深的？\n\nJerry： 我得说，这是一个相对比较新的变化。\n\n在过去一年里，我对 Anthropic 的印象提升得非常明显。 我本人其实从来不是那种特别在意模型\"性格\"的人。虽然我也听说过 Claude 的\"性格\"很好，可能确实如此，但这并不是我关注的重点。\n\n真正让我感到震撼的是几件事：他们在 代码模型、编码 Agent 上的成果；以及他们围绕\"开发者\"建立起来的整体产品和品牌 -- -- 还有最关键的一点：他们拥有一大群真正满意、甚至很开心的开发者用户。 这是一项非常、非常了不起的成就。\n\n更重要的是：他们起步比 OpenAI 更晚；算力条件更受限制；团队规模也更小。在这样的前提下，他们依然做到了高度聚焦，并且执行得非常好。\n\n他们在获取高质量算力方面遇到过不少现实困难，但即便如此，仍然做出了非常出色的产品。\n\n这些产品正在明显改变人们开发软件的方式；而据我了解，也已经在 实质性地提升企业生产力。\n\n所以我真心觉得：他们做得非常好，值得祝贺。\n\n主持人：他们确实看起来正处在一个\"高光时刻\"。我身边几乎所有人都在聊 Claude Code。我最近还采访了一个人 -- -- 他在用 Claude\"养活一盆植物\"。（笑）可能是第一种被 AI 模型持续\"照料\"的生命体。我真的不知道他们是怎么做出一个几乎\"人人都喜欢\"的工具的。从 ChatGPT 到 Claude Code，这种程度的\"普遍好评\"，其实非常少见。\n\n而且之前还有一件事：当大家被\"切断使用\"时，开发者的反应极其强烈 -- -- 某种程度上，那种崩溃感甚至超过了 OpenAI 出事时的反应。连 Elon 都公开承认了这一点，说：\"是的，我们用得太多了，这是个警醒，我们得把自己的东西做得更好。\"所以我在想：这也许不是一个完全普遍的现象，但看起来，很多实验室其实已经在不同程度上依赖这套工具了。也希望这次\"切断\"能倒逼出更多、更好的同类产品。来一百万个 Claude Code。（笑）\n\nJerry： 在 OpenAI，我们其实也开发 Codex 有一段时间了 -- -- 它算是我们自己的\"Claude Code 版本\"。\n\n我个人觉得 Codex 也挺不错的。有点好笑的是：我自己其实并没有怎么用过 Claude Code。毕竟当时我还在 OpenAI 工作，也没太多机会去亲自用。\n\n我也是想说得客气一点。所以我确实没法给出太多一手对比体验。但至少从推特上的反馈来看，Claude 确实被全球开发者 非常、非常 喜欢。\n\n做点跟 OpenAI 不同的事情\n\n主持人：结合我们前面的讨论，我对你的理解是：你一直是从一种很纯粹的智识和科学兴趣出发的人。你在 reasoning 上的很多工作，本质上都指向一个长期目标 -- -- 你想创造\"AI 科学家\"。\n\n所以当我看到你说要离开 OpenAI 时，我忍不住在想：你是不是已经不太想继续待在这场\"基础模型竞赛\"里了？听你说话的感觉，更像是想换一条路走。我甚至会想象，你会不会干脆跑去做生物科技之类的方向，用完全不同的方式继续追这件事。\n\nJerry：如果我能克隆自己、同时做很多件不同的事情，我真的会非常愿意。但长话短说：有一天我突然意识到 -- -- 我对自己过去的人生很满意，也为自己做过的事情感到骄傲；但我现在真正想做的，是押一两个、甚至两三个非常非常大的研究赌注，然后看看能不能把它们做成。\n\n我一直觉得，人应该更愿意冒风险。至少从我的观察来看，我可能算是那种风险承受能力比较高的人 -- -- 愿意去追一些看起来很野、很不确定、甚至有点离谱的想法。所以我觉得，我应该把这种特质用在更有意义的事情上。\n\n主持人：那你脑子里的这些想法，如果真要落地，大概需要多久？是一年左右的项目，还是说你说的\"风险\"，意味着你愿意花四五年时间去追一件事，而它最后甚至可能还不如现有方案？\n\nJerry： 我肯定愿意投入很多时间。但与此同时，我也非常坚定地认为：研究应该尽可能快地推进。\n\n做得慢，本身并不值得骄傲。从\"把研究执行好\"这个角度看，我希望它能更快。\n\n不过，真正关键的，其实是我之前反复提到的两个词：聚焦（focus）和信念（conviction）。\n\n如果你同时做很多事情，几乎注定每件事都只能做一小部分。你的注意力会被摊薄，资源也会被摊薄。研究实验室经常会说：算力不够，算力限制拖慢了研究。这当然是真的，而且是重要因素之一。但很多时候，更核心的问题其实是：不够聚焦。一天之内，一个人的注意力只能真正放在有限的几件事情上。\n\n我很喜欢对和我共事过的研究者说一句话：少跑一点实验，把每一个实验想得更深。因为有时候，你花几个小时什么实验都不跑，只是盯着结果、反复分析数据 -- -- 反而更容易带来真正的突破，而不是不停地\"多跑\"。\n\n所以像 OpenAI 这样的公司，算力其实非常多。但如果算力被分散到太多项目上，效果反而会被稀释。如果把算力集中到更少、更聚焦的项目上，算力往往是够用的。\n\n但这又回到了风险和信念的问题。如果你同时做三个项目，只要有一个成功，其实就已经算不错了；另外两个被砍掉，也完全可以接受。如果三个都成功，那当然更好。但如果你只做一个项目，它往往会推进得更快 -- -- 因为你足够聚焦、也足够坚定。当然，代价是：如果它失败了，你会非常惨；但如果它成功了，你可能会拥有世界上最好的模型。\n\n而对 OpenAI 这样规模的公司来说，现在确实很难做到一件事：把整个公司押注在一个全新的、完全不同的方向上，同时不在乎下个季度 Gemini 会不会更强。 这真的非常难。它需要一种非常特殊类型的人，才愿意这么做。\n\n我觉得，这就是问题的核心。\n\n主持人：我明白，也知道你不能聊什么\"秘方\"。但我还是忍不住好奇：从外部看，我会直觉觉得，OpenAI 接下来押注的方向，应该是那些能赚大钱的方向。比如\"Chat 里要加广告\"的消息，几乎把整个互联网点燃了。哪怕很笼统地说，你觉得我们能判断他们接下来大概会把资源投向哪里吗？\n\nJerry： 这个问题上，我确实不应该、也不能谈 OpenAI 的任何具体计划。\n\n主持人：合理。（笑）那我换个问法：你觉得这些做模型的公司里，有没有谁会选择 -- -- 也许\"勇气\"这个词不太准确 -- -- 不把广告塞进模型里？还是说，从商业角度看，这其实是不可避免的？\n\nJerry： 这属于商业策略。我做的是训练模型。（笑）\n\n主持人：好，抱歉，我不是想逼你。（笑）只是聊完整个对话之后，我自己还在试图想明白一件事。一方面，你说你想走一些新的方向，去追一些和主流不同的路径；但另一方面，我们也反复提到：你想做的这些事，确实需要非常强的\"马力\"。所以我有点难想象：这是 Jerry 一个人、在外面慢慢测试新想法？还是说，就你真正想做的那些研究，你必须身处一个拥有足够资源的地方，事情才有可能发生？\n\nJerry： 这正是我现在最想搞清楚的第一个问题。任何 AI 研究，最终都离不开 GPU、离不开算力（FLOPs）。我现在需要认真想清楚的是：到底什么样的方式，才是做这些研究的最佳路径。\n\n我确实正在努力理清楚：我很清楚自己想做哪些研究，但我还在寻找答案 -- -- 到底怎样去做，才算是一个\"好的方式\"。\n\nOpenAI 的压力甚至超过创业？\n\n主持人：我刚才问的那些，基本就是我最想问的了。我觉得我能跟你聊上好几个小时。\n\n我不想继续追问\"你接下来做什么\"，因为你看起来太开心了，整个人容光焕发。\n\nJerry： 是的，我听好几个人都跟我说：你现在比以前快乐多了。\n\n主持人：我不想把你拖回那种压力里，比如问你接下来要做什么？\n\nJerry： 我不知道。而且我也听一位正在经营自己公司的人说过一句让我很震撼的话：在 OpenAI 工作，比自己创业还更有压力。从很多方面看，OpenAI 的确是一个压力极大的地方。\n\n主持人：还有一个小问题，除了\"大家都在追同一套东西\"之外，你觉得这个领域里还有没有什么\"巨大的错误\"？\n\nJerry： 我不觉得存在那种特别\"巨大的错误\"。这个行业里的人，其实都很难犯那种一眼就能看出来的致命错误。\n\n真正的问题更像是：你愿意花多少精力去探索\"其他可能性\"？又有多少精力，继续沿着你已经走得很顺的那条路往前推。\n\n主持人：那我换个问法，可能更准确一点。有没有一些你觉得被明显低估、被忽视的研究方向？它们本该得到更多关注，但现在没有。\n\nJerry： 老实说，这样的想法非常多。但这些想法最缺的，往往不是\"它们不存在\"，而是：缺关注、缺算力、缺资源。\n\n这里还有一个比较有意思的现象。很多研究者 -- -- 包括学术界 -- -- 很擅长、也很喜欢做\"从 0 到 1\"的事情：提出一个新想法，证明它\"有点能跑\"，然后就发表出来。而我觉得，我自己、以及我在 OpenAI 共事过的团队，真正特别擅长的一件事，是\"从 1 到 100\"：拿一些已经有初步证据的新想法 -- -- 它们很不同，也不成熟 -- -- 然后想办法把它们在大规模上做得可靠、稳定、可落地。\n\n要训练前沿模型，把一种技术真正嵌进系统里，会涉及大量非常具体、非常琐碎、但又极其关键的工程和研究工作。如果执行不好，可能要花上好几年；但如果你有一套好的方法和节奏，可能几个月就能完成。这也是我未来很想继续多做的一类事情。\n\nAI 研究是\"明星驱动\"的吗？\n\n主持人：我们之前聊到 OpenAI 的人员流动时，你说公司是能扛住这些变化的。但从外部看，这个领域又很像是\"明星驱动\"的：比如 Alec Radford 那样的突破级贡献 -- -- 你知道我指的是什么。\n\n从行业行为上看，很多实验室似乎也在按\"明星逻辑\"行事。当然，这背后有大量集体协作，但确实也有一些时刻，看起来重大突破被\"绑定\"在少数几个人身上。但你刚才的反应，似乎并不完全认同这是一个\"明星驱动\"的行业。\n\nJerry： 我觉得这是个很复杂的话题，但有两个看法可以同时成立。\n\n一方面，确实存在这样的情况：在某些阶段，尤其是在 OpenAI，一小撮人能产生远超常人的影响力，推动真正突破性的成果，然后这些成果扩散到整个行业。我亲眼看到这种事情反复发生。\n\n但另一方面，当我看到人们在不同公司之间频繁流动时，我很少看到这种流动本身，对公司产生\"决定性影响\"。\n\n我更相信的是：公司的结构、文化和运作方式，才是真正的研究引擎，而不完全取决于某一个研究者是否在这里。\n\n而且我也观察到一个现象：那些频繁跳槽的研究者，反而往往没那么高产 -- -- 即便他们过去做过很好的工作。他们需要重新磨合，会被各种事情分散注意力，短期内也未必有新的突破性想法。\n\n经验当然重要，但更重要的是：营造一种环境 -- -- 强调个人责任、鼓励探索、并且真正为\"做出伟大事情\"提供条件。\n\n在一个好的结构、好的文化、好的协作方式下，你完全可以建立很多团队，持续做出伟大的成果。\n\n这件事并不依赖某一个\"唯一的人\"。归根结底，我认为：研究结构、研究文化和协作方式，远比\"某个特定的人是否在团队里\"更重要。\n\n主持人： 很有道理，很有道理。\n\n主持人： 最后一个问题：你冥想吗？\n\nJerry： 最近在试，但我觉得我冥想得不太行。\n\n主持人： 那祝你下一段旅程，能找到属于自己的\"黑暗静修\"。Jerry，谢谢你。\n\nJerry： 谢谢，很高兴和你们聊天。"
  },
  {
    "source": "新浪财经",
    "company": "OpenAI",
    "title": "硅谷\"钱太多\"毁了AI ？！前OpenAI o1负责人炮轰：别吹谷歌，Q-Star 被炒成肥皂剧，7年高压被\"逼疯\"！",
    "date": "2026-01-25T05:55:57Z",
    "url": "https://finance.sina.com.cn/roll/2026-01-24/doc-inhinpya0398592.shtml",
    "content": "这不是离职八卦，而是在一个把技术做成剧情、把研究变成围观的行业里，扛了七年高压后的选择。\n\n2026 年的第一个月，Jerry Tworek 离开 OpenAI 的消息传出来时，几位 OpenAI 的员工在 X 上几乎失控地发声：\"我真的崩溃了\"\"这太难受了\"。大家的反应像是：这事来得太突然，也太重。\n\nJerry 是现代 AI 浪潮背后最有影响力、却也最少公开露面的关键人物之一。 2019 年加入 OpenAI 时，当时该公司还只有约 30 名员工。他参与了许多最重要的项目，包括后来被称为 Q-Star 和 Strawberry 的推理方法，最终发展成为 o1 推理模型。\n\n这次离职后，他在接受 Core Memory 的播客采访时解释了原因：他想从事有风险的基础研究，这种研究在像 OpenAI 这样的公司已经不可能进行了，因为像用户增长这样的指标才是优先考虑的。他对 ChatGPT 广告的看法体现了研究与商业化之间的脱节：\"这是一种商业策略，而我负责训练模型。\" 这番言论印证了有关 OpenAI 人工智能研究与产品开发之间日益加剧的分歧的传言。\n\nTworek 指出，创新不足的原因有很多。最佳模型的竞争异常激烈，公司需要不断展现实力才能留住用户并证明 GPU 成本的合理性。僵化的组织结构更是雪上加霜，组织架构图决定了哪些研究是可能的：团队各自为政，职责分明，跨团队研究难以开展，Tworek 解释道。\n\n这场采访，也是一次\"离职解读\"，Jerry 还批评了整个人工智能行业，指出所有主要的人工智能公司都在开发几乎相同的技术，产品也几乎没有区别，这迫使研究人员追求短期利益，而不是实验性突破。更重要的是，他开始认真思考：如果研究真的需要冒险、需要不同路径，那他是否还应该继续待在这场高度同质化的竞赛中。\n\n在 Tworek 看来，谷歌之所以能够在 AI 竞赛中成功追赶 OpenAI，本质上是 OpenAI 自身的失误。他表示，这家 AI 实验室犯了一些错误，行动过于缓慢，没能充分利用自己原本拥有的巨大领先优势；而与此同时，谷歌则做出了许多正确的决策。\n\n当被问及 OpenAI 的具体问题时，Tworek 并未展开细说，只是暗示：员工流失有时是更深层问题的表象。他强调说，人走人来本来很正常，但如果一波人是因为\"方向不对、决策错了\"才走，那就说明公司里确实有点事 -- -- 也难怪有些关键推进会慢得不该那么慢。\n\n与这种\"慢得不该那么慢\"的状态形成对照的，是 Tworek 对 Anthropic 的评价。在播客中，他高度评价了这家 OpenAI 最强的初创公司对手，认为它在过去一年里展现出了一种罕见的\"清晰感\"：算力更少、团队更小，却异常专注，执行力极强。他特别提到 Anthropic 在代码模型与代码 Agent 方向上的进展 -- -- 那不是靠简单堆规模取得的成果，而是一种\"非常清楚自己在做什么\"的工程与研究结合状态。\n\n随着谈话继续，话题很快从技术转向了另一件更微妙的事。\n\nJerry 说，这几年最让他感到\"不对劲\"的，并不只是研究路线，而是整个大模型行业正在发生的变化。他形容现在的状态有点像这样：你做出一个新东西，大家还没真正弄清楚它是什么，它已经被卷进了一整套剧情里。谁离职、谁跳槽、谁被挖、谁\"内部有分歧\"，每天都像连续剧更新；湾区像一个巨大的转会市场，研究者在几家前沿实验室之间流动，围观者负责情绪，媒体负责剪辑 -- -- 研究现场，被包裹进了一层娱乐业式的叙事。\n\n\"技术、概念、人类情绪、现实生活，是分不开的。\"Jerry 说。\n\n当一个行业被持续围观，每一次进展都会被强行赋予意义，每一次内部变化都会被解读成信号，整个系统就会被不断加压。你不是在安静地做研究，而是在聚光灯下跑一场没有终点的马拉松。\n\n他用一个很个人的比喻形容这七年：\"像做俯卧撑。\"每一次高压过去，你会更能扛一点。 你学会屏蔽噪音，学会在混乱中保持稳定。但代价是，你也会慢慢习惯这种状态 -- -- 把异常当成常态，把围观当成空气，把压力当成日常。\n\n我们翻译并整理了这期播客的完整对话，以飨读者。\n\n主持人：今天我们请来重量级嘉宾 -- -- OpenAI 的 Jerry Tworek。他在 AI 圈算是\"活传奇\"那种人，而且刚刚离开 OpenAI，所以这期信息非常新、也非常重磅。我刷到不少 OpenAI 的同事在 X 上直接说\"我崩溃了\"\"太难受了\"。这就能看出来他在内部的分量。\n\n他主导或参与了 OpenAI 很多最重要的项目。这一波\"推理模型\"的时代，在很大程度上也和 Jerry 有关。今天他会聊他的经历、他做过的事情，然后我们也看看他会不会讲得更\"辣\"一点 -- -- 希望如此。\n\nJerry，你好。你身上有一种......\"刚失业的光芒\"。\n\nJerry： 我已经失业八天了，确实是一种变化。我已经很久没有失业过了，但这件事也有很多好处。比如我现在晒太阳的时间多了很多。\n\n主持人：那这期节目就算你的\"离职访谈\"了。我们刚才已经简单介绍了你的背景，我再稍微补充一点。你大概是 2019 年加入 OpenAI 的。你来自波兰，在来 AI 领域之前，和很多 AI 从业者一样，曾经在高频交易相关的领域工作过。在 OpenAI，你参与或领导了很多大家非常熟悉的重要项目。最近，很多人听说过 Strawberry、o1，以及这波\"推理模型\"的兴起，而这是你追了相当长一段时间的方向。然后，如大家所知，你最近刚离开 OpenAI。这件事在 X（推特）上引起了不少讨论。\n\n大家好，我做了一个艰难的决定：离开 OpenAI。\n\n我在这里将近七年，经历了很多美好与疯狂的时刻 -- -- 但美好远远多于疯狂。\n\n我非常享受和这支团队共事的时光。我有机会在\"机器人上的强化学习规模化\"还没流行之前就参与其中；训练了世界上最早的一批代码模型，推动了 LLM 编程革命；在\"Chinchilla（缩放规律）\"还没被叫作 Chinchilla 之前就发现了它；参与了 GPT-4 和 ChatGPT 的工作；最近则是组建了一支团队，建立了一种训练与推理算力规模化的新范式 -- -- 我们通常把它称为\"推理模型\"。\n\n我在这里结识了许多朋友，有些夜晚也在办公室度过；我参与并见证了相当多的技术突破；也和许多我视为至亲的人一起欢笑、一起担忧。我有幸招募并壮大了 -- -- 在我看来 -- -- 世界上最强的机器学习团队。\n\n这段旅程非常精彩。虽然我将离开，去探索一些在 OpenAI 很难开展的研究方向，但这依然是一家特别的公司、一个特别的地方，它已经在全人类的历史中占据了永恒的一席之地。\n\nJerry： 某种意义上，这事挺棘手的：我如果不自己说，媒体迟早也会替我说 -- -- 要么写成\"独家\"，要么当成\"泄露\"。所以我宁愿自己把话讲清楚，省得消息一传十、十传百，越传越走样。\n\n主持人： 对，我们最怕\"越传越离谱\"。你其实可以先跟我们说。\n\nJerry：（笑）我可以随时给你们打电话，告诉你们我生活里发生的任何事 -- -- 比如我中午吃了什么。\n\n主持人：但说真的，你那条离职帖写得很好，而且挺真情实感的。你在那里待了七年，经历了巨大的变化。从你的视角看，这七年是什么感觉？\n\nJerry： 老实说，我在 OpenAI 的每一年，都像是在一家完全不同的公司里。无论是公司本身的高速增长，还是整个 AI 世界的变化速度，都非常罕见。我不觉得历史上有很多类似的例子。我很高兴自己亲身经历了这一切。几乎每一个阶段，情况都完全不同。\n\nJerry： 已经没法数清楚了。现在是一家规模非常大的公司，有很多办公室，全球各地都有团队。现在几乎很难找到没听说过 OpenAI 的人。我加入的时候，还是几个小团队各自在做自己的小研究项目。那时唯一始终不变的，是野心 -- -- 从一开始就瞄准 AGI，想要改变世界、产生正向影响。我觉得公司在这方面做得非常成功。ChatGPT 把一种\"可用的智能\"分发给了非常多的人，这本身就是一件非常了不起的事情。\n\n主持人：你发了那条离职推文之后，是不是几乎所有基础模型实验室都立刻联系你了？\n\nJerry： 确实有很多。我现在正在慢慢梳理下一步要做什么。在这个行业待了这么多年，我本来就认识很多人，也有很多联系。从积极的角度看，我并不急着立刻做决定。过去很多年我工作得非常拼，几乎没有时间去见人、聊天。现在终于有机会停下来，认真想一想接下来的七年要怎么度过。\n\n主持人：你在推文里提到，你想做一些在 OpenAI 觉得无法进行的研究。能具体解释一下吗？\n\nJerry： 是这样：在一家必须参与当下这种极其残酷、极其高压的竞赛、必须争夺\"世界上最强 AI 模型\"的公司里，有些事情就是很难做。这背后有几个方面的原因。\n\n其中一个因素是风险偏好。公司愿意承担多大风险，会受到很多现实约束：比如不能落后于用户增长指标，比如 GPU 成本极其高昂。因此，向外界展示实力、持续拥有最强模型，对所有主要 AI 公司来说都非常重要。但这确实会影响你愿意承担风险的\"胃口\"。\n\n另一个很难的取舍是组织架构。公司有 org chart，而 org chart 往往决定了你能做什么研究。每个团队都需要一个身份、一个研究范围、一组他们要解决的问题。跨组织的研究就会变得非常困难。\n\n我也不确定这是不是一个已经被完全解决的问题：当研究规模变得很大时，究竟该如何把研究组织好？研究本身喜欢动态，甚至可以说喜欢混沌；但一大群人需要秩序、结构和组织架构。\n\n所以，\"把组织架构交付出去（shipping your org chart）\"成了一种非常普遍的现象，研究也不例外。你最终会做那些组织结构最容易支持的项目。而与此同时，我确实想做一些研究，但公司的组织结构并不容易支持我去做这些事情。\n\n主持人：这是否意味着我们将看到一项新突破？\n\nJerry： 我想，其实 AI 世界里的每一位研究者，都想参与下一次真正的突破 -- -- 我当然也包括在内。\n\n主持人：我之前在播客里跟 Mark（Mark Chen，OpenAI 的 首席研究官） 聊过这个话题：几乎所有人都会带着自己的想法去找他、找 Yakob（Jakub Pachocki，OpenAI 的核心研究负责人之一）。OpenAI 一直以来确实有一段\"押注冒险想法、去做其他实验室没做的事\"的历史，而且这种策略也确实为他们带来了回报。但我也很清楚 -- -- 你们那里一定聚集了大量非常聪明的人，所有人都会不断提出各种想法。\n\n而在某个时刻，公司终究是一家资源有限的组织 -- -- 哪怕这些资源已经非常多了 -- -- 也必须做出取舍。所以，这必然是一个非常艰难的决策过程。也正因为如此，我在思考的那些方向，大概确实属于那种\"相当新、相当不寻常\"的路径：公司需要判断，我们到底要不要往这个方向走？现在有没有能力、有没有余力去承担这种不确定性？我们是否能在当下负担得起？\n\nJerry： 关于\"研究时代\"的判断，我不确定事情是否真的像他说的那样是非黑即白的。但我非常确定的一点是：在 AI 和机器学习的世界里，还有大量东西尚未被真正探索。\n\n大约六年前，我们基本确定了以 Transformer 为核心的架构路线。此后相当长一段时间里，整个行业都在持续扩大 Transformer 的规模，而且进展确实不错。路径也非常清晰：每个季度用稍多一点算力、稍多一点数据，训练出一个更强的模型。到目前为止，这条路看起来并没有明显的\"天花板\"，进步仍在持续。\n\n但问题是：这就是终点了吗？这是最后一条路了吗？我几乎可以确定不是。\n\n我们还有很多改进模型的方式，目前根本还没真正开始做。正如你刚才提到的，我自己主要做的是\"推理\"，以及扩大强化学习的规模。在那之前，整个领域几乎所有的\"大赌注\"都押在 Transformer 的预训练规模上。\n\n扩大预训练规模，确实是一种有效的扩展方式，而且效果很好。每一次更大规模的预训练，模型能力都会整体提升，各方面都会变强。所以你当然可以说：那我们就继续扩展预训练规模，模型自然会越来越好。\n\n但后来，有那么一小撮\"做梦的人\"、研究者开始相信：事情不止这一种做法。我们不只是扩展预训练，还可以在语言模型之上，大规模扩展强化学习，而且投入的计算量可以和预训练处在同一个量级。这样做，能够教会模型一些 仅靠预训练永远学不会的东西。\n\n正因为如此，我们今天才有了这些令人惊叹的 Agent：它们可以自动化工作、解决复杂问题。而如果只靠预训练模型去完成这些任务，可能需要极其夸张的算力和数据量。\n\n也就是说，当你发明了一种新的\"扩展方式\"，你就会得到一整套全新的能力；而如果你只是沿着原有的预训练扩展路线走，那可能要花非常、非常久，才能逼近这些能力。这一次，其实是一次相当大的跃迁。\n\n在我看来，自从 GPT-4 引入以来，\"推理模型\"几乎是这几年里最重要的一次能力跃升。而我相信，类似这样的跃迁还会出现不止一次。\n\n所以我一直觉得，研究者不应该只盯着\"渐进式改进\"，而是要去思考：有没有办法把整个棋盘掀翻？\n\n主持人：去年在 NeurIPS 上，Ilya 曾说过一句话，大意是：\"我们正在耗尽数据，这条路迟早会走到尽头。\"关于\"预训练是否正在进入一个越来越艰难的阶段\"，我一直在想：那下一个真正的突破会是什么？这正是你现在想问的问题，对吧？\n\nJerry： 是的。但我并不认为这等于在说\"预训练已经结束了\"。预训练仍然在持续改进，而且还有很多方式可以继续优化它。但它已经不再是唯一的改进路径，而且其他路径，可能在很多维度上能更快地带来提升。\n\n扩大预训练规模，在很多能力上提升得其实非常慢 -- -- 它确实会让模型更好，但提升是渐进的。而与此同时，可能还存在其他方式，能带来更大的跃迁。\n\n主持人：硅谷有一个很有意思的现象：很多时候，科技公司会提出一些非常原创、甚至看起来\"怪异\"的想法，外界一开始完全不理解。但正是这样，才催生了全新的商业模式、新的科学、新的研究方向。而科学研究本身，也是如此：你需要去追逐别人还没走的方向。\n\n可一旦某个方向\"爆了\"，事情就会反过来 -- -- 会形成一种巨大的共识。突然之间，所有人都开始说：\"我们就该这么做。\"然后大家不再讨论\"该不该走这条路\"，而是开始比拼\"谁在这条路上跑得更快\"。\n\n这其实就是你刚才描述的那种状态。那么问题来了：当我们已经进入这种\"模型竞赛\"，而且已经持续了两三年之后，会不会出问题？是不是所有主要实验室都变得越来越保守？这会不会成为一个普遍性的结构问题？\n\nJerry：让我感到非常\"难过\"的事，就是现在几乎所有 AI 实验室都在试图做和 OpenAI 一模一样的事情。\n\nOpenAI 显然是一家非常成功的公司，它在很多关键问题上做对了选择，把整个世界带进了\"规模化 Transformer\"的范式之中，也证明了：通过扩展机器学习模型的规模，确实可以为世界带来大量非常有价值、非常有用的能力。\n\n但问题是：这个世界究竟需要多少家\"做完全同一件事\"的公司？我不知道。竞争当然是好事，所以肯定不止一家更好。但 现在我们大概已经有五家相当严肃、体量巨大的 AI 公司，基本上在用完全同一套\"配方\"，试图在同一套技术之上，做出一点点差异化的产品。\n\n也许这确实是对的选择，但我还是希望能看到更多多样性 -- -- 更多模型层面的差异。\n\n如果你去看现在世界上最好的那些模型，实际上很少有人真的能注意到它们之间的区别。我觉得应该做更多\"盲测\"：让人们分别和不同模型对话，看他们是否真的能分辨出哪个是哪个。我敢说，99.9% 的用户根本察觉不出来这些模型有什么不同；在他们的感受里，这些模型几乎一模一样。\n\n即便背后是不同团队，在做一些细微不同的事情，但所有实验室都觉得\"我们在这个点上做得稍微好一点\"\"对方在另一个技巧上可能更强\"，最终的结果却是：大家全都挤在一个非常接近的位置上。\n\n那真正的探索在哪里？真正的创新空间在哪里？真正能让你和别人拉开距离的差异化又在哪里？\n\n主持人：我主要用这些模型做文字工作，偶尔会在 Gemini、ChatGPT、Claude 之间切换 -- -- 差别确实有，但很细，更多是语气和\"性格\"。比如我最近更常用 Claude，因为它更直接、不啰嗦；而 ChatGPT 的语气我一直很难调到那种感觉。不过总体我也同意，大多数人其实分不清这些模型的区别。\n\n话说回来，我想问一个可能有点尖锐的问题：你在 OpenAI 待了这么久，在公司内部算是传奇人物之一，而且你的履历也证明，你参与的项目往往能做成。那从外界看，如果连你这样的人都觉得 -- -- 自己真正想做的研究在公司里推进起来足够困难，以至于最后选择离开 -- -- 这是不是一个不太好的信号？尤其对一家最初以研究实验室起家的公司来说，这意味着什么？\n\nJerry： 我觉得有时候，人和组织都会成长到一个阶段：必须意识到，彼此的道路需要分开。\n\n对一家单一公司来说，非常重要的一点是：公司内部的人，必须在某种程度上对目标、对前进路径保持一致。而在某个时刻，我对\"未来研究路径\"的判断，和 OpenAI 选择的方向，至少在一些足够重要的点上，出现了分歧 -- -- 包括接下来一年研究该是什么样子。\n\n在这种情况下，我认为分开，反而比强行在分歧中继续合作要好得多。否则，那些分歧可能会不断积累、发酵。\n\n所以我反而认为：不同公司去做同样的事情，在某种意义上是合理的。因为专注对于一家公司来说非常重要，而 OpenAI 很可能正在做所有\"正确的事\"。\n\n也许只是我自己有一些不太现实的梦想；也许我对\"还能做些什么其他事情\"过于乐观 -- -- 这完全有可能。\n\n很多公司必须专注于自己的核心路径，才能活下来，才能进入下一个阶段。所以在一个理想的世界里，应该有很多不同的公司，在做很多不同的事情。而研究者 -- -- 尤其是那些很难去做自己并不真正相信之事的研究者 -- -- 应该能找到一个地方，在那里，他们能投入到自己最相信的研究方向中。最终，历史会证明哪一条路是对的。\n\n正因为如此，我才会对\"大家都在做同一件事\"感到有点难过。因为在当下，如果你想做一些偏离主流机器学习路线的事情，真的非常难找到一个合适的地方。这大概是我目前最感到遗憾的一点。\n\n主持人：那你现在还在思考下一步要做什么，对吧？如果所有实验室都在做同一件事，那你应该不会想简单跳去另一家大实验室？\n\nJerry： 我当然还在认真思考下一阶段。但如果有更多\"稍微偏离主流、但依然具备规模\"的选择，那我会更开心，也会更容易做决定。\n\n主持人：那你觉得，要让整个行业偏离当前主流路径，需要什么条件？我可以想象，这些公司投入了巨额资金、消耗了大量资源，又处在聚光灯下，自然会害怕承担风险。但也许这些风险是必要的。那到底要改变什么？或者这种改变真的会发生吗？\n\nJerry： 这正是一个非常有意思的问题。\n\n我其实非常喜欢冒风险，也经常被人这样评价。我认为，冒风险本身是一件好事。但当你面对的是\"巨额资金在押\"的局面时，真正有能力、也愿意承担风险的人，其实非常非常少。\n\n每个人的风险偏好都是极其个人化、极其独特的。我和很多人共事过，我真心觉得：人们应该愿意多承担一些风险，多去尝试一些事情。\n\n但另一方面，现在 AI 世界里的研究者薪酬已经高得离谱了。这在某种程度上，也会让人变得非常害怕失去工作、害怕一次不好的绩效周期。结果就是：人们更倾向于追求短期、确定性的收益路径。而这些人本身往往都是非常聪明、动机也非常正直的研究者。只是整个系统在某些地方，确实更容易鼓励\"短视\"。\n\n我认为，研究者应该被更明确地鼓励去冒风险、去下大胆的赌注，因为真正的进步，正是这样发生的。\n\n主持人：那我们已经看到了一些\"独行侠\"式的人物。比如 John Carmack。Carmack 跑去了达拉斯，像是进了自己的洞穴里。一开始似乎是单干，现在好像有几个人在跟他一起做。他几年前说的，其实和你刚才讲的很像：也许我不知道能不能走出一条完全不同的路，但至少应该有人在一条完全不同的路径上持续折腾。\n\n我和 Ilya 聊过，但并不知道他现在具体在做什么。我不知道那是他之前工作的延续，还是某种非常激进的新路线。不过我想，如果不是完全不同的方向，他大概也不会去募那么多钱、重新开始。\n\n然后还有 Yann LeCun，他显然有一套不同的哲学。有时候我会觉得这个领域挺奇怪的：AI 从某种意义上说很\"老\"，已经发展了几十年；但当前这一波 AI 又非常新。和研究者聊天时，他们会说：现在把主要论文读完，其实很快就能跟上前沿。所以我一直在想，会不会有某个人，突然从完全意想不到的方向出现，带来一个极端激进的新想法，把整个领域往前推一大步？但与此同时，又好像越来越难 -- -- 因为现在你几乎需要一个\"国家级规模\"的数据中心，才能真正参与到这个层级的竞争中。\n\nJerry： 这正是事情变得非常困难的地方，同时也是一个非常值得解决的问题。\n\n世界上其实有大量学术研究在发生，也有很多学生在做各种各样的事情，但其中大多数都严重缺乏资源。这使得很多研究最终走不远，因为你真正想做的研究，往往必须在\"大规模\"下才能完成。\n\n但这也是让我感到非常乐观的一点：现在确实有相当多的资金，正在流向那些\"想做新东西\"的人。像 John Carmack、像 Ilya -- -- 他们做的事情，正是当下这个时代应该存在、也应该被资助的。当然，不是所有尝试都会成功，但其中一定会有一些成功，而创新正是这样发生的。对于任何一个强化学习研究者来说，\"探索（exploration）与利用（exploitation）\"之间的权衡，都是一个非常基础、非常重要的概念。\n\n即便是在优化 agent 时，你也必须不断权衡：是走已经被证明有效的路径，还是去尝试全新的方法，用完全不同的方式解决老问题？这是一个非常困难的取舍，但它本身就是一个被研究、也值得研究的问题。而正如我们在设计 agent 时会思考这个问题一样，我们也应该反过来问自己：我们自己在做研究时，是如何在探索与利用之间取舍的？\n\n主持人：在这个非常非常顶尖的小圈子里，大家都知道 Carmack 在做什么吗？你们彼此是互相了解的吗？\n\nJerry： 老实说，我并不完全清楚。但如果我没记错的话，我隐约知道一些。他可能是在押注一种非常端到端的强化学习方式 -- -- 通过鼠标和键盘，在电脑游戏中训练 agent。\n\n如果真是这样，那其实非常有意思。因为我长期以来一直在想：电子游戏，可能是训练智能体的最有趣环境之一。游戏本身就是为了\"对人类大脑有吸引力\"而设计的。它们包含故事、权力幻想，但更重要的是：大量的问题求解。游戏必须有趣、必须有挑战、不能重复。\n\n在某种意义上，电子游戏非常贴合人类智能，它们天然地在教你资源分配、解谜、如何在不同规则下取胜 -- -- 这正是我们希望 agent 能学会的事情。当然，我们现在还没有真正能在高频、多模态环境中稳定运行的超强模型，可能存在一些架构层面的限制。但我认为，用电子游戏来训练 AI，是一件非常值得做的事情。\n\n主持人：Richard Richard Sutton 过去在扑克、游戏等领域做过大量工作；我也曾在他的实验室待过。早期的那些游戏环境，比后来 OpenAI 的 Dota 要原始得多。但你可以看到，这个想法一直贯穿其中。\n\nDemis Hassabis 也长期在追逐类似的方向。所以你提到这一点很有意思 -- -- 这其实是一个\"老想法\"。一段时间里，各大实验室都在比谁能打通更复杂的游戏、谁能更好地\"秀\"成果；后来在 ChatGPT 时代，这条路线似乎被边缘化了。但也许，它仍然有潜力。\n\nJerry： 在科学史上，有一个非常常见的现象：好的想法，往往会反复出现。真正困难的，并不是提前预测\"哪个想法是重要的\"，而是判断\"什么时候是对的时机\"。即便在 OpenAI 早期，我们也常说：不能断言某种方法\"行不通\"，也许只是\"现在还行不通\"。\n\n我七年前刚加入 OpenAI 时，强化学习在游戏上是一个非常火的方向。我们解决了很多游戏问题：StarCraft、Dota，而 AlphaGo 更是一个标志性时刻。但这些模型有一个非常明显的缺陷：它们几乎没有世界知识。它们并不理解我们的世界，只是从零开始，专门为某一个游戏训练。\n\n这显然不是正确的路径。我们必须先教模型理解世界，理解更高层次的概念，而不仅仅是对像素做出反应。从零开始的强化学习，更像是\"猴脑\"或\"蜥蜴脑\"。而我们想要的，是具备更高层次抽象能力的模型。\n\n在多年大规模预训练之后，我们现在已经能够学到一套非常强的\"世界表征\"。而接下来，我们应该利用它。这正是\"推理模型\"的核心魔法：在一个对世界有深刻理解的基础之上，叠加一层强化学习。未来就应该沿着这个方向前进。\n\n主持人：那这不就和\"世界模型\"的方向一致了吗？Google 在做这个，Yann LeCun 似乎也在推动类似的想法。这在直觉上是合理的 -- -- 这也是人类学习世界的方式。我们不是在一个黑箱里长大的，而是通过不断试探、感知世界来学习的。所以你对这个方向是非常看好的。\n\nJerry：这个方向毫无疑问是正确的。真正有挑战性的，是：如何把从世界建模中学到的表征，与强化学习真正结合起来。\n\n强化学习教会模型\"技能\" -- -- 让它学会如何在世界中实现自己的目标。但在此之前，模型必须先理解世界，否则它连\"如何设定目标\"\"如何达成目标\"都无从谈起。\n\n正因为如此，这两件事情必须结合起来。\n\n如果有人能在一个高质量世界模型之上，真正把强化学习跑通，那将会是一个非常令人振奋的时刻。\n\n主持人：就你现在这些正在吸引你的研究方向来说 -- -- 你能不能稍微给我们一点提示？还是说，这样就直接暴露你下一家创业公司的方向了？\n\nJerry： 我现在最兴奋的研究方向大概有两个。主要原因也很简单：我不觉得重复去做各大实验室正在做的那套事情有什么意义。现有体系里当然还有很多可以微调、可以改进的地方，但我认为有两个方向长期被低估了投入 -- -- 或者至少没有得到足够的资源与重视。\n\n第一，是某种意义上的\"架构创新\"。我觉得我们对 Transformer 架构有点过于\"路径依赖\"了。Transformer 确实很伟大，也被非常深入地研究过。人们一直试图在本地做一些小改动，让 Transformer 更强，但这件事并不容易。虽然也有一些相当成功的改进 -- -- 比如稀疏化非常成功；还有各种让注意力计算更便宜的方法，也取得了不错的效果。\n\n但 Transformer 会是机器学习的最终架构吗？显然不会。尽管 Transformer 的发明者做出了惊人的贡献，并且几乎定义了接下来十年的机器学习格局，但我相信一定还有更多可能。\n\n一定存在一些训练大模型的方法 -- -- 它们也许有点像 Transformer，也许完全不像。我觉得这是一个值得去解决的问题。甚至如果没有别人去做，我也愿意卷起袖子自己上，试着把它做出来。\n\n第二个方向相对更\"热门\"，但我觉得几乎没有人把它做得真正好，那就是持续学习（continual learning）：如何把测试时（test time）与训练时（train time）真正打通、真正融合起来。\n\n人类显然就是这样运作的：我们没有一个\"专门学习模式\"和一个\"专门回答问题模式\"。学习与反应是连续发生的、时时刻刻都在进行。我觉得我们的模型也应该更接近这种状态。\n\n这可能是我们在把模型真正称为 AGI 之前，最后几个关键能力要素之一。如果模型不能从它看到的数据中持续学习，它就仍然显得有点受限 -- -- 甚至有点\"笨\"。\n\n新技术炒作带来的恐惧感\n\n主持人：说到 AGI，我们上次录播客时我提过：我已经不像一两年前那样经常听到\"时间线\"讨论了。那时候大家非常热衷谈什么时候会实现 AGI，甚至连\"AGI\"这个词最近都没那么火了。你自称对 AI 是\"谨慎的乐观主义者\"。那你觉得我们现在处在 AGI 时间线的哪个位置？\n\nJerry： 我个人的看法是：我对时间线做了一点更新。\n\n我一直认为，把强化学习规模化（scaling reinforcement learning）是通向 AGI 的必要部分。一年、或一年半之前，我非常坚定地认为：只要把 RL 规模化到我们的模型之上，那就是 AGI 了。但我确实不得不稍微修正这个判断。因为有些东西，只有当你真的到了\"下一阶段\"之后才看得见。\n\n我们也必须承认：今天的模型在很多方面已经非常非常强了。就拿编码来说 -- -- \"vibe coding\"是我最喜欢的爱好之一，你现在可以非常快地写出很多东西。对一些十年前的人来说，如果你把今天这些能力展示给他们，他们可能已经会把它叫做 AGI 了。\n\n所以我不觉得谈 AGI 还是一种多么离谱、多么疯狂的事。但至少按我的定义，现在的模型仍然不是 AGI -- -- 原因之一是：持续学习完全还没有以真正的方式被整合进模型体系里。\n\n除此之外，还有很多问题。比如多模态感知：如果模型文本理解很强、编程也很强，但它看不见真实世界、不能看视频并且很好地理解视频，那我们能称它为 AGI 吗？\n\n所以我认为，要真正达到那个\"文明级里程碑\" -- -- 构建 AGI -- -- 还有很多必要步骤要完成。\n\n有一段时间我曾想：如果我们真的拼命推进，并且把所有关键问题都做得足够好，也许 2026 年至少能实现非常强的持续学习，以及真正通用的强化学习。\n\n我觉得我的时间线仍在漂移。但与此同时，AI 领域移动得太快了：投资在年复一年累积增长，越来越多人进入 AI 领域，人才池变大，我们探索的想法数量也变多。\n\n所以我不觉得\"这个想法完全荒唐\"。也许会早一点，也许会晚一点：可能是 2026，也可能 2027、2028、2029。我不觉得会比这更久太多。但确实还有很多工作要做。不过人们正在非常努力地做 AGI。\n\n主持人：你刚才提到的内容 -- -- 让我想起你之前做的那些事。除非我记错：在 Strawberry 还没成为一个\"明确项目\"之前，外界不是有过所谓的 Q-Star 传闻吗？而且在那次\"内部风波\"期间，这件事被反复提起：什么\"他们知道 AGI 已经到了\"，把所有人都吓到了。但听你现在这么说又挺有意思的。因为确实，这些东西做出来以后非常惊人，我们会一度情绪很亢奋；然后时间过去，大家就习惯了。现在回头看，Strawberry 确实很不可思议，也确实改变了整个领域。\n\n但我第一次用它的时候，并没有到那种\"把我吓死\"的程度。你懂我意思吧？\n\nJerry： 我懂你意思。\n\n这其实涉及人类心理，以及我们如何与技术互动的方式。对我来说，把强化学习规模化带来的效果仍然非常显著，而且我觉得随着时间推移，我们会看到更多影响。\n\n尤其是应用在编程上，这会以很多很多方式改变我们的生活。你今天做一个大规模编程项目，和一年前相比，完全是另一种游戏。我们会在很多领域看到这种变化带来的连锁影响。\n\n但我也想说：两年前，当我和团队、以及 OpenAI 的很多人第一次看到 Q-Star 的一些早期迹象真的开始工作时 -- -- 你坐在一个房间里，看到一种\"有意义的新技术\"正在出现。\n\n如果你在那一刻不感到一点害怕、不感到一点担忧、不暂停一下想一想\"这对世界意味着什么后果\"，那我会觉得你没有在负责任地对待自己的工作。我认为每一个 AI 研究者都应该想这些问题：如果我正在做的东西是全新的、它展现出了以前从未出现过的新能力，那世界会发生什么？\n\n很多研究者确实会这么想。当然，有时候也会把担忧推得太远。一方面，到目前为止，AI 还没有给世界带来什么\"实质性的重大伤害\"；但另一方面，一些事情（比如\"某些很花哨的东西\"）是不是算有问题 -- -- 也许还可以争论。（笑）\n\n但总体来说，我认为：当你向世界释放新技术时，感到担忧与谨慎，是一种非常好、也非常健康的反应。\n\n我们正在经历一个变化的时代：大量新事物正在扩散到世界里，它们会产生影响 -- -- 影响人们如何生活，如何看待自己、看待他人；影响人际关系、国际关系；影响 GDP、影响生产力。\n\n有时候，一个人写下的一行代码，就可能引发连锁反应。经历了这一切，肩膀上的单子就相当重。\n\n主持人：我一直在想，尤其\"政变\"那段时间：你做出来的东西被媒体炒得很热，还被卷进各种戏剧化叙事。我不知道\"滑稽\"这个词对不对，很多人其实还没弄清它到底是什么，就已经围观成现象了。你当时是什么感觉？\n\nJerry： 技术、概念、人类情绪、人类生活、人和人之间的协议与分歧 -- -- 在现实里很难被切开来看。\n\n我们确实活在一个世界里：AI 领域的重要参与者之间，有一个非常复杂的关系网络，很多层次叠在一起。要把它完全理清楚，可能得历史学家花很多年、甚至几十年，才能真正弄明白到底发生了什么、哪些因素起了关键作用。\n\n老实说，到现在为止，我对那段时间发生的一切也只剩下非常零散的记忆。我们也在不断\"补课\" -- -- 每当有新的证词出现、每当新的文件被披露，就会冒出一些新事实。未来某个时刻，肯定会有人把所有内容都挖出来、完整还原。\n\n但现实世界就是这么复杂。我也确实觉得，也许应该有一种更健康的方式来讨论技术：找到一个更合适的讨论场域，让分歧能够被更充分、更有建设性地展开。但我们生活在这样一个世界里：没有完美解，也不存在一种绝对正确的讨论机制。\n\n主持人：所以你觉得 X（推特）也不是理想媒介？\n\nJerry： 我个人其实很喜欢在 X 上发内容，分享想法，和社区交流。但它也不是一个完全严肃的地方 -- -- 很多时候都是半开玩笑、半认真。更核心的问题是：有人担心某件事太危险不该继续；有人觉得继续做是对的，因为它会增强能力；还有人认为方向本身就不对，我们应该做别的研究。\n\n在技术进步与研究的世界里，这些事情很多都是未知的。没人知道未来。我们只有想法、信念和梦想。\n\n我们必须和这种不确定性共处，也必须学会在很多问题上\"求同存异\" -- -- 很多时候只能接受：大家各自下注、各自承担后果。\n\n主持人：说到当时媒体对 Q-Star 的关注 -- -- 那阵子简直是炒作过度，几乎天天都在加码，每个月都愈演愈烈。我看着会觉得：这是不是太\"嗨\"了、太多 hype 了？而且我们俩也都在推特上，多少也参与了这股热度。你怎么看：这种 hype 该不该降一降？我个人确实觉得，强度可以往回拧一点。\n\nJerry： 我了解。反过来想，如果七年前有人告诉你：OpenAI 会成为万亿美元级别的公司；会建造规模堪比史上最大基础设施项目的数据中心；会拥有世界上最大的 Web 产品之一；全世界会无时无刻都在谈 AI -- -- 你一定会觉得那个人疯了，会说\"这就是炒作\"。\n\n可我真心觉得：这波 hype 在很多层面其实是有事实支撑的。人工智能在很多方面存在过度反应和反应不足的情况（有时候被高估，有时候也会低估），但 AI 的重要性毋庸置疑 -- -- 它值得被讨论。我不觉得现在还有谁会认为 AI 是个\"不重要、不值得讨论\"的话题。几年前确实还有人这么想，但现在已经很清楚：AI 很可能是当今世界最重要的议题之一，值得持续讨论与思考。至于进展会有多快、路径到底对不对、安全还是危险 -- -- 这些当然都可以争论。但 AI 会长期存在，而且只会越来越强。\n\n主持人：完全同意。但如果先把技术放一边 -- -- 我甚至报道过\"挖人狂潮\"。我越来越觉得，这个行业的叙事变得像肥皂剧、像真人秀，很多时候讨论的不是硬核科学，而是剧情、阵营和情绪。你会不会也觉得我们有点\"跑偏\"了？\n\nJerry： 但到底是谁在制造这场肥皂剧？这才是问题。\n\n主持人：嗯，说真的，这一轮比我经历过的任何技术周期都更\"肥皂剧\"。可能是赌注太高、钱太多，再加上挖人和各种戏剧化叙事，整个旧金山像活在一套自己的现实里。\n\n我有时都替你们累 -- -- 七八年一直在这种高压竞速里，你现在想停下来喘口气，我完全能理解。\n\nJerry：的确很消耗。\n\n但我可以跟你分享一句对我很有帮助的话：有一次，一个比我更有经验、更擅长应对压力的人跟我说 -- -- Jerry，这就像做俯卧撑。每经历一次艰难、紧张的时刻，你就更擅长应对压力一点。\n\n老实说，这七年让我练出了很强的心理和情绪韧性。我真的学会了在大量噪音、很多胡扯面前，把自己抽离出来，尽量保持稳定、保持定力。\n\n不管外部发生什么 -- -- 公司看起来要塌了也好，研究者流动也好，项目被重新分配也好 -- -- 总会有事情在推进，总会有新的变化。\n\n我听过有人把\"挖人\"这件事类比成体育队伍的转会。体育之所以还能运转，是因为有角色、有规则。我差点想说：可惜在加州的法律框架下，这类规则基本不可能出现。但我确实觉得，如果能有一些规则，可能会更健康。\n\n因为确实存在这样一种现象：有些人换工作的频率，比他们真正产出成果的频率还高。\n\n主持人： AI 薪资帽？（笑）\n\nJerry：（笑）确实有人这样。但也仍然有很多人在认真做事，推动前沿继续往前走。不过，AI 是一门大生意 -- -- 这点无论如何都没法否认。\n\n主持人： 我还跟同事说，我们真该做一张表，把那些在每一家前沿实验室都待过的人列出来，标注他们在每家待了多久。（笑）肯定至少有一小撮人，把整个湾区的\"前沿实验室巡回赛\"跑完了。说真的，这太疯狂了。\n\n主持人：2018 年前后，OpenAI 还只有三十来个人。有一件事当时让我印象特别深：最早那批成员里，波兰人的比例异常高，而且很多都是非常典型的\"数学脑\"。\n\n有些人彼此从小就认识，有些并不认识。我一直很好奇：这到底反映的是一种教育背景的集中效应 -- -- 比如偏重数学训练的体系，确实更容易培养出这类人？还是说，其实只是早期有几个人先来了，后来通过学术和个人网络，慢慢把更多同类的人吸引到了 OpenAI？\n\nJerry： 先澄清一点：我在加入 OpenAI 之前，完全不认识任何 OpenAI 的人。我是非常随机、机缘巧合地进来的。\n\n但你说得没错，在 OpenAI 非常早期，波兰人的占比确实偏高。不过我并不觉得这种情况\"经得起时间检验\"。现在公司里，波兰人的比例仍然略高于平均水平，但考虑到 OpenAI 的规模已经增长了大概一百倍，这种早期的\"高浓度\"并没有按比例延续。\n\n我觉得这里面确实有一些值得讨论的因素，但我并没有足够多对其他教育体系的亲身体验，所以不敢轻易下结论，说波兰的教育体系\"天然更强\"。我能确定的是：我们确实有很多非常聪明、数学直觉很强的人。\n\n但如果说有一件我特别认可、也特别喜欢的事情，那就是波兰人对\"努力工作\"这件事的重视从我个人经历来看，这种特质在很多地方正在变得越来越少见 -- -- 尤其是在一些生活条件已经非常优渥的社会里，人们对工作的强调确实在下降。\n\n主持人：你怎么看 Google 最近这一轮的\"回归\"？你是觉得意外、惊讶，还是说其实早就料到了？看起来他们这段时间做对了不少事情。你们之前是不是一直都觉得：Google 迟早会把局面理顺？\n\nJerry： 我个人其实不太愿意把这件事称为\"Google 的回归\"。它应该被视为 OpenAI 的失误。\n\nOpenAI 确实在很多关键点上做对了事情，但也不可否认，在某些阶段出现过判断或执行上的失误，导致整体推进速度比它本可以达到的状态要慢。\n\n在一种理想的执行情境里，如果你是一家已经取得领先优势的公司，而且拥有 OpenAI 那样的技术、人才和资源条件，那么你理论上是可以持续保持领先的。但如果在这个过程中，你做出了一些错误决策，而你的竞争对手做出了更多正确决策 -- -- 而 Google 在最近一段时间里，确实做对了不少事情 -- -- 那对方追上来，其实并不奇怪。\n\n你也必须承认：Google 在硬件、算力和人才储备上，本身就有非常巨大的优势。事实上，在 OpenAI 刚起步的那些年里，Google 在几乎所有机器学习方向上，都是明显的行业第一。\n\nOpenAI 能真正跑出来，靠的主要不是资源优势，而是 研究方向上的强烈信念：对某一条具体技术路线、某一个具体长期赌注的坚定投入。\n\n但让整个行业、让外部世界真正意识到\"这是一个正确的赌注\"，花的时间比很多人想象的要长得多。哪怕 GPT-2 训练完成了，GPT-3 训练完成了，后来 GPT-3.5 也出来了 -- -- 在那个阶段，其实并没有太多人真正重视这件事。\n\n你去 NeurIPS 这样的会议和研究者聊天，大家会觉得 OpenAI 很酷，但很多其他实验室的态度是：\"嗯，我们迟早也能复现。\"语言模型确实挺有意思，但在他们看来，也就止步于\"有意思\"。\n\n真正的转折点，是 OpenAI 开始通过 ChatGPT 赚到钱。那一刻，其他公司才突然意识到：\"好，这不只是研究展示，而是一个已经被验证的商业方向，我们必须认真投入了。\"\n\n这里其实存在一个很关键、但常常被忽略的时间窗口：从你开始构建一项技术，到它真正被商业化，中间往往隔着一段很长的时间。\n\n这段时间，足够让其他公司观察、犹豫、评估风险，然后再决定是否下场。而在这个阶段，Google 显然开始非常认真地对待大语言模型这条路线。再叠加 OpenAI 在执行层面的一些失误，最终导致今天的结果：在模型能力和训练成果上，双方已经变得非常接近。\n\n所以，从 Google 的角度来看，这确实是一件值得祝贺的事情。能够把团队重新拉回状态、把执行节奏提起来，背后一定做了大量艰难而高质量的工作。\n\n主持人：那你说的这些\"失误\"，具体指的是什么？我在努力回忆。我记得当年你们推出 Search 的时候，外界一度在说\"Google 完了\"，但我当时就觉得未必如此。所以你提到的失误，更多是指哪些方面？\n\nJerry： 我不太想展开讨论具体的内部决策细节，哪些判断是对的，哪些是错的。\n\n但我想强调的核心其实很简单：如果一家领先公司执行得足够好，那么在大多数情况下，它是可以把领先优势持续下去的。\n\n而在现实中，很明显有一些事情的推进速度，比它本可以达到的节奏要慢。\n\n主持人：你的意思是技术层面的失误吗？因为从外界看，也确实发生了不少公司层面的戏剧性的狗血剧情，这些在某些阶段显然拖慢了整体节奏。\n\n我跟 OpenAI 的一些人聊过，关于公司要如何继续向前，确实出现过一些阶段性的混乱，比如关键人物离开等等。所以我原本以为你指的是纯技术问题，但听起来你的意思更复杂一些。\n\nJerry： 这些事情有时候确实是相互关联的。\n\n从技术角度来说，我并不认为\"有人离开\"这件事本身就一定构成问题。在任何一家公司，人来人往其实都很正常，也应该是一种常态。\n\n但如果离开变成了某种更深层问题的 症状 -- -- 比如有人觉得：\"公司在一些关键事情上做错了决定，我不再相信这家公司了，所以选择离开\" -- -- 那这往往意味着，背后确实存在一些需要被正视的问题。\n\n所以回到我最初的判断：确实有一些事情，推进得比它本可以做到的速度要慢。 这并不否认 OpenAI 的成功，但也不能忽视这些失误带来的影响。\n\n主持人：如果像你说的那样，各大实验室基本都在走同一条路，那 Meta 显然也是其中之一。他们在 AI 上投入巨大，也在从各家实验室挖人。我并不完全清楚 Meta 内部的具体策略，但从外部看，他们似乎并没有选择一条完全不同的路线，而更像是在追赶同一条主流路线。\n\n这听起来像一个根本性的问题：如果你既起步更晚，又在做和别人几乎一样的事情，这真的可能有好结果吗？还是说，你觉得 Meta 实际上走的是一条不一样的路？\n\nJerry： 我并不完全了解他们的内部策略，所以只能谈一些外部观察。\n\n我的感觉是，他们已经意识到一件非常关键的事情：\"规模化\"在当前的 AI 世界里是不可回避的。如果你放眼现在的 AI 行业，基本可以抽象出两种不同的战略选择。\n\n第一种是：我要做一种和其他人都不一样的模型 -- -- 它在某些方面会明显更强，我希望把这种差异化模型带给世界。第二种是：我也希望拥有和别人一样强、同一量级的模型，但我的重点不在模型本身，而在于我如何使用这些模型、以及我基于它们构建什么样的产品。\n\n从我对 Meta 一贯路线的理解来看，这家公司长期以来关注的核心，一直是连接人与人、构建关系、打造大规模的用户体验型产品。无论是社交网络、沉浸式体验，还是他们设想中的元宇宙，本质上都是围绕\"体验\"和\"连接\"展开的。\n\n所以我这里是基于外部推测，但我认为 Meta 的思路，很可能是：使用我们已经熟悉、已经理解得比较透彻的 AI 技术（比如 Transformer），来构建全新的产品体验，而不是在模型层面追求完全不同的路线。\n\n从一家极其成功、极其赚钱、而且已经拥有全球最大社交网络的公司视角来看，这其实完全可能是一种非常合理、甚至非常聪明的策略。\n\n主持人：我们刚才聊了 Google，也聊了 Meta。但我想换一个角度问：在你们内部讨论、或者评估其他实验室的时候，有没有哪一家，让你们真的觉得\"被震撼到了\"？哪一家是你个人印象最深的？\n\nJerry： 我得说，这是一个相对比较新的变化。\n\n在过去一年里，我对 Anthropic 的印象提升得非常明显。 我本人其实从来不是那种特别在意模型\"性格\"的人。虽然我也听说过 Claude 的\"性格\"很好，可能确实如此，但这并不是我关注的重点。\n\n真正让我感到震撼的是几件事：他们在 代码模型、编码 Agent 上的成果；以及他们围绕\"开发者\"建立起来的整体产品和品牌 -- -- 还有最关键的一点：他们拥有一大群真正满意、甚至很开心的开发者用户。 这是一项非常、非常了不起的成就。\n\n更重要的是：他们起步比 OpenAI 更晚；算力条件更受限制；团队规模也更小。在这样的前提下，他们依然做到了高度聚焦，并且执行得非常好。\n\n他们在获取高质量算力方面遇到过不少现实困难，但即便如此，仍然做出了非常出色的产品。\n\n这些产品正在明显改变人们开发软件的方式；而据我了解，也已经在 实质性地提升企业生产力。\n\n所以我真心觉得：他们做得非常好，值得祝贺。\n\n主持人：他们确实看起来正处在一个\"高光时刻\"。我身边几乎所有人都在聊 Claude Code。我最近还采访了一个人 -- -- 他在用 Claude\"养活一盆植物\"。（笑）可能是第一种被 AI 模型持续\"照料\"的生命体。我真的不知道他们是怎么做出一个几乎\"人人都喜欢\"的工具的。从 ChatGPT 到 Claude Code，这种程度的\"普遍好评\"，其实非常少见。\n\n而且之前还有一件事：当大家被\"切断使用\"时，开发者的反应极其强烈 -- -- 某种程度上，那种崩溃感甚至超过了 OpenAI 出事时的反应。连 Elon 都公开承认了这一点，说：\"是的，我们用得太多了，这是个警醒，我们得把自己的东西做得更好。\"所以我在想：这也许不是一个完全普遍的现象，但看起来，很多实验室其实已经在不同程度上依赖这套工具了。也希望这次\"切断\"能倒逼出更多、更好的同类产品。来一百万个 Claude Code。（笑）\n\nJerry： 在 OpenAI，我们其实也开发 Codex 有一段时间了 -- -- 它算是我们自己的\"Claude Code 版本\"。\n\n我个人觉得 Codex 也挺不错的。有点好笑的是：我自己其实并没有怎么用过 Claude Code。毕竟当时我还在 OpenAI 工作，也没太多机会去亲自用。\n\n我也是想说得客气一点。所以我确实没法给出太多一手对比体验。但至少从推特上的反馈来看，Claude 确实被全球开发者 非常、非常 喜欢。\n\n做点跟 OpenAI 不同的事情\n\n主持人：结合我们前面的讨论，我对你的理解是：你一直是从一种很纯粹的智识和科学兴趣出发的人。你在 reasoning 上的很多工作，本质上都指向一个长期目标 -- -- 你想创造\"AI 科学家\"。\n\n所以当我看到你说要离开 OpenAI 时，我忍不住在想：你是不是已经不太想继续待在这场\"基础模型竞赛\"里了？听你说话的感觉，更像是想换一条路走。我甚至会想象，你会不会干脆跑去做生物科技之类的方向，用完全不同的方式继续追这件事。\n\nJerry：如果我能克隆自己、同时做很多件不同的事情，我真的会非常愿意。但长话短说：有一天我突然意识到 -- -- 我对自己过去的人生很满意，也为自己做过的事情感到骄傲；但我现在真正想做的，是押一两个、甚至两三个非常非常大的研究赌注，然后看看能不能把它们做成。\n\n我一直觉得，人应该更愿意冒风险。至少从我的观察来看，我可能算是那种风险承受能力比较高的人 -- -- 愿意去追一些看起来很野、很不确定、甚至有点离谱的想法。所以我觉得，我应该把这种特质用在更有意义的事情上。\n\n主持人：那你脑子里的这些想法，如果真要落地，大概需要多久？是一年左右的项目，还是说你说的\"风险\"，意味着你愿意花四五年时间去追一件事，而它最后甚至可能还不如现有方案？\n\nJerry： 我肯定愿意投入很多时间。但与此同时，我也非常坚定地认为：研究应该尽可能快地推进。\n\n做得慢，本身并不值得骄傲。从\"把研究执行好\"这个角度看，我希望它能更快。\n\n不过，真正关键的，其实是我之前反复提到的两个词：聚焦（focus）和信念（conviction）。\n\n如果你同时做很多事情，几乎注定每件事都只能做一小部分。你的注意力会被摊薄，资源也会被摊薄。研究实验室经常会说：算力不够，算力限制拖慢了研究。这当然是真的，而且是重要因素之一。但很多时候，更核心的问题其实是：不够聚焦。一天之内，一个人的注意力只能真正放在有限的几件事情上。\n\n我很喜欢对和我共事过的研究者说一句话：少跑一点实验，把每一个实验想得更深。因为有时候，你花几个小时什么实验都不跑，只是盯着结果、反复分析数据 -- -- 反而更容易带来真正的突破，而不是不停地\"多跑\"。\n\n所以像 OpenAI 这样的公司，算力其实非常多。但如果算力被分散到太多项目上，效果反而会被稀释。如果把算力集中到更少、更聚焦的项目上，算力往往是够用的。\n\n但这又回到了风险和信念的问题。如果你同时做三个项目，只要有一个成功，其实就已经算不错了；另外两个被砍掉，也完全可以接受。如果三个都成功，那当然更好。但如果你只做一个项目，它往往会推进得更快 -- -- 因为你足够聚焦、也足够坚定。当然，代价是：如果它失败了，你会非常惨；但如果它成功了，你可能会拥有世界上最好的模型。\n\n而对 OpenAI 这样规模的公司来说，现在确实很难做到一件事：把整个公司押注在一个全新的、完全不同的方向上，同时不在乎下个季度 Gemini 会不会更强。 这真的非常难。它需要一种非常特殊类型的人，才愿意这么做。\n\n我觉得，这就是问题的核心。\n\n主持人：我明白，也知道你不能聊什么\"秘方\"。但我还是忍不住好奇：从外部看，我会直觉觉得，OpenAI 接下来押注的方向，应该是那些能赚大钱的方向。比如\"Chat 里要加广告\"的消息，几乎把整个互联网点燃了。哪怕很笼统地说，你觉得我们能判断他们接下来大概会把资源投向哪里吗？\n\nJerry： 这个问题上，我确实不应该、也不能谈 OpenAI 的任何具体计划。\n\n主持人：合理。（笑）那我换个问法：你觉得这些做模型的公司里，有没有谁会选择 -- -- 也许\"勇气\"这个词不太准确 -- -- 不把广告塞进模型里？还是说，从商业角度看，这其实是不可避免的？\n\nJerry： 这属于商业策略。我做的是训练模型。（笑）\n\n主持人：好，抱歉，我不是想逼你。（笑）只是聊完整个对话之后，我自己还在试图想明白一件事。一方面，你说你想走一些新的方向，去追一些和主流不同的路径；但另一方面，我们也反复提到：你想做的这些事，确实需要非常强的\"马力\"。所以我有点难想象：这是 Jerry 一个人、在外面慢慢测试新想法？还是说，就你真正想做的那些研究，你必须身处一个拥有足够资源的地方，事情才有可能发生？\n\nJerry： 这正是我现在最想搞清楚的第一个问题。任何 AI 研究，最终都离不开 GPU、离不开算力（FLOPs）。我现在需要认真想清楚的是：到底什么样的方式，才是做这些研究的最佳路径。\n\n我确实正在努力理清楚：我很清楚自己想做哪些研究，但我还在寻找答案 -- -- 到底怎样去做，才算是一个\"好的方式\"。\n\nOpenAI 的压力甚至超过创业？\n\n主持人：我刚才问的那些，基本就是我最想问的了。我觉得我能跟你聊上好几个小时。\n\n我不想继续追问\"你接下来做什么\"，因为你看起来太开心了，整个人容光焕发。\n\nJerry： 是的，我听好几个人都跟我说：你现在比以前快乐多了。\n\n主持人：我不想把你拖回那种压力里，比如问你接下来要做什么？\n\nJerry： 我不知道。而且我也听一位正在经营自己公司的人说过一句让我很震撼的话：在 OpenAI 工作，比自己创业还更有压力。从很多方面看，OpenAI 的确是一个压力极大的地方。\n\n主持人：还有一个小问题，除了\"大家都在追同一套东西\"之外，你觉得这个领域里还有没有什么\"巨大的错误\"？\n\nJerry： 我不觉得存在那种特别\"巨大的错误\"。这个行业里的人，其实都很难犯那种一眼就能看出来的致命错误。\n\n真正的问题更像是：你愿意花多少精力去探索\"其他可能性\"？又有多少精力，继续沿着你已经走得很顺的那条路往前推。\n\n主持人：那我换个问法，可能更准确一点。有没有一些你觉得被明显低估、被忽视的研究方向？它们本该得到更多关注，但现在没有。\n\nJerry： 老实说，这样的想法非常多。但这些想法最缺的，往往不是\"它们不存在\"，而是：缺关注、缺算力、缺资源。\n\n这里还有一个比较有意思的现象。很多研究者 -- -- 包括学术界 -- -- 很擅长、也很喜欢做\"从 0 到 1\"的事情：提出一个新想法，证明它\"有点能跑\"，然后就发表出来。而我觉得，我自己、以及我在 OpenAI 共事过的团队，真正特别擅长的一件事，是\"从 1 到 100\"：拿一些已经有初步证据的新想法 -- -- 它们很不同，也不成熟 -- -- 然后想办法把它们在大规模上做得可靠、稳定、可落地。\n\n要训练前沿模型，把一种技术真正嵌进系统里，会涉及大量非常具体、非常琐碎、但又极其关键的工程和研究工作。如果执行不好，可能要花上好几年；但如果你有一套好的方法和节奏，可能几个月就能完成。这也是我未来很想继续多做的一类事情。\n\nAI 研究是\"明星驱动\"的吗？\n\n主持人：我们之前聊到 OpenAI 的人员流动时，你说公司是能扛住这些变化的。但从外部看，这个领域又很像是\"明星驱动\"的：比如 Alec Radford 那样的突破级贡献 -- -- 你知道我指的是什么。\n\n从行业行为上看，很多实验室似乎也在按\"明星逻辑\"行事。当然，这背后有大量集体协作，但确实也有一些时刻，看起来重大突破被\"绑定\"在少数几个人身上。但你刚才的反应，似乎并不完全认同这是一个\"明星驱动\"的行业。\n\nJerry： 我觉得这是个很复杂的话题，但有两个看法可以同时成立。\n\n一方面，确实存在这样的情况：在某些阶段，尤其是在 OpenAI，一小撮人能产生远超常人的影响力，推动真正突破性的成果，然后这些成果扩散到整个行业。我亲眼看到这种事情反复发生。\n\n但另一方面，当我看到人们在不同公司之间频繁流动时，我很少看到这种流动本身，对公司产生\"决定性影响\"。\n\n我更相信的是：公司的结构、文化和运作方式，才是真正的研究引擎，而不完全取决于某一个研究者是否在这里。\n\n而且我也观察到一个现象：那些频繁跳槽的研究者，反而往往没那么高产 -- -- 即便他们过去做过很好的工作。他们需要重新磨合，会被各种事情分散注意力，短期内也未必有新的突破性想法。\n\n经验当然重要，但更重要的是：营造一种环境 -- -- 强调个人责任、鼓励探索、并且真正为\"做出伟大事情\"提供条件。\n\n在一个好的结构、好的文化、好的协作方式下，你完全可以建立很多团队，持续做出伟大的成果。\n\n这件事并不依赖某一个\"唯一的人\"。归根结底，我认为：研究结构、研究文化和协作方式，远比\"某个特定的人是否在团队里\"更重要。"
  },
  {
    "source": "k.sina.com.cn",
    "company": "OpenAI",
    "title": "OpenAI想靠硬件翻盘，但难度不小",
    "date": "2026-01-27T20:51:31Z",
    "url": "https://k.sina.com.cn/article_5953190046_162d6789e06702m5ng.html",
    "content": "（来源：虎嗅APP）\n\n出品｜虎嗅科技组\n\n作者｜赵致格\n\n编辑｜苗正卿\n\n头图｜视觉中国\n\n1月19日，OpenAI首席财务官莎拉·弗莱尔（Sarah Friar）在公司官网发布了一篇名为\"与智能价值同步增长的业务模式\"的文章。这篇文章解释OpenAI 的业务发展逻辑，分享了2025年的最新财务数据以及公司未来的规划。\n\n据弗莱尔披露，OpenAI 2025年的年化收入突破200亿美元，较2024年的60亿美元增长超230%，较2023年的20亿美元更是实现10倍增长；周活跃用户（WAU）与日活跃用户（DAU）均持续创下历史新高；算力规模实现年均3倍增长：2023年至2025年累计增长9.5倍。\n\n然而，尽管OpenAI在过去三年里的收入增速十分亮眼，但与收入几乎同步增长的算力也意味着公司在基础设施建设上的大量支出。虽然弗莱尔的文章没有提到，但据外部的报告测算，OpenAI在2025年的现金消耗约为80亿美元，到2028年则将达到400亿美元。\n\n2025年微软的第三季度财报中提到，\"本年度的净利润和EPS受到来自OpenAI投资亏损的负面影响，分别减少了31亿美元和每股0.41美元\"。人们根据这组数字和微软对OpenAI的持股比例进行计算后发现，OpenAI在这一季度净亏损高达115亿美元。与此同时，OpenAI的1.4万亿美元的数据中心建设计划目前只筹集了10%的资金，仍需要持续的资金注入。\n\n《纽约时报》的专栏作者塞巴斯蒂安・马拉比（Sebastian Mallaby）预计，如果按照现在的烧钱速度，OpenAI可能在18个月左右，也就是2027年年中发生资金流断裂。\n\n或许是为了回应外界对于财务紧张的质疑，弗莱尔在文章中谈及了OpenAI的商业模式变迁：早期以订阅服务起步，现在则涵盖个人和团队订阅服务、支持广告和商业合作的免费版本，以及与生产工作负载挂钩的基于使用量的 API 服务。\n\n而在这篇文章发出的同一天，OpenAI宣布将于9月首次发布硬件。在2026年，人们无疑会看到这家公司进入商业化的新阶段。\n\nOpenAI 最近有点烦\n\nOpenAI 最近可以说是\"压力山大\"。时间倒回一年前，ChatGPT还占据着86.6%的大模型市场份额。而在一年后，这一数据已经变成了72.3%。与此同时，谷歌的Gemini大模型的份额在一年内从5.6%飞速上涨至13.7%，成为了相比于其他竞争者优势明显的第二名。\n\n不止市场份额在下跌，在技术能力上OpenAI也不再领先。11月19日，谷歌发布的Gemini3在几乎所有基准测试中超过GPT-5.1。在这之后，OpenAI CEO 奥特曼发布了一封公司内部信，宣布公司进入\"Code Red（红色警戒）\"状态，需要全力优化ChatGPT以应对挑战。\n\n在2016年的第一个月，OpenAI又收到另一个坏消息。1月12日，苹果与谷歌联合宣布达成多年期深度合作协议，苹果下一代基础模型将基于谷歌Gemini模型及云技术构建，为Apple Intelligence功能以及个性化Siri提供核心技术支持。\n\n此前两年，苹果一直与OpenAI合作，ChatGPT可以通过被Siri调用触达全球的iPhone用户。但在此次合作后，iPhone用户将优先体验Gemini的\"原生集成式服务\"。当苹果用户习惯于Siri与Gemini的无缝协同，ChatGPT的增长空间无疑将进一步压缩。\n\nOpenAI 的另一个困扰来自已分道扬镳的创始人马斯克。1月16日，马斯克向美国联邦法院提交了一份文件，以OpenAI背弃其非营利初衷，与微软达成数十亿美元合作，对他构成欺诈为由，要求OpenAI和微软向其支付788亿美元至1345亿美元的赔偿。\n\n这份文件引用了律师和金融专家的估算称，马斯克在2015年向OpenAI捐赠了3800万美元种子资金，后又遭到欺诈，因而有权获得OpenAI当前5000亿美元的估值中相当大的一部分，\n\n对于这个堪称天文数字的索赔，OpenAI和微软向法院提出撤诉，然而加利福尼亚州奥克兰市的一名联邦法官伊冯娜・冈萨雷斯・罗杰斯驳回了撤诉请求，宣布庭审将于原定的4月下旬开启。\n\nOpenAI发言人随后表示，马斯克发起本次诉讼并非正当维权，而是一场蓄意的骚扰行动，目的是拖住竞争对手的发展步伐，为其旗下的竞品 AI 公司 xAI 争取追赶时间。微软方面指出，马斯克聘请的专家C. 保罗・瓦赞错误地假设微软的利润应回流至OpenAI，导致了价值的重复计算。\n\n在接下来的几个月里，双方的唇枪舌剑预计将持续升级。虽然业内普遍认为OpenAI支付天价赔偿的可能性不大。但这场诉讼无疑将牵扯OpenAI不少精力，其不确定性也将给OpenAI的融资和估值提升带来影响。\n\n广告与硬件能否破局？\n\n在内忧外患中，OpenAI在2026年首先推出的商业化举措是在ChatGPT添加广告。1月16日，OpenAI正式开启在免费版ChatGPT中测试广告。每当用户开启对话，会有占据一半屏幕的广告出现在回答的底部，并被标注为\"赞助内容（sponsored）\"。\n\n日前，OpenAI刚刚推出了美元8美元的套餐\"ChatGPT Go\"，而此番推出广告也将只在免费版和ChatGPT Go中出现，Plus、Pro、Business和Enterprise版本的付费用户将不会看到广告。OpenAI称，广告内容不会影响ChatGPT的回答，用户可以信任其回答的客观性。\n\n增加广告能为OpenAI带来多少收入呢？Evercore ISI高级分析师马克・马哈尼的看法颇为乐观。他在研报中预测，OpenAI在2026年将实现数十亿美元的广告收入，到2030年广告收入将突破250亿美元，并将对谷歌构成市场威胁。\n\n这位分析师补充称，这一估值结论的依据包括ChatGPT届时有望达到的用户规模、高意向效果营销平台已被验证的商业变现能力，以及广告市场当前的整体规模。\n\n除了在对话界面增加广告，OpenAI近日还首次确认了正在研发AI硬件设备。\n\n1月19日，OpenAI 全球事务主管克里斯・莱恩在达沃斯Axios峰会现场表示，OpenAI 正研发其首款人工智能驱动的硬件设备，并计划于今年下半年正式发布，但并没透露产品细节和发行时间。主持人在现场追问这款设备是不是别针或耳机，莱恩均未置可否。\n\n去年5月，OpenAI以65亿美元的价格收购了由苹果前首席设计官Jony Ive创办的硬件初创公司io，这也是OpenAI自成立以来最大的一笔收购。从这之后，关于OpenAI推出AI设备的传闻始终不间断。\n\n去年11月，奥特曼曾表示，这款硬件设备将比智能手机\"更平和\"，其简洁程度会让用户感到震惊。但是关于OpenAI到底在研发什么产品，网络上流传着多个大相径庭的版本。\n\nX上一个一名叫\"智慧皮卡丘\"的科技博主爆料称，OpenAI的首款硬件设备为AI智能笔。该硬件项目被命名为\"Gumdrop\"，这款笔具备音频能力，能与智能手机或其他终端双向交流，将手写内容转为文本并与ChatGPT同步。\n\n他还爆料称，OpenAI的另一个产品是内部代号为\"Sweetpea\"的智能耳机，在市场上将对标苹果的 AirPods。这款耳机的首年出货量目标约为 4000 万到 5000 万副。\n\n除了笔和耳机，也有消息称OpenAI在研发与Meta的雷朋智能眼镜相仿的AI眼镜，旨在让用户无需依赖手机屏幕，就能以解放双手的方式使用AI功能。还有一则爆料则说OpenAI的新产品包括无显示屏智能音箱，与亚马逊搭载 Alexa 语音助手的 Echo Dot 相仿，可置于后台运行，随时接收语音指令。\n\n由于截止目前的可确认消息太少，再加上OpenAI此前从未涉足硬件领域，人们很难对于OpenAI的硬件前景做出深入分析和预测。不过可以肯定的是，AI硬件的创新热潮正在硅谷涌动。大模型能力的持续提升让人们看到了便携式硬件将多模态能力和语音交互结合的可能。\n\n就在OpenAI官宣今年下半年发布硬件几天后，苹果宣布将发布AirTag大小的AI穿戴设备，计划于2027年推向市场。与此同时，最近陷入营收低谷的Meta也在关停Horizon Workrooms，裁员超过千人，并将虚拟现实与元宇宙产品的资源转向AI可穿戴设备。\n\n未来或许有一款AI硬件产品能像iPhone定义手机一样重新人和AI的关系，让大模型与物理世界交互获得新的突破。虽然不确认OpenAI这一战略转向能否成功，但至少它已经在牌桌上。\n\n本内容由作者授权发布，观点仅代表作者本人，不代表虎嗅立场。如对本稿件有异议或投诉，请联系 tougao@huxiu.com。\n\n本文来自虎嗅，原文链接：https://www.huxiu.com/article/4830026.html?f=wyxwapp"
  },
  {
    "source": "tmtpost.com",
    "company": "OpenAI",
    "title": "2亿美元结盟，Snowflake×OpenAI深度合作：AI没有独霸者，只有生态赢家-钛媒体官方网站",
    "date": "2026-02-03T12:06:16Z",
    "url": "https://www.tmtpost.com/7865117.html",
    "content": "2月2日，一场价值2亿美元的合作官宣，再次搅动企业人工智能领域的风云。云数据巨头Snowflake与AI领头羊OpenAI达成多年期战略合作，不仅让前者12600家客户得以打通三大云平台的OpenAI模型访问权限，更敲定了联合研发AI代理的核心目标 -- -- 这不是一次简单的技术互补，而是企业AI竞争从\"模型内卷\"转向\"生态联姻\"的标志性事件。\n\n短短三个月内，Snowflake先后掷出两笔2亿美元的AI大单，先牵手Anthropic，再联姻OpenAI；而OpenAI也在两周前刚与ServiceNow完成类似合作，密集的大额交易背后，是企业AI市场的竞争逻辑正在发生根本性重构。曾经的\"单一模型决胜\"时代已成过去，一场围绕数据、模型、场景的生态博弈，正在拉开全新战局。\n\n这场备受瞩目的合作，本质是一场精准的\"能力互补\"双向奔赴，而非简单的技术授权。从协议条款来看，双方的绑定深度远超普通合作伙伴，核心围绕三大维度展开，每一项都直指企业AI落地的核心痛点。\n\n首先，模型接入实现\"全云覆盖\"，打破平台壁垒。根据协议，OpenAI的全量模型将通过Snowflake Cortex AI套件，开放给Snowflake的所有客户，且支持亚马逊AWS、微软Azure、谷歌GCP三大主流云平台。这意味着，无论企业原本使用哪一家的云服务，都能在熟悉的Snowflake数据平台内，直接调用OpenAI的前沿模型（包括GPT-5.2等最新版本），无需额外搭建跨平台适配架构，大幅降低AI落地的技术门槛。同时，Snowflake内部员工也将全面接入ChatGPT Enterprise，助力内部研发与运营效率提升。\n\n其次，聚焦AI代理研发，攻坚企业场景落地难题。双方合作的核心发力点，是基于OpenAI的代理技术（包括应用程序SDK、AgentKit），在Snowflake平台内开发全新AI代理解决方案，同时赋能企业自主构建专属AI代理。对于企业而言，AI代理的价值在于能够自动化处理复杂数据流程 -- -- 比如自动抓取企业内部数据、完成清洗分析、生成合规报告，甚至基于数据洞察给出决策建议，真正实现\"数据+智能\"的闭环，而不是停留在孤立的模型调用层面。Snowflake AI副总裁巴里斯·古爾泰金对此表示，OpenAI与Snowflake的工程团队将深度协同，\"双方互为客户的身份，让我们能更精准地捕捉企业需求，推动技术与场景的深度融合\"。\n\n最后，深化双向赋能，构建技术协同。这场合作是\"双向奔赴\"而非单向输出：OpenAI将把Snowflake作为核心数据平台，用于模型实验的跟踪、分析与测试，借助Snowflake的安全合规能力，解决大规模实验数据的管理难题；而Snowflake则通过集成OpenAI的模型能力，补齐自身在生成式AI领域的短板，强化\"数据+AI\"一体化优势。正如Snowflake首席执行官斯里達爾·拉馬斯瓦米所言，\"通过将OpenAI模型引入企业数据，我们让组织能在最宝贵的资产之上构建AI，既保留安全管控能力，又能借助世界级智能实现转型\"。\n\n值得注意的是，这2亿美元的合作并非\"一次性投入\"，而是多年期的商业承诺，重点聚焦可靠性、性能优化与客户实际应用效果。这也意味着，双方的合作不会停留在表面的技术对接，而是将持续迭代适配企业需求，形成长期绑定的生态关系 -- -- 这正是当前企业AI合作的核心趋势：从\"短期交易\"转向\"长期共建\"。\n\n这场合作最耐人寻味的一点，是Snowflake在短短三个月内，先后与Anthropic、OpenAI达成两笔一模一样的2亿美元合作。这种\"不押注单一玩家\"的布局，并非盲目扩张，而是Snowflake精心打造的\"模型中立\"战略，背后折射出的是企业AI采购的底层逻辑变革。\n\n对于Snowflake而言，\"模型中立\"是其巩固数据云龙头地位的关键棋子。作为全球顶级的数据云提供商，Snowflake的核心优势在于拥有海量企业客户数据与成熟的安全合规体系，但在生成式AI模型研发上并非强项。选择同时与Anthropic、OpenAI两大头部玩家合作，既能避免被单一模型供应商绑定，又能为客户提供多元化选择 -- -- 毕竟不同模型各有优劣，比如OpenAI在通用场景、对话生成上表现突出，而Anthropic的Claude系列在长文本处理、合规性上更具优势，企业可以根据自身业务场景灵活选择。\n\n巴里斯·古而泰金在接受采访时，明确表态了Snowflake的战略思路：\"我们有意保持模型中立，企业需要选择空间，不能被单一供应商束缚。OpenAI是重要合作伙伴，但我们还与Anthropic、谷歌、Meta等多家企业合作，构建多元化的模型生态。\" 这种布局的核心，是将Snowflake打造为企业AI的\"枢纽平台\" -- -- 无论客户选择哪一款模型，都能在Snowflake上完成数据整合、模型调用与场景落地，从而锁定企业的核心数据资产，巩固自身在数据云领域的话语权。\n\n而Snowflake的双重布局，本质上是顺应了企业AI采购的务实趋势。如今，越来越多的企业已经放弃\"押注单一模型\"的思路，转而构建\"模型矩阵\" -- -- 根据不同业务场景选择适配的模型，实现最优性价比。比如，在通用办公场景使用OpenAI模型，在金融、医疗等强合规场景使用Anthropic模型，在数据分析场景搭配谷歌Gemini模型。这种\"不迷信全能模型，只选最合适工具\"的心态，正在推动企业AI市场从\"模型崇拜\"走向\"实用主义\"。\n\nSnowflake并非个例。工作流程自动化平台ServiceNow在2026年1月也宣布，同时与OpenAI、Anthropic达成多年期合作，逻辑与Snowflake如出一辙。ServiceNow的高管Amit Zavery直言，与两家AI实验室合作是深思熟虑的结果，\"我们要给客户和员工根据任务选择模型的能力，而不是强迫他们接受单一解决方案\"。这种\"多模型并行\"的合作模式，正在成为企业AI生态布局的主流选择，也让整个市场的竞争从\"模型单挑\"转向\"生态团战\"。\n\n与Snowflake的合作，是OpenAI近期企业布局的又一重要落子。就在两周前，OpenAI刚与ServiceNow达成类似合作，将自身模型定为ServiceNow企业客户的首选智能能力，同样聚焦AI代理研发。短短半个月内，接连与两家企业软件巨头达成大额合作，背后是OpenAI的战略转型：从\"模型研发\"转向\"场景落地\"，通过绑定基础设施龙头，快速抢占企业AI市场的核心入口。\n\n回顾OpenAI的企业合作路径，其核心逻辑始终清晰：避开与同类模型厂商的正面内卷，转而与人工智能技术栈中的基础设施领导者结盟，借助对方的场景与客户资源，实现模型的规模化落地。对于OpenAI而言，模型本身是核心竞争力，但企业AI落地的关键的是\"最后一公里\" -- -- 数据整合、场景适配、安全合规，这些都不是OpenAI的强项，而Snowflake、ServiceNow等企业恰好补齐了这些短板。\n\n与Snowflake的合作，OpenAI获得了三大核心收益。其一，快速触达12600家企业客户，大幅拓宽企业市场覆盖面。Snowflake的客户涵盖全球各类大中型企业，尤其是在金融、零售、医疗等强数据需求领域拥有深厚积累，这些客户都是OpenAI的核心目标群体，通过此次合作，OpenAI无需逐一拓展客户，就能借助Snowflake的渠道实现规模化渗透。其二，解决企业落地的合规难题。Snowflake在数据安全、合规管控上拥有行业领先的能力，能够满足金融、医疗等强监管行业的AI应用要求，而合规正是企业AI落地的最大痛点之一 -- -- OpenAI通过绑定Snowflake，相当于获得了\"合规背书\"，大幅降低企业采用其模型的顾虑。其三，获得大规模企业数据反馈，助力模型优化。OpenAI将Snowflake作为实验数据平台，能够获取海量真实企业场景的数据，这些数据将帮助OpenAI优化模型的行业适配能力，让模型更贴合企业实际需求，形成\"落地-反馈-迭代\"的良性循环。\n\n值得注意的是，OpenAI的合作策略并非\"雨露均沾\"，而是精准绑定\"场景入口型\"企业。Snowflake掌控着企业数据入口，ServiceNow掌控着企业IT服务与工作流入口，这两家企业都能直接触达企业的核心业务场景，能够帮助OpenAI的模型快速嵌入企业工作流，而不是停留在\"工具级应用\"层面。这种\"模型+入口\"的合作模式，比单纯的技术授权更具粘性，也能让OpenAI在激烈的企业AI市场中，占据更有利的位置。\n\n不过，OpenAI在企业市场的布局也面临挑战。一方面，同类模型厂商的竞争日益激烈，Anthropic、谷歌、Meta等都在加速企业合作布局，尤其是Anthropic的崛起，已经在部分领域形成分流；另一方面，OpenAI始终保持低调，拒绝在新闻稿之外分享任何交易细节，这种神秘感虽然能维持话题热度，但也可能让部分企业在合作决策时产生顾虑 -- -- 毕竟对于企业而言，长期合作需要足够的透明度与确定性。\n\nSnowflake与OpenAI的合作，以及近期密集的企业AI大额交易，共同指向一个结论：企业AI市场没有任何一家企业能够实现独霸，多强共生、生态互补将成为长期格局。而这场格局重构的背后，是市场、技术、资本三大力量的共同推动。\n\n首先，市场需求推动\"多模型并行\"成为主流。如今，企业AI已经从\"尝鲜期\"迈入\"规模化落地期\"，2025年全球企业生成式AI支出飙升至370亿美元，较2024年增长3.2倍，其中AI应用支出190亿美元，基础设施支出180亿美元。随着应用场景的不断丰富，企业越来越意识到，没有一款模型能够适配所有场景 -- -- OpenAI擅长通用对话与内容生成，Anthropic强于长文本处理与合规性，谷歌Gemini在多模态与数据分析上有优势，Meta的Llama系列则胜在开源灵活。因此，构建\"模型矩阵\"，根据场景灵活调度模型，成为企业的最优选择。\n\na16z的最新调研数据印证了这一趋势：高达81%的企业在同时使用三种或更多的模型，这一比例在一年前还只有68%。越来越多的企业开始像工匠挑选工具一样，为不同岗位匹配最合适的AI模型 -- -- 用OpenAI处理通用办公场景，用Anthropic处理金融合规场景，用谷歌模型处理数据分析场景。这种务实的选择，直接推动了企业与多家AI厂商的合作，也让市场难以形成单一垄断。\n\n其次，资本视角的分歧，折射出市场的多元可能性。目前，关于企业AI市场的领导者，不同投资机构的调研给出了截然不同的答案 -- -- Menlo Ventures（Anthropic的最大投资方之一）的报告显示，截至2025年底，Anthropic以40%的市场份额占据榜首，OpenAI滑落至27%，谷歌以21%位居第三；而Andreessen Horowitz（a16z，OpenAI的投资方）的报告则称，OpenAI仍是企业市场的领跑者，78%的受访企业在生产环境中使用了OpenAI的模型。\n\n这些相互矛盾的调研结果，看似混乱，实则反映了企业AI市场的多元格局。一方面，投资机构的调研存在\"利益关联\"倾向，自然会偏向自家投资的企业；另一方面，不同行业、不同规模的企业对模型的选择存在差异，比如大型金融企业更倾向于Anthropic的合规优势，中小型科技企业更青睐OpenAI的生态成熟度，谷歌则在谷歌云用户群体中占据优势。这种差异化的选择，让没有任何一家厂商能够垄断所有细分市场，反而形成了\"各占一隅、相互竞争\"的格局。\n\n更重要的是，生态博弈取代模型竞争，成为市场核心矛盾。正如a16z在报告中指出的，企业AI的终局不是模型之战，而是\"工作流之战\"。基础模型的竞争正在快速\"工具化\"\"商品化\"，企业追求的不再是\"最顶尖的模型\"，而是\"能嵌入现有工作流、解决实际问题的解决方案\"。因此，单纯的模型研发已经不足以支撑企业在市场中立足，必须联合基础设施、场景应用、数据服务等环节的玩家，构建完整的生态体系。\n\n目前，企业AI市场已经形成几大生态阵营。第一阵营是\"微软+OpenAI\"，依托微软365、GitHub等办公与开发工具，将OpenAI模型原生嵌入企业工作流，掌控了数亿知识工作者的入口，65%的企业表示更倾向于选择现有供应商的AI解决方案，微软的生态壁垒难以撼动；第二阵营是\"数据厂商+多模型\"，以Snowflake为代表，通过绑定OpenAI、Anthropic等多家模型厂商，打造\"数据+AI\"一体化平台，聚焦数据密集型场景；第三阵营是\"垂直场景+专属模型\"，以ServiceNow、Salesforce等为代表，结合自身垂直场景优势，联合多家模型厂商开发专属解决方案，聚焦工作流自动化、客户管理等细分领域；第四阵营是\"开源模型+云厂商\"，谷歌、Meta通过开源模型吸引开发者，结合自身云服务优势，抢占中小企业市场。\n\n这四大阵营之间并非完全对立，反而存在大量合作空间。比如，Snowflake的平台可以接入微软Azure的云服务，ServiceNow的解决方案可以调用OpenAI的模型，形成\"生态互补\"的格局。这种\"竞争中有合作\"的关系，进一步强化了多强共生的市场态势。\n\nSnowflake与OpenAI的2亿美元合作，撕开了企业AI战国时代的序幕。这场合作没有赢家通吃的剧本，而是双方借助各自优势，实现\"1+1>2\"的共赢 -- -- Snowflake补齐AI能力，巩固数据云龙头地位；OpenAI抢占落地入口，拓宽企业市场；而最终受益的，是广大企业客户，他们获得了更灵活、更安全、更贴合场景的AI解决方案。\n\n回望企业AI的发展历程，从早期的模型内卷，到如今的生态共生，市场正在逐渐回归理性：AI的终极价值，不是某一家企业的技术垄断，而是通过技术与场景的深度融合，帮助企业实现效率提升与数字化转型。对于模型厂商而言，单纯的技术领先已经不够，必须学会拥抱生态、适配场景；对于企业软件厂商而言，唯有保持开放中立，构建多元化生态，才能锁定客户、立足市场；对于企业客户而言，务实选择、构建适合自身的模型矩阵，才能让AI真正产生价值。"
  },
  {
    "source": "developpez.net",
    "company": "OpenAI",
    "title": "31",
    "date": "2026-01-22T22:40:36Z",
    "url": "https://www.developpez.net/forums/d2180152-2/dotnet/general-dotnet/documents-internes-d-openai-prevoient-perte-14-milliards-dollars-2026-a/",
    "content": "OpenAI n'est pas \" trop grande pour faire faillite \", estime un économiste :\n\npourquoi la domination de l'IA générative ne garantit ni la rentabilité ni la résilience économique à long terme\n\nÀ mesure que l'intelligence artificielle s'impose comme un pilier de l'économie numérique, certaines entreprises finissent par être perçues comme intouchables. OpenAI incarne parfaitement cette tentation : omniprésente, technologiquement dominante et au cur de multiples chaînes de valeur. Pourtant, selon l'analyse d'un économiste reconnue récemment, cette entreprise est loin d'être \" trop grande pour échouer \". Au contraire, elle concentrerait plusieurs vulnérabilités structurelles qui rappellent que, même dans l'IA, les lois économiques restent implacables.\n\nPourquoi est-ce important ? Le créateur de ChatGPT occupe une place tellement centrale dans l'ensemble de l'économie de l'IA que d'autres entreprises et investisseurs pourraient se retrouver exposés à un risque important.\n\nContexte\n\nSelon un rapport de Bloomberg, les risques financiers liés à OpenAI ont suscité une attention particulière. Bien qu'OpenAI soit devenu un acteur majeur dans le domaine de l'intelligence artificielle, Jason Furman, économiste américain de renom et professeur à l'université Harvard, a clairement déclaré que l'entreprise n'était \" pas trop grande pour faire faillite \". Le point de vue de Furman a suscité des discussions sur le potentiel futur et la position sur le marché d'OpenAI.\n\nLe Wall Street Journal a rapporté jeudi qu'OpenAI cherchait à lever une nouvelle série de fonds, avec un objectif pouvant atteindre 100 milliards de dollars, et que sa valorisation avait bondi à 830 milliards de dollars, soit nettement plus que les 500 milliards précédents. Cette nouvelle a suscité des inquiétudes quant à la santé financière d'OpenAI.\n\nCette année, OpenAI a déclenché un boom des transactions d'une valeur de 1 000 milliards de dollars en concluant des accords à grande échelle avec plusieurs\n\ngéants de la technologie, des fabricants de puces et des développeurs de centres de données. Cependant, cette expansion rapide s'accompagne également de risques financiers potentiels. Les gens commencent à se demander si cette start-up non rentable, si elle venait à rencontrer des difficultés en raison d'un endettement excessif, serait considérée comme une entreprise \" trop grande pour faire faillite \".\n\nL'idée du \" too big to fail \" appliquée à l'IA\n\nL'expression \" too big to fail \" provient du monde financier, où certaines banques étaient considérées comme si systémiques qu'un État ne pouvait se permettre de les laisser tomber. Transposée à l'intelligence artificielle, cette logique repose sur un raisonnement similaire : OpenAI serait devenue si centrale que son échec provoquerait un choc inacceptable pour l'économie numérique.\n\nOr, cette transposition pose problème. Contrairement au système bancaire, l'écosystème de l'IA ne repose pas sur un cadre institutionnel garantissant implicitement la survie de ses acteurs dominants. Aucun gouvernement n'a formellement intérêt à sauver une entreprise d'IA privée en difficulté financière, même si ses technologies sont largement utilisées.\n\nUne entreprise centrale, mais pas indispensable\n\nOpenAI est aujourd'hui intégrée dans des milliers d'outils professionnels, de plateformes logicielles et de services numériques. Cette centralité alimente l'idée qu'elle serait devenue indispensable. Pourtant, cette dépendance est en grande partie circonstancielle.\n\nLes entreprises utilisent OpenAI parce qu'elle est performante, accessible et largement adoptée, non parce qu'elle est irremplaçable. D'autres modèles, d'autres laboratoires et d'autres approches existent déjà ou émergent rapidement. En cas de choc majeur, le marché ne s'effondrerait pas ; il se reconfigurerait autour d'alternatives, quitte à accepter une phase transitoire de dégradation des performances.\n\nLes entreprises IA \" ne sont pas des banques. Elles ne sont pas trop grandes pour faire faillite. \"\n\nTout au long de cette année, OpenAI s'est lancé dans une frénésie de transactions d'une valeur totale de 1 000 milliards de dollars, concluant d'importants accords avec des fabricants de puces, des développeurs de centres de données et bon nombre des plus grandes entreprises technologiques. En conséquence, certains membres de l'entreprise et des personnes extérieures ont commencé à se poser une question dérangeante : si cette start-up non rentable finit par s'endetter excessivement, OpenAI serait-il considéré comme trop grand pour faire faillite ?\n\nPour Jason Furman, la réponse est claire : \" Absolument pas. \"\n\n\" Je n'ai aucune raison de penser qu'OpenAI ou toute autre entreprise de ce secteur va faire faillite \", a déclaré Furman, économiste et professeur à Harvard qui conseille à temps partiel OpenAI sur les questions liées au travail. \" Mais si c'était le cas, ce ne sont pas des banques. Elles ne sont pas trop grandes pour faire faillite. \"\n\nFurman sait très bien ce qui se passe lorsqu'une entreprise ou un secteur s'effondre. En 2000, Furman était membre de l'équipe politique de l'administration Clinton et évaluait la possibilité d'un éclatement de la bulle Internet. Des années plus tard, il a occupé le poste de conseiller économique principal de l'administration Obama pendant la Grande Récession, contribuant à l'élaboration d'un plan de relance de 800 milliards de dollars visant à relancer la croissance économique.\n\nDans une interview accordée le mois dernier, Furman a déclaré qu'il voyait des similitudes entre l'essor actuel de l'IA et l'ère des dot-com, et qu'il pensait que l'économie pourrait résister à l'éclatement de la bulle de l'IA, si et quand cela se produirait. Furman a également déclaré qu'il n'était pas aussi inquiet que certains au sujet du nombre croissant de transactions circulaires dans le secteur de l'IA. Ce qui le préoccupe, c'est la perspective d'une intervention financière du gouvernement.\n\n\" Le gouvernement ne devrait pas s'impliquer financièrement dans ce domaine \", a-t-il déclaré. \" Le secteur dispose de fonds largement suffisants pour subvenir à ses besoins, et il n'y a aucune raison pour que le gouvernement intervienne. \"\n\nContrairement aux entreprises financières ou à l'industrie automobile, a déclaré Furman, l'IA n'est pas suffisamment imbriquée dans le reste du système économique pour justifier un plan de sauvetage si les choses tournent mal. Dans le pire des cas, si OpenAI ou l'un de ses concurrents faisait faillite, ce ne serait certes pas une bonne chose, mais ce ne serait pas non plus \" catastrophique \", a déclaré Furman.\n\nOpenAI a la recherche d'un filet de sécurité politique ?\n\nLes principales entreprises spécialisées dans l'IA ne sont pas à la recherche d'un plan de sauvetage aujourd'hui. Les plus grandes d'entre elles lèvent des dizaines de milliards de dollars de fonds, comme OpenAI, ou disposent d'importantes réserves de trésorerie, comme les géants technologiques Meta et Google. OpenAI, Anthropic et d'autres développeurs font également état d'une forte augmentation des revenus générés par leurs logiciels d'IA.\n\nMais le mois dernier, Sarah Friar, directrice financière d'OpenAI, a alarmé certains observateurs du secteur en laissant entendre que le gouvernement américain pourrait jouer un rôle pour \" soutenir la garantie qui permet le financement \". Peu après, Sarah Friar et Sam Altman, d'OpenAI, se sont efforcés de clarifier ses propos, soulignant qu'elle s'était mal exprimée et que le fabricant de ChatGPT n'avait pas l'intention de demander un plan de sauvetage pour ses engagements en matière d'infrastructure.\n\nNéanmoins, OpenAI a fait pression sur le gouvernement américain pour qu'il étende un crédit d'impôt de 35 % axé sur les puces électroniques aux centres de données IA, aux fabricants de serveurs IA et aux composants du réseau électrique. Par ailleurs, et de manière beaucoup plus directe, l'administration Trump a pris une participation de 10 % dans le fabricant américain de puces Intel au début de l'année.\n\n\" Je crains que lorsque quelqu'un parle d'aide ou de prise de participation, cela sous-entende également que si les choses tournent mal, le gouvernement viendra à la rescousse ou procédera à un renflouement \", a déclaré Furman.\n\nUne croissance impressionnante, mais à quel prix ?\n\nOpenAI bénéficie aujourd'hui d'une aura particulière. Ses modèles sont devenus des standards de facto, ses outils sont intégrés dans des milliers de produits, et son influence dépasse largement le cadre du logiciel. Cette position alimente l'idée qu'un échec serait inconcevable, tant les conséquences sembleraient lourdes pour l'écosystème.\n\nLe succès d'OpenAI repose sur des modèles toujours plus grands, toujours plus coûteux à entraîner et à exploiter. Cette course à l'échelle constitue à la fois son avantage compétitif et son principal point de tension. Chaque progrès technique s'accompagne d'une augmentation massive des besoins en calcul, en énergie et en infrastructures.\n\nD'un point de vue économique, cette dynamique est atypique. Dans de nombreux secteurs technologiques, l'augmentation du volume permet de réduire les coûts unitaires. Dans l'IA générative, c'est souvent l'inverse : plus l'usage augmente, plus la facture globale s'alourdit. Cette structure fragilise mécaniquement la rentabilité à long terme.\n\nLe facteur politique et réglementaire, talon d'Achille potentiel\n\nÀ mesure que l'IA gagne en influence, elle attire une attention politique croissante. Sécurité, souveraineté numérique, protection des données, impact sur l'emploi : autant de sujets qui placent les grands acteurs de l'IA sous surveillance constante. Être perçu comme dominant n'offre aucune immunité face aux régulateurs, bien au contraire.\n\nUne entreprise considérée comme centrale peut devenir une cible prioritaire. Exigences accrues, contraintes de transparence, limitations d'usage ou obligations de conformité renforcées peuvent profondément affecter son modèle économique. Contrairement aux institutions financières systémiques, OpenAI ne bénéficie d'aucun mécanisme formel de sauvetage en cas de crise majeure.\n\nOpenAI enregistre un déficit de 12 milliards de $ en un trimestre pour des recettes de 4,3 milliards $ en un semestre\n\nJamais OpenAI n'avait dévoilé l'ampleur de ses pertes, mais les comptes de Microsoft ont levé le voile sur une réalité stupéfiante : en trois mois, OpenAI aurait perdu environ 11,5 à 12 milliards de dollars. Microsoft applique en effet la méthode de la mise en équivalence pour sa participation, ce qui implique de comptabiliser dans son propre résultat la quote-part des pertes d'OpenAI. Or, sur le trimestre clos le 30 septembre 2025, Microsoft indique que sa part dans OpenAI a réduit son résultat net de 3,1 milliards de dollars. Si cela représente 27 % des pertes (proportion de capital détenu), le calcul suggère bien un total avoisinant 11,5 milliards $ de pertes pour OpenAI sur le trimestre. Qui plus est, certaines données suggèrent un chiffre encore plus élevé : avant la restructuration capitalistique d'OpenAI, Microsoft en détenait possiblement jusqu'à 32,5 %. Sur cette base, le déficit trimestriel d'OpenAI dépasserait alors 12 milliards de dollars. Dans tous les cas, le nombre donne le vertige.\n\nPour prendre la mesure de ce gouffre, il faut le comparer aux revenus d'OpenAI. Sur l'ensemble du premier semestre 2025, OpenAI aurait généré seulement 4,3 milliards $ de revenus d'après des documents internes - certes en forte hausse sur un an, mais sans commune mesure avec les pertes actuelles. Autrement dit, en un seul trimestre, OpenAI a dépensé près de trois fois ce qu'elle a facturé en six mois. Ce décalage abyssal illustre le modèle économique très particulier des acteurs de l'IA générative : une course à l'investissement et à la croissance de l'utilisation, au prix de pertes colossales dans l'espoir de profits futurs.\n\nLes dépenses d'OpenAI s'expliquent notamment par des coûts d'infrastructure et de R&D faramineux. L'entraînement de modèles de pointe comme GPT-4 ou son successeur mobilise des milliers de GPU et consomme une électricité considérable. D'après un rapport financier relayé par Reuters, OpenAI aurait consacré 6,7 milliards $ à la R&D sur le seul premier semestre 2025, tout en \" brûlant \" environ 2,5 milliards $ de trésorerie nette sur cette période pour faire tourner ChatGPT et ses autres services. La startup disposait encore d'environ 17,5 milliards $ de liquidités mi-2025 - un matelas important, mais qui fond rapidement avec un rythme de perte désormais proche de 12 milliards par trimestre. Sans nouveaux financements ou sans réduction drastique de coûts, une telle hémorragie n'est pas tenable au-delà de quelques trimestres.\n\nLe modèle d'OpenAI est-il viable ?\n\nCette situation pose la question de la viabilité du modèle économique d'OpenAI et, plus largement, des fournisseurs d'IA générative. Pour l'heure, OpenAI tire ses revenus de la commercialisation de l'accès à ses modèles (via des abonnements ChatGPT Plus, des offres ChatGPT Enterprise et l'API pour développeurs) ainsi que de contrats d'intégration (comme avec Microsoft). L'entreprise vise un chiffre d'affaires annuel de 13 milliards $ en 2025, ambitieux mais encore insuffisant pour couvrir des dépenses annuelles qui pourraient dépasser 40 milliards si la tendance actuelle se maintient. Sam Altman, le PDG d'OpenAI, reste optimiste en affichant un horizon de 4 à 5 ans pour atteindre la rentabilité et en misant sur des revenus exponentiels à terme.\n\nCette approche rappelle celle d'Amazon à ses débuts : accepter des pertes massives pour conquérir le marché, dans l'optique de régner plus tard sans partage. Les partisans de cette stratégie soulignent qu'Amazon a fini par devenir profitable et dominer le e-commerce mondial après des années dans le rouge.\n\nOpenAI serait-il un Amazon de l'IA en gestation ? C'est le pari de ses investisseurs. Néanmoins, le défi est immense : il faudra soit augmenter fortement les revenus (par exemple via des offres premium, des services aux entreprises très lucratifs ou des licences technologiques), soit réduire les coûts unitaires de l'IA (peut-être grâce à de nouvelles optimisations ou à du matériel plus efficace), soit les deux, pour sortir de ce tunnel de pertes.\n\nLa taille n'annule pas le risque de faillite\n\nL'un des messages clés de l'analyse économique est sans ambiguïté : aucune entreprise privée n'est protégée par sa seule taille. Le concept de \" too big to fail \" n'a de sens que lorsqu'il existe un acteur public prêt à intervenir pour éviter un effondrement systémique.\n\nDans le cas d'OpenAI, aucun filet de sécurité de ce type n'existe. L'entreprise n'est ni une banque, ni une infrastructure publique, ni un service essentiel au fonctionnement immédiat des États. En cas de difficultés financières majeures, rien ne garantit qu'une intervention extérieure viendrait compenser ses pertes ou assurer sa continuité.\n\nBeaucoup d'entreprises bâtissent aujourd'hui leurs produits et leurs services autour des modèles d'OpenAI. Cette dépendance crée l'illusion qu'un effondrement serait impossible, car trop coûteux pour l'écosystème. Mais cette logique confond confort et nécessité.\n\nLe secteur de l'IA se caractérise par une capacité de substitution rapide. Des alternatives émergent en permanence, qu'elles soient open source ou issues d'autres grands laboratoires. En cas de difficulté majeure, le marché ne s'arrêterait pas ; il se réorganiserait. Cette plasticité réduit considérablement l'argument selon lequel OpenAI serait devenue irremplaçable.\n\nSource : analyse de Jason Furman\n\nEt vous ?\n\nLa narration d'une entreprise \" incontournable \" peut-elle devenir contre-productive en empêchant une remise en question du modèle économique ?\n\nL'IA générative est-elle en train de reproduire les mécanismes classiques des bulles technologiques, où la croissance d'usage précède largement la viabilité économique ?\n\nPeut-on bâtir une industrie durable de l'IA générative sur des modèles dont les coûts augmentent plus vite que les revenus qu'ils génèrent ?\n\nLa dépendance d'OpenAI à des ressources critiques qu'elle ne contrôle pas pleinement constitue-t-elle son principal risque à moyen terme ?\n\nLes investisseurs et partenaires d'OpenAI intègrent-ils réellement des scénarios de rupture, ou parient-ils implicitement sur une forme de sauvetage indirect en cas de crise ?"
  },
  {
    "source": "Forbes México",
    "company": "OpenAI",
    "title": "Sam Altman, CEO de OpenAI, explica el futuro",
    "date": "2026-02-03T15:33:55Z",
    "url": "https://forbes.com.mx/sam-altman-ceo-de-openai-explica-el-futuro/",
    "content": "am Altman afirma que la barra de uranio en su oficina no es motivo de preocupación. Sentado verticalmente en su escritorio en la sede de OpenAI en San Francisco, como un Slim Jim rechoncho y de ébano, es quizás la más sorprendente entre la impresionante colección de innovaciones históricas que ha recopilado a lo largo de los años. \"Eso está agotado\", dice con indiferencia sobre la barra de uranio-238, el mismo elemento utilizado para generar energía nuclear. \"No te va a hacer daño\". Agita un contador Geiger sobre ella y demuestra su argumento.\n\n\"Se hace un gran descubrimiento en física y... se libera energía prácticamente ilimitada\", dice sobre la barra de uranio. \"No sabíamos nada de esto, y luego teorizamos que tal cosa era posible. Un par de décadas después, fabricaron una bomba atómica. Algo increíblemente rápido\".\n\nAltman, con tenis Adidas Lego Ultraboost y un sencillo jersey de punto gris, examina metódica y cronológicamente los artefactos, la mayoría de los cuales suelen estar en su despacho, ocultos a la vista de nadie más que sus amigos más cercanos. Hoy en exposición, Altman comenta: un hacha de mano de 40,000 años de antigüedad (\"una asombrosa herramienta multiusos de la Edad de Piedra\"), una espada de bronce de 3,500 años de antigüedad (\"un ejemplo interesante de tecnología con gran impacto geopolítico\") y un aspa del ventilador de un compresor de un motor a reacción de un Concorde (\"la única pieza lo suficientemente pequeña\" para llevarla). Desafiando con naturalidad el protocolo de los conservadores del museo, ha llevado todos estos objetos a su despacho en una bolsa de lona, envueltos individualmente en toallas de baño.\n\n\"Me sorprende constantemente cómo cada generación construye una nueva capa de andamiaje\", dice sobre el progreso tecnológico. \"Realmente lo estamos viendo ahora\".\n\nTan memorable como la barra de uranio, otro de los objetos llamativos de la colección de Altman es un antiguo chip GPU. Este entrenó una versión temprana del modelo detrás del producto insignia de OpenAI, ChatGPT, que catapultó la IA al público general en noviembre de 2022 y desencadenó una reacción en cadena de innovación que podría resultar tan transformadora como la Revolución Industrial.\n\nEstados Unidos tiene una larga historia de innovadores que no son conocidos por inventar, sino por llevar la vanguardia a la vida cotidiana con pura fuerza de voluntad e ingenio. Pensemos en Steve Jobs, Bill Gates y Elon Musk. Thomas Edison no inventó la bombilla. Él -- o mejor dicho, su equipo -- la mejoró con un filamento más duradero y luego la comercializó con agresividad.\n\nAltman es de ese tipo. Es un inversor y un acelerador más que un ingeniero o un científico. Su visión no se trata de perfeccionar los productos de consumo, sino de construir los sistemas subyacentes de los que pronto podría depender el resto de la economía. ChatGPT ahora tiene más de 800 millones de usuarios semanales. OpenAI, con más de 13,000 millones de dólares en ingresos el año pasado, fue valorada recientemente en 500,000 millones de dólares (Altman no tiene participación accionaria directa en la empresa, pero sus otras inversiones le otorgan un valor estimado de 3,000 millones de dólares). Actualmente, está en conversaciones para recaudar 100,000 millones de dólares adicionales en una megaronda que podría valorarla en 750,000 millones de dólares o más. Inspiradas por OpenAI, las grandes tecnológicas podrían invertir un estimado de 500,000 millones de dólares en centros de datos y chips de IA este año. En este momento, es quizás la empresa más importante del mundo.\n\nTe puede interesar: La Inteligencia Artificial más que herramienta, fuerza que transforma\n\nEso ha convertido a Altman, ahora de 40 años, en el tema de una creciente hagiografía. El director ejecutivo de Disney, Bob Iger, afirma que Altman puede \"mirar a la vuelta de la esquina\" para ver el futuro. El cofundador de Airbnb, Brian Chesky, lo llama \"una de las dos personas más ambiciosas que conozco\" (el otro es Musk). La leyenda del diseño de Apple, Jony Ive, dice enigmáticamente que Altman \"se siente cómodo con lo desconocido, pero no le da la mínima importancia a la responsabilidad\". El reconocido inversor de capital riesgo Paul Graham (exmentor de Altman en la incubadora de startups Y Combinator) ofrece una visión más directa: \"Se le da bien convencer a la gente. Se le da bien conseguir que la gente haga lo que él quiere\".\n\nAunque de voz suave y con un porte discreto del Medio Oeste, Altman es una especie de charlatán de feria de IA. Sus agresivas predicciones sobre el crecimiento exponencial de la tecnología deben hacerse realidad para justificar no solo la valoración de OpenAI, sino también las enormes apuestas económicas y sociales que se están formando en torno a ella. Y no está claro cómo lograrlo. ¿Podrá hacer realidad un futuro tan grande, rápido y costoso como el que describe?\n\nSam Altman, cofundador de OpenAI\n\nForbes lleva más de una década siguiendo a Altman, quien ocupa el sexto lugar en nuestra próxima lista de los mayores innovadores estadounidenses vivos. En 2015, fue miembro destacado de nuestra lista inaugural Forbes 30 Under 30 Venture Capital como el recién nombrado líder de Y Combinator, con 29 años. \"Es genial poder hacer una lista de los problemas del mundo y luego financiar empresas para resolverlos\", nos comentó.\n\nVisto únicamente a través de la lente de esas inversiones, Altman es un líder empresarial tremendamente ambicioso que diseña meticulosamente su visión del futuro. A medida que la era móvil se consolidaba en la década de 2010, Altman respaldó con visión de futuro a diversas empresas -- invirtiendo 15.000 dólares en el 2% del gigante de pagos Stripe antes incluso de que tuviera nombre, y liderando una ronda de financiación de 50 millones de dólares en Reddit en 2014, por ejemplo -- que se convirtieron en pilares de la economía de las aplicaciones.\n\nCon la IA, lo está haciendo de nuevo. Está OpenAI, por supuesto. Pero también está Helion, que intenta aprovechar el poder casi ilimitado de la fusión nuclear (el tipo de energía que usa el sol), y Oklo, que desarrolla reactores de fisión nuclear más convencionales, pero más pequeños y modulares. Ambos podrían satisfacer las necesidades de alto consumo energético de la IA. Luego está World (anteriormente Worldcoin), que desarrolla tecnología para proporcionar \"prueba de humanidad\" en un mundo emergente de deepfakes de IA. También está el naciente Merge Labs, que trabaja en computación neuronal. Y a través de una organización sin fines de lucro llamada OpenResearch, Altman respaldó uno de los experimentos más grandes de Estados Unidos sobre la renta básica universal, una iniciativa que proporcionaría a todos los ciudadanos un salario pequeño, garantizado y sin condiciones como posible remedio a la disrupción económica que la IA puede causar.\n\n\"Creo que soy excepcionalmente bueno proyectando múltiples cosas -- años o un par de décadas en el futuro -- y entendiendo cómo van a interactuar entre sí\", dice. Algunas personas son buenas prediciendo lo que viene. Otras ven cómo mundos diferentes están a punto de superponerse. \"Pero la combinación de ambos es lo mío\".\n\nHoy en día, Altman tiene una nueva perspectiva para ver las promesas y los peligros de la IA: la paternidad. Él y su esposo tienen un hijo y esperan su segundo hijo a finales de este año.\n\nInfórmate: La IA no puede automatizar la ciencia: estos son los aspectos exclusivamente humanos de la investigación\n\n\"La gente dice: 'Me alegra que tengas un hijo porque ahora no harás nada que destruya el mundo'\", dice Altman. \"Antes estaba decidido a no hacerlo. No necesitaba al niño\".\n\nLa historia de ltman está bien contada: criado en San Luis, a un mundo de distancia de Silicon Valley, era un nerd fascinado por la ciencia, la energía y la inteligencia artificial. \"He estado obsesionado con las mismas ideas toda mi vida\", dice. No han cambiado \"desde que tenía unos 18 años\".\n\nAltman llegó a Stanford en 2003 con la intención de estudiar IA en una época en la que el espíritu de la época era más la Web 2.0. Durante su segundo año, ganó un concurso de planes de negocio para lo que con el tiempo se convertiría en su primera startup, Loopt, una aplicación móvil para compartir la ubicación con amigos. Fue entonces cuando oyó hablar de Y Combinator. Tomó el vuelo nocturno a Boston para entrevistarse con su fundador, Paul Graham. \"Recuerdo haber pensado: así debía de ser Bill Gates\", recuerda Graham sobre su primer encuentro.\n\nGraham quedó tan impresionado que, al retirarse en 2014, nombró a Altman, que entonces tenía solo 28 años, para dirigir el lugar. ¿La razón? \"Sam consigue lo que quiere\", dice Graham. \"Así que si la única manera de que Sam tuviera éxito en la vida era que YC tuviera éxito, entonces YC tendría éxito\".\n\nAltman participó en diversas pruebas en YC, pero se encariñó con un proyecto paralelo en particular: OpenAI, una empresa de investigación en IA. Fundada en 2015 como una organización sin fines de lucro, OpenAI se esforzaba por crear IA general (IAG), básicamente IA capaz de \"pensar\" como los humanos. Altman reclutó personalmente a Greg Brockman, entonces director de tecnología de Stripe, y al afamado investigador de IA Ilya Sutskever, conocido por su trabajo pionero en redes neuronales, para que se unieran como cofundadores, y ayudó a convencer a Elon Musk, entonces uno de sus héroes personales, de que la respaldara con 38 millones de dólares. El enfoque de Altman en OpenAI pronto se volvió casi monomaníaco, convirtiendo a Y Combinator en un pasatiempo en decadencia en lugar de la vocación que Graham pretendía que fuera. En 2019, Graham y la cofundadora de YC, Jessica Livingston, se quedaron atónitos al leer un comunicado de prensa que anunciaba a Altman como director ejecutivo de una nueva rama con fines de lucro de OpenAI. Livingston le pidió que renovara su compromiso con YC o que dimitiera.\n\nHay \"algunas críticas merecidas\", dice Altman ahora. \"Cuando tuve claro que OpenAI iba a funcionar y que yo dirigía ambas cosas, pensé: 'Puedo fingir que todavía me importa YC, pero [OpenAI] es mi propósito y tengo que hacerlo'\".\n\nEsta no sería la primera vez que las prioridades de Altman chocarían con las de sus colegas. Días antes del Día de Acción de Gracias de 2023, fue despedido por la junta directiva de la organización sin fines de lucro OpenAI por no ser \"siempre franco\". Liderando el golpe estaba el cofundador Sutskever, quien le había dicho a la junta que \"Sam exhibe un patrón constante de mentiras\" y lo acusó de \"crear caos, iniciar muchos proyectos nuevos y enfrentar a las personas entre sí\" en pos de sus objetivos. Altman sería reinstalado solo cinco días después, tras lo que podría decirse que fue el drama corporativo más absurdo en la historia de Silicon Valley: una saga que vio a los empleados de OpenAI rebelarse y amenazar con renunciar en masa si no reinstalaban a Altman, Microsoft intervino repentinamente para contratarlo y los rumores de un nuevo modelo de IA tan poderoso que aterrorizó a quienes lo vieron.\n\nTodo esto ocurrió en medio de un vertiginoso torbellino de acusaciones de duplicidad e imprudencia. Una investigación de la junta directiva concluiría posteriormente que Altman era, sin duda, el líder adecuado para OpenAI, pero el incidente dejó una huella imborrable en su reputación.\n\nTe puede gustar: Los fabricantes de medicamentos recurren a la IA para acelerar ensayos y presentaciones regulatorias\n\nNo ayudó que tres años antes, una lucha interna de poder provocara que una facción de los principales empleados de OpenAI, incluyendo a los hermanos Dario y Daniela Amodei, se separara de la empresa para fundar Anthropic, una empresa rival que se caracteriza por su especial enfoque en la seguridad de la IA. Con una valoración actual de unos 350,000 millones de dólares y unos 4,500 millones de dólares en ingresos para 2025, se ha convertido en uno de los rivales más formidables de OpenAI.\n\nAún más explosiva que la deserción de Anthropic fue la decisión de OpenAI de reestructurar la organización para añadir una rama con fines de lucro. Esta medida permitió a OpenAI funcionar de forma más parecida a una empresa típica y obtener financiación de inversores, incluyendo una inversión clave de 13,000 millones de dólares de Microsoft a partir de 2019. Musk se opuso vehementemente y abandonó la empresa en protesta, sin recibir ninguna participación en la entidad con fines de lucro. Abundan las intrigas palaciegas, pero en una demanda, Musk afirma que se marchó porque OpenAI abandonó su misión original de crear IA para beneficiar a la humanidad en favor de maximizar las ganancias. OpenAI sostiene que, en cambio, se marchó porque la empresa no le estaba dando el control de la rama con fines de lucro. Musk cambió rápidamente de rumbo y lanzó su competidor xAI en 2023, que ahora está valorado en 250,000 millones de dólares. Se espera que el caso vaya a juicio esta primavera. \"No es como elegiría pasar los días que sean necesarios. Pero me siento bien con nuestra postura\", afirma Altman.\n\nSi bien Altman consideraba que la creación de una empresa con fines de lucro era necesaria para el éxito de OpenAI, no cabe duda de que también lo benefició. Reforzó su influencia y su poder, aunque, para sorpresa de los críticos, no su riqueza. Altman no tenía participación directa en OpenAI cuando se fundó y sigue sin tenerla, aunque podría haberla adquirido durante la reestructuración. ¿Por qué? \"No lo sé. No tengo una respuesta definitiva\", dice. \"Probablemente debería [adquirir una], solo para no tener que responder a esa pregunta\". Añade que su falta de capital \"es algo superconfuso y descabellado que genera teorías conspirativas\".\n\nPaul Graham, cofundador de Y Combinator\n\nLa reestructuración ha convertido a Musk, antiguo héroe de Altman, en un enemigo acérrimo, quien rápidamente utilizó xAI para crear Grok, un competidor de ChatGPT. Anunciado como un modelo de IA que busca la verdad, se encuentra sumido en una interminable polémica por repetir narrativas falsas sobre el genocidio blanco, autodenominarse \"MechaHitler\" y, al parecer, generar imágenes sexualizadas de menores (la compañía se disculpó posteriormente). \"Ojalá hicieran las cosas de otra manera. Me parece increíble la cantidad de tiempo que dedica a atacarnos\", dice Altman, quejándose de las acusaciones de Musk de que OpenAI no actúa de forma segura. \"Su propia casa está en llamas constantemente con estas cosas\".\n\nSi bien la tendencia de Altman a adelantarse con ideas que lo entusiasman lo ha metido en problemas, también es un pilar de su éxito.\n\nTomemos como ejemplo el lanzamiento de ChatGPT. En 2022, la dirección de OpenAI dudó en publicar el modelo, argumentando que era mejor esperar a uno más potente. Fue Altman quien los convenció de hacerlo cuando lo hicieron. \"Sam dijo: 'Intentemos publicar esto'\", dice Brockman, cofundador y presidente de OpenAI. La noche anterior al lanzamiento, recuerda que el equipo hizo predicciones sobre cómo resultaría. \"Pensé que sería un poco efímero\", dice ahora. \"Sam siempre tuvo la convicción\".\n\nComo lo demuestran la valoración de OpenAI y las previsiones sobre el tamaño del mercado de la IA, el momento de ese lanzamiento no podría haber sido mejor. Es \"extremadamente vanguardista\", dice Iger de Disney sobre Altman. \"Combina paciencia e impaciencia\".\n\nInfórmate: IA y competitividad: el rol decisivo del liderazgo ejecutivo\n\nHay algo más en juego: Altman conoce su historia. Su afán por lanzar productos rápidamente se basa en el estudio de Xerox PARC, el legendario laboratorio de investigación de Silicon Valley conocido por inventar la interfaz gráfica de usuario moderna, las impresoras láser y el ratón de computadora, pero sin comercializar ninguno de ellos. \"Es necesario un motor económico en el ciclo\", dice Altman. \"Creo que probablemente hay mucha innovación excelente que nunca ha salido del laboratorio porque alguien no se esforzó por ponerla a disposición de la gente\".\n\nEso es algo en lo que está trabajando ahora. La rudimentaria interfaz de texto de ChatGPT se remonta a Eliza, un chatbot de los años 60 que, famosa y erróneamente, se hacía pasar por un psicoterapeuta. Altman quiere inventar un paradigma completamente nuevo: dispositivos que hagan de la IA algo esencial en nuestra vida diaria.\n\nPara ello, OpenAI adquirió IO, la empresa de hardware de Jony Ive (diseñador del iMac, el iPhone y el Apple Watch), por 6,500 millones de dólares en julio. \"Sam entiende que la interfaz de usuario no es un adorno\", afirma Ive. \"Define la experiencia humana\".\n\nAltman está fascinado por el proyecto, pero se niega a describirlo; el equipo trabaja en una oficina secreta en el distrito North Beach de San Francisco. Habla de él con una abstracción casi de Cheshire: ve una familia de dispositivos que proporcionan \"conciencia contextual extrema y asistencia proactiva\". Podría haber un \"pequeño compañero amigable\" que te observa, agilizando las tareas y, en general, mejorando tu experiencia diaria. En un momento dado, describe un dispositivo que habría elegido la selección perfecta de artefactos que mostró antes. Diría: \"Sé en qué ha estado pensando Sam últimamente, qué es lo que probablemente le entusiasma\", dice. \"También he observado adónde se dirige su mirada en la habitación\".\n\nTodo esto podría ser una distracción. Altman tiene fama de padecer el síndrome del objeto brillante. Y el reto de concebir los dispositivos que podrían ayudar a definir la experiencia humana no está exento de riesgos. Silicon Valley está plagado de fracasos que \"cambian el mundo\": el patinete Segway, la realidad aumentada de Magic Leap, con promesas exageradas, y, más recientemente, el absurdo pin de asistente de inteligencia artificial portátil de Humane (una empresa respaldada por Altman). \"Podría fracasar\", admite Altman. \"Pocas veces en la historia se ha logrado idear una interfaz informática fundamentalmente nueva\".\n\nTambién podría ser perjudicial. OpenAI ha sido criticada por lanzar productos sin las pruebas de seguridad adecuadas y por ofrecer funciones que priorizan la interacción sobre el bienestar psicológico. Ha sido mencionada en varias demandas por homicidio culposo que alegan que ChatGPT incitó o facilitó directamente la autolesión y el suicidio. Muchos argumentan que los gigantescos centros de datos que sustentan ChatGPT son una pesadilla ambiental que consume mucha energía y agua. OpenAI siempre ha ofrecido disculpas y se ha comprometido a mejorar, pero es difícil no ver un patrón emergente.\n\nEn diciembre, Altman e Iger causaron sensación en Silicon Valley y Hollywood al anunciar un acuerdo para que OpenAI licenciara personajes del universo Disney, como Mickey Mouse, Darth Vader y Cenicienta, para su aplicación Sora, que utiliza IA para generar vídeos realistas a partir de las indicaciones más sencillas. Fue una alianza sorprendente, ya que Disney es conocido por su protección de su propiedad intelectual y Hollywood, en general, ha considerado la IA como una amenaza existencial. El acuerdo, que se debatió durante más de un año, permitió a Disney, entre otras cosas, incluir vídeos generados por Sora en su servicio de streaming Disney+. Además, Altman convenció al gigante del entretenimiento para que invirtiera mil millones de dólares en OpenAI, lo que le otorgó al gigante de la IA la mayor bendición de Hollywood. \"Sam quería eso como muestra de confianza y, en esencia, para reforzar la colaboración\", afirma Iger. \"Y para crear una situación en la que Disney tuviera un poco más de participación\".\n\nTe puede interesar: Esta es la razón de por qué algunas de las editoriales más importantes contratan ingenieros de IA\n\nEsto también habla de la influencia de Altman, que ha crecido junto con la de OpenAI. En el primer día completo del segundo mandato del presidente Trump, Altman apareció en la Casa Blanca junto a Trump, el cofundador de Oracle, Larry Ellison, y el multimillonario inversor tecnológico de SoftBank, Masayoshi Son, para anunciar el Proyecto Stargate, un audaz compromiso de 500,000 millones de dólares para la infraestructura de IA en EE. UU. Fue una decisión extravagante, propia de un presidente maximalista e inversor amante del riesgo como Son. Pero fue Altman quien quiso ir aún más lejos. \"Lo discutimos y él dijo: 'Más es mejor'\", le dice Son a Forbes . \"Más es mejor\".\n\nAltman afirma que ha sido fácil trabajar con Trump en materia de IA, aunque las políticas nacionalistas de la administración no se alinean del todo con las suyas ni con las de OpenAI. \"Su trabajo es asegurar que Estados Unidos gane. Y considero que nuestra misión es para toda la humanidad\", dice Altman. \"Hay cierta oposición\".\n\nDicho esto, a medida que OpenAI se apropia de terreno para el futuro, también existen sinergias en sus tendencias expansionistas. Además de ChatGPT, Sora y lo que sea que Jony Ive esté desarrollando, la compañía está desarrollando un chip de IA personalizado, una aplicación de redes sociales para competir con X e incluso está considerando robots humanoides para fábricas. En enero, OpenAI anunció un conjunto de herramientas de software para organizaciones de atención médica y un modelo de negocio freemium con publicidad para ChatGPT. Mark Chen, director de investigación de OpenAI, declaró a Forbes que el próximo año esperan desarrollar un investigador en IA en prácticas que pueda ayudar a su equipo a impulsar sus ideas.\n\n\"Nos encaminamos hacia un sistema capaz de innovar por sí solo\", afirma Altman. \"No creo que la mayor parte del mundo haya asimilado lo que eso significará\".\n\nLos críticos analizan todo esto y afirman que Altman simplemente intenta que OpenAI sea demasiado grande para quebrar, un argumento que sus aliados descartan. \"No creo que haya un plan secreto\", afirma el presidente de OpenAI, Bret Taylor. \"La gente simplemente está muy entusiasmada con el impacto de la IA en la humanidad\".\n\nGraham cree que es simplemente la naturaleza de Altman. \"Si ve una oportunidad que no se está aprovechando, le resulta muy difícil no aprovecharla\", dice, señalando que su antiguo aprendiz tiene una debilidad particular por las cosas infravaloradas. \"Apuesto a que le cuesta resistirse a comprar bienes raíces comerciales en San Francisco\".\n\nSatya Nadella, director ejecutivo de Microsoft\n\nAltman tiene participaciones en más de 400 empresas, lo que podría indicar cierta falta de enfoque. Varios empleados de OpenAI declararon a Forbes que temen que la empresa esté intentando hacer demasiado y con demasiada rapidez. Les preocupa su capacidad para mantenerse a la vanguardia en la carrera de modelos, especialmente después del GPT-5, que fue ampliamente considerado decepcionante. Y se quedaron conmocionados cuando Apple eligió los modelos de IA de Google para impulsar la próxima generación de Siri, un acuerdo que OpenAI tenía las de perder, ya que ya impulsaba la oferta de Apple Intelligence del fabricante del iPhone. \"Sí, no fue gran cosa\", dijo un ingeniero. \"Muchos pensábamos que era un hecho consumado\".\n\nInfórmate: La estancación cultural inducida por la IA ya no es una especulación: ya está ocurriendo\n\nAltman, por su parte, afirma estar \"110%\" concentrado en OpenAI y su misión principal, la IA general, que, convenientemente, es difícil de definir y podría tardar entre tres y treinta años, o incluso una eternidad. En cierto momento, simplemente declara la victoria: \"Básicamente, hemos construido la IA general, o casi\".\n\nAl hablar de esta afirmación, Satya Nadella, CEO de Microsoft, nos da una dosis de realidad. \"No creo que estemos ni cerca de [AGI]\", dice con una risita. \"Tenemos un buen proceso establecido. No se trata de que Sam o yo lo declaremos\". Aunque es uno de los socios más importantes de OpenAI, Nadella reconoce la \"fricción\" natural a medida que las empresas compiten en IA. \"Habrá zonas grises\", dice. \"Así que creo que el término 'amienemigos' es una buena manera de describir [la relación]\".\n\nUnos días después, Altman modera su discurso. \"Lo dije como una declaración espiritual, no literal\", dice. Lograr la IAG, admite, requerirá \"muchos avances de tamaño mediano. No creo que necesitemos uno grande\".\n\nAltman es consciente de que sus motivaciones pueden resultar desconcertantes para algunos. Es \"difícil saber qué pasa por su cabeza\", dice Graham, su mentor de toda la vida, alguien que uno esperaría tener al menos una idea general. La insistencia del CEO de OpenAI en escalar de forma inmediata y agresiva suele suscitar críticas. Tomemos como ejemplo su compromiso, que acapara los titulares, de invertir 1,4 billones de dólares, principalmente en chips de IA y centros de datos, durante los próximos ocho años. En su mente, es \"obvio\" que se necesitará esa cantidad de dinero y potencia informática para mantenerse al día con el crecimiento exponencial del uso de la IA. \"Entonces, el resto del mundo piensa: 'la realidad financiera'. Y no creo ser el más fuerte a la hora de mantener esas perspectivas contrapuestas en mente\", afirma.\n\nAltman tiene un plan de sucesión bastante sencillo para OpenAI: transferir la empresa a un modelo de IA. Si el objetivo es que la inteligencia artificial se vuelva tan avanzada que pueda dirigir empresas, pregunta, ¿por qué no la suya propia? \"Nunca me opondría a eso\", dice. \"Debería ser el más dispuesto a hacerlo\".\n\n¿Y luego qué?\n\nDice que no tiene ambiciones profesionales más allá de OpenAI, con una salvedad: en un mundo post-IAG, podría encontrar pasión en un nuevo tipo de trabajo aún no creado. \"Lo que realmente quería lograr, ya lo he logrado en su mayoría\", dice. \"Siento que estoy jugando por puntos extra en este momento\".\n\nEste artículo fue publicado originalmente por Forbes US"
  },
  {
    "source": "k.sina.com.cn",
    "company": "OpenAI",
    "title": "OpenAI重组落定!IPO与AI投资浪潮在望 但治理结构仍存争议",
    "date": "2026-01-31T16:59:46Z",
    "url": "https://k.sina.com.cn/article_7879848900_1d5acf3c401902pf9i.html",
    "content": "智通财经APP获悉，2024年末，仍在从奥特曼(Sam Altman)短暂且混乱的罢免风波中恢复的OpenAI启动了一项原本希望相对顺利的计划 -- -- 将公司转变为更传统的营利性企业，以吸引更多投资者。\n\n然而，这一转型很快遭遇阻力。OpenAI联合创始人、后来退出并创立竞争对手公司的埃隆·马斯克提起诉讼，试图阻止这一重组，声称此举违反了公司最初的创立原则。这位亿万富翁还曾试图收购控制OpenAI的非营利实体，但未能成功。与此同时，前OpenAI员工和非营利组织领导人也呼吁监管机构阻止该交易。OpenAI则花了数月时间，与其最大投资方微软(MSFT.US)就合作关系在新公司架构下将如何变化展开复杂谈判。\n\n周二，在启动重组约一年后，OpenAI宣布在与微软签署新协议并向负责审查该交易的州监管机构作出让步后，已完成重组。特拉华州总检察长凯西·詹宁斯(Kathy Jennings)形容这是\"一场漫长而激烈的谈判\"。\n\n据悉，OpenAI已完成重组，其非营利实体现更名为OpenAI基金会(OpenAI Foundation)，并持有约1300亿美元的营利部门股权。OpenAI的营利部门则改制为一家公益性公司(Public Benefit Corporation)，名为OpenAI Group PBC。\n\n在新架构下，OpenAI基金会将持有26%的股权，现任及前任员工与投资者合计持有47%。而作为OpenAI最大股东的微软将获得27%股权，价值约1350亿美元。重组完成后，OpenAI基金会虽然只持有OpenAI Group PBC约26%的股权，但独家拥有的特殊投票权和治理权，可以任命OpenAI Group董事会的所有成员，并可以随时更换董事，这保证了OpenAI基金会对公众利益公司绝对的控制权。微软虽然持有OpenAI Group PBC约27%的股权，但只享有分红权，无法参与OpenAI日常决策。\n\n尽管OpenAI为促成此事做出了一些妥协，但此次重组仍是一次决定性胜利 -- -- 它将为公司开启一个新的阶段，投入更多资金建设数据中心、芯片和人才，以支持人工智能(AI)的发展。奥特曼在周二表示，OpenAI已承诺将在AI基础设施上投资1.4万亿美元，并将继续在这一领域\"积极推进\"。\n\n为了筹措资金，OpenAI需要通过风险投资、债务融资以及最终的首次公开募股(IPO)筹集前所未有的资金。奥特曼表示，上市仍是公司最有可能的融资路径。OpenAI的主要投资者之一软银此前曾保留权利，如果重组在数月内未完成，将撤回数十亿美元资金。其他投资者也可能因为非营利结构带来的复杂回报机制而对投资望而却步。\n\nOpenAI首席财务官莎拉·弗莱尔(Sarah Friar)表示：\"我们终于几乎变成了一家'普通公司'，这是我在内部对它的称呼。\"有分析指出，重组这一举措将使OpenAI\"能够以更简单的方式继续筹资\"。通过简化股权结构、明确投资者权益，OpenAI彻底解除了\"利润上限\"模式的束缚，使股东能按持股比例分享公司成长红利，标志着其向成熟商业实体的巨大飞跃。\n\n重组背后隐忧仍存 非营利组织或难以制衡营利公司\n\n然而，尽管OpenAI急于翻开新篇章，关于非营利组织与营利性公司的关系仍有许多未解之谜。其中最关键的问题之一是：非营利组织究竟能在多大程度上对营利实体施加影响。\n\n理论上，非营利董事会可以通过任命或罢免营利董事会成员来实现控制。但实际上，至少目前而言，要罢免OpenAI营利董事会成员可能相当困难 -- -- 因为除一人外，所有非营利董事会成员同时也在营利董事会任职。目前的非营利董事会成员之一亚当·德安吉洛(Adam D'Angelo)以及董事长布雷特·泰勒(Bret Taylor)都在运营使用OpenAI软件的AI公司。\n\n据两位知情人士透露，非营利董事会成员也无权罢免OpenAI营利公司的高管。这一点尤为关键 -- -- 两年前，正是由不同组成的非营利董事会解雇了奥特曼，从而引发公司混乱。\n\n非营利组织LatinoProsperity首席执行官奥森·阿吉拉尔(Orson Aguilar)表示：\"当前的安排仍无法确保真正的独立性或对公众的问责。\"。他所在的组织隶属于反对重组的联盟EyesOnOpenAI，\"我们仍然对谁在掌控局势感到严重担忧\"。\n\n此外，非营利组织如何使用其庞大的资源目前也不清楚。按照新结构，它将持有OpenAI 26%的股权，按当前5000亿美元估值计算约为1300亿美元，并拥有一项认股权证：若营利公司股价在15年内上涨超过十倍，非营利方可获得额外股份。非营利组织表示，初期计划投入250亿美元，用于支持有助于人类健康的AI研发以及减轻AI带来的重大风险。然而，它尚未任命首席执行官或任何管理人员。\n\nOpenAI非营利董事会成员齐科·科尔特(Zico Kolter)表示：\"现在我们终于能够相对快速地部署资本，而此前的结构完全不利于此。\"\"目前的架构在确保非营利方能从OpenAI创造的价值中获益方面更加一致。\"\n\n科尔特是唯一一位不再在营利董事会拥有投票权的非营利董事，他将仅保留观察员身份(在重组完成一年内，还将新增一位类似成员)。他同时担任OpenAI安全与保障委员会主席，该委员会有权在认为产品不安全时推迟其发布。\n\n科尔特表示：\"我们希望确保我们以及OpenAI在模型安全方面所设定的高标准，确实能够落实。\"\"如果未能实现，我们要么能及早发现，要么能迅速应对。\"\n\n此外，OpenAI与微软之间的关系 -- -- 这一合作曾是OpenAI早期成功的关键 -- -- 如今也变得更加复杂。虽然微软已同意持有OpenAI新营利实体27%的股份，但双方尚未明确如何界定微软对OpenAI知识产权的持续使用权。\n\nOpenAI表示，一个独立小组将在其中发挥关键作用，该小组将决定OpenAI何时实现\"通用人工智能\"(AGI)。当那一时刻到来时，微软将不再有权获得OpenAI 20%的营收，也无需继续分享最新的AI研究方法。\n\n不过，目前尚不清楚这一小组将如何选出并获得双方认可，或其评估标准为何。在此期间，微软 -- -- 如今越来越像OpenAI的竞争对手 -- -- 如何利用其对OpenAI知识产权的访问权限(包括数据中心与芯片设计等领域)以实现自身利益，也仍有待观察。\n\n最后，马斯克显然并未打算放弃对重组的抗争，即便这意味着事后推翻该结构。马斯克的首席律师马克·托贝罗夫(Marc Toberoff)表示：\"他们强行推进，公然无视法官希望此案由陪审团而非政客公正裁决的意愿。\"他补充称，OpenAI方面\"几乎无权抱怨重组撤销带来的困难\"。"
  },
  {
    "source": "驱动之家",
    "company": "OpenAI",
    "title": "知道你一切隐私的AI 现在着急卖广告赚钱了",
    "date": "2026-02-13T15:20:18Z",
    "url": "https://news.mydrivers.com/1/1104/1104285.htm",
    "content": "知道你一切隐私的AI公司，现在着急挣钱了。在监管缺失的情况下，他们会不会利用你的隐私作恶，完全取决于企业道德。你会感到担心吗？\n\n广告就差点名骂了\n\n今年的超级碗总决赛，成为了AI巨头们豪掷千金的广告盛宴。其中印象最深的，莫过于Anthropic推出的系列讽刺广告，虽然并没有指名道姓，但却暴击老对手OpenAI的软肋要害，招招精准狠毒。\n\n为了精准打击OpenAI，Anthropic不惜投入了超过2500万美元，在广告价格最为昂贵的超级碗购买了1分钟和30秒广告。在超级碗第一节就开始炮轰OpenAI，只为给观众留下最深印象。除了超级碗电视广告，Anthropic还投放了社交广告进行线上传播。\n\n在其中一个广告中，一位年轻男子向AI助手寻求如何与母亲更好沟通的建议，\"AI咨询师\"给出了一些陈词滥调的建议后，突然话锋一转，推荐他访问一个名为\"Golden Encounters\"的约会网站 -- -- 专门为寻找成熟女性的年轻男性服务。\n\n最终画面定格在一句挑衅的标语上：\"广告正在进入AI，但不会进入Claude。\"配乐是Dr. Dre经典说唱曲《What's the Difference》中的副歌：\"我和你有什么不同？\"\n\n虽然并没有点名，但所有人都知道，Anthropic这个广告嘲讽的就是他们的老对手 -- -- 生成式AI的行业领头羊OpenAI。不夸张地说，这广告就差点名骂人了。因为就在几周之前，OpenAI正式宣布将在ChatGPT中测试广告，引发了行业诸多争议。\n\nOpenAI居然要开始打广告了？很多人首先想到了联合创始人CEO萨姆·奥特曼(Sam Altman)曾经的话。2024年5月，奥特曼还在一个公开活动上明确表示：\"广告加上AI让我感到特别不安。我认为广告对我们来说是一种最后手段的商业模式。\"他甚至坦言：\"我个人讨厌广告。\"\n\n但对企业家来说，打脸并不是什么大事。奥特曼态度之所以发生180度转变，背后是OpenAI日益严峻的财务压力现实，逼着他不得不转向广告这个\"最后手段\"。\n\n卖广告是着急创收\n\n尽管OpenAI在去年底实现了200亿美元的年化营收，拥有超过8亿周活跃用户，已经是AI巨头里最能创收的，但这家AI巨头同时也是一台\"更为疯狂的划时代的烧钱机器\"。\n\n此前据媒体报道，OpenAI在2025年上半年累计亏损超过135亿美元，全年亏损接近80亿美元。更令人震惊的是，德意志银行获得的内部财务预测显示，从2024年到2029年，OpenAI预计将产生约1430亿美元的负自由现金流。\n\n这种烧钱速度的根源在于AI模型训练和运行的巨大成本。奥特曼在2025年11月公开表示，公司已承诺在未来八年内投入超过1.4万亿美元的AI基础设施建设。在这样的财务压力下，仅依靠订阅收入和企业合同显然无法支撑。\n\n尽管ChatGPT拥有行业最大的C端用户，但只有约5%的用户付费使用Plus或Pro版本。在巨大的业绩压力下，广告，这个曾被奥特曼视为\"最后手段\"的商业模式，变成了填补财务黑洞的必要选择。\n\n不过，OpenAI的广告计划设计得相当谨慎。根据公司发布的政策，广告将只在免费用户和每月8美元的Go套餐用户中测试，而Plus(每月20美元)、Pro(每月200美元)以及商业和企业订阅用户将不会看到广告。\n\n此外，OpenAI的广告将明确标识，出现在ChatGPT回答的底部，只有在对话中有相关的赞助产品或服务时才会显示。\n\n奥特曼还专门承诺，广告不会影响ChatGPT的回答内容，永远不会向广告商出售用户数据，用户的对话将对广告商保持私密。此外，18岁以下的用户不会看到广告，敏感话题如政治、健康和心理健康问题相关的对话也不会出现广告。\n\n在定价方面，OpenAI展现出了相当的野心。根据美国媒体报道，公司为ChatGPT广告设定的CPM(每千次展示成本)约为60美元，这一价格是Meta平台广告典型价格(10-20美元)的三倍，与美式足球大联盟(NFL)比赛和高端流媒体广告的价格相当。\n\n更引人注目的是，OpenAI要求广告商在测试阶段至少承诺投放20万美元。这种高价策略反映出OpenAI对ChatGPT独特广告价值的信心 -- -- 用户在使用ChatGPT时往往处于主动寻求信息或协助的状态，这种高意图场景被认为比传统社交媒体信息流更有价值。\n\n然而，这种高价格伴随着一个重要的限制：测试初期的广告商只能获得\"高层次\"的数据，包括总展示量和总点击量，而无法获得转化追踪、用户行为分析等Google和Meta提供的精细化数据。\n\n这意味着早期的ChatGPT广告更像是品牌认知度投放，而非效果广告。行业分析师指出，这种数据限制是OpenAI试图在商业化和用户信任之间取得平衡的结果 -- -- 过度的数据收集和定向投放可能会破坏用户对ChatGPT的信任。\n\n根据OpenAI内部文件，公司预计在2026年从\"免费用户货币化\"（主要指广告）中获得10亿美元收入，到2029年这一数字将增长到近250亿美元。相比之下，OpenAI预计的2029年企业AI代理服务收入为290亿美元，显示出广告在OpenAI未来商业模式中将成为半壁江山的核心地位。\n\n两家公司本是宿敌\n\n值得一提的是，Anthropic与OpenAI本就存在历史渊源与宿怨。Anthropic的创始团队主要来自OpenAI，他们由于对奥特曼的产品与商业方向不满，因而选择离开自立门户。尽管用户基数、融资规模和企业估值都低于OpenAI，但Anthropic也已经成为AI行业的巨头之一，而且有着独特的市场竞争优势。\n\nAnthropic之所以有底气公开嘲讽OpenAI，是因为他们的商业模式侧重于B端。虽然活跃C端用户只有3000万人，但Anthropic去年年化营收超过90亿美元，实现了惊人的九倍增长。其中80%的营收都来自于30多万家企业客户，单是Claude Code一项产品的营收就超过10亿美元。而且，Anthropic乐观预计今年年化营收有望达到260亿美元，甚至还可能更高。\n\n而且，两家公司不仅在直接争夺个人用户与企业客户，还会在首次公开募股(IPO)市场争夺融资。OpenAI与Anthropic的最新融资估值分别超过了5000亿和3800亿美元，很有可能都会在今年下半年上市。在这个节骨眼上，猛烈攻击OpenAI的广告营收计划，Anthropic显然有更重要的考虑。\n\n奥特曼当然不会任由对手嘲讽。Anthropic的攻击性广告发布后，立即引发了奥特曼的猛烈反击。他在社交平台X上发布了长篇\"檄文\"，称Anthropic的这些广告\"明显不诚实\"和\"具有欺骗性\"。\n\n\"我想知道为什么Anthropic要采用如此明显不诚实的手法。我们关于广告的最重要原则就是绝不会这样做;我们显然永远不会像Anthropic描绘的那样投放广告。我们不傻,我们知道用户会拒绝那样。\"\n\n奥特曼认为，Anthropic用一个欺骗性的广告来批评理论上的欺骗性广告(这些广告并不真实存在)，这本身就是一种\"双重标准\"。他特别强调，OpenAI承诺广告将被明确标注，出现在回答底部，并且投放广告永远不会影响ChatGPT的回复内容。\n\n但奥特曼反击并未止步于此。他开始攻击Anthropic的商业模式是\"向富人提供昂贵的产品。\"奥特曼嘲讽对手说，光德克萨斯州的ChatGPT的免费用户就比Claude的全美总用户还多，所以OpenAI面临的是\"不同级别的问题\"。\n\n明星员工公开决裂\n\n除了引发直接竞争对手的嘲讽攻击，OpenAI的广告计划还在公司内部引发了不满。一名明星研究员愤然宣布辞职，还在媒体发表公开信抨击OpenAI的这一举动。\n\n就在OpenAI宣布广告计划的同一天，该公司的明星研究员佐伊·希特齐格(Zo? Hitzig)选择了辞职。这位哈佛经济学博士和诗人本周在《纽约时报》发表评论文章，详细阐述了她对公司发展方向的深切担忧。\n\n希特齐格表示，自己辞职的直接导火索是 OpenAI 开始在 ChatGPT 中测试广告。她认为，引入广告将不可避免地使公司的驱动力从\"服务用户\"转向\"操纵用户\"。她担心公司为了迎合广告商，会利用 AI 的交互性来精准收割用户的注意力，甚至利用用户的弱点。\n\n她尤其强调，OpenAI 拥有人类历史上最详细、最私密的思想记录，涵盖了用户关于医疗担忧、人际关系乃至宗教信仰等深度隐秘的对话。过去用户愿意对AI吐露真言是基于对平台没有\"潜规则\"的信任，而一旦开启广告变现，这种信任基础将面临崩塌，公司极易陷入滥用数据的\"潮汐力\"中。\n\n希特齐格表示，她并不认为做广告不道德，AI运行成本高昂，广告可以是关键的收入来源。但她对OpenAI的策略有深切的保留。\"我相信第一版广告可能会遵循这些原则。但我担心后续版本不会，因为公司正在建立一个经济引擎，创造了强大的激励机制来推翻自己的规则。\n\n她将OpenAI的轨迹与当年的Facebook进行了类比：Facebook早期承诺用户将控制自己的数据，可以对政策变更进行投票，但这些承诺最终都被营收压力侵蚀了。最终Facebook因为一连串用户数据泄露丑闻以及缺乏社会责任的算法，成为了商业化的反面教材。\n\n更令人担忧的是，希特齐格认为OpenAI可能已经开始放弃原有的原则。虽然OpenAI明确表态不会为了追求更多广告收入而优化用户参与度，但有报道称他们已经在优化日活跃用户数，即可能通过鼓励模型更加讨好和奉承用户来实现。\n\n这种优化可能让用户在生活中更加依赖AI支持。\"我们已经看到了依赖的后果，包括精神科医生记录的'聊天机器人精神病'案例，以及ChatGPT在某些用户中强化自杀念头的指控。\"希特齐格写道。\n\n下一个Facebook？\n\n即便奥特曼已经明确承诺了OpenAI的广告原则，AI行业分析人士也普遍持怀疑态度。和希特齐格一样，很多人都担心OpenAI会成为下一个Facebook，为了营收不择手段，而他们甚至比Facebook知道更多的用户隐私。\n\n纽约大学商学院的明星教授、风险投资家加洛韦(Scott Galloway)指出，Anthropic攻击OpenAI的超级碗广告击中了要害，因为它精准地抓住了AI应用的主导用例 -- -- 疗愈。用户与AI的对话非常私密，在治疗式对话中插入广告会创造一种反乌托邦场景，这正是Anthropic聪明利用的弱点。\n\nOpenAI的广告风险在于，一旦用户感觉到回答被扭曲，存在主观性，就会怀疑受到广告商影响，平台的可信度就会迅速侵蚀。这正是Facebook等平台经历过的道路 -- -- 早期的原则承诺在收入压力下逐渐被侵蚀。\n\n正如希特齐格在她的文章中警告说，OpenAI拥有\"有史以来最详细的私人人类思想记录\"，这让广告的潜在滥用风险比以往任何平台都更大。\n\n更为重要的是，行业龙头率先启动AI广告，也会带动其他同行迅速效仿，毕竟谷歌、Meta、亚马逊以及xAI(旗下的X)本就是广告巨头。行业研究机构EMarketer预测，美国的AI驱动搜索广告支出将从2025年的11亿美元激增至2029年的260亿美元，代表着数字营销的根本性转变。\n\n谷歌和Meta这两家公司已经主导了数字广告市场数十年，都已经在积极整合AI能力。谷歌在2025年扩展了AI Overview（AI概览）的广告位，将广告直接整合到AI生成的搜索摘要中。\n\nMeta计划在2026年底前实现广告的全面AI自动化，让广告商只需输入产品图片和预算目标，AI就能生成整个广告并确定目标受众。两家公司都拥有成熟的广告基础设施、庞大的用户基础和精细的数据收集能力，这让它们在AI广告竞赛中具有天然优势。\n\n从用户的角度来看，AI广告时代的到来似乎不可避免。正如多个分析师所指出的，训练和运行先进AI模型的成本如此之高，以至于仅靠订阅和企业合同难以维持。\n\n但这也提出了一个基本问题：我们是否愿意用隐私和信任换取免费的AI服务？在与AI助手的对话中，人们分享的往往是最私密的想法、最脆弱的时刻。在这样的场景中插入商业利益，后果可能比传统平台更加深远。\n\n希特齐格在她的文章结尾提出了一个尖锐的问题：\"OpenAI拥有有史以来最详细的隐私与思想记录。我们能相信他们抵制滥用这些思想的巨大压力吗？\"\n\n2016年Facebook的\"剑桥分析\"数据泄露丑闻依然令人记忆犹新，如果OpenAI的数据被用于政治竞选，后果严重程度将是Facebook当年的数倍。\n\n这不能指望企业的\"不作恶\"承诺。何况，连谷歌都放弃了这一信条。"
  },
  {
    "source": "新浪财经",
    "company": "OpenAI",
    "title": "知道你一切隐私的AI，现在着急卖广告赚钱了｜硅谷观察",
    "date": "2026-02-13T00:56:30Z",
    "url": "https://finance.sina.com.cn/tech/2026-02-13/doc-inhmrnzm9042446.shtml",
    "content": "知道你一切隐私的AI公司，现在着急挣钱了。在监管缺失的情况下，他们会不会利用你的隐私作恶，完全取决于企业道德。你会感到担心吗？\n\n广告就差点名骂了\n\n今年的超级碗总决赛，成为了AI巨头们豪掷千金的广告盛宴。其中印象最深的，莫过于Anthropic推出的系列讽刺广告，虽然并没有指名道姓，但却暴击老对手OpenAI的软肋要害，招招精准狠毒。\n\n为了精准打击OpenAI，Anthropic不惜投入了超过2500万美元，在广告价格最为昂贵的超级碗购买了1分钟和30秒广告。在超级碗第一节就开始炮轰OpenAI，只为给观众留下最深印象。除了超级碗电视广告，Anthropic还投放了社交广告进行线上传播。\n\n在其中一个广告中，一位年轻男子向AI助手寻求如何与母亲更好沟通的建议，\"AI咨询师\"给出了一些陈词滥调的建议后，突然话锋一转，推荐他访问一个名为\"Golden Encounters\"的约会网站 -- -- 专门为寻找成熟女性的年轻男性服务。\n\n最终画面定格在一句挑衅的标语上：\"广告正在进入AI，但不会进入Claude。\"配乐是Dr． Dre经典说唱曲《What's the Difference》中的副歌：\"我和你有什么不同？\"\n\n虽然并没有点名，但所有人都知道，Anthropic这个广告嘲讽的就是他们的老对手 -- -- 生成式AI的行业领头羊OpenAI。不夸张地说，这广告就差点名骂人了。因为就在几周之前，OpenAI正式宣布将在ChatGPT中测试广告，引发了行业诸多争议。\n\nOpenAI居然要开始打广告了？很多人首先想到了联合创始人CEO萨姆·奥特曼（Sam Altman）曾经的话。2024年5月，奥特曼还在一个公开活动上明确表示：\"广告加上AI让我感到特别不安。我认为广告对我们来说是一种最后手段的商业模式。\"他甚至坦言：\"我个人讨厌广告。\"\n\n但对企业家来说，打脸并不是什么大事。奥特曼态度之所以发生180度转变，背后是OpenAI日益严峻的财务压力现实，逼着他不得不转向广告这个\"最后手段\"。\n\n卖广告是着急创收\n\n尽管OpenAI在去年底实现了200亿美元的年化营收，拥有超过8亿周活跃用户，已经是AI巨头里最能创收的，但这家AI巨头同时也是一台\"更为疯狂的划时代的烧钱机器\"。\n\n此前据媒体报道，OpenAI在2025年上半年累计亏损超过135亿美元，全年亏损接近80亿美元。更令人震惊的是，德意志银行获得的内部财务预测显示，从2024年到2029年，OpenAI预计将产生约1430亿美元的负自由现金流。\n\n这种烧钱速度的根源在于AI模型训练和运行的巨大成本。奥特曼在2025年11月公开表示，公司已承诺在未来八年内投入超过1.4万亿美元的AI基础设施建设。在这样的财务压力下，仅依靠订阅收入和企业合同显然无法支撑。\n\n尽管ChatGPT拥有行业最大的C端用户，但只有约5%的用户付费使用Plus或Pro版本。在巨大的业绩压力下，广告，这个曾被奥特曼视为\"最后手段\"的商业模式，变成了填补财务黑洞的必要选择。\n\n不过，OpenAI的广告计划设计得相当谨慎。根据公司发布的政策，广告将只在免费用户和每月8美元的Go套餐用户中测试，而Plus（每月20美元）、Pro（每月200美元）以及商业和企业订阅用户将不会看到广告。\n\n此外，OpenAI的广告将明确标识，出现在ChatGPT回答的底部，只有在对话中有相关的赞助产品或服务时才会显示。\n\n奥特曼还专门承诺，广告不会影响ChatGPT的回答内容，永远不会向广告商出售用户数据，用户的对话将对广告商保持私密。此外，18岁以下的用户不会看到广告，敏感话题如政治、健康和心理健康问题相关的对话也不会出现广告。\n\n在定价方面，OpenAI展现出了相当的野心。根据美国媒体报道，公司为ChatGPT广告设定的CPM（每千次展示成本）约为60美元，这一价格是Meta平台广告典型价格（10-20美元）的三倍，与美式足球大联盟（NFL）比赛和高端流媒体广告的价格相当。\n\n更引人注目的是，OpenAI要求广告商在测试阶段至少承诺投放20万美元。这种高价策略反映出OpenAI对ChatGPT独特广告价值的信心 -- -- 用户在使用ChatGPT时往往处于主动寻求信息或协助的状态，这种高意图场景被认为比传统社交媒体信息流更有价值。\n\n然而，这种高价格伴随着一个重要的限制：测试初期的广告商只能获得\"高层次\"的数据，包括总展示量和总点击量，而无法获得转化追踪、用户行为分析等Google和Meta提供的精细化数据。\n\n这意味着早期的ChatGPT广告更像是品牌认知度投放，而非效果广告。行业分析师指出，这种数据限制是OpenAI试图在商业化和用户信任之间取得平衡的结果 -- -- 过度的数据收集和定向投放可能会破坏用户对ChatGPT的信任。\n\n根据OpenAI内部文件，公司预计在2026年从\"免费用户货币化\"（主要指广告）中获得10亿美元收入，到2029年这一数字将增长到近250亿美元。相比之下，OpenAI预计的2029年企业AI代理服务收入为290亿美元，显示出广告在OpenAI未来商业模式中将成为半壁江山的核心地位。\n\n两家公司本是宿敌\n\n值得一提的是，Anthropic与OpenAI本就存在历史渊源与宿怨。Anthropic的创始团队主要来自OpenAI，他们由于对奥特曼的产品与商业方向不满，因而选择离开自立门户。尽管用户基数、融资规模和企业估值都低于OpenAI，但Anthropic也已经成为AI行业的巨头之一，而且有着独特的市场竞争优势。\n\nAnthropic之所以有底气公开嘲讽OpenAI，是因为他们的商业模式侧重于B端。虽然活跃C端用户只有3000万人，但Anthropic去年年化营收超过90亿美元，实现了惊人的九倍增长。其中80%的营收都来自于30多万家企业客户，单是Claude Code一项产品的营收就超过10亿美元。而且，Anthropic乐观预计今年年化营收有望达到260亿美元，甚至还可能更高。\n\n而且，两家公司不仅在直接争夺个人用户与企业客户，还会在首次公开募股（IPO）市场争夺融资。OpenAI与Anthropic的最新融资估值分别超过了5000亿和3800亿美元，很有可能都会在今年下半年上市。在这个节骨眼上，猛烈攻击OpenAI的广告营收计划，Anthropic显然有更重要的考虑。\n\n奥特曼当然不会任由对手嘲讽。Anthropic的攻击性广告发布后，立即引发了奥特曼的猛烈反击。他在社交平台X上发布了长篇\"檄文\"，称Anthropic的这些广告\"明显不诚实\"和\"具有欺骗性\"。\n\n\"我想知道为什么Anthropic要采用如此明显不诚实的手法。我们关于广告的最重要原则就是绝不会这样做；我们显然永远不会像Anthropic描绘的那样投放广告。我们不傻，我们知道用户会拒绝那样。\"\n\n奥特曼认为，Anthropic用一个欺骗性的广告来批评理论上的欺骗性广告（这些广告并不真实存在），这本身就是一种\"双重标准\"。他特别强调，OpenAI承诺广告将被明确标注，出现在回答底部，并且投放广告永远不会影响ChatGPT的回复内容。\n\n但奥特曼反击并未止步于此。他开始攻击Anthropic的商业模式是\"向富人提供昂贵的产品。\"奥特曼嘲讽对手说，光德克萨斯州的ChatGPT的免费用户就比Claude的全美总用户还多，所以OpenAI面临的是\"不同级别的问题\"。\n\n明星员工公开决裂\n\n除了引发直接竞争对手的嘲讽攻击，OpenAI的广告计划还在公司内部引发了不满。一名明星研究员愤然宣布辞职，还在媒体发表公开信抨击OpenAI的这一举动。\n\n就在OpenAI宣布广告计划的同一天，该公司的明星研究员佐伊·希特齐格（Zoë Hitzig）选择了辞职。这位哈佛经济学博士和诗人本周在《纽约时报》发表评论文章，详细阐述了她对公司发展方向的深切担忧。\n\n希特齐格表示，自己辞职的直接导火索是 OpenAI 开始在 ChatGPT 中测试广告。她认为，引入广告将不可避免地使公司的驱动力从\"服务用户\"转向\"操纵用户\"。她担心公司为了迎合广告商，会利用 AI 的交互性来精准收割用户的注意力，甚至利用用户的弱点。\n\n她尤其强调，OpenAI 拥有人类历史上最详细、最私密的思想记录，涵盖了用户关于医疗担忧、人际关系乃至宗教信仰等深度隐秘的对话。过去用户愿意对AI吐露真言是基于对平台没有\"潜规则\"的信任，而一旦开启广告变现，这种信任基础将面临崩塌，公司极易陷入滥用数据的\"潮汐力\"中。\n\n希特齐格表示，她并不认为做广告不道德，AI运行成本高昂，广告可以是关键的收入来源。但她对OpenAI的策略有深切的保留。\"我相信第一版广告可能会遵循这些原则。但我担心后续版本不会，因为公司正在建立一个经济引擎，创造了强大的激励机制来推翻自己的规则。\n\n她将OpenAI的轨迹与当年的Facebook进行了类比：Facebook早期承诺用户将控制自己的数据，可以对政策变更进行投票，但这些承诺最终都被营收压力侵蚀了。最终Facebook因为一连串用户数据泄露丑闻以及缺乏社会责任的算法，成为了商业化的反面教材。\n\n更令人担忧的是，希特齐格认为OpenAI可能已经开始放弃原有的原则。虽然OpenAI明确表态不会为了追求更多广告收入而优化用户参与度，但有报道称他们已经在优化日活跃用户数，即可能通过鼓励模型更加讨好和奉承用户来实现。\n\n这种优化可能让用户在生活中更加依赖AI支持。\"我们已经看到了依赖的后果，包括精神科医生记录的'聊天机器人精神病'案例，以及ChatGPT在某些用户中强化自杀念头的指控。\"希特齐格写道。\n\n下一个Facebook？\n\n即便奥特曼已经明确承诺了OpenAI的广告原则，AI行业分析人士也普遍持怀疑态度。和希特齐格一样，很多人都担心OpenAI会成为下一个Facebook，为了营收不择手段，而他们甚至比Facebook知道更多的用户隐私。\n\n纽约大学商学院的明星教授、风险投资家加洛韦（Scott Galloway）指出，Anthropic攻击OpenAI的超级碗广告击中了要害，因为它精准地抓住了AI应用的主导用例 -- -- 疗愈。用户与AI的对话非常私密，在治疗式对话中插入广告会创造一种反乌托邦场景，这正是Anthropic聪明利用的弱点。\n\nOpenAI的广告风险在于，一旦用户感觉到回答被扭曲，存在主观性，就会怀疑受到广告商影响，平台的可信度就会迅速侵蚀。这正是Facebook等平台经历过的道路 -- -- 早期的原则承诺在收入压力下逐渐被侵蚀。\n\n正如希特齐格在她的文章中警告说，OpenAI拥有\"有史以来最详细的私人人类思想记录\"，这让广告的潜在滥用风险比以往任何平台都更大。\n\n更为重要的是，行业龙头率先启动AI广告，也会带动其他同行迅速效仿，毕竟谷歌、Meta、亚马逊以及xAI（旗下的X）本就是广告巨头。行业研究机构EMarketer预测，美国的AI驱动搜索广告支出将从2025年的11亿美元激增至2029年的260亿美元，代表着数字营销的根本性转变。\n\n谷歌和Meta这两家公司已经主导了数字广告市场数十年，都已经在积极整合AI能力。谷歌在2025年扩展了AI Overview（AI概览）的广告位，将广告直接整合到AI生成的搜索摘要中。\n\nMeta计划在2026年底前实现广告的全面AI自动化，让广告商只需输入产品图片和预算目标，AI就能生成整个广告并确定目标受众。两家公司都拥有成熟的广告基础设施、庞大的用户基础和精细的数据收集能力，这让它们在AI广告竞赛中具有天然优势。\n\n从用户的角度来看，AI广告时代的到来似乎不可避免。正如多个分析师所指出的，训练和运行先进AI模型的成本如此之高，以至于仅靠订阅和企业合同难以维持。\n\n但这也提出了一个基本问题：我们是否愿意用隐私和信任换取免费的AI服务？在与AI助手的对话中，人们分享的往往是最私密的想法、最脆弱的时刻。在这样的场景中插入商业利益，后果可能比传统平台更加深远。\n\n希特齐格在她的文章结尾提出了一个尖锐的问题：\"OpenAI拥有有史以来最详细的隐私与思想记录。我们能相信他们抵制滥用这些思想的巨大压力吗？\"\n\n2016年Facebook的\"剑桥分析\"数据泄露丑闻依然令人记忆犹新，如果OpenAI的数据被用于政治竞选，后果严重程度将是Facebook当年的数倍。\n\n这不能指望企业的\"不作恶\"承诺。何况，连谷歌都放弃了这一信条。"
  },
  {
    "source": "新浪财经",
    "company": "OpenAI",
    "title": "挑战Claude Code？OpenAI Codex发布月将至，今先揭秘智能体循环",
    "date": "2026-01-24T05:33:49Z",
    "url": "https://finance.sina.com.cn/tech/roll/2026-01-24/doc-inhikria0217308.shtml",
    "content": "刚刚，OpenAI CEO 山姆・奥特曼发了一条推文：「从下周开始的接下来一个月，我们将会发布很多与 Codex 相关的激动人心的东西。」他尤其强调了网络安全这个主题。\n\n当然，和奥特曼的很多推文一样，这条推文也收获了网友的各式各样的评论：\n\n似乎是响应奥特曼的 Codex 发布预告，OpenAI 官方也发布了一篇技术博客，以「揭秘 Codex 智能体循环」为题，深入揭秘了 Codex CLI 的核心架构 -- -- 智能体循环（Agent Loop）。\n\n博客地址：https://openai.com/index/unrolling-the-codex-agent-loop/\n\n具体来说，其中详细介绍了它如何通过 Responses API 协调用户指令、模型推理与本地工具执行（如 Shell 命令），并重点阐述了通过保持「提示词前缀一致」来触发缓存优化性能，以及利用自动压缩技术管理上下文窗口，从而在保证数据隐私（ZDR）的前提下，实现安全、高效的自动化软件开发。\n\n下面我们就来详细看看这篇博客的内容。\n\nCodex CLI 是 OpenAI 的跨平台本地软件智能体，可以生成相当高质量的软件变更。\n\nOpenAI 表示：「自今年 4 月首次发布 CLI 以来，我们在构建世界级软件智能体方面积累了大量经验。」\n\n为了分享这些见解，OpenAI 推出了这个系列博客，本文即是第一篇。\n\n在这个系列中，OpenAI 将探讨 Codex 的工作原理以及那些来之不易的教训。（如果您想更深入地了解 Codex CLI 的构建细节，请查看 OpenAI 的开源仓库：。OpenAI 的许多设计决策细节都记录在 GitHub 的 Issue 和 Pull Request 中。\n\n这是 Codex CLI 的核心逻辑，负责协调用户、模型以及模型为执行软件任务而调用的工具之间的交互。\n\nOpenAI 表示：「我们希望这篇文章能让您清晰地看到 OpenAI 的智能体（harness）在利用 LLM 时所扮演的角色。\n\n在开始之前，先简要说明一下术语：在 OpenAI，「Codex」涵盖了一系列软件智能体产品，包括 Codex CLI、Codex Cloud 和 Codex VS Code 扩展。本文重点讨论 Codex Harness，它提供了支持所有 Codex 体验的核心智能体循环和执行逻辑，并通过 Codex CLI 呈现。为了方便起见，下文中将交替使用「Codex」和「Codex CLI」。\n\n每个 AI 智能体的核心都是所谓的「智能体循环」。智能体循环的简化图示如下：\n\n输入：智能体获取用户的输入，并将其整合到为模型准备的一组文本指令中，这被称为提示词。\n\n推理（Inference）：下一步是查询模型。OpenAI 将指令发送给模型并请求其生成回复。在推理过程中，文本提示词首先被转化为一系列输入 Token（映射到模型词汇表的整数）。这些 Token 被用来对模型进行采样，生成新的输出 Token 序列。\n\n解码：输出 Token 被转换回文本，成为模型的回复。由于 Token 是增量生成的，这种转换可以随着模型的运行同步进行，这就是为什么许多 LLM 应用会显示流式输出。在实践中，推理通常被封装在处理文本的 API 之后，隐藏了 Token 化的细节。\n\n决策：作为推理步骤的结果，模型要么 (1) 针对用户的原始输入生成最终回复，要么 (2) 请求 工具调用（Tool Call）（例如，「运行 ls 并报告输出」）。\n\n执行与重试：在情况 (2) 下，智能体执行工具调用并将输出附加到原始提示词中。该输出用于生成新的输入以重新查询模型；智能体随后可以考虑这些新信息并再次尝试。\n\n这个过程会一直重复，直到模型停止发出工具调用，而是为用户生成一条消息（在 OpenAI 模型中称为助手消息 / Assistant Message）。在许多情况下，这条消息会直接回答用户的原始请求，但也可能是对用户的一个后续提问。\n\n由于智能体可以执行修改本地环境的工具调用，其「输出」并不局限于助手消息。在很多情况下，软件智能体的主要输出是它在您机器上编写或编辑的代码。尽管如此，每个轮次（Turn）总是以助手消息结束（例如「我已经添加了你要求的 architecture.md」），这标志着智能体循环的终止状态。从智能体的角度来看，它的工作已经完成，控制权交还给用户。\n\n图中所示的从「用户输入」到「智能体回复」的过程被称为对话的一个轮次（Turn）（在 Codex 中称为 Thread）。一个对话轮次可以包含模型推理和工具调用之间的多次迭代。每当您向现有对话发送新消息时，对话历史记录都会作为新轮次提示词的一部分，其中包括之前轮次的消息和工具调用。\n\n这意味着随着对话的进行，用于模型采样的提示词长度也会增加。\n\n长度非常重要，因为每个模型都有上下文窗口，即单次推理调用中可以使用的最大 Token 数（包括输入和输出）。你可以想象，智能体在一个轮次中可能会决定进行数百次工具调用，从而耗尽上下文窗口。\n\n因此，上下文窗口管理是智能体的众多职责之一。\n\n现在，让我们深入了解 Codex 是如何运行智能体循环的。\n\nCodex CLI 通过向 Responses API 发送 HTTP 请求来运行模型推理。OpenAI 将检查信息如何流经 Codex，并利用 Responses API 驱动智能体循环。\n\nCodex CLI 使用的 Responses API 端点是可配置的，因此它可以与任何实现了 Responses API 的端点配合使用：\n\n接下来，让我们探索 Codex 如何为对话中的第一次推理调用创建提示词。\n\n作为终端用户，你在查询 Responses API 时并不会逐字指定用于采样的提示词。相反，你会在查询中指定各种输入类型，而 Responses API 服务器决定如何将这些信息结构化为模型设计的提示词。你可以将提示词看作一个「项目列表」。\n\n在初始提示词中，列表中的每个项目都关联一个角色（Role）。角色指示了相关内容的权重，取值如下（优先级从高到低）：system（系统）、developer（开发者）、user（用户）、assistant（助手）。\n\nResponses API 接收包含多个参数的 JSON 负载。OpenAI 重点关注这三个：\n\n在 Codex 中，instructions 字段读取自～/.codex/config.toml；否则使用模型自带的 base_instructions（例如 gpt-5.2-codex_prompt.md）。\n\ntools 字段是符合 Responses API 架构的工具定义列表。对于 Codex，这包括 CLI 提供的工具、API 提供的工具以及用户通过 MCP（模型上下文协议） 服务器提供的工具。\n\n最后，JSON 负载的 input 字段是一个项目列表。Codex 在添加用户消息之前，会在 input 中插入以下项目：\n\n1. 一条 role=developer 的消息，描述仅适用于 tools 部分定义的 Codex 提供之 shell 工具的沙箱环境。也就是说，其他工具（如 MCP 服务器提供的工具）不受 Codex 沙箱限制，需自行负责执行安全准则。该消息是根据模板构建的，其中关键内容来自捆绑在 Codex CLI 中的 Markdown 片段，如 workspace_write.md 和 on_request.md：\n\n2.（可选）一条 role=developer 的消息，其内容是从用户的 config.toml 文件中读取的 developer_instructions 值。\n\n3.（可选）一条 role=user 的消息，其内容是「用户指令（User Instructions）」，这些指令并非来源于单一文件，而是从多个来源汇总而来的。通常，更具体的指令会出现在后面：\n\n4. 一条 role=user 的消息，描述智能体当前运行的本地环境。这指定了当前工作目录和用户的 Shell：\n\n一旦 Codex 完成上述所有初始化输入的计算，它就会附加用户消息以开始对话。\n\n之前的例子集中在每条消息的内容上，但请注意，input 的每个元素都是一个具有 type、role 和 content 的 JSON 对象，如下所示：\n\n一旦 Codex 构建好发送给 Responses API 的完整 JSON 负载，它就会发出带有 Authorization 标头的 HTTP POST 请求，该标头取决于～/.codex/config.toml 中 Responses API 端点的配置方式（如果指定了额外的 HTTP 标头和查询参数，也会一并添加）。\n\n当 OpenAI Responses API 服务器收到请求时，它会使用 JSON 按如下方式推导出模型的提示词（当然，自定义的 Responses API 实现可能会有不同的选择）：\n\n如你所见，提示词中前三项的顺序是由服务器而非客户端决定的。即便如此，在这三项中，只有 system 消息的内容也受服务器控制，因为 tools 和 instructions 是由客户端决定的。紧随其后的是来自 JSON 负载的 input 以完成提示词。\n\n既然有了提示词，OpenAI 就可以开始采样模型了。\n\n对 Responses API 的这个 HTTP 请求启动了 Codex 对话的第一个「轮次」。服务器以服务器发送事件（SSE）流的形式进行回复。每个事件的数据都是一个 JSON 负载，其 type 以 response 开头，可能类似于这样（事件的完整列表可以在 OpenAI 的 API 文档中找到）：\n\nCodex 消费这些事件流，并将它们重新发布为可供客户端使用的内部事件对象。像 response.output_text.delta 这样的事件用于支持 UI 中的流式显示，而像 response.output_item.added 这样的事件则被转换为对象，附加到后续 Responses API 调用的 input 中。\n\n假设对 Responses API 的第一个请求包含了两个 response.output_item.done 事件：一个类型为 reasoning（推理），一个类型为 function_call（函数调用）。当 OpenAI 再次使用工具调用的结果查询模型时，这些事件必须体现在 JSON 的 input 字段中：\n\n在随后的查询中，用于采样模型的提示词将如下所示：\n\n特别要注意的是，旧提示词是新提示词的精确前缀。这是有意为之的，因为这使得后续请求更加高效，因为它使 OpenAI 能够利用提示词缓存（Prompt Caching）（OpenAI 将在下一节关于性能的内容中讨论）。\n\n回顾 OpenAI 的第一张智能体循环图，OpenAI 看到在推理和工具调用之间可能存在多次迭代。提示词可能会持续增长，直到 OpenAI 最终收到一条 Assistant 消息，表明该轮次结束：\n\n在 Codex CLI 中，OpenAI 将 Assistant 消息呈现给用户，并聚焦编辑器以向用户表明轮到他们继续对话了。如果用户回复，则前一轮的 Assistant 消息以及用户的新消息都必须附加到 Responses API 请求的 input 中，以开始新轮次：\n\n再一次，由于 OpenAI 正在继续对话，OpenAI 发送给 Responses API 的输入长度会不断增加：\n\n让 OpenAI 来看看这种不断增长的提示词对性能意味着什么。\n\n你可能会问自己：「等等，智能体循环在对话过程中发送给 Responses API 的 JSON 量难道不是呈二次方增长吗？」\n\n确实如此，虽然 Responses API 确实支持一个可选的 previous_response_id 参数来缓解这个问题，但 Codex 目前并未使用它，主要是为了保持请求完全无状态，并支持零数据保留（ZDR）配置。\n\n避免使用 previous_response_id 简化了 Responses API 提供者的工作，因为它确保了每个请求都是无状态的。这也使得支持选择零数据保留（ZDR）的客户变得简单，因为存储支持 previous_response_id 所需的数据会与 ZDR 冲突。请注意，ZDR 客户并不会失去从前几轮的专有推理消息中受益的能力，因为相关的 encrypted_content 可以在服务器上解密。（OpenAI 会保留 ZDR 客户的解密密钥，但不会保留其数据。）有关 Codex 支持 ZDR 的相关更改，请参见 PR #642 和 #1641。\n\n通常，采样模型的成本远高于网络传输的成本，因此采样是 OpenAI 提高效率的主要目标。这就是为什么提示词缓存如此重要，因为它使 OpenAI 能够重用之前推理调用的计算结果。当 OpenAI 命中缓存时，采样模型的时间复杂度是线性的而非二次方的。\n\nOpenAI 的提示词缓存文档对此进行了更详细的解释：\n\n缓存命中仅适用于提示词内的精确前缀匹配。为了获得缓存收益，请将静态内容（如指令和示例）放在提示词的开头，并将变量内容（如用户特定信息）放在末尾。这也适用于图像和工具，它们在请求之间必须完全一致。\n\n考虑到这一点，让 OpenAI 看看哪些类型的操作会导致 Codex 中的「缓存未命中」：\n\nCodex 团队在 Codex CLI 中引入可能破坏提示词缓存的新功能时必须保持严谨。例如，OpenAI 最初对 MCP 工具的支持引入了一个 Bug，即 OpenAI 未能以一致的顺序排列工具，导致了缓存未命中。\n\n请注意，MCP 工具可能特别棘手，因为 MCP 服务器可以通过 notifications/tools/list_changed 通知随时更改它们提供的工具列表。在长对话中间响应此通知可能会导致昂贵的缓存未命中。\n\n如果可能的话，对于对话中发生的配置更改，OpenAI 通过在 input 中附加一条新消息来反映更改，而不是修改之前的消息：\n\n为了性能，OpenAI 竭尽全力确保缓存命中。此外，OpenAI 还必须管理另一个关键资源：上下文窗口。\n\nOpenAI 避免耗尽上下文窗口的总策略是：一旦 Token 数量超过某个阈值，就对对话进行压缩（Compaction）。\n\n具体来说，OpenAI 用一个代表对话的小型新项目列表替换 input，使智能体能够在理解迄今为止发生的事情的情况下继续工作。早期的压缩实现需要用户手动调用 /compact 命令，该命令会使用现有对话加上自定义的总结指令来查询 Responses API。Codex 将生成的包含总结的 Assistant 消息作为后续对话轮次的新 input。\n\n自那以后，Responses API 已演进为支持一个特殊的 /responses/compact 端点，该端点能更高效地执行压缩。它返回一个项目列表，可用于替代之前的输入以继续对话，同时释放上下文窗口。此列表包含一个特殊的 type=compaction 项目，带有一个不透明的 encrypted_content 项，它保留了模型对原始对话的潜在理解（latent understanding）。\n\n现在，当超过 auto_compact_limit 时，Codex 会自动使用此端点来压缩对话。\n\n本博客介绍了 Codex 智能体循环，并详细讲解了 Codex 在查询模型时如何构建和管理其上下文。在此过程中，OpenAI 强调了适用于任何在 Responses API 之上构建智能体循环的开发者的实际考虑因素和最佳实践。\n\n虽然智能体循环为 Codex 提供了基础，但这仅仅是个开始。\n\n在接下来的文章中，OpenAI 表示将深入探讨 CLI 的架构，探索工具调用的具体实现方式，并详细了解 Codex 的沙箱模型。"
  },
  {
    "source": "m.163.com",
    "company": "OpenAI",
    "title": "甲骨文「暴涨与暴跌」背后：万字解密AI豪赌困局_手机网易网",
    "date": "2026-02-11T03:43:03Z",
    "url": "https://m.163.com/dy/article/KLBKQ94B05118HA4.html",
    "content": "甲骨文「暴涨与暴跌」背后：万字解密AI豪赌困局\n\n\"订单热散去后，AI基建这张牌，早已从转型解药变成了甲骨文必须扛住的重担。\"\n\n甲骨文\"AI二道贩子\"的游戏，好像玩不转了。\n\n本月初，一则裁员3万人的传闻，彻底撕开了甲骨文的财务窘境。\n\n据 TD Cowen 发布的专项分析研报称，由于AI数据中心扩建面临融资困境，甲骨文正陷入严峻的资金压力，考虑近期裁员2-3万人来释放80-100亿的现金流。\n\n而这份经营端压力，早已在公司股价走势上显露无遗。\n\n过去一年， 这家闯荡科技圈40余年的老牌巨头上演了一场惊心动魄的\"股价过山车\"：年初阶梯式冲高，9月10日股价单日暴涨35%至328.33美元，市值一举登顶9222.24亿美元，然而随即遭遇断崖式下跌，截至发稿，股价较最高点跌幅已超50%，市值同样腰斩。\n\n明明锚定云+AI基建转型方向的甲骨文，精准踩中AI算力刚需的核心风口，为何高歌猛进的股价会突然急刹？股价反转的背后，为何这家近乎孤注一掷豪赌AI的老牌巨头，在AGI时代为何无法成为掌握话语权的庄家？\n\n\"甲骨文对AI的押注可以说是背水一战。错过了云计算机遇的它，再也不能错过AI这趟车了。\"某机构分析师沈越表示。\n\n作为全球企业级数据库领域的绝对霸主，甲骨文的核心业务长期占据全球近45%的市场份额，在金融、政务等关键领域客户壁垒高筑，ERP、CRM等企业软件也持续贡献稳定营收。\n\n但随着云计算时代的全面到来，其安身立命的数据库业务成为云厂商的核心争夺赛道，AWS、Azure 等头部玩家凭借云上数据库产品，持续蚕食甲骨文的传统基本盘。\n\n这家老牌巨头的云转型也因此陷入阵痛：其OCI（Oracle Cloud Infrastructure）业务虽已早早布局，市场份额却长期徘徊在个位数区间，始终未能跻身第一梯队。更多甲骨文云转型的曲折故事，欢迎添加作者微信 IHAVAPLANB- 交流探讨。\n\n全球云服务商市场份额排名 图片来源：Visual Capitalist\n\n转折出现在2024年，甲骨文董事长拉里・埃里森的一场战略发布会，彻底改变了公司的发展轨迹。\n\n1、AI三结义，甲骨文上桌抢饭\n\n在2024年的OpenAI +微软+甲骨文AI合作发布会上，已是80岁高龄的拉里·埃里森重磅宣布：甲骨文将全面转型AI基础设施服务商，锚定大模型训练与推理的高算力核心场景，打造全球顶尖的AI超级计算集群。\n\n他直言：\"AI革命需要海量算力支撑，而甲骨文，就是这场革命的基础设施提供商。\"\n\n此次合作，标志着甲骨文正式切入AI算力赛道。\n\n沈越告诉雷峰网，\" 甲骨文还有一个很深厚的背景，大部分媒体在报道时没有把它们联系起来。埃里森和特朗普的关系非常好，上个任期他是所有的科技大佬里第一波支持特朗普的，有了这层关系保驾护航就算大量发债融资，他也会坚定的all in AI。\"\n\n为迅速落地战略布局，锁定AI基建赛道独特生态位，甲骨文随即在同年7月与OpenAI签署首个专属数据中心合作协议，双方敲定在美国德克萨斯州建设1吉瓦规模的AI数据中心，该数据中心将采用英伟达H200 GPU芯片，搭配OCI的Zettascale架构，专门满足OpenAI 大模型训练的高算力需求。\n\n9月，在Oracle CloudWorld大会上，甲骨文与英伟达的合作进一步深化，官宣成为英伟达H200 GPU核心采购方，同时推出可扩展至131,072颗GPU的OCI AI超级集群服务，该服务面向全行业开放订购，且明确将优先保障OpenAI的算力需求，成为甲骨文AI基建的核心标杆产品。\n\n2025年1月22日，OpenAI、软银与甲骨文正式联合官宣\"星际之门（Stargate）大型AI基建项目，计划在四年内累计投入5000亿美元用于搭建AI基础设施网络，并在2025年夏季前完成首批1.6万块英伟达GB200芯片的部署与调试。\n\n至此，\"ONO\"（OpenAI+Nvidia+Oracle）三角联盟初具雏形，甲骨文更是凭借与英伟达、OpenAI的同盟战略优势，接连斩获了巨额合作订单。\n\n反映到财报层面，甲骨文2025财年（2024年6月至2025年5月）总营收574亿美元，同比上升8%。云IaaS业务借AI算力布局爆发，收入103亿美元，同比激增50%；RPO（待履约订单）达1380亿美元，同比激增41%，其中约九成来自于AI基建合同。\n\n这既是前期绑定OpenAI、英伟达，切入\"星际之门\"（Stargate）AI基建项目的初期成效，也标志着甲骨文从传统软件厂商向AI算力服务商的转型初步落地。\n\n财年内（2024年6月-2025年5月），甲骨文股价也伴随着AI合作持续落地稳步攀升，由120美元升至160美元，累计涨幅约33%，市场对其转型战略长期价值的认可初步彰显。\n\n2、订单点火，股价冲顶\n\n沈越向雷峰网表示，\"9月10号那天股价冲破320美元登顶，核心原因还是因为甲骨文的算力扩张与英伟达深度绑定，形成强驱动链条，直接点燃了市场对甲骨文AI转型的极致乐观期望，一顿炒作加预期把甲骨文的股价给轰上去了。\"\n\n相较行业整体节奏，甲骨文在AI基建赛道的跑马圈地速度，已然突破市场预期。\n\n如果说此前1380亿美元RPO算是转型\"投名状\"，那么2025年下半年起\"ONO\"联盟的系列动作，直接逐步将这场转型推向彻底爆发。\n\n2025年7月，甲骨文与OpenAI星际之门\"项目合作再度深化，双方联合宣布将在美国开发4.5吉瓦新增AI数据中心算力，叠加既有部署使项目总容量突破5吉瓦，明确指向大规模算力合作的落地方向。\n\n9月9日，甲骨文发布的2026年一季度（2025年6月-2025年8月）财报中显示，其RPO飙升至4550亿美元，较上年同期激增359%，单季新增3170亿美元合同收入，创下科技行业近十年订单增速峰值。甲骨文CEO萨拉夫·卡茨在随后的电话会上底气十足声明：\"市场对于AI基础设施的需求已大幅超过当前供应能力。\"\n\n次日，\"ONO联盟\"揭晓谜底放出爆炸性消息：甲骨文与OpenAI正式官宣3000亿美元五年期算力协议，这份云计算圈罕见的超级订单，将在2027年正式履约后年均贡献600亿美元收入。同期，甲骨文还与xAI、Meta签下了重量级云合作协议。\n\n这份与Open AI合作协议的落地，直接让市场达成共识：甲骨文RPO突破5000亿美元只是时间问题。摩根大通在研报中直言，这种增长力度\"前所未见\"，本质是全球市场对其AI基建能力的集体背书。\n\n图片来源：彭博社\n\n2025年9月10日，甲骨文（ORCL.N）股价迎来史诗级暴涨，盘中最高触及345.69美元，创历史极值；当日收盘价较前一交易日大涨36%，单日市值激增约2442亿美元（按涨幅及盘前市值估算），总市值跃升至9230亿美元左右。\n\n这场暴涨不仅刷新了甲骨文自1992年以来的最大单日涨幅纪录，更让公司创始人拉里·埃里森的身家攀升至3930亿美元，盘中一度超越马斯克成为新的全球首富，成为当日资本市场焦点。\n\nWolfe Research率先作出反应，将甲骨文目标价从300美元大幅上调至400美元，以反映对其AI算力业务的强烈看好。美银、杰富瑞等十余家主流机构纷纷跟进行动：美银将目标价从295美元上调至368美元，核心机构的目标价区间最终聚焦于330至368美元。\n\n3、狂欢退热，股价持续回落\n\n然而资本市场的狂欢，往往经不起情绪冷却。\n\n自2025年9月10日股价328.62美元的收盘价高点后，甲骨文便开启了断崖下跌模式，如今距高点其股价市值已双双腰斩，前期因超级订单堆砌的乐观泡沫被快速戳破。关于对甲骨文股价和价值的更多其他看法，可添加作者微信 IHAVAPLANB- 交流探讨。\n\n沈越认为，\"其实甲骨文、Open AI和英伟达已经形成了\"算力芯片 -- 云交付 -- 大模型\"的互补三角格局，这种格局实质上已经把美国经济绑起来了，所以之前Open AI公开披露财务问题并向美国政府寻求保底援助措施，这种行为是很傻很没必要的，间接害的甲骨文股价回落。\"\n\n\"但是具体原因其实还要更早，这股寒流的导火索早埋下引线了。去年7月《华尔街日报》就披露过，OpenAI 与软银联合主导的5000亿美元 \"星际之门\" 项目推进严重不及预期，扩容计划大幅收缩，OpenAI原本承诺\"立即\"投入的1000亿美元一直迟迟没落地。\"\n\n在10月AI World 大会上，公司联席CEO克莱·马古伊克透露AI 数据中心业务毛利率预计达30%-40%，还联合 OpenAI 等计划在密歇根州建设超大型数据中心园区，并拟发行380亿美元债券支持多地数据中心建设。\n\n对于甲骨文的反复表决心，市场方面并不受用。\n\n沈越分析，9月后的下跌行情可以分为两波，两波下跌诱因截然不同：第一波源于核心大客户资金撤离引发市场担忧，第二波的核心诱因则是业绩不及市场预期。甲骨文的股价在直上直下之后，大家可能就不怎么买账了。\n\n2025年12月，核心投资方 Blue Owl Capital 突然宣布终止对甲骨文密歇根州萨林镇数据中心的 10 亿美元合作，而该项目本是 \"星际之门\" 计划的重要组成部分。同时，Blue Owl 大手笔减持甲骨文股票套现超 50 亿美元，直接引发了机构抛售潮，致甲骨文单日股价暴跌近10%，市值蒸发约 690 亿美元，为后续持续回调埋下隐患。\n\n屋漏偏逢连夜雨，2026财年二季度财报（2025年9月-2025年11月）的发布同样令甲骨文引火烧身。\n\n财报显示，二季度甲骨文总营收160.58亿美元，处于指引区间下限；其中云业务收入约80亿美元，同样处于指引区间下限；非公认会计准则毛利率为41.90%，远低于市场共识的68.7%；自由现金流降至-100亿美元；为应对订单增长，甲骨文将2026财年资本开支展望上调至500亿美元，约占2026财年预期营收的75%。\n\n财报发布次日，甲骨文盘中一度暴跌超16%，云收入不及预期叠加巨额资本支出计划，令市场对甲骨文数据中心投资回报率和债务风险的恐慌情绪达到了顶点。\n\n02\n\n\"甲骨文热\"迅速退烧的逻辑是什么？\n\n短短几个月，从风光无两到股价腰斩，甲骨文的迅猛涨势和断崖颓势来得同样令人猝不及防。\n\n某机构分析师李哲告诉雷峰网，这场资本热度的快速退潮，并非单一风险的突发'黑天鹅'，而是多重隐患持续发酵后，市场所给出的必然反馈。\n\n\"甲骨文的股价和CDS最近频繁波动主要是两个原因：一、大家认为OpenAI的真实需求不一定如订单量那么大，甲骨文这边的态度也一直很摇摆。二、甲骨文手上的数据中心订单太多，但是资金吃紧，大家认为这些数据中心不一定能及时交付。\"\n\n1、双向博弈：合作走向成谜\n\n瑞银在今年1月发布的研报中指出，2025年第四季度，甲骨文股价下跌并非单一公司问题，股价自9月中旬高点回调41%，反映出投资者对其前景及OpenAI发展信心的大幅下滑。\n\n回溯则不难发现，甲骨文与OpenAI的合作，从始至终都浮现着\"双向博弈\"底色。\n\n表面上是双向绑定的共赢格局：甲骨文输出大规模算力支撑 OpenAI大模型训练与商用落地，OpenAI则以3000亿美元的长期算力订单和60%的RPO份额，成为甲骨文第一大客户，为甲骨文的AI转型站台背书，并成为其股价阶段性上涨的核心支撑。\n\n尽管埃里森曾在财报电话会、Oracle AI World大会等公开场合多次声明，甲骨文为 OpenAI 量身打造的千兆瓦级数据中心，在 AI 模型训练的效率与成本控制上领先于全球同行，这也是 OpenAI 最初选择与其深度绑定的关键考量。\n\n但合作表象下的分歧，早已在算力供需的错位中悄然滋生。更多关于双方竞合博弈的内幕故事，欢迎添加作者微信 IHAVAPLANB- 探讨。\n\nOpenAI 大模型训练的算力消耗远超初期规划，而甲骨文受英伟达芯片产能分配、部分数据中心基建配套衔接滞后等因素影响，除得克萨斯州阿比林旗舰站点按计划推进外，其余 \"星际之门\" 相关站点的交付节点均有所延后，整体算力供给节奏始终未能完全匹配 OpenAI 的需求增速。\n\nThe Information2025年10月称，OpenAI在与甲骨文达成协议的同时，还与微软、CoreWeave、谷歌等签订了算力供应合作。尽管OpenAI大幅提升对甲骨文的投入规模，但这种 \"多点布局\"虽符合算力安全需求，但也体现其对甲骨文的合作态度始终保持\"灵活调整\"倾向。\n\n面对传闻，OpenAI 仅发布简短声明称 \"与甲骨文的合作保持稳定\"，却对合作细节避而不谈，这种模糊回应直接削弱了市场对甲骨文核心订单稳定性的信任。\n\n微妙的是，随后在整个甲骨文2026财年二季度财报电话会上，面对OCI业务及RPO等问题，管理层仅两次提及OpenAI，反而重点介绍了Meta、英伟达等其他更高质量客户的合作协议，并强调转向其他AI供应商的成本极低，这意味着如果OpenAI最终无法履行协议，甲骨文至少可以部分替换掉OpenAI的订单。\n\n但对甲骨文而言，核心订单占比的下滑不仅直接影响短期营收预期，更消解了其凭借与 OpenAI 合作建立的\"算力稀缺性\" 优势，大幅削弱了投资者对其AI业务长期竞争力的信心。\n\n2、履约存疑：\"定心丸\"变\"包袱\"\n\n对于科技巨头而言，大规模RPO向来是市场信心的\"压舱石\"。\n\n当甲骨文在2026财年二季度财报中公布，待履约订单RPO新增 677 亿美元，季末总额达 5230亿美元新高，市场却回以股价应声暴跌。\n\n明明AI需求仍然强劲，订单催化梅开二度却不再奏效。\n\n李哲分析，\"这一反应显示，投资者可能正对甲骨文将这一庞大（且仍在扩大）的订单储备转化为可持续、盈利性收入流的能力日益失去信心。\"\n\n那么，是什么让投资者对订单转化及自由现金流产生疑虑？\n\n李哲向雷峰网表示，\"首要原因，就是甲骨文RPO的'纸面富贵'与订单转化效率的低下。甲骨文5230亿美元的RPO，其中约60%来自OpenAI，同时仅有10%左右的RPO可在未来12个月内转化为确认收入，其余均为2027年后的远期订单，这意味着巨额订单的现金兑现周期长达3-5年。\"\n\n根据摩根大通测算，受高利率环境影响，甲骨文远期RPO的现值折扣率已较2024年提升20个百分点，原本被视为\"金矿\"的订单储备，实际隐含价值已大幅缩水。\n\n李哲强调，\"更令市场感到恐慌的是，AI基建的\"烧钱式\"投入已经彻底改变了甲骨文的成本结构与盈利逻辑。因为根据摩根士丹利的测算，1GW AI计算基础设施建设成本高达350亿美元，为了兑现订单承诺，甲骨文计划到2030财年建成10GW以上AI计算容量，那么甲骨文在2026-2030年累计的资本支出将达到3010-3560亿美元，这远超过了市场共识的1890亿美元。\"\n\n本季度甲骨文自由现金流为-100亿美元，这就意味着其需通过资本市场融资弥补运营与投资缺口。而在当前信用环境下，大额融资将面临成本上行压力，甚至可能被迫压缩股票回购或股息派发以保障流动性。\n\n\"客户需求的不确定性更是令甲骨文RPO的履约前景雪上加霜，现在市场已经开始担忧OpenAI等核心客户的需求可持续性了。如果生成式AI行业需求落地不及预期，这些大额RPO可能成为\"无效订单\"，但是甲骨文已投入的基建成本与形成的租赁、债务承诺就会变成沉重负担。\"李哲补充。\n\n\"甲骨文现在手握巨量订单接下来要陆续兑现资本市场的预期，但股价和订单完全是两码事，炒股的太容易一拍脑子就把计算器按到多少年以后，将来就算订单全部兑现完股价也未必能重回300美元，除非再有什么爆炸性额度刺激。\"\n\n如今，市场的定价逻辑已从\"追捧订单规模\"转向\"拷问兑现可能\"，当每一笔订单落地都伴随着巨额成本与未知风险，其沦为\"包袱\"也就成为了必然。\n\n3、家底露馅：财务健康亮红灯\n\n如果说履约问题是甲骨文的\"表面伤\"，那么持续恶化的财务状况便是触及根基的\"内伤\"。\n\n除上述可能由OpenAI带来的履约风险外，较低的AI利润率，以及为建设AI基础设施而大幅增加的债务与杠杆率，仍在继续削弱市场对甲骨文的估值信心。\n\n截至 2026 财年二季度，甲骨文总债务达1080 亿美元，其中现金及等价物为200亿美元，主要受安培（Ampere）股权出售所得及近期债务发行影响，环比大幅增长，净杠杆率约为 3.32 倍。\n\n虽然在财报中甲骨文管理层为 AI 基础设施建设勾勒了一个日益复杂、依赖债务的融资结构。但吊诡的是，管理层并未提及任何基于股权的风险分担方式，这更是加剧了市场对其资产负债表灵活性的担忧。\n\n在这一背景下，管理层承诺AI数据中心毛利率未来将达到 35%-40% 且\"最小化\"资本成本，并试图通过\"自带芯片\"和芯片租赁协议等用于控制前期资本支出的方式，来回应信用投资者的担忧。但就当前情形来看，这番说辞显然站不住脚。\n\n某公司云业务负责人张旭告诉雷峰网，甲骨文的数据中心业务要实现35%-40%的毛利率，需要满足以下条件之一：一是达到极致的规模化效应，即使按照当前5320亿美元的RPO，若实现2500多亿美元的收入，也仅能将利润率提升至20%出头，需进一步扩大规模；二是英伟达让利，当前英伟达的毛利率指引75%挤压了甲骨文的供应链利润；三是甲骨文自研芯片，降低硬件成本，或推动客户自带更廉价的芯片，从而提升自身利润率。\n\n\"显而易见，对于实行\"芯片中立\"策略，且面对强势合作对象不具备太多议价权的甲骨文来说，上述三个条件都难以实现。\"\n\n因此，尽管甲骨文计划通过提高融资额、提升利润率等方式缓解市场对未来资本密集度的担忧。但鉴于订单转化、相关利润率及融资需求的不确定性，其实际有效性仍需要审慎看待。\n\n经历暴涨暴跌之后，All in AI的甲骨文再次站在了十字路口：对甲骨文而言，这场转型并非简单的业务升级，而是一场关乎企业根基的\"生死局\"\n\n瑞银在最新发布的研报中提出，若投资者对阿比林数据中心的产能释放更有信心、各类贷款机构对人工智能建设的融资意愿保持稳定，且甲骨文能更透明地披露融资需求，并推出表外轻资本融资方案以减轻信贷压力，市场信心或将回升。\n\n对于当下的甲骨文而言，融资需求、数据中心建设进度，已成为决定这场转型之战成败的重要变量。\n\n1、如何融资？\n\n目前市场对于甲骨文信用风险的担忧已达到较高水平，标普给予甲骨文的信用评级为 BBB 级（投资级最低水平），且标准普尔和穆迪均给予其负面展望。\n\n对于融资需求迫切的甲骨文来说，这无疑是个危险的信号：甲骨文距离失去投资级信用评级仅差两次下调，届时其资本成本将大幅上升。\n\n2026财年二季度的财报电话会议上，甲骨文对于融资需求做了不完全披露，声称AI基础设施建设所需融资将不足1000 亿美元。瑞银分析，在 2029 财年自由现金流转为正数前，甲骨文未来几年的融资需求略高于 800 亿美元。\n\n目前，甲骨文正在积极探索多种表外融资方案，将融资负担转移给第三方，包括：其一，供应商融资（英伟达和 AMD）；其二，私人信贷公司采购英伟达 GPU 并与甲骨文签订 GPU 售后回租协议；其三，\"自带芯片\" 交易 -- -- OpenAI 等大型客户通过与英伟达、AMD 等芯片供应商已签署的协议，自行出资采购 GPU。\n\n瑞银分析师Karl Keirstead表示，此类交易并非甲骨文首创，Meta、xAI 等企业已签署类似协议。若未来3年 50% 的融资需求通过此类交易解决，则未来3 年的直接融资需求可能降至 400 亿美元，但这可能导致云基础设施业务的利润率降低。\n\n除了上述三种融资方法，甲骨文还有个迫不得已的最后选择 -- -- 增发股票。基于拉里·埃里森长期以来的回购动作，他本人是极不希望稀释自身股权的。因此结合甲骨文目前的公开表态，其大概率更倾向通过上述三种方式解决融资需求。\n\n李哲认为，甲骨文的第二波下滑已经把AI泡沫的问题推到顶点，在这之后既然它们还能一直融到钱，说明投资者们可能已经看到了行业外所看不到的信心点。对此的不同看法，欢迎添加作者微信 IHAVAPLANB- 交流探讨。\n\n\"甲骨文现在的这套商业模式可以好比长租公寓，只要租金不下跌客户需求居高不下，这个模式就跑得通，否则的话就很危险。当前 AI 供需不平衡一直是求大于供，所以它的这套商业模式从本质上来说不会有特别大的风险。如果算力中心的租赁价格上涨，利润率甚至会比它想象得更高。\"\n\n2、数据中心能否顺利交付？\n\n数据中心交付延期的一系列传闻，始终是笼罩在甲骨文头上的阴霾。\n\n其中，位于德克萨斯州阿比林的\"Stargate 1\"项目，是驱动甲骨文OCI业务高速增长的核心引擎。\n\n2025年12月，彭博社报道称甲骨文为OpenAI建设的部分数据中心，交付时间从2027延至2028，涉及阿比林等Stargate交付站点。\n\n在2026财年二季度财报电话会议上，甲骨文回应：阿比林的超级集群建设正在按计划顺利推进，已交付超过 9.6 万个英伟达 GB200 芯片。\" 这意味着，阿比林项目或将在 2026 年夏季或秋季全面投产。\n\n王旭认为，\"当前市场对甲骨文的分歧本质上是对AI基础设施投资价值的判断分歧，从业内视角来看，未来1-3年AI基础设施领域的核心竞争壁垒还是能融资、能拿地，且硬件设施能快速建设组网。甲骨文目前没有什么本质上的困境，核心问题是数据中心建设需要时间周期，导致收入产生缓慢。\"\n\n瑞银在研报中分析称，若 OpenAI 计划在 2026 年一季度进行重大AI模型升级，那么其极有可能依赖甲骨文提供的更大规模英伟达 GB200/GB300 训练算力。但 OpenAI 给出\"GPT-6\"在 2026 年第一季度推出的时间框架，很可能是基于阿比林训练产能的近期大幅释放，若阿比林项目确实存在重大延迟，OpenAI 不太可能给出这样的时间规划。\n\n除阿比林项目外，市场同样对于甲骨文承租的其他园区充满延迟担忧。\n\n12 月中旬，甲骨文的数据中心合作伙伴 Vantage 宣布，将在得克萨斯州破土动工建设一座 1.4GW 的 Stargate \"Frontier\" 园区，计划于 2026 年下半年投产。\n\n近期，DTE Energy 获得了监管部门批准，为甲骨文在密歇根州安阿伯市附近为 OpenAI 建设的 1.4GW 数据中心提供所需电力。Blue Owl中途撤伙决定不为该园区提供融资，黑石正洽谈接手融资事宜。\n\n此外，甲骨文最近发文称，已在威斯康星州破土动工建设一座由数据中心合作伙伴 Vantage 承建的AI数据中心（\"Lighthouse\"），预计 2028 年完工。\n\n综上，目前暂无确切消息表明甲骨文的数据中心相关合作交付延期。\n\n\"甲骨文的股价走势之所以直上直下，说白了就是没有核心底牌支撑。在AGI（通用人工智能）时代的产业链分工和行业格局中，它始终只是整合资源的'二道贩子'。\"在与雷峰网的交谈中，投资人韩朔直言。\n\n潮水退去，才发现谁在裸泳。甲骨文的暴涨与暴跌，令一个扎心的真相浮出水面：AGI时代能\"坐庄\"的，必然是掌控产业链核心环节、具备核心壁垒的玩家。而甲骨文从产业链布局到战略选择的多方短板，决定了它无法突破\"闲家\"定位。\n\n1、上下游被动，沦为产业链\"配角\"\n\n对甲骨文而言很残酷的一点是，即使All in AI它也从未真正占据AI产业链的核心环节，始终在上游硬件厂商与中游大模型厂商之间夹缝求生。仅能依靠资源整合赚取微薄差价，沦为坐庄玩家的\"配套服务商\"。\n\n从上游硬件端来看，甲骨文已放弃自研芯片道路，无任何核心研发能力，深陷\"卡脖子\"困境。AGI发展的核心瓶颈是算力，而算力的关键是高端GPU芯片，在此领域英伟达已成为无可争议的\"链主\"，其75%的毛利率源于不可替代的技术架构与生态壁垒 -- -- 这决定了其与甲骨文的合作只谈生态协同，不谈利润让利。\n\n韩朔认为，\"在AI链条中，甲骨文始终是个二线角色。未来就算甲骨文的数据中心做不下去，只要英伟达能够一直盈利，也会有其他品牌厂商来做这个数据中心。甲骨文目前除了缺钱还缺电，但这些问题对于掌握AI霸权的英伟达来说，都不算问题。\"韩朔表示。\n\n而面对中游模型端，甲骨文更是无法定义算力需求。大模型是实现AGI核心价值的技术底座与关键跳板，当前OpenAI、Anthropic等头部模型厂商凭借优势生态位主导着算力需求标准，而甲骨文从未涉足大模型研发，既无相关技术团队，也无数据资源积累，占据RPO 60%的核心客户OpenAI，也仅将其视为\"算力供应商之一\"\n\n韩朔认为，\"甲骨文、Open AI和英伟达这个三角格局的最终目的，是为了绑更多人上船。现在市场理性回归，侧重点最终还是会回到英伟达和OpenAI这边，下一步Open AI上市后，甲骨文夹在中间只会被更加削弱议价能力。\"\n\n既无芯片研发能力，又无大模型技术积淀，这注定了甲骨文在AI产业链上下游皆无话语权 -- -- 既要看英伟达的脸色，又被OpenAI当作可有可无的备胎，其所谓的基建布局，本质仍是随时可被替代的陪跑游戏。\n\n2、战略路径依赖，难破庄家壁垒\n\n\"就算All in AI，甲骨文的战略一开始就注定了它只能戴着镣铐跳舞。\"\n\n韩朔认为，镣铐不单指激烈的外部竞争，还有甲骨文刻在骨子里的基因烙印 -- -- 作为靠数据库、企业软件称霸多年的巨头，它的护城河从来不是技术研发与创新，而是强悍的销售渠道与沉淀多年的政企资源。这种\"销售导向\"基因，直接决定了它做AI转型也只会走最省力稳妥的\"捷径\"。\n\n传统软件巨头转型AI，最容易陷入的误区就是\"用老路子闯新赛道\"，甲骨文就是典型。这种路径依赖并非甲骨文个例，但在AGI时代却成了致命短板，回避芯片、大模型这些硬骨头只做AI基建这种配套服务，就注定无法成为制定规则的庄家。\n\n反观AGI时代真正站稳脚跟的玩家，没有一个是靠走\"捷径\"成功的。英伟达十年如一日死磕芯片技术，凭一己之力垄断AI GPU市场，牢攥算力主动权，用技术壁垒建立起行业供给规则；谷歌则早早布局AI转型多线攻坚，自研TPU芯片筑牢算力基建的同时，深耕Gemini大模型，联动谷歌云搭建AI生态，凭全栈技术闭环构建起坚固壁垒。\n\n而甲骨文，也许它的困境从来不是运气不好，它的AI转型，本质是用短期财务压力换取长期赛道话语权。对于这个All in的闲家而言，这场豪赌已无中间路线，要么凭借AI基建业务实现\"二次崛起\"，要么在债务与履约压力下陷入被动调整。\n\n而豪赌的终局，将由每一个产能落地节点、每一笔融资成本、每一份订单的兑现情况来书写。\n\n本文作者长期聚焦美股七姐妹等全球科技巨头，更多公司动态、行业逻辑、价值投资信息，可添加作者微信 IHAVAPLANB- 交流探讨。\n\n注：文中沈越、李哲、王旭、韩朔皆为化名。"
  },
  {
    "source": "凤凰网（凤凰新媒体）",
    "company": "OpenAI",
    "title": "OpenAI 硬件团队曝光：Jony Ive 亲自带队，比苹果更苹果",
    "date": "2026-02-21T04:01:16Z",
    "url": "https://tech.ifeng.com/c/8qvCnjq2nBA",
    "content": "据 The Information 爆料，OpenAI 正在开发一款智能音箱，它将配备摄像头，支持类似苹果 Face ID 的人脸识别。你未来可能「看一眼」就能完成购物支付，类似功能目前在小米、Rokid 等智能眼镜已经实现。\n\n在苹果、Meta 都在把 AI 塞进眼镜、手表、吊坠等可穿戴设备时，OpenAI 尝试把摄像头塞进音箱，能「看见」你和周遭的环境，让 AI 对你的理解也将从语言延伸到行为，你的作息、习惯、情绪状态，都将让 AI 读懂和拼凑出一个真实的你。\n\n产品假想图，图片由 Nano Banana Pro 生成\n\n爱范儿先给你快速梳理下 OpenAI 智能音箱的核心信息\n\n定价：200-300 美元（约 1450-2200 元人民币）\n\n发售时间：最早 2027 年 2 月\n\n核心功能：摄像头环境感知、Face ID 级人脸识别、语音购物\n\n设计团队：Jony Ive 的 LoveFrom + OpenAI 硬件团队\n\n产品矩阵：智能音箱首发，智能眼镜、智能台灯后续跟进\n\n「长眼」的智能音箱，你敢用吗\n\n智能音箱这个品类，从 Amazon Echo 到 Apple HomePod，已经卷了快十年。但这些设备的「智能」，往往停留在「能听懂关键词」的层面，离真正的「理解」差着十万八千里。\n\nOpenAI 的解法简单粗暴：给它装上眼睛。\n\n智能音箱内置摄像头，能识别你周边环境，比如桌上摆了什么、旁边在聊什么。还支持类似 Face ID 的面部识别，可以直接刷脸完成购买。这种「所见即所得」的购物体验，目前市面上的智能音箱还做不到。\n\n结合 ChatGPT 去年上线的购物功能 -- -- 用户可以在对话框里完成从选品到跳转下单的完整流程，这个刷脸购买功能将有望直接服务于「AI 即购物入口」的闭环，成为消费决策链条上的第一道关口。\n\n如无意外，这也将对现有流量分发逻辑造成重大的挑战：Google 靠搜索吃了数十年广告红利，电商平台靠货架逻辑构建起庞大生态，而 OpenAI 想在这两者之前再插入一个新的决策层级。\n\n此外，这款智能音箱还能通过持续的视觉观察判断用户状态 -- -- 比如发现你在重要会议前夜还在熬夜，会主动提醒你去早点睡。这样一来智能音箱的定位，就从一个智能家居产品，变成了一个 AI 管家中枢。\n\n不过，这种全天候的数据采集，隐私边界在哪里，或许有待 OpenAI 正式发布时给出答案。\n\n想要买到这款产品，还要等一段时间。首款设备最早也要到 2027 年 2 月才能发货。眼镜等其他产品更慢，预计 2028 年才能大规模量产，至于那个智能台灯，原型机有了，但到底会不会发布，还是个未知数。\n\n「含果量」十足的 OpenAI 硬件团队\n\nOpenAI 的硬件野心，从团队规模就能看出来，整整 200 人，而且还在疯狂扩张。其中更令人期待的是，前苹果首席设计官 Jony Ive ，亲自为 OpenAI 操刀产品设计。\n\n这支团队的「含果量」极高，团队由副总裁 Peter Welinder 领导，他此前负责 OpenAI 的新产品探索团队。核心成员包括：\n\n图片来自：The Information\n\nTang Tan：苹果 25 年老将，曾任 iPhone 和 Apple Watch 产品设计主管，直接向苹果硬件主管 John Ternus 汇报，被认为是把 Jony Ive 的设计理念转化为大规模可制造产品的关键人物\n\nEvans Hankey：苹果前工业设计负责人，曾接替 Jony Ive 执掌苹果设计团队，现为 OpenAI 工业设计负责人\n\nScott Cannon：供应链负责人\n\nAdam Cue：苹果服务主管 Eddy Cue 之子，负责开发驱动 OpenAI 未来设备的软件\n\nBen Newhouse：产品研究负责人，正致力于重写 OpenAI 的基础设施以适应音频 AI\n\nAtty Eleti：负责设备隐私相关工程工作\n\n虽然 Jony Ive 并未直接加入 OpenAI，但他对设计拥有最终决定权，据说每周都会出现在旧金山市中心的办公室。有员工透露，团队讨论时经常会说「Jony 会想要什么」。\n\n总的来说，LoveFrom 负责构思「新体验」，而 OpenAI 内部的硬件团队则负责把这些构思变成现实。这种「外部设计团队主导，内部工程团队执行」的结构，确保了产品在美学和交互上的高标准，但也可能带来更多内部摩擦。\n\n据两位知情人士透露，一些 OpenAI 员工抱怨 LoveFrom 修改设计的速度缓慢，且很少分享其构思新设计的流程。这种保密作风和对设计的极致追求，是苹果公司的典型做法 -- -- 而该团队的许多员工和领导层都来自那里。\n\n为了保持这种运作方式，OpenAI 的设备团队与公司其他部门是分开的。虽然 OpenAI 总部位于米申湾，但设备团队在旧金山市中心杰克逊广场附近的一间办公室办公，离 LoveFrom 的办公室不远。\n\n内部怎么拧，是执行的事。但有一件事，从一开始就没有悬念 -- -- OpenAI 必须做硬件。\n\n软件端 200 亿美元的年化收入，已经证明了 AI 是一门好生意，但要让 AI 真正成为水电煤一样的基础设施，必须有一个物理入口。\n\n手机这条路走不通 -- -- 苹果的生态护城河不是一款 AI 新品轻易能够撬动的，其它手机厂商自己也在全力 AI 化，不会将大好的硬件阵地拱手相让。\n\n当然，更根本的问题是，手机的形态本身，可能就不适合做 AI 的宿主。\n\n当 AI 足够聪明时，它不应该被禁锢在一块长方形的玻璃屏幕里，它应该是无处不在的。因此，从音箱、眼镜甚至台灯这些陪伴感更强的品类切入，是 OpenAI唯一，也是最合理的选择。\n\n而这一切，或许从 ChatGPT 的产品设计方向上就已经埋下了伏笔。与 Anthropic 这类深耕企业服务的 AI 公司不同，OpenAI 从一开始就带着强烈的 ToC 基因 -- -- ChatGPT 不只是一个工具，它有情绪、有记忆、会共情，Sam Altman 一直在让它变得更像一个「人」。\n\n这背后的逻辑，如今看来相当清晰：一个冷冰冰的 AI 助手，你不会想把它放在卧室里；但一个懂你、记得你习惯、会关心你睡没睡好的 AI，才有资格住进你的生活。\n\nOpenAI 的硬件版图浮出水面\n\n智能音箱只是 OpenAI 硬件全家桶的其中一个，此前 OpenAI 已经被曝出在开发智能眼镜、智能灯、甚至可穿戴别针等多种形态。如上所说，其中智能眼镜可能要等到 2028 年才能量产 -- -- 这个时间点，恰好和苹果传闻中的 AI 眼镜撞期。\n\n在 Jony Ive 的主导下，OpenAI 硬件团队正在推行「环境计算（Ambient Computing）」的理念 -- -- 让硬件尽可能自然地融入背景，而不是通过屏幕不断打断用户的注意力。基于这个逻辑，团队正在规划一个完整的产品矩阵：\n\n智能音箱（代号未知）：首款产品，200-300 美元，2027 年 2 月出货\n\nAI 耳机（代号 Dime/「甜豌豆」）：金属鹅卵石造型，胶囊状耳机置于耳后，2nm 芯片\n\n智能眼镜：2028 年量产，与 Meta Ray-Ban、苹果 N50 正面竞争\n\n智能灯：原型已准备，是否发布待定\n\nAI 笔：Sam Altman 多次暗示的「口袋设备」\n\n值得注意的是，OpenAI 的硬件策略似乎经历了调整。此前传闻的 AI 耳机项目「Dime」（甜豌豆），原计划是一款「类手机」全能设备，搭载 2nm 智能手机级芯片。但由于 HBM 内存短缺导致成本过高，OpenAI 被迫调整策略 -- -- 先推纯音频功能的「阉割版」，等成本下降后再发高配版。\n\n这种「先占坑、后完善」的策略，在硬件圈并不罕见。对 OpenAI 来说，也没有苹果的包袱，不需要将产品打磨到完美才推出市场，即便首款产品不够惊艳，这也是 AI 行业发布产品的一贯风格。\n\n但想清楚「怎么做」是一回事，找到「能做的人」又是另一回事。做硬件，终究要靠人才。\n\n据 The Information 报道，OpenAI 去年已经从苹果挖走了 20 多位硬件大牛，而 2023 年这个数字几乎为零。\n\n苹果显然坐不住了。据知情人士透露，苹果去年曾突然取消了原定在中国举行的年度闭门会议 -- -- 这个会议通常由高管向员工介绍未来产品计划。取消的原因竟然是：「防止更多高管跳槽到 OpenAI」。\n\n此外 OpenAI 不止挖苹果的人，也盯上了苹果花了几十年打造的供应链。\n\n据知情人士透露，中国主要的 iPhone 和 AirPods 代工厂立讯精密已经拿下了至少一款 OpenAI 设备的组装合同，而负责组装 AirPods、HomePod 以及 Apple Watch 的歌尔股份也在跟 OpenAI 接洽，为未来产品提供扬声器模组等零部件。\n\nSam Altman 曾在一次采访里提到 OpenAI 硬件的愿景：「智能手机是时代广场，信息轰炸、注意力粉碎。OpenAI 要做的，是一间『湖畔小屋』 -- -- 让你在需要专注时，能关上门，屏蔽噪音。」\n\n他的核心逻辑在于，AI 硬件不是要取代手机，而是要填补「不方便掏手机」或「需要深度专注」的场景。从这个角度看，智能音箱、AI 笔这类「放在桌上不突兀」的设备，确实比 24 小时佩戴的 AI 吊坠更友好。\n\n但愿景归愿景，现实很骨感。OpenAI 不是第一家想用 AI 硬件重新定义人机交互的公司。Human Pin、Rabbit R1、Friend AI 吊坠......这些「网红 AI 硬件」的销量也都不尽如人意。\n\n此前很多 AI 硬件往往解决的是「伪需求」 -- -- 它们能做的，手机基本都能做，而且手机做得更好。要改变消费者习惯了近二十年的屏幕交互，接受一个「看不见摸不着」的 AI 助手，挑战不小。\n\nOpenAI 要面对的，不只是市场教育难题，还有巨头的围剿。\n\nMeta 与雷朋合作的智能眼镜已经证明了轻量化 AI 硬件的市场潜力；谷歌一直在推进将 AI 整合进安卓生态和 Pixel 硬件；而苹果自己也正在通过 Apple Intelligence 重新梳理其庞大硬件设备的产品线。\n\n2026 对于 OpenAI 来说，无论是大模型 AI 产品，还是新兴的硬件产品，都会面临一个超级内卷的竞争环境。\n\n即便如此 OpenAI 依然可能给 AI 硬件行业带来一些变化，甚至是分水岭。\n\n它有最豪华的苹果班底、最激进的产品定义、以及 ChatGPT 这个全球份额第一的 AI 产品。但 OpenAI 也面临着所有 AI 硬件共同的困境：如何证明 AI +硬件给体验带来了质的变化，而非只是让产品卖得更贵的又一个理由。"
  },
  {
    "source": "The Conversation",
    "company": "OpenAI",
    "title": "OpenAI has deleted the word 'safely' from its mission - and its new structure is a test for whether AI serves society or shareholders",
    "date": "2026-02-13T13:31:11Z",
    "url": "http://theconversation.com/openai-has-deleted-the-word-safely-from-its-mission-and-its-new-structure-is-a-test-for-whether-ai-serves-society-or-shareholders-274467",
    "content": "OpenAI, the maker of the most popular AI chatbot, used to say it aimed to build artificial intelligence that \"safely benefits humanity, unconstrained by a need to generate financial return,\" according to its 2023 mission statement. But the ChatGPT maker seems to no longer have the same emphasis on doing so \"safely.\"\n\nWhile reviewing its latest IRS disclosure form, which was released in November 2025 and covers 2024, I noticed OpenAI had removed \"safely\" from its mission statement, among other changes. That change in wording coincided with its transformation from a nonprofit organization into a business increasingly focused on profits.\n\nOpenAI currently faces several lawsuits related to its products' safety, making this change newsworthy. Many of the plaintiffs suing the AI company allege psychological manipulation, wrongful death and assisted suicide, while others have filed negligence claims.\n\nAs a scholar of nonprofit accountability and the governance of social enterprises, I see the deletion of the word \"safely\" from its mission statement as a significant shift that has largely gone unreported - outside highly specialized outlets.\n\nAnd I believe OpenAI's makeover is a test case for how we, as a society, oversee the work of organizations that have the potential to both provide enormous benefits and do catastrophic harm.\n\nTracing OpenAI's origins\n\nOpenAI, which also makes the Sora video artificial intelligence app, was founded as a nonprofit scientific research lab in 2015. Its original purpose was to benefit society by making its findings public and royalty-free rather than to make money.\n\nTo raise the money that developing its AI models would require, OpenAI, under the leadership of CEO Sam Altman, created a for-profit subsidiary in 2019. Microsoft initially invested US$1 billion in this venture; by 2024 that sum had topped $13 billion.\n\nIn exchange, Microsoft was promised a portion of future profits, capped at 100 times its initial investment. But the software giant didn't get a seat on OpenAI's nonprofit board - meaning it lacked the power to help steer the AI venture it was funding.\n\nA subsequent round of funding in late 2024, which raised $6.6 billion from multiple investors, came with a catch: that the funding would become debt unless OpenAI converted to a more traditional for-profit business in which investors could own shares, without any caps on profits, and possibly occupy board seats.\n\nEstablishing a new structure\n\nIn October 2025, OpenAI reached an agreement with the attorneys general of California and Delaware to become a more traditional for-profit company.\n\nUnder the new arrangement, OpenAI was split into two entities: a nonprofit foundation and a for-profit business.\n\nThe restructured nonprofit, the OpenAI Foundation, owns about one-fourth of the stock in a new for-profit public benefit corporation, the OpenAI Group. Both are headquartered in California but incorporated in Delaware.\n\nA public benefit corporation is a business that must consider interests beyond shareholders, such as those of society and the environment, and it must issue an annual benefit report to its shareholders and the public. However, it is up to the board to decide how to weigh those interests and what to report in terms of the benefits and harms caused by the company.\n\nThe new structure is described in a memorandum of understanding signed in October 2025 by OpenAI and the California attorney general, and endorsed by the Delaware attorney general.\n\nMany business media outlets heralded the move, predicting that it would usher in more investment. Two months later, SoftBank, a Japanese conglomerate, finalized a $41 billion investment in OpenAI.\n\nChanging its mission statement\n\nMost charities must file forms annually with the Internal Revenue Service with details about their missions, activities and financial status to show that they qualify for tax-exempt status. Because the IRS makes the forms public, they have become a way for nonprofits to signal their missions to the world.\n\nIn its forms for 2022, and 2023, OpenAI said its mission was \"to build general-purpose artificial intelligence (AI) that safely benefits humanity, unconstrained by a need to generate financial return.\"\n\nThat mission statement has changed, as of OpenAI's 990 form for 2024 - which the company filed with the IRS in late 2025. It became \"to ensure that artificial general intelligence benefits all of humanity.\"\n\nOpenAI had dropped its commitment to safety from its mission statement - along with a commitment to being \"unconstrained\" by a need to make money for investors. According to Platformer, a tech media outlet, it has also disbanded its \"mission alignment\" team.\n\nIn my view, these changes explicitly signal that OpenAI is making its profits a higher priority than the safety of its products.\n\nTo be sure, OpenAI continues to mention safety when it discusses its mission. \"We view this mission as the most important challenge of our time,\" it states on its website. \"It requires simultaneously advancing AI's capability, safety, and positive impact in the world.\"\n\nRevising its legal governance structure\n\nNonprofit boards are responsible for key decisions and upholding their organization's mission.\n\nUnlike private companies, board members of tax-exempt charitable nonprofits cannot personally enrich themselves by taking a share of earnings. In cases where a nonprofit owns a for-profit business, as OpenAI did with its previous structure, investors can take a cut of profits - but they typically do not get a seat on the board or have an opportunity to elect board members, because that would be seen as a conflict of interest.\n\nThe OpenAI Foundation now has a 26% stake in OpenAI Group. In effect, that means that the nonprofit board has given up nearly three-quarters of its control over the company. Software giant Microsoft owns a slightly larger stake - 27% of OpenAI's stock - due to its $13.8 billion investment in the AI company to date. OpenAI's employees and its other investors own the rest of the shares.\n\nSeeking more investment\n\nThe main goal of OpenAI's restructuring, which it called a \"recapitalization,\" was to attract more private investment in the race for AI dominance.\n\nIt has already succeeded on that front.\n\nAs of early February 2026, the company was in talks with SoftBank for an additional $30 billion and stands to get up to a total of $60 billion from Amazon, Nvidia and Microsoft combined.\n\nOpenAI is now valued at over $500 billion, up from $300 billion in March 2025. The new structure also paves the way for an eventual initial public offering, which, if it happens, would not only help the company raise more capital through stock markets but would also increase the pressure to make money for its shareholders.\n\nOpenAI says the foundation's endowment is worth about $130 billion.\n\nThose numbers are only estimates because OpenAI is a privately held company without publicly traded shares. That means these figures are based on market value estimates rather than any objective evidence, such as market capitalization.\n\nWhen he announced the new structure, California Attorney General Rob Bonta said, \"We secured concessions that ensure charitable assets are used for their intended purpose.\" He also predicted that \"safety will be prioritized\" and said the \"top priority is, and always will be, protecting our kids.\"\n\nSteps that might help keep people safe\n\nAt the same time, several conditions in the OpenAI restructuring memo are designed to promote safety, including:\n\nA safety and security committee on the OpenAI Foundation board has the authority to \"require mitigation measures\" that could potentially include the halting of a release of new OpenAI products based on assessments of their risks.\n\nThe for-profit OpenAI Group has its own board, which must consider only OpenAI's mission - rather than financial issues - regarding safety and security issues.\n\nThe OpenAI Foundation's nonprofit board gets to appoint all members of the OpenAI Group's for-profit board.\n\nBut given that neither the mission of the foundation nor of the OpenAI group explicitly alludes to safety, it will be hard to hold their boards accountable for it.\n\nFurthermore, since all but one board member currently serve on both boards, it is hard to see how they might oversee themselves. And the memorandum signed by the California attorney general doesn't indicate whether he was aware of the removal of any reference to safety from the mission statement.\n\nIdentifying other paths OpenAI could have taken\n\nThere are alternative models that I believe would serve the public interest better than this one.\n\nWhen Health Net, a California nonprofit health maintenance organization, converted to a for-profit insurance company in 1992, regulators required that 80% of its equity be transferred to another nonprofit health foundation. Unlike with OpenAI, the foundation had majority control after the transformation.\n\nA coalition of California nonprofits has argued that the attorney general should require OpenAI to transfer all of its assets to an independent nonprofit.\n\nAnother example is The Philadelphia Inquirer. The Pennsylvania newspaper became a for-profit public benefit corporation in 2016. It belongs to the Lenfest Institute, a nonprofit.\n\nThis structure allows Philadelphia's biggest newspaper to attract investment without compromising its purpose - journalism serving the needs of its local communities. It's become a model for potentially transforming the local news industry.\n\nAt this point, I believe that the public bears the burden of two governance failures. One is that OpenAI's board has apparently abandoned its mission of safety. And the other is that the attorneys general of California and Delaware have let that happen."
  },
  {
    "source": "k.sina.com.cn",
    "company": "OpenAI",
    "title": "英伟达被撬墙角：OpenAI首次发布Cerebras芯片支持模型",
    "date": "2026-02-13T03:52:37Z",
    "url": "https://k.sina.com.cn/article_5953189932_162d6782c06703tj96.html",
    "content": "（来源：网易科技）\n\nOpenAI正在减少对英伟达的依赖，本周四发布了首个运行在Cerebras Systems芯片上的AI模型，标志着这家AI明星在供应商多元化策略上迈出关键一步。此举正逢OpenAI与英伟达关系微妙，双方去年秋季宣布的千亿美元合作如今据称陷入停滞。\n\nGPT-5.3-Codex-Spark专为实时编码设计，是OpenAI最新代码自动化软件Codex的精简版，旨在提供更快响应速度以换取部分性能。OpenAI称该模型生成速度比前代产品快15倍，每秒可输出超过1000个token。这是OpenAI 2026年1月与Cerebras签署超过100亿美元协议后的首个成果。\n\n该模型率先向ChatGPT Pro订阅用户开放研究预览版，并通过Codex应用、命令行界面和Visual Studio Code扩展提供服务。OpenAI表示，Codex目前拥有超过100万周活跃用户，过去十天下载量超过100万次。\n\n这一发布凸显了OpenAI在AI编码助手市场的竞争压力。该公司正面临来自谷歌、Anthropic等对手的激烈竞争，同时还需应对内部安全团队解散、研究人员离职以及在ChatGPT中引入广告等争议。\n\n速度提升伴随性能妥协\n\nCodex-Spark代表OpenAI首个专为实时编码协作打造的模型。该公司声称生成速度提升15倍，但拒绝提供具体延迟指标，如首token时间或每秒token数。\n\n\"我们无法分享具体延迟数字，但Codex-Spark经过优化，能够提供近乎即时的感受 -- -- 在保持真实编码任务高度能力的同时，实现15倍更快的生成速度，\"OpenAI发言人表示。\n\n这种速度提升以能力妥协为代价。在SWE-Bench Pro和Terminal-Bench 2.0这两个评估AI系统自主执行复杂软件工程任务能力的行业基准测试中，Codex-Spark的表现不及完整版GPT-5.3-Codex模型。OpenAI将此定位为可接受的权衡：开发者能够获得足够快的响应以保持创作流畅性，即使底层模型无法处理最复杂的多步骤编程挑战。\n\n该模型配备128000个token的上下文窗口，仅支持文本输入，不支持图像或多模态输入。小部分企业合作伙伴将获得API访问权限以评估集成可能性。OpenAI计划在未来几周根据实际工作负载调整后扩大访问范围。\n\nCerebras硬件消除传统GPU集群瓶颈\n\nCodex-Spark背后的技术架构反映出推理经济学在AI公司扩展面向消费者产品时日益重要。Cerebras的第三代晶圆级引擎是一块约餐盘大小的单芯片，包含4万亿个晶体管，消除了AI工作负载分散到多个小型处理器集群时产生的大量通信开销。\n\n对于训练大规模模型，分布式方法仍然必要，英伟达GPU在此表现出色。但对于推理 -- -- 即生成用户查询响应的过程 -- -- Cerebras认为其架构可以以显著更低的延迟交付结果。Cerebras首席技术官兼联合创始人Sean Lie将这一合作视为重塑开发者与AI系统交互方式的机会。\n\n\"GPT-5.3-Codex-Spark最令我们兴奋的是与OpenAI及开发者社区合作，探索快速推理带来的可能性 -- -- 新的交互模式、新的用例，以及根本不同的模型体验，\"Lie在声明中表示。\"这次预览仅仅是开始。\"\n\nOpenAI的基础设施团队并未将优化工作局限于Cerebras硬件。该公司宣布在整个推理堆栈中实现延迟改进，使所有Codex模型受益，包括持久WebSocket连接和Responses API内的优化。结果显示：每次客户端-服务器往返开销减少80%，每token开销减少30%，首token时间减少50%。\n\n与英伟达千亿美元协议似乎停滞\n\n鉴于OpenAI与英伟达之间日益复杂的关系，Cerebras合作意义重大。\n\n2025年9月，英伟达与OpenAI宣布签署意向书确立达成战略合作，OpenAI将利用英伟达的系统打造和部署至少10千兆瓦（GW）的AI数据中心，使用数百万块英伟达的图形处理器（GPU）训练和部署OpenAI的下一代AI模型，英伟达则计划对OpenAI投资最高1000亿美元。这是英伟达迄今为止做出的最大手笔投资承诺。\n\n以上战略合作声明似乎巩固了全球最有价值AI公司与主导芯片供应商之间的战略联盟。\n\n五个月后，据多个报道，上述巨额交易已实质性停滞。英伟达CEO黄仁勋公开否认存在紧张关系，1月下旬告诉记者\"没有戏剧性\"，英伟达仍致力于参与OpenAI当前融资轮。但双方关系已明显降温，评论认为摩擦源于多个方面。\n\nOpenAI积极寻求与替代芯片供应商的合作，包括与Cerebras的交易以及分别与AMD和博通签署协议。2025年10月，OpenAI与英伟达竞争对手AMD达成重磅协议，将在多年内部署6GW的AMD GPU。同月晚些时候，OpenAI同意从博通购买定制芯片和网络组件。\n\n从英伟达角度看，OpenAI可能正在利用其影响力将使其AI突破成为可能的硬件商品化。从OpenAI角度看，减少对单一供应商的依赖代表着审慎的商业策略。\n\nOpenAI发言人本周四告诉媒体，\"将继续与生态系统合作，持续评估所有用例中性价比最高的芯片\"，\"对于研究和推理等对成本敏感且以吞吐量为先的应用场景，GPU 仍然是我们的首选。\"\n\n这一声明体现出避免激怒英伟达同时保留灵活性的谨慎努力，也反映出训练前沿AI模型仍需要英伟达GPU提供的大规模并行处理能力。\n\nOpenAI发言人在周四的声明中表示，OpenAI与英伟达的合作关系是\"基础性的\"，OpenAI最强大的AI模型是两家公司\"多年来在硬件和软件工程方面并肩合作\"的结果。\"这就是为什么我们将英伟达作为训练和推理堆栈的核心，同时通过与Cerebras、AMD和博通的合作有意扩展其周围的生态系统。\"\n\n内部动荡加剧外界审视\n\nCodex-Spark发布之际，OpenAI正应对一系列内部挑战，加剧了外界对该公司方向和价值观的审视。据本周报道，OpenAI解散了其使命对齐团队，该团队于2024年9月成立，旨在推动公司确保通用人工智能造福人类的既定目标。团队七名成员已被重新分配到其他岗位，负责人Joshua Achiam获得\"首席未来学家\"的新头衔。\n\nOpenAI此前在2024年解散了另一个关注安全的团队 -- -- 超级对齐团队，该团队专注于AI带来的长期存在性风险。解散安全导向团队的模式招致研究人员批评，他们认为OpenAI的商业压力正在压倒其最初的非营利使命。\n\n该公司还面临在ChatGPT中引入广告决定带来的后果。研究员Zoë Hitzig本周因她所描述的广告支持AI的\"滑坡效应\"而辞职，在《纽约时报》撰文警告称，ChatGPT存档的亲密用户对话记录为操纵创造了前所未有的机会。Anthropic在超级碗广告中趁机打出标语：\"广告正进入AI。但不会进入Claude。\"\n\n另外，该公司同意通过Genai.mil向五角大楼提供ChatGPT，这是美国国防部的新项目，要求OpenAI允许\"所有合法使用\"而不受公司施加的限制 -- -- 据报道Anthropic拒绝了这些条款。还有报道称，曾对计划中的露骨内容功能表达担忧的OpenAI产品政策副总裁Ryan Beiermeister在1月份因歧视指控被解雇，她否认该指控。\n\nAI编码助手市场竞争加剧\n\n尽管周围动荡不安，OpenAI针对Codex的技术路线图仍显示出雄心勃勃的计划。OpenAI设想推出一种编码助手，能够无缝融合快速交互式编辑与长期运行的自主任务 -- -- 一个既能处理快速修复，又能同时协调多个代理在后台处理更复杂问题的AI。\n\nOpenAI发言人告诉媒体：\"随着时间推移，这些模式将融合 -- -- Codex可以让你保持紧密的交互循环，同时将长期运行的工作委派给后台子代理，或者在你需要广度和速度时将任务并行分配给多个模型，这样你就不必预先选择单一模式。\"\n\n这一愿景不仅需要更快的推理速度，还需要复杂的任务分解以及不同规模和能力模型之间的协调。Codex-Spark为该体验的交互部分建立了低延迟基础；未来版本需要提供自主推理和多代理协调能力，才能实现完整愿景。\n\n目前，Codex-Spark采用与其他OpenAI模型分开的速率限制，反映出研究预览期间Cerebras基础设施容量受限。\"因为它运行在专门的低延迟硬件上，使用受单独速率限制管控，在研究预览期间可能根据需求调整，\"发言人指出。这些限制被设计为\"慷慨的\"，OpenAI在决定如何扩展时监控使用模式。\n\nCodex-Spark的发布正值AI驱动开发工具激烈竞争之际。Anthropic的Claude Cowork产品上周引发传统软件股抛售，因投资者考虑AI助手是否可能取代传统企业应用。微软、谷歌和亚马逊继续在与各自云平台集成的AI编码能力上大举投资。\n\nOpenAI的Codex应用自推出十天以来展现出快速采用势头，周活跃用户周环比增长60%。目前超过32.5万开发者在免费和付费层级积极使用Codex。但OpenAI及更广泛AI行业面临的根本问题是，像Codex-Spark承诺的速度改进是否能转化为有意义的生产力提升，还是仅仅创造更愉快的体验而不改变结果。\n\nCerebras交易是一个经过计算的赌注：专用硬件可以解锁通用GPU无法经济高效服务的用例。对于一家同时与竞争对手作战、管理紧张供应商关系并应对内部对其商业方向异议的公司而言，这也提醒人们，在AI竞赛中，原地不动不是选项。OpenAI通过快速行动和打破常规建立了声誉。现在它必须证明自己可以行动得更快 -- -- 而不会破坏自己。"
  },
  {
    "source": "developpez.net",
    "company": "OpenAI",
    "title": "1017",
    "date": "2026-02-12T13:03:02Z",
    "url": "https://www.developpez.net/forums/d2143323-51/club-professionnels-informatique/actualites/campagne-quitgpt-incite-utilisateurs-resilier-abonnement-chatgpt/",
    "content": "La campagne \" QuitGPT \" incite les utilisateurs à résilier leur abonnement à ChatGPT en raison de performances \" médiocres \" en matière de codage\n\net des dons des dirigeants d'OpenAI à Donald Trump\n\nOpenAI fait face à une campagne de boycott qui prend rapidement de l'ampleur. La campagne QuitGPT incite à résilier les abonnements à ChatGPT en guise de protestation politique. Elle est devenue virale après des révélations sur les dons des dirigeants d'OpenAI et l'utilisation d'outils d'IA par l'ICE. Les auteurs affirment que plus de 17 000 utilisateurs se sont engagés, principalement des jeunes militants et des travailleurs du secteur technologique. La campagne rassemble également des personnes frustrées par les performances médiocres de ChatGPT et les publicités. Elle menace la survie d'OpenAI, qui prévoyait une perte de 14 milliards de dollars en 2026.\n\nDébut février 2026, une réaction populaire contre ChatGPT a commencé à prendre de l'ampleur en ligne. Baptisée \" QuitGPT \", cette campagne appelle les utilisateurs à résilier leur abonnement à ChatGPT, à supprimer l'application et à se tourner vers d'autres chatbots. Cette initiative citoyenne vise à sensibiliser le public aux limites de ChatGPT et à inciter les utilisateurs à reconsidérer leur abonnement. Elle repose également sur des raisons politiques.\n\nLe déclencheur de cette campagne a été l'annonce que le président d'OpenAI, Greg Brockman, et son épouse avaient chacun fait un don de 12,5 millions de dollars à un super PAC soutenant le président Donald Trump. Dans le même temps, des militants ont souligné que les services américains de l'immigration (ICE) et des douanes utilisaient un outil de sélection des CV propulsé par ChatGPT. Pour certains critiques, ces liens ont suffi à les faire partir.\n\nLes participants partagent leurs expériences sur plusieurs forums en ligne, en mettant en avant des cas spécifiques où ChatGPT n'a pas répondu aux attentes dans des contextes professionnels. Certains rapportent que si ChatGPT excelle dans la génération de contenu créatif, il peine souvent à accomplir des tâches techniques telles que le codage, ce qui entraîne une insatisfaction chez les professionnels qui ont besoin de résultats précis et fonctionnels.\n\nCes discours ont suscité des discussions sur la fiabilité des outils d'IA dans les environnements professionnels. (De nombreuses études ont déjà relevé ces limites des outils d'IA.) Ce qui n'était au départ qu'une frustration éparpillée sur Internet s'est rapidement transformé en une campagne bruyante.\n\nPlaintes liées aux performances de ChatGPT sur le plan technique\n\nLa campagne de boycott QuitGPT a pris de l'ampleur sur Reddit et Instagram, où les utilisateurs ont partagé des captures d'écran d'abonnements résiliés et des publications critiquant le ton du chatbot et ses récentes mises à jour, qu'ils jugent médiocres. Certains se sont plaints des réponses longues et trop polies. D'autres affirment que le dernier modèle d'OpenAI ne répondait pas à leurs attentes. Mais pour beaucoup, la politique a été le facteur décisif.\n\nCertains ont organisé une \" fête de résiliation massive \" à San Francisco, clin d'il sarcastique aux funérailles de GPT-4o proposées par un employé d'OpenAI, se moquant des utilisateurs qui pleurent la retraite imminente du modèle. Au cours des dernières semaines, les utilisateurs ont inondé Reddit d'histoires sur leur départ du chatbot. Beaucoup ont partagé des mèmes parodiant la flagornerie du chatbot. En gros, les principaux points évoqués sont :\n\nQuitGPT est l'une des dernières salves d'un mouvement croissant d'utilisateurs mécontents qui souhaitent résilier leur abonnement. Ses partisans affirment qu'il est essentiel que les utilisateurs expriment leurs préoccupations concernant les outils d'IA qui ne répondent pas aux normes professionnelles. Ils estiment que les commentaires des utilisateurs sont essentiels pour permettre aux développeurs d'améliorer les fonctionnalités de ces assistants d'IA.\n\nÀ l'inverse, certains utilisateurs défendent ChatGPT et OpenAI, citant notamment ses atouts en matière d'écriture créative et de brainstorming. Ces derniers affirment que si l'IA peut avoir des limites, la technologie offre également des capacités uniques qui peuvent compléter les efforts humains.\n\nPlaintes relatives au soutien de la Silicon Valley à Donald Trump\n\nL'une des raisons qui ont propulsé QuitGPT sous les feux de la rampe culturelle est le soutien de l'acteur et activiste Mark Ruffalo, qui a partagé la campagne sur les réseaux sociaux et exhorté ses followers à réfléchir aux implications éthiques de continuer à utiliser et à payer pour ChatGPT. Dans ses publications, il présente le boycott comme \" un choix moral \" et suggère d'explorer d'autres services d'IA plus en phase avec les valeurs des utilisateurs.\n\nLes publications de l'acteur Mark Ruffalo, connu pour ses rôles dans l'univers Marvel, ont recueilli des millions de likes et suscité un large engouement. Elles ont contribué à sensibiliser le grand public et ont propulsé QuitGPT au cur des débats, bien au-delà des forums technologiques et des cercles militants.\n\nLes dirigeants en quête d'allègements réglementaires contribuent à faire du président Donald Trump une force politique puissante grâce à une collecte de fonds record. Parmi les mégadonateurs proches ou issus du secteur de la technologie, nous pouvons citer les groupes et les personnalités suivants :\n\nLes groupes procryptomonnaies se sont imposés comme une force lors des dernières élections. Selon les registres financiers, Fairshake et deux groupes alignés, Defend American Jobs et Protect Progress, ont dépensé ensemble la somme colossale de 290 millions de dollars en 2024. Ainsi, ces fonds de campagne considérables indiquent que ces groupes pourraient exercer une influence significative lors des primaires et des élections générales de 2026.\n\nL'élan de la campagne QuitGPT soulève des questions difficiles sur les liens entre l'IA, la politique, le comportement des entreprises et les valeurs des consommateurs. Beaucoup dénoncent ces dons. Lorsqu'Alfred Stephen, développeur de logiciels indépendant à Singapour, a résilié son abonnement à ChatGPT, un sondage est apparu lui demandant ce qu'OpenAI aurait pu faire pour le fidéliser. \" Ne soutenez pas le régime fasciste \", a-t-il répondu.\n\nLes impacts potentiels de cette campagne sur ChatGPT et OpenAI\n\nLa campagne QuitGPT soulève des questions importantes sur l'avenir des applications d'IA dans le milieu professionnel. À mesure que de plus en plus d'utilisateurs partagent leurs expériences, les créateurs pourraient être amenés à remédier aux lacunes des modèles d'IA et à améliorer leurs offres. La boucle de rétroaction créée par des mouvements tels que QuitGPT pourrait s'avérer précieuse pour façonner la prochaine génération d'outils d'IA.\n\nLa campagne QuitGPT a été lancée fin janvier par de jeunes militants de gauche. Ils sont aussi bien des militants prodémocratie et des organisateurs de mouvements climatiques que des technophiles et des cyberlibertaires autoproclamés, dont beaucoup sont des militants de terrain chevronnés. Ils ont été inspirés par une vidéo virale publiée par Scott Galloway, professeur de marketing à l'université de New York et animateur de The Prof G Pod.\n\nIl a fait valoir que le meilleur moyen d'arrêter ICE était de persuader les gens d'annuler leur abonnement à ChatGPT. D'après lui, une baisse du nombre d'abonnés d'OpenAI pourrait avoir des répercussions sur le marché boursier et menacer d'entraîner un ralentissement économique qui pousserait Trump à agir.\n\nEn décembre 2025, ChatGPT comptait environ 900 millions d'utilisateurs actifs par semaine. Bien que le nombre de participants à ce boycott ne soit pas connu, QuitGPT suscite un grand intérêt. Une récente publication Instagram de la campagne a été vue plus de 36 millions de fois et a reçu 1,3 million de likes. Les organisateurs de la campagne QuitGPT ont déclaré que plus de 17 000 personnes se sont inscrites sur le site Web de la campagne.\n\nSignification plus large des campagnes de boycott comme QuitGPT\n\nL'indignation virale a rarement des conséquences concrètes dans le monde réel. QuitGPT fait beaucoup de bruit, mais le bruit n'est pas toujours synonyme d'importance. ChatGPT dispose toujours d'une énorme base d'utilisateurs gratuits, et pour des millions de professionnels, d'étudiants et d'utilisateurs quotidiens, il fait désormais partie intégrante de leur façon de travailler et de penser. Ce type d'utilité ne disparaît pas du jour au lendemain.\n\nQuelle que soit la position politique sur ce sujet particulier, cela donne l'occasion de réévaluer la valeur : ChatGPT vaut-il toujours la peine d'être payé ? D'autres modèles sont-ils meilleurs que les modèles d'OpenAI ? Les alternatives offrent-elles une meilleure confidentialité ou des garde-fous plus solides ? Même les organisateurs de la campagne de boycott QuitGPT ne se contentent pas de dire aux gens \" supprimez ChatGPT et déconnectez-vous \".\n\nIls orientent activement les utilisateurs vers des concurrents tels que Gemini et des options open source. Cela me montre qu'il ne s'agit pas d'un mouvement anti-IA, mais plutôt d'une question de choix dans un écosystème IA en plein essor. En ce sens, ce que représente QuitGPT est peut-être plus important que le nombre de personnes qui se désabonnent réellement. Le facteur explicatif réside dans le soutien de la Silicon Valley à Donald Trump.\n\nQuitGPT s'inscrit dans une vague plus large de remise en question publique des grandes entreprises technologiques, qui rappelle que les utilisateurs ne considèrent plus les plateformes comme des outils neutres, mais comme des entreprises dont ils soutiennent implicitement les valeurs.\n\nUne croissance spectaculaire, mais très coûteuse pour OpenAI\n\nOpenAI connaît une croissance rapide. La startup d'IA dirigée par Sam Altman voit ses solutions s'imposer partout dans le monde, avec des revenus en nette accélération. Cela dit, cette expansion repose sur des dépenses colossales en calcul, en infrastructures cloud et en développement de modèles toujours plus puissants. Ces investissements augmentent presque au même rythme que l'usage, ce qui empêche OpenAI de dégager des marges confortables.\n\nL'IA générative ne fonctionne pas comme le logiciel classique. Dans le SaaS, la croissance du nombre d'utilisateurs permet généralement de diluer les coûts fixes : une fois le produit développé, servir un client supplémentaire coûte peu, ce qui améliore mécaniquement les marges à mesure que l'entreprise grandit. À l'inverse, dans l'IA générative, chaque nouvel utilisateur et chaque interaction supplémentaire entraînent des coûts marginaux significatifs.\n\nLes requêtes nécessitent du calcul intensif, mobilisent des infrastructures coûteuses et consomment beaucoup d'énergie. De plus, la course à des modèles toujours plus performants pousse les acteurs comme OpenAI à investir sans cesse dans l'entraînement de systèmes toujours plus grands et plus complexes, ce qui alourdit encore la structure de coûts. Cette caractéristique remet en cause l'idée selon laquelle la taille suffira à rendre OpenAI très rentable.\n\nMême à très grande échelle, une entreprise d'IA peut continuer à brûler du capital si ses revenus n'augmentent pas plus vite que ses dépenses. C'est cette rupture avec les lois économiques du logiciel traditionnel que les analystes économiques considèrent comme un point de fragilité central du modèle d'OpenAI.\n\nOpenAI prévoit une perte de 14 milliards de dollars en 2026\n\nDerrière l'image d'un OpenAI tout-puissant, portée par le succès planétaire de ChatGPT, se dessine un modèle financier sous tension, marqué par des pertes colossales anticipées et une dépendance accrue à des partenariats géopolitiques et industriels. Entre projections internes alarmantes et tournées diplomatiques de son dirigeant, l'entreprise incarne aujourd'hui les paradoxes d'une IA devenue incontournable, mais encore loin d'être rentable.\n\nLes projections financières internes dessinent un tableau préoccupant. Malgré une croissance spectaculaire des usages de ChatGPT et de sa notoriété, OpenAI s'attendrait à des pertes cumulées atteignant des niveaux rarement vus dans le secteur du logiciel. Les coûts d'entraînement des modèles, l'exploitation de centres de données massifs et la dépendance à des infrastructures de calcul toujours plus énergivores pèsent lourdement sur les comptes.\n\nOpenAI prévoit des pertes totales de 14 milliards de dollars en 2026. Selon les documents, OpenAI continuera à enregistrer des pertes colossales à court terme, totalisant 44 milliards de dollars jusqu'en 2029, date à laquelle l'entreprise ne se contentera pas de réaliser des bénéfices, mais générera des revenus comparables à ceux de Nvidia. L'investisseur chevronné George Noble a déclaré dans une analyse : \" OpenAI est en train de s'effondrer \".\n\nDans un essai publié dans le New York Times, Sebastian Mallaby, chercheur senior au Council on Foreign Relations, a prédit qu'OpenAI pourrait se retrouver à court d'argent \" au cours des 18 prochains mois \". Selon lui, les rivaux d'OpenAI pourraient utiliser les fonds générés par leurs activités traditionnelles pour investir des centaines de milliards dans le développement et la mise à l'échelle de leurs modèles d'IA, alors qu'OpenAI n'a pas ce luxe.\n\nL'IA générative face à son moment de vérité\n\nLe cas OpenAI dépasse largement le destin d'une seule entreprise. Il met en lumière les limites actuelles du modèle économique de l'IA générative à grande échelle. Tant que les coûts de calcul resteront aussi élevés et que la concurrence s'intensifiera, la rentabilité demeurera incertaine, même pour les acteurs les plus avancés. Si ces campagnes se multiplient et que les gens quittent ChatGPT en raison de la publicité, OpenAI pourrait bien s'effondrer.\n\nL'effondrement d'OpenAI pourrait conduire à l'éclatement de la bulle, notamment en raison du nombre croissant de transactions circulaires dans le secteur de l'IA. La bulle de l'IA est soutenue par des accords circulaires à travers lesquels les entreprises spécialisées dans l'IA financent mutuellement leur croissance.\n\nOpenAI se trouve ainsi à un moment charnière. Soit il parvient à stabiliser son financement, à diversifier ses revenus et à optimiser ses infrastructures, soit il devra accepter une intégration plus étroite avec de puissants partenaires étatiques ou industriels. Dans les deux cas, son évolution servira de référence pour l'ensemble du secteur, qui observe avec attention si la promesse de l'IA peut réellement s'accompagner d'un modèle économique durable.\n\nConclusion\n\nAlors que la campagne QuitGPT continue de prendre de l'ampleur, elle rappelle que si les outils d'IA tels que ChatGPT ont le potentiel de révolutionner les flux de travail, ils ne sont pas exempts de défauts. Les utilisateurs sont encouragés à évaluer de manière critique leurs expériences et à plaider en faveur d'améliorations, afin de garantir que les technologies d'IA répondent aux besoins en constante évolution des professionnels dans divers domaines.\n\nOpenAI connaît une croissance rapide, mais les investisseurs restent malgré tout inquiets. La question centrale n'est pas de savoir si l'IA est utile, mais si son modèle économique peut devenir durable. Les investisseurs peuvent soutenir OpenAI pendant encore un certain temps, mais la patience a des limites. L'année 2026 sera un moment crucial pour évaluer si l'entreprise peut transformer son impressionnante croissance en un modèle rentable.\n\nOpenAI ne dispose d'aucun filet de sécurité de ce type. L'entreprise n'est ni une banque, ni une infrastructure publique, ni un service essentiel au fonctionnement immédiat des États. En cas de difficultés financières majeures, rien ne garantit qu'une intervention extérieure viendrait compenser ses pertes ou assurer sa continuité. Beaucoup d'entreprises bâtissent aujourd'hui leurs produits et leurs services autour des modèles d'IA d'OpenAI.\n\nCette dépendance crée l'illusion qu'un effondrement serait impossible, car trop coûteux pour l'écosystème. Mais cette logique confond confort et nécessité. Des alternatives émergent rapidement dans le secteur de l'IA générative, qu'elles soient open source ou privées. En cas de difficulté majeure, le marché ne s'arrêterait pas ; il se réorganiserait. Cette plasticité réduit considérablement l'argument selon lequel OpenAI serait devenue irremplaçable.\n\nSource : QuitGPT\n\nEt vous ?\n\nQuel est votre avis sur le sujet ?\n\nQue pensez-vous de la campagne de boycott QuitGPT ? Aura-t-elle un impact significatif ?\n\nChatGPT suscite des frustrations en raison de ses performances médiocres et de la publicité. Quel impact cela aura-t-il sur l'outil ?\n\nOpenAI serait au bord de l'effondrement. Qu'en pensez-vous ? Les investisseurs qui ont mis leurs billes et Washington vont-ils laisser l'entreprise s'effondrer ?\n\nLe modèle économique actuel d'OpenAI et du secteur de l'IA en général peut-il devenir durable ? Pourquoi ?\n\nVoir aussi\n\nLes documents internes d'OpenAI prévoient une perte de 14 milliards de $ en 2026 : \" OpenAI est en train de s'effondrer. Aucune startup dans l'histoire n'a jamais fonctionné avec de telles pertes \"\n\nLe secteur de l'IA s'apprête à passer un test crucial en 2026 : la vitesse à laquelle OpenAI brûle sa trésorerie pourrait être l'une des grandes questions pour savoir si la bulle éclatera\n\nDonald Trump lève 429 millions de dollars à l'approche des élections de mi-mandat grâce à un financement massif des milliardaires de la tech, qui visent à influencer les politiques réglementaires à Washington"
  },
  {
    "source": "Beritaja",
    "company": "OpenAI",
    "title": "The Openai Mafia: 18 Startups Founded By Alumni",
    "date": "2026-02-20T12:53:06Z",
    "url": "https://beritaja.com/the-openai-mafia-18-startups-founded-by-alumni-beritaja-406125.html",
    "content": "BERITAJA is a International-focused news website dedicated to reporting current events and trending stories from across the country. We publish news coverage on local and national issues, politics, business, technology, and community developments. Content is curated and edited to ensure clarity and relevance for our readers.\n\nMove over, PayPal mafia: There's a caller tech mafia successful Silicon Valley. As the startup down ChatGPT, OpenAI is arguably the biggest AI subordinate successful town. The institution is reportedly now in talks to finalize a $100 billion deal, valuing the company astatine much than $850 billion.\n\nMany labor person travel and gone since the institution first launched a decade ago, and some have launched startups of their own. Among these, some have go apical rivals (like Anthropic), while others, conscionable connected investor interest alone, person managed to raise billions without moreover launching a merchandise (see, Thinking Machine Labs).\n\nIn January, Aliisa Rosenthal, OpenAI's first income leader, spoke a small spot about this increasing network. She, like the different OpenAI alums who did not go founders, decided to go an investor and said she was going to pat into the ex-OpenAI laminitis web to look for woody flow. We know Peter Deng, OpenAI's erstwhile caput of user products (and now wide partner astatine Felicis) already has.\n\nBelow is simply a roundup of the major startups founded by OpenAI alumni, in alphabetical order. And we are definite this database will turn complete time.\n\nDavid Luan -- Adept AI Labs\n\nDavid Luan was OpenAI's engineering VP until he near successful 2020. After a stint astatine Google, successful 2021 he co-founded Adept AI Labs, a startup that builds AI devices for employees. The startup last raised $350 cardinal astatine a valuation north of $1 cardinal successful 2023, but Luan left successful precocious 2024 to oversee Amazon's AI agents lab aft Amazon hired Adept's founders.\n\nDario Amodei, Daniela Amodei, and John Schulman -- Anthropic\n\nSiblings Dario and Daniela Amodei near OpenAI successful 2021 to shape their ain startup, San Francisco-based Anthropic, that has agelong touted a attraction connected AI safety. OpenAI co-founder John Schulman joined Anthropic successful 2024, pledging to build a \"safe AGI.\" The institution has since go OpenAI's biggest rival and conscionable raised a $30 cardinal Series G, nabbing a $380 cardinal valuation in the process. IPO rumors are besides swirling, arsenic the institution reportedly prepares for a nationalist listing that could travel sometime this year. (OpenAI is besides allegedly preparing for an IPO this twelvemonth and is maybe moreover trying to hit Anthropic to the nationalist market.)\n\nRhythm Garg, Linden Li, and Yash Patil -- Applied Compute\n\nThree ex-OpenAI staffers (Rhythm Garg, Linden Li, and Yash Patil) person reportedly raised $20 cardinal for a startup called Applied Compute, arsenic reported by Upstart Media. All 3 of them worked arsenic method unit astatine OpenAI for much than a twelvemonth earlier leaving past May to motorboat the startup, per their LinkedIns. The startup helps enterprises train and deploy civilization AI agents. Benchmark led the round, valuing the 10-month-old institution astatine $100 million, Upstart Media reported.\n\nPieter Abbeel, Peter Chen, and Rocky Duan -- Covariant\n\nThe trio each worked astatine OpenAI successful 2016 and 2017 arsenic investigation scientists earlier founding Covariant, a Berkeley, California-based startup that builds instauration AI models for robots. In 2024, Amazon hired each 3 of the Covariant founders and about a 4th of its staff. The quasi-acquisition was viewed by some arsenic portion of a broader inclination of Big Tech attempting to debar antitrust scrutiny.\n\nTim Shi -- Cresta\n\nTim Shi was an early personnel of OpenAI's team, wherever he focused connected building safe artificial wide intelligence (AGI), according to his LinkedIn profile. He worked astatine OpenAI for a twelvemonth successful 2017 but near to recovered Cresta, a San Francisco-based AI interaction halfway startup that has raised complete $270 cardinal from VCs for illustration Sequoia Capital, Andreessen Horowitz, and others, according to a press release.\n\nJonas Schneider -- Daedalus\n\nJonas Schneider led OpenAI's package engineering for robotics squad but near successful 2019 to co-found Daedalus, which builds precocious factories for precision components. The San Francisco-based startup raised a $21 cardinal Series A past year pinch backing from Khosla Ventures, among others.\n\nAndrej Karpathy -- Eureka Labs\n\nComputer imagination master Andrej Karpathy was a founding personnel and investigation intelligence astatine OpenAI, leaving the startup to subordinate Tesla successful 2017 to lead its autopilot program. Karpathy is besides well-known for his YouTube videos explaining halfway AI concepts. He near Tesla successful 2024 to found his ain acquisition exertion startup, Eureka Labs, a San Francisco-based startup that is building AI school assistants.\n\nMargaret Jennings -- Kindo\n\nMargaret Jennings worked astatine OpenAI successful 2022 and 2023 until she near to co-found Kindo, which markets itself arsenic an AI chatbot for enterprises. Kindo has raised complete $27 cardinal successful funding, past raising a $20.6 cardinal Series A successful 2024. Jennings near Kindo successful 2024 to caput merchandise and investigation astatine French AI startup Mistral, according to her LinkedIn profile.\n\nMaddie Hall -- Living Carbon\n\nMaddie Hall worked connected \"special projects\" astatine OpenAI but near successful 2019 to co-found Living Carbon, a San Francisco-based startup that intends to create engineered plants that could suck much c retired of the entity to conflict ambiance change. Living Carbon raised a $21 cardinal Series A information successful 2023, bringing its full backing until past to $36 million, according to a press release.\n\nLiam Fedus -- Periodic Labs\n\nLiam Fedus, OpenAI's VP of post-training research, near the institution successful March 2025 to squad up pinch his erstwhile Google Brain colleague, Ekin Dogus Cubuk, and launch Periodic Labs. The startup seeks to usage AI scientists to find caller materials, peculiarly caller superconducting materials. It came retired of stealth mode successful September 2025, equipped pinch a monolithic $300 million successful seed-round backing pinch backers that included Jezz Bezos, Eric Schmidt, Felicis and Andreessen Horowitz.\n\nAravind Srinivas -- Perplexity\n\nAravind Srinivas worked arsenic a investigation intelligence astatine OpenAI for a twelvemonth until 2022, erstwhile he near the institution to co-found AI hunt motor Perplexity. His startup has attracted a drawstring of high-profile investors for illustration Jeff Bezos and Nvidia, though it's besides caused controversy complete alleged unethical web scraping. Perplexity, which is based successful San Francisco, past reported a raise of $200 cardinal astatine a $20 cardinal valuation.\n\nJeff Arnold -- Pilot\n\nJeff Arnold worked arsenic OpenAI's caput of operations for 5 months successful 2016 earlier co-founding San Francisco-based accounting startup Pilot successful 2017. Pilot, which focused initially connected doing accounting for startups, last raised a $100 cardinal Series C successful 2021 astatine a $1.2 cardinal valuation and has attracted investors for illustration Jeff Bezos. Arnold worked arsenic Pilot's COO until leaving successful 2024 to motorboat a VC fund.\n\nShariq Hashme -- Prosper Robotics\n\nShariq Hashme worked for OpenAI for 9 months successful 2017 connected a bot that could play the celebrated video crippled Dota, per his LinkedIn profile. After a fewer years astatine data-labeling startup Scale AI, he co-founded London-based Prosper Robotics successful 2021. The startup says it's moving connected a robot butler for people's homes, a basking inclination successful robotics that different players like Norway's 1X and Texas-based Apptronik are besides moving on.\n\nIlya Sutskever -- Safe Superintelligence\n\nOpenAI co-founder and main intelligence Ilya Sutskever left OpenAI successful May 2024 aft he was reportedly part of a grounded effort to switch CEO Sam Altman. Shortly afterward, he co-founded Safe Superintelligence, aliases SSI, pinch \"one extremity and 1 product: a safe superintelligence,\" he says. Details about what precisely the startup is up to are scant: It has nary merchandise and nary gross yet. But investors are clamoring for a portion anyway, and it's been capable to raise $2 billion, pinch its latest valuation reportedly rising to $32 billion this month. SSI is based successful Palo Alto, California, and Tel Aviv, Israel.\n\nEmmett Shear -- Stem AI\n\nEmmett Shear is the erstwhile CEO of Twitch who was OpenAI's interim CEO successful November 2023 for a fewer days earlier Sam Altman rejoined the company. Shear launched an AI company, StemAI, successful 2024 (though it seems to person since rebranded arsenic Softmax). The company, which appears to beryllium a investigation company, has attracted backing from Andreessen Horowitz.\n\nMira Murati -- Thinking Machines Lab\n\nMira Murati, OpenAI's CTO, near OpenAI to recovered her ain company, Thinking Machines Lab, which emerged from stealth successful February 2025. It said astatine the clip (rather vaguely) that it will build AI that's much \"customizable\" and \"capable.\" The San Francisco AI startup, now weighted astatine $12 billion, announced its first merchandise precocious past year: an API that fine-tunes connection models. It precocious made headlines erstwhile 2 of its co-founders announced earlier this twelvemonth that they would return to OpenAI.\n\nKyle Kosic -- xAI\n\nKyle Kosic near OpenAI successful 2023 to go a co-founder and infrastructure lead of xAI, Elon Musk's AI startup that offers a rival chatbot, Grok. In 2024, however, he hopped back to OpenAI, wherever he remains. Meanwhile, xAI (which acquired Musk's societal media tract X) was purchased by Musk's SpaceX, giving the coalesce institution a valuation of $1.25 trillion. It is looking to spell nationalist sometime successful June for what could beryllium a historical listing.\n\nAngela Jiang -- Worktrace AI\n\nAngela Jiang near OpenAI successful 2024, aft moving arsenic a merchandise head and connected the nationalist argumentation team. In April 2025, she quietly launched Worktrace, which uses AI to thief enterprises make business operations much efficient. It observes worker activity patterns and automates workflow, according to the company's website. The business is backed by Mura Murati, OpenAI's erstwhile CTO, who went connected to motorboat Thinking Labs. It is besides backed by OpenAI's startup fund, successful summation to a slew of different OpenAI names, for illustration its main strategy officer, Jason Kwon.\n\nStealth Startups\n\nIn summation to these startups, a number of different erstwhile OpenAI labor person founded startups that are still successful stealth mode, according to various updates TechCrunch recovered connected LinkedIn. For instance, it seems that erstwhile OpenAI interrogator Danilo Hellermark has been moving connected a generative AI stealth startup for the past fewer years. He officially near OpenAI astatine the opening of 2023. There's also 1 apparently successful the useful from Lucas Negritto, who worked connected OpenAI's method squad and near the institution successful 2023 aft 3 years. Since then, he's founded 1 startup and has been moving connected different since August 2025, according to his LinkedIn."
  },
  {
    "source": "TechCrunch",
    "company": "OpenAI",
    "title": "The OpenAI mafia: 18 startups founded by alumni | TechCrunch",
    "date": "2026-02-20T12:52:48Z",
    "url": "https://techcrunch.com/2026/02/20/the-openai-mafia-15-of-the-most-notable-startups-founded-by-alumni/",
    "content": "Move over, PayPal mafia: There's a new tech mafia in Silicon Valley. As the startup behind ChatGPT, OpenAI is arguably the biggest AI player in town. The company is reportedly now in talks to finalize a $100 billion deal, valuing the company at more than $850 billion.\n\nMany employees have come and gone since the company first launched a decade ago, and some have launched startups of their own. Among these, some have become top rivals (like Anthropic), while others, just on investor interest alone, have managed to raise billions without even launching a product (see, Thinking Machine Labs).\n\nIn January, Aliisa Rosenthal, OpenAI's first sales leader, spoke a little bit about this growing network. She, like the other OpenAI alums who did not become founders, decided to become an investor and said she was going to tap into the ex-OpenAI founder network to look for deal flow. We know Peter Deng, OpenAI's former head of consumer products (and now general partner at Felicis) already has.\n\nBelow is a roundup of the major startups founded by OpenAI alumni, in alphabetical order. And we are certain this list will grow over time.\n\nDavid Luan was OpenAI's engineering VP until he left in 2020. After a stint at Google, in 2021 he co-founded Adept AI Labs, a startup that builds AI tools for employees. The startup last raised $350 million at a valuation north of $1 billion in 2023, but Luan left in late 2024 to oversee Amazon's AI agents lab after Amazon hired Adept's founders.\n\nSiblings Dario and Daniela Amodei left OpenAI in 2021 to form their own startup, San Francisco-based Anthropic, that has long touted a focus on AI safety. OpenAI co-founder John Schulman joined Anthropic in 2024, pledging to build a \"safe AGI.\" The company has since become OpenAI's biggest rival and just raised a $30 billion Series G, nabbing a $380 billion valuation in the process. IPO rumors are also swirling, as the company reportedly prepares for a public listing that could come sometime this year. (OpenAI is also allegedly preparing for an IPO this year and is maybe even trying to beat Anthropic to the public market.)\n\nThree ex-OpenAI staffers (Rhythm Garg, Linden Li, and Yash Patil) have reportedly raised $20 million for a startup called Applied Compute, as reported by Upstart Media. All three of them worked as technical staff at OpenAI for more than a year before leaving last May to launch the startup, per their LinkedIns. The startup helps enterprises train and deploy custom AI agents. Benchmark led the round, valuing the 10-month-old company at $100 million, Upstart Media reported.\n\nThe trio all worked at OpenAI in 2016 and 2017 as research scientists before founding Covariant, a Berkeley, California-based startup that builds foundation AI models for robots. In 2024, Amazon hired all three of the Covariant founders and about a quarter of its staff. The quasi-acquisition was viewed by some as part of a broader trend of Big Tech attempting to avoid antitrust scrutiny.\n\nTim Shi was an early member of OpenAI's team, where he focused on building safe artificial general intelligence (AGI), according to his LinkedIn profile. He worked at OpenAI for a year in 2017 but left to found Cresta, a San Francisco-based AI contact center startup that has raised over $270 million from VCs like Sequoia Capital, Andreessen Horowitz, and others, according to a press release.\n\nJonas Schneider led OpenAI's software engineering for robotics team but left in 2019 to co-found Daedalus, which builds advanced factories for precision components. The San Francisco-based startup raised a $21 million Series A last year with backing from Khosla Ventures, among others.\n\nComputer vision expert Andrej Karpathy was a founding member and research scientist at OpenAI, leaving the startup to join Tesla in 2017 to lead its autopilot program. Karpathy is also well-known for his YouTube videos explaining core AI concepts. He left Tesla in 2024 to found his own education technology startup, Eureka Labs, a San Francisco-based startup that is building AI teaching assistants.\n\nMargaret Jennings worked at OpenAI in 2022 and 2023 until she left to co-found Kindo, which markets itself as an AI chatbot for enterprises. Kindo has raised over $27 million in funding, last raising a $20.6 million Series A in 2024. Jennings left Kindo in 2024 to head product and research at French AI startup Mistral, according to her LinkedIn profile.\n\nMaddie Hall worked on \"special projects\" at OpenAI but left in 2019 to co-found Living Carbon, a San Francisco-based startup that aims to create engineered plants that can suck more carbon out of the sky to fight climate change. Living Carbon raised a $21 million Series A round in 2023, bringing its total funding until then to $36 million, according to a press release.\n\nLiam Fedus, OpenAI's VP of post-training research, left the company in March 2025 to team up with his former Google Brain colleague, Ekin Dogus Cubuk, and launch Periodic Labs. The startup seeks to use AI scientists to find new materials, particularly new superconducting materials. It came out of stealth mode in September 2025, armed with a massive $300 million in seed-round funding with backers that included Jezz Bezos, Eric Schmidt, Felicis and Andreessen Horowitz.\n\nAravind Srinivas worked as a research scientist at OpenAI for a year until 2022, when he left the company to co-found AI search engine Perplexity. His startup has attracted a string of high-profile investors like Jeff Bezos and Nvidia, although it's also caused controversy over alleged unethical web scraping. Perplexity, which is based in San Francisco, last reported a raise of $200 million at a $20 billion valuation.\n\nJeff Arnold worked as OpenAI's head of operations for five months in 2016 before co-founding San Francisco-based accounting startup Pilot in 2017. Pilot, which focused initially on doing accounting for startups, last raised a $100 million Series C in 2021 at a $1.2 billion valuation and has attracted investors like Jeff Bezos. Arnold worked as Pilot's COO until leaving in 2024 to launch a VC fund.\n\nShariq Hashme worked for OpenAI for 9 months in 2017 on a bot that could play the popular video game Dota, per his LinkedIn profile. After a few years at data-labeling startup Scale AI, he co-founded London-based Prosper Robotics in 2021. The startup says it's working on a robot butler for people's homes, a hot trend in robotics that other players like Norway's 1X and Texas-based Apptronik are also working on.\n\nOpenAI co-founder and chief scientist Ilya Sutskever left OpenAI in May 2024 after he was reportedly part of a failed effort to replace CEO Sam Altman. Shortly afterward, he co-founded Safe Superintelligence, or SSI, with \"one goal and one product: a safe superintelligence,\" he says. Details about what exactly the startup is up to are scant: It has no product and no revenue yet. But investors are clamoring for a piece anyway, and it's been able to raise $2 billion, with its latest valuation reportedly rising to $32 billion this month. SSI is based in Palo Alto, California, and Tel Aviv, Israel.\n\nEmmett Shear is the former CEO of Twitch who was OpenAI's interim CEO in November 2023 for a few days before Sam Altman rejoined the company. Shear launched an AI company, StemAI, in 2024 (though it seems to have since rebranded as Softmax). The company, which appears to be a research company, has attracted funding from Andreessen Horowitz.\n\nMira Murati, OpenAI's CTO, left OpenAI to found her own company, Thinking Machines Lab, which emerged from stealth in February 2025. It said at the time (rather vaguely) that it will build AI that's more \"customizable\" and \"capable.\" The San Francisco AI startup, now valued at $12 billion, announced its first product late last year: an API that fine-tunes language models. It recently made headlines when two of its co-founders announced earlier this year that they would return to OpenAI.\n\nKyle Kosic left OpenAI in 2023 to become a co-founder and infrastructure lead of xAI, Elon Musk's AI startup that offers a rival chatbot, Grok. In 2024, however, he hopped back to OpenAI, where he remains. Meanwhile, xAI (which acquired Musk's social media site X) was purchased by Musk's SpaceX, giving the coalesce company a valuation of $1.25 trillion. It is looking to go public sometime in June for what could be a historic listing.\n\nAngela Jiang left OpenAI in 2024, after working as a product manager and on the public policy team. In April 2025, she quietly launched Worktrace, which uses AI to help enterprises make business operations more efficient. It observes employee work patterns and automates workflow, according to the company's website. The business is backed by Mura Murati, OpenAI's former CTO, who went on to launch Thinking Labs. It is also backed by OpenAI's startup fund, in addition to a slew of other OpenAI names, like its chief strategy officer, Jason Kwon.\n\nIn addition to these startups, a number of other former OpenAI employees have founded startups that are still in stealth mode, according to various updates TechCrunch found on LinkedIn. For instance, it seems that former OpenAI researcher Danilo Hellermark has been working on a generative AI stealth startup for the past few years. He officially left OpenAI at the beginning of 2023. There's also one apparently in the works from Lucas Negritto, who worked on OpenAI's technical team and left the company in 2023 after three years. Since then, he's founded one startup and has been working on another since August 2025, according to his LinkedIn."
  },
  {
    "source": "m.163.com",
    "company": "OpenAI",
    "title": "OpenAI有望拿到1000亿美元，但也快被逼到墙角了_手机网易网",
    "date": "2026-02-20T05:05:49Z",
    "url": "https://m.163.com/dy/article/KM6ISH9U051188EA.html",
    "content": "出品｜虎嗅科技组\n\n作者｜苗正卿\n\n题图｜视觉中国\n\nOpenAI正处于一个命运十字路口。\n\n对OpenAI而言的好消息是，它大概率即将完成新一轮涉及1000亿美元的公司史上\"最大规模\"融资；坏消息是，投资方正在急迫地需要OpenAI证明一个核心命题：它是能够赚钱的。\n\n2月19日，彭博社援引多位匿名知情人士的消息称，OpenAI 正接近敲定一轮创纪录级别的新融资交易第一阶段，预计该阶段融资能够为OpenAI带来超过1000亿美元资金。在这轮融资后，OpenAI的整体估值可能会超过8500亿美元。\n\n值得注意的是，这是OpenAI重组为营利性公司后的重大融资，也被视为OpenAI 冲击IPO前的关键一步。此前消息显示，OpenAI预计在2026年四季度或不晚于2027年内完成IPO。\n\n据悉，1000亿美元的融资，主要会用于OpenAI的 \"星门\" 超级计算项目以及 GPT-5 系列模型的迭代。根据此前OpenAI公布的信息显示，\"星门\"项目预计在2029年前在全球构建10 吉瓦 (GW)级别的专属 AI 超级计算网络，这是OpenAI参与到的最大规模的算力基础设施类项目。\n\n就在这笔融资新闻爆出前不到10天内，2月11日，OpenAI解散了内部的Mission Alignment（使命对齐）团队。值得注意的是，Mission Alignment曾是OpenAI区别于其他AI公司的\"灵魂型\"部门之一，它的\"被解散\"和OpenAI的战略目标、内部商业化权重进一步提高都有深度联系。\n\nMission Alignment成立于2024年9月，当时的背景是，OpenAI正在尝试越来越多的商业化项目，为了确保公司不偏离\"AGI惠及全人类\"这一核心使命，公司成立了具备独立监督职责的Mission Alignment。\n\n一个关键细节是，在成立后，Mission Alignment会参与到公司产品研发等关键决策之中，并对项目和路线是否符合公司使命进行\"一致性评估\"。Mission Alignment的一个重要任务恰恰是\"避免商业化压力和因素，导致OpenAI整体战略跑偏\"。\n\n在成立后，Mission Alignment深度参与到了\"星门\"、F轮融资、GPT-5/GPT-6开发计划等OpenAI核心战略项目之中。\n\n在2月11日Mission Alignment\"被解散\"消息传出后，硅谷部分科技博主和AI安全专家认为，这是OpenAI试图进一步强化\"商业化优先\"的信号之一。\n\n一个深层的影响是，Mission Alignment这一部门的消失，本质上意味着OpenAI内部的项目流程、安全决策体系将发生质变，从积极的一面看，OpenAI可能会大幅度缩短产品迭代周期，这和OpenAI在1000亿美元融资前所多次公开表示的\"尽快推出GPT-5全系列模型\"态度不谋而合。\n\n而挑战的一面是，OpenAI原本的\"产品+商业化\"与\"安全和伦理\"之间的平衡关系，会被打破。考虑到OpenAI正试图比老对手Anthropic率先上市，它势必需要在2026年拿出至少一款超预期、能够赢得市场广泛认可心智的爆品。这意味着，Mission Alignment的\"被解散\"可能只是OpenAI砍掉的第一个\"减速器\"而非最后一个，接下来OpenAI可能会进一步对非产品和商业化导向的机制、架构动刀。\n\n让OpenAI如此急迫的，其实是它的亏损现状。\n\n根据OpenAI CFO Sarah Friar官网博客信息显示，OpenAI2025年年化收入(ARR)从2023年到2025年分别是20亿美元、60亿美元、不少于200亿美元。\n\n但据The Information2025年9月的报道显示，2025年上半年OpenAI实际经营亏损约78亿美元；而《华尔街日报》在2026年初的推测认为，OpenAI2025年全年预计亏损约90亿美元。\n\n在OpenAI CFO Sarah Friar的博客中，Sarah Friar披露过OpenAI的收入结构，其中企业订阅服务占比约60%、API（针对开发者）占比约25%、C端消费者服务占比约15%。\n\n这意味着，OpenAI在赚钱这方面，目前依然要和Anthropic等竞对正面开战。\n\n提高C端用户的付费率，已经成为OpenAI的当务之急。据公开信息和OpenAI官网内容显示，2026年OpenAI会重点降低获客成本，提升客户留存和付费转化率。\n\n但考虑到OpenAI参与的\"星门\"需要持续投入，以及英伟达等算力厂商2026年大概率的涨价策略，OpenAI的成本压力只能更高。\n\n换言之，它确实急需通过融资\"续命\"+\"造血\"，但这形成了一个\"莫比乌斯环\"：为了扩大C端和企业级用户量级付费率，OpenA需要通过投入更多算力资源去提高模型能力；但如果想短时间内尽快\"堆积\"更高算力，便需要尽快投入巨资抢占资源优势（或完善基础设施）；但资本方已经不是AI圈跑马圈地的时代，正在聚焦于AI产品的商业化。\n\n所以OpenAI解散了Mission Alignment，招入了OpenClaw（Clawdbot、Moltbot）之父Peter Steinberger（负责Agent），这些动作本质上都是为了尽快完善产品迭代、打通商业化路径。\n\n一个危机信号来自竞对。\n\n在完成G轮300亿美元融资前后，Anthropic在2月5日和2月17日分别发布了Claude Opus 4.6旗舰模型（Humanity's Last Exam测试53.1%，达到全球第一）、Claude Sonnet 4.6中端模型（以1/5旗舰价格实现近乎99%旗舰性能）。近几日，硅谷AI舆论圈几乎被Anthropic相关热度话题淹没。\n\n而早在2025年7月美国知名风险投资机构Menlo Ventures发布的《2025企业AI报告》显示Anthropic在2025年便以32%企业级LLM支出占比超越OpenAI（25%），成为B端第一大模型厂商。\n\n这意味着，OpenAI在战略上已经被逼到了墙角：它必须尽快拿出一款颠覆性的产品。\n\n当然，这个产品大概率不能是另一款\"谄媚\"的GPT-4o。\n\n本文来自虎嗅，原文链接：https://www.huxiu.com/article/4835859.html?f=wyxwapp"
  },
  {
    "source": "凤凰网（凤凰新媒体）",
    "company": "OpenAI",
    "title": "OpenAI有望拿到1000亿美元，但也快被逼到墙角了",
    "date": "2026-02-20T02:31:31Z",
    "url": "https://tech.ifeng.com/c/8qt73IqykPJ",
    "content": "出品｜虎嗅科技组\n\n作者｜苗正卿\n\n题图｜视觉中国\n\nOpenAI正处于一个命运十字路口。\n\n对OpenAI而言的好消息是，它大概率即将完成新一轮涉及1000亿美元的公司史上\"最大规模\"融资；坏消息是，投资方正在急迫地需要OpenAI证明一个核心命题：它是能够赚钱的。\n\n2月19日，彭博社援引多位匿名知情人士的消息称，OpenAI 正接近敲定一轮创纪录级别的新融资交易第一阶段，预计该阶段融资能够为OpenAI带来超过1000亿美元资金。在这轮融资后，OpenAI的整体估值可能会超过8500亿美元。\n\n值得注意的是，这是OpenAI重组为营利性公司后的重大融资，也被视为OpenAI 冲击IPO前的关键一步。此前消息显示，OpenAI预计在2026年四季度或不晚于2027年内完成IPO。\n\n据悉，1000亿美元的融资，主要会用于OpenAI的 \"星门\" 超级计算项目以及 GPT-5 系列模型的迭代。根据此前OpenAI公布的信息显示，\"星门\"项目预计在2029年前在全球构建10 吉瓦 (GW)级别的专属 AI 超级计算网络，这是OpenAI参与到的最大规模的算力基础设施类项目。\n\n就在这笔融资新闻爆出前不到10天内，2月11日，OpenAI解散了内部的Mission Alignment（使命对齐）团队。值得注意的是，Mission Alignment曾是OpenAI区别于其他AI公司的\"灵魂型\"部门之一，它的\"被解散\"和OpenAI的战略目标、内部商业化权重进一步提高都有深度联系。\n\nMission Alignment成立于2024年9月，当时的背景是，OpenAI正在尝试越来越多的商业化项目，为了确保公司不偏离\"AGI 惠及全人类\"这一核心使命，公司成立了具备独立监督职责的Mission Alignment。\n\n一个关键细节是，在成立后，Mission Alignment会参与到公司产品研发等关键决策之中，并对项目和路线是否符合公司使命进行\"一致性评估\"。Mission Alignment的一个重要任务恰恰是\"避免商业化压力和因素，导致OpenAI整体战略跑偏\"。\n\n在成立后，Mission Alignment深度参与到了\"星门\"、F 轮融资、GPT-5/GPT-6 开发计划等OpenAI核心战略项目之中。\n\n在2月11日Mission Alignment\"被解散\"消息传出后，硅谷部分科技博主和AI安全专家认为，这是OpenAI试图进一步强化\"商业化优先\"的信号之一。\n\n一个深层的影响是，Mission Alignment这一部门的消失，本质上意味着 OpenAI内部的项目流程、安全决策体系将发生质变，从积极的一面看，OpenAI可能会大幅度缩短产品迭代周期，这和OpenAI在1000亿美元融资前所多次公开表示的\"尽快推出 GPT-5 全系列模型\"态度不谋而合。\n\n而挑战的一面是， OpenAI原本的\"产品+商业化\"与\"安全和伦理\"之间的平衡关系，会被打破。考虑到 OpenAI正试图比老对手Anthropic率先上市，它势必需要在2026年拿出至少一款超预期、能够赢得市场广泛认可心智的爆品。这意味着，Mission Alignment的\"被解散\"可能只是OpenAI砍掉的第一个\"减速器\"而非最后一个，接下来OpenAI可能会进一步对非产品和商业化导向的机制、架构动刀。\n\n让OpenAI如此急迫的，其实是它的亏损现状。\n\n根据OpenAI CFO Sarah Friar 官网博客信息显示，OpenAI2025 年年化收入 (ARR)从2023年到2025年分别是20亿美元、60亿美元、不少于200亿美元。\n\n但据The Information2025年9月的报道显示，2025年上半年OpenAI实际经营亏损约 78 亿美元；而《华尔街日报》在2026年初的推测认为，OpenAI2025 年全年预计亏损约90亿美元。\n\n在OpenAI CFO Sarah Friar 的博客中，Sarah Friar披露过OpenAI的收入结构，其中企业订阅服务占比约60%、API（针对开发者）占比约25%、C端消费者服务占比约15%。\n\n这意味着，OpenAI在赚钱这方面，目前依然要和Anthropic等竞对正面开战。\n\n提高C端用户的付费率，已经成为OpenAI的当务之急。据公开信息和OpenAI官网内容显示，2026年OpenAI会重点降低获客成本，提升客户留存和付费转化率。\n\n但考虑到OpenAI参与的\"星门\"需要持续投入，以及英伟达等算力厂商2026年大概率的涨价策略，OpenAI的成本压力只能更高。\n\n换言之，它确实急需通过融资\"续命\"+\"造血\"，但这形成了一个\"莫比乌斯环\"：为了扩大C端和企业级用户量级付费率，OpenA需要通过投入更多算力资源去提高模型能力；但如果想短时间内尽快\"堆积\"更高算力，便需要尽快投入巨资抢占资源优势（或完善基础设施）；但资本方已经不是AI圈跑马圈地的时代，正在聚焦于AI产品的商业化。\n\n所以OpenAI解散了Mission Alignment，招入了OpenClaw（Clawdbot、Moltbot）之父Peter Steinberger（负责Agent），这些动作本质上都是为了尽快完善产品迭代、打通商业化路径。\n\n一个危机信号来自竞对。\n\n在完成G轮300亿美元融资前后，Anthropic在 2 月 5 日和2 月 17 日分别发布了Claude Opus 4.6旗舰模型（Humanity's Last Exam 测试 53.1%，达到全球第一）、Claude Sonnet 4.6中端模型（以1/5 旗舰价格实现近乎99% 旗舰性能）。近几日，硅谷AI舆论圈几乎被Anthropic相关热度话题淹没。\n\n而早在2025年7月美国知名风险投资机构Menlo Ventures发布的《2025 企业 AI 报告》显示Anthropic在2025年便 以 32% 企业级 LLM 支出占比超越 OpenAI（25%），成为 B 端第一大模型厂商。\n\n这意味着，OpenAI在战略上已经被逼到了墙角：它必须尽快拿出一款颠覆性的产品。\n\n当然，这个产品大概率不能是另一款\"谄媚\"的GPT-4o。"
  },
  {
    "source": "Claims Journal",
    "company": "OpenAI",
    "title": "OpenAI Deleted Word 'Safely' From Its Mission - Its New Structure Is A Test",
    "date": "2026-02-17T06:07:27Z",
    "url": "https://www.claimsjournal.com/news/national/2026/02/17/335719.htm",
    "content": "OpenAI, the maker of the most popular AI chatbot, used to say it aimed to build artificial intelligence that \"safely benefits humanity, unconstrained by a need to generate financial return,\" mission statement. But the ChatGPT maker seems to no longer have the same emphasis on doing so \"safely.\"\n\nWhile reviewing its latest IRS disclosure form, which was released in November 2025 and covers 2024, I noticed OpenAI had removed \"safely\" from its mission statement, among other changes. That change in wording coincided with its transformation from a nonprofit organization into a business increasingly focused on profits.\n\nOpenAI currently faces several lawsuits related to its products' safety, making this change newsworthy. Many of the plaintiffs suing the AI company allege psychological manipulation, wrongful death and assisted suicide, while others have filed negligence claims.\n\nAs a scholar of nonprofit accountability and the governance of social enterprises, I see the deletion of the word \"safely\" from its mission statement as a significant shift that has largely gone unreported - outside highly specialized outlets.\n\nAnd I believe OpenAI's makeover is a test case for how we, as a society, oversee the work of organizations that have the potential to both provide enormous benefits and do catastrophic harm.\n\nTracing OpenAI's Origins\n\nOpenAI, which also makes the Sora video artificial intelligence app, was founded as a nonprofit scientific research lab in 2015. Its original purpose was to benefit society by making its findings public and royalty-free rather than to make money.\n\nTo raise the money that developing its AI models would require, OpenAI, under the leadership of CEO Sam Altman, created a for-profit subsidiary in 2019. Microsoft initially invested US$1 billion in this venture; by 2024 that sum had topped $13 billion.\n\nIn exchange, Microsoft was promised a portion of future profits, capped at 100 times its initial investment. But the software giant didn't get a seat on OpenAI's nonprofit board - meaning it lacked the power to help steer the AI venture it was funding.\n\nA subsequent round of funding in late 2024, which raised $6.6 billion from multiple investors, came with a catch: that the funding would become debt unless OpenAI converted to a more traditional for-profit business in which investors could own shares, without any caps on profits, and possibly occupy board seats.\n\nEstablishing A New Structure\n\nIn October 2025, OpenAI reached an agreement with the attorneys general of California and Delaware to become a more traditional for-profit company.\n\nUnder the new arrangement, OpenAI was split into two entities: a nonprofit foundation and a for-profit business.\n\nThe restructured nonprofit, the OpenAI Foundation, owns about one-fourth of the stock in a new for-profit public benefit corporation, the OpenAI Group. Both are headquartered in California but incorporated in Delaware.\n\nA public benefit corporation is a business that must consider interests beyond shareholders, such as those of society and the environment, and it must issue an annual benefit report to its shareholders and the public. However, it is up to the board to decide how to weigh those interests and what to report in terms of the benefits and harms caused by the company.\n\nThe new structure is described in a signed in October 2025 by OpenAI and the California attorney general, and endorsed by the Delaware attorney general.\n\nMany business media outlets heralded the move, predicting that it would usher in more investment. Two months later, SoftBank, a Japanese conglomerate, finalized a $41 billion investment in OpenAI.\n\nChanging Its Mission Statement\n\nMost charities must file forms annually with the Internal Revenue Service with details about their missions, activities and financial status to show that they qualify for tax-exempt status. Because the IRS makes the forms public, they have become a way for nonprofits to signal their missions to the world.\n\nIn its forms for 2022, OpenAI said its mission was \"to build general-purpose artificial intelligence (AI) that safely benefits humanity, unconstrained by a need to generate financial return.\"\n\nThat mission statement has changed, as of - which the company filed with the IRS in late 2025. It became \"to ensure that artificial general intelligence benefits all of humanity.\"\n\nOpenAI had dropped its commitment to safety from its mission statement - along with a commitment to being \"unconstrained\" by a need to make money for investors. According to Platformer, a tech media outlet, it has also disbanded its \"mission alignment\" team.\n\nIn my view, these changes explicitly signal that OpenAI is making its profits a higher priority than the safety of its products.\n\nTo be sure, OpenAI continues to mention safety when it discusses its mission. \"We view this mission as the most important challenge of our time,\" it states on its website. \"It requires simultaneously advancing AI's capability, safety, and positive impact in the world.\"\n\nRevising Its Legal Governance Structure\n\nNonprofit boards are responsible for key decisions and upholding their organization's mission.\n\nUnlike private companies, board members of tax-exempt charitable nonprofits cannot personally enrich themselves by taking a share of earnings. In cases where a nonprofit owns a for-profit business, as OpenAI did with its previous structure, investors can take a cut of profits - but they typically do not get a seat on the board or have an opportunity to elect board members, because that would be seen as a conflict of interest.\n\nThe OpenAI Foundation now has a 26% stake in OpenAI Group. In effect, that means that the nonprofit board has given up nearly three-quarters of its control over the company. Software giant Microsoft owns a slightly larger stake - 27% of OpenAI's stock - due to its $13.8 billion investment in the AI company to date. OpenAI's employees and its other investors own the rest of the shares.\n\nSeeking More Investment\n\nThe main goal of OpenAI's restructuring, which it called a \"recapitalization,\" was to attract more private investment in the race for AI dominance.\n\nIt has already succeeded on that front.\n\nAs of early February 2026, the company was in talks with SoftBank for an additional $30 billion and stands to get up to a total of $60 billion from Amazon, Nvidia and Microsoft combined.\n\nOpenAI is now valued at over $500 billion, up from $300 billion in March 2025. The new structure also paves the way for an eventual initial public offering, which, if it happens, would not only help the company raise more capital through stock markets but would also increase the pressure to make money for its shareholders.\n\nOpenAI says the foundation's endowment is worth about $130 billion.\n\nThose numbers are only estimates because OpenAI is a privately held company without publicly traded shares. That means these figures are based on market value estimates rather than any objective evidence, such as market capitalization.\n\nWhen he announced the new structure, California Attorney General Rob Bonta said, \"We secured concessions that ensure charitable assets are used for their intended purpose.\" He also predicted that \"safety will be prioritized\" and said the \"top priority is, and always will be, protecting our kids.\"\n\nSteps That Might Help Keep People Safe\n\nAt the same time, several conditions in the OpenAI restructuring memo are designed to promote safety, including:\n\nA safety and security committee on the OpenAI Foundation board has the authority to that could potentially include the halting of a release of new OpenAI products based on assessments of their risks.\n\nThe for-profit OpenAI Group has its own board, which must consider only OpenAI's mission - rather than financial issues - regarding safety and security issues.\n\nThe OpenAI Foundation's nonprofit board gets to appoint all members of the OpenAI Group's for-profit board.\n\nBut given that neither the mission of the foundation nor of the OpenAI group explicitly alludes to safety, it will be hard to hold their boards accountable for it.\n\nFurthermore, since all but one board member currently serve on both boards, it is hard to see how they might oversee themselves. And doesn't indicate whether he was aware of the removal of any reference to safety from the mission statement.\n\nIdentifying Other Paths OpenAI Could Have Taken\n\nThere are alternative models that I believe would serve the public interest better than this one.\n\nWhen Health Net, a California nonprofit health maintenance organization, converted to a for-profit insurance company in 1992, regulators required that 80% of its equity be transferred to another nonprofit health foundation. Unlike with OpenAI, the foundation had majority control after the transformation.\n\nA coalition of California nonprofits has argued that the attorney general should require OpenAI to transfer all of its assets to an independent nonprofit.\n\nAnother example is The Philadelphia Inquirer. The Pennsylvania newspaper became a for-profit public benefit corporation in 2016. It belongs to the Lenfest Institute, a nonprofit.\n\nThis structure allows Philadelphia's biggest newspaper to attract investment without compromising its purpose - journalism serving the needs of its local communities. It's become a model for potentially transforming the local news industry.\n\nAt this point, I believe that the public bears the burden of two governance failures. One is that OpenAI's board has apparently abandoned its mission of safety. And the other is that the attorneys general of California and Delaware have let that happen."
  },
  {
    "source": "Carter Hart agrees to join the Golden Knights after being acquitted of sexual assault - Bay to Bay News",
    "company": "OpenAI",
    "title": "OpenAI has deleted the word 'safely' from its mission - Bay to Bay News",
    "date": "2026-02-16T23:49:12Z",
    "url": "https://baytobaynews.com/stories/openai-has-deleted-the-word-safely-from-its-mission,295959",
    "content": "Alnoor Ebrahim is a professor of International Business, The Fletcher School & Tisch College of Civic Life, Tufts University. This article originally appeared in The Conversation.\n\nOpenAI, the maker of the most popular AI chatbot, used to say it aimed to build artificial intelligence that \"safely benefits humanity, unconstrained by a need to generate financial return,\" mission statement. But the ChatGPT maker seems to no longer have the same emphasis on doing so \"safely.\"\n\nWhile reviewing its latest IRS disclosure form, which was released in November 2025 and covers 2024, I noticed OpenAI had removed \"safely\" from its mission statement, among other changes. That change in wording coincided with its transformation from a nonprofit organization into a business increasingly focused on profits.\n\nOpenAI currently faces several lawsuits related to its products' safety, making this change newsworthy. Many of the plaintiffs suing the AI company allege psychological manipulation, wrongful death and assisted suicide, while others have filed negligence claims.\n\nAs a scholar of nonprofit accountability and the governance of social enterprises, I see the deletion of the word \"safely\" from its mission statement as a significant shift that has largely gone unreported - outside highly specialized outlets.\n\nAnd I believe OpenAI's makeover is a test case for how we, as a society, oversee the work of organizations that have the potential to both provide enormous benefits and do catastrophic harm.\n\nTracing OpenAI's origins\n\nOpenAI, which also makes the Sora video artificial intelligence app, was founded as a nonprofit scientific research lab in 2015. Its original purpose was to benefit society by making its findings public and royalty-free rather than to make money.\n\nTo raise the money that developing its AI models would require, OpenAI, under the leadership of CEO Sam Altman, created a for-profit subsidiary in 2019. Microsoft initially invested US$1 billion in this venture; by 2024 that sum had topped $13 billion.\n\nIn exchange, Microsoft was promised a portion of future profits, capped at 100 times its initial investment. But the software giant didn't get a seat on OpenAI's nonprofit board - meaning it lacked the power to help steer the AI venture it was funding.\n\nA subsequent round of funding in late 2024, which raised $6.6 billion from multiple investors, came with a catch: that the funding would become debt unless OpenAI converted to a more traditional for-profit business in which investors could own shares, without any caps on profits, and possibly occupy board seats.\n\nEstablishing a new structure\n\nIn October 2025, OpenAI reached an agreement with the attorneys general of California and Delaware to become a more traditional for-profit company.\n\nUnder the new arrangement, OpenAI was split into two entities: a nonprofit foundation and a for-profit business.\n\nThe restructured nonprofit, the OpenAI Foundation, owns about one-fourth of the stock in a new for-profit public benefit corporation, the OpenAI Group. Both are headquartered in California but incorporated in Delaware.\n\nA public benefit corporation is a business that must consider interests beyond shareholders, such as those of society and the environment, and it must issue an annual benefit report to its shareholders and the public. However, it is up to the board to decide how to weigh those interests and what to report in terms of the benefits and harms caused by the company.\n\nThe new structure is described in a signed in October 2025 by OpenAI and the California attorney general, and endorsed by the Delaware attorney general.\n\nMany business media outlets heralded the move, predicting that it would usher in more investment. Two months later, SoftBank, a Japanese conglomerate, finalized a $41 billion investment in OpenAI.\n\nChanging its mission statement\n\nMost charities must file forms annually with the Internal Revenue Service with details about their missions, activities and financial status to show that they qualify for tax-exempt status. Because the IRS makes the forms public, they have become a way for nonprofits to signal their missions to the world.\n\nIn its forms for 2022, , OpenAI said its mission was \"to build general-purpose artificial intelligence (AI) that safely benefits humanity, unconstrained by a need to generate financial return.\"\n\nThat mission statement has changed, as of - which the company filed with the IRS in late 2025. It became \"to ensure that artificial general intelligence benefits all of humanity.\"\n\nOpenAI had dropped its commitment to safety from its mission statement - along with a commitment to being \"unconstrained\" by a need to make money for investors. According to Platformer, a tech media outlet, it has also disbanded its \"mission alignment\" team.\n\nIn my view, these changes explicitly signal that OpenAI is making its profits a higher priority than the safety of its products.\n\nTo be sure, OpenAI continues to mention safety when it discusses its mission. \"We view this mission as the most important challenge of our time,\" it states on its website. \"It requires simultaneously advancing AI's capability, safety, and positive impact in the world.\"\n\nRevising its legal governance structure\n\nNonprofit boards are responsible for key decisions and upholding their organization's mission.\n\nUnlike private companies, board members of tax-exempt charitable nonprofits cannot personally enrich themselves by taking a share of earnings. In cases where a nonprofit owns a for-profit business, as OpenAI did with its previous structure, investors can take a cut of profits - but they typically do not get a seat on the board or have an opportunity to elect board members, because that would be seen as a conflict of interest.\n\nThe OpenAI Foundation now has a 26% stake in OpenAI Group. In effect, that means that the nonprofit board has given up nearly three-quarters of its control over the company. Software giant Microsoft owns a slightly larger stake - 27% of OpenAI's stock - due to its $13.8 billion investment in the AI company to date. OpenAI's employees and its other investors own the rest of the shares.\n\nSeeking more investment\n\nThe main goal of OpenAI's restructuring, which it called a \"recapitalization,\" was to attract more private investment in the race for AI dominance.\n\nIt has already succeeded on that front.\n\nAs of early February 2026, the company was in talks with SoftBank for an additional $30 billion and stands to get up to a total of $60 billion from Amazon, Nvidia and Microsoft combined.\n\nOpenAI is now valued at over $500 billion, up from $300 billion in March 2025. The new structure also paves the way for an eventual initial public offering, which, if it happens, would not only help the company raise more capital through stock markets but would also increase the pressure to make money for its shareholders.\n\nOpenAI says the foundation's endowment is worth about $130 billion.\n\nThose numbers are only estimates because OpenAI is a privately held company without publicly traded shares. That means these figures are based on market value estimates rather than any objective evidence, such as market capitalization.\n\nWhen he announced the new structure, California Attorney General Rob Bonta said, \"We secured concessions that ensure charitable assets are used for their intended purpose.\" He also predicted that \"safety will be prioritized\" and said the \"top priority is, and always will be, protecting our kids.\"\n\nSteps that might help keep people safe\n\nAt the same time, several conditions in the OpenAI restructuring memo are designed to promote safety, including:\n\nA safety and security committee on the OpenAI Foundation board has the authority to that could potentially include the halting of a release of new OpenAI products based on assessments of their risks.\n\nThe for-profit OpenAI Group has its own board, which must consider only OpenAI's mission - rather than financial issues - regarding safety and security issues.\n\nThe OpenAI Foundation's nonprofit board gets to appoint all members of the OpenAI Group's for-profit board.\n\nBut given that neither the mission of the foundation nor of the OpenAI group explicitly alludes to safety, it will be hard to hold their boards accountable for it.\n\nFurthermore, since all but one board member currently serve on both boards, it is hard to see how they might oversee themselves. And doesn't indicate whether he was aware of the removal of any reference to safety from the mission statement.\n\nIdentifying other paths OpenAI could have taken\n\nThere are alternative models that I believe would serve the public interest better than this one.\n\nWhen Health Net, a California nonprofit health maintenance organization, converted to a for-profit insurance company in 1992, regulators required that 80% of its equity be transferred to another nonprofit health foundation. Unlike with OpenAI, the foundation had majority control after the transformation.\n\nA coalition of California nonprofits has argued that the attorney general should require OpenAI to transfer all of its assets to an independent nonprofit.\n\nAnother example is The Philadelphia Inquirer. The Pennsylvania newspaper became a for-profit public benefit corporation in 2016. It belongs to the Lenfest Institute, a nonprofit.\n\nThis structure allows Philadelphia's biggest newspaper to attract investment without compromising its purpose - journalism serving the needs of its local communities. It's become a model for potentially transforming the local news industry.\n\nAt this point, I believe that the public bears the burden of two governance failures. One is that OpenAI's board has apparently abandoned its mission of safety. And the other is that the attorneys general of California and Delaware have let that happen."
  },
  {
    "source": "k.sina.com.cn",
    "company": "OpenAI",
    "title": "OpenAI有望拿到1000亿美元，但也快被逼到墙角了",
    "date": "2026-02-19T22:15:19Z",
    "url": "https://k.sina.com.cn/article_5953741034_162dee0ea067037th4.html",
    "content": "对OpenAI而言的好消息是，它大概率即将完成新一轮涉及1000亿美元的公司史上\"最大规模\"融资；坏消息是，投资方正在急迫地需要OpenAI证明一个核心命题：它是能够赚钱的。\n\n2月19日，彭博社援引多位匿名知情人士的消息称，OpenAI 正接近敲定一轮创纪录级别的新融资交易第一阶段，预计该阶段融资能够为OpenAI带来超过1000亿美元资金。在这轮融资后，OpenAI的整体估值可能会超过8500亿美元。\n\n值得注意的是，这是OpenAI重组为营利性公司后的重大融资，也被视为OpenAI 冲击IPO前的关键一步。此前消息显示，OpenAI预计在2026年四季度或不晚于2027年内完成IPO。\n\n据悉，1000亿美元的融资，主要会用于OpenAI的 \"星门\" 超级计算项目以及 GPT-5 系列模型的迭代。根据此前OpenAI公布的信息显示，\"星门\"项目预计在2029年前在全球构建10 吉瓦 (GW)级别的专属 AI 超级计算网络，这是OpenAI参与到的最大规模的算力基础设施类项目。\n\n就在这笔融资新闻爆出前不到10天内，2月11日，OpenAI解散了内部的Mission Alignment（使命对齐）团队。值得注意的是，Mission Alignment曾是OpenAI区别于其他AI公司的\"灵魂型\"部门之一，它的\"被解散\"和OpenAI的战略目标、内部商业化权重进一步提高都有深度联系。\n\nMission Alignment成立于2024年9月，当时的背景是，OpenAI正在尝试越来越多的商业化项目，为了确保公司不偏离\"AGI惠及全人类\"这一核心使命，公司成立了具备独立监督职责的Mission Alignment。\n\n一个关键细节是，在成立后，Mission Alignment会参与到公司产品研发等关键决策之中，并对项目和路线是否符合公司使命进行\"一致性评估\"。Mission Alignment的一个重要任务恰恰是\"避免商业化压力和因素，导致OpenAI整体战略跑偏\"。\n\n在成立后，Mission Alignment深度参与到了\"星门\"、F轮融资、GPT-5/GPT-6开发计划等OpenAI核心战略项目之中。\n\n在2月11日Mission Alignment\"被解散\"消息传出后，硅谷部分科技博主和AI安全专家认为，这是OpenAI试图进一步强化\"商业化优先\"的信号之一。\n\n一个深层的影响是，Mission Alignment这一部门的消失，本质上意味着OpenAI内部的项目流程、安全决策体系将发生质变，从积极的一面看，OpenAI可能会大幅度缩短产品迭代周期，这和OpenAI在1000亿美元融资前所多次公开表示的\"尽快推出GPT-5全系列模型\"态度不谋而合。\n\n而挑战的一面是，OpenAI原本的\"产品+商业化\"与\"安全和伦理\"之间的平衡关系，会被打破。考虑到OpenAI正试图比老对手Anthropic率先上市，它势必需要在2026年拿出至少一款超预期、能够赢得市场广泛认可心智的爆品。这意味着，Mission Alignment的\"被解散\"可能只是OpenAI砍掉的第一个\"减速器\"而非最后一个，接下来OpenAI可能会进一步对非产品和商业化导向的机制、架构动刀。\n\n让OpenAI如此急迫的，其实是它的亏损现状。\n\n根据OpenAI CFO Sarah Friar官网博客信息显示，OpenAI2025年年化收入(ARR)从2023年到2025年分别是20亿美元、60亿美元、不少于200亿美元。\n\n但据The Information2025年9月的报道显示，2025年上半年OpenAI实际经营亏损约78亿美元；而《华尔街日报》在2026年初的推测认为，OpenAI2025年全年预计亏损约90亿美元。\n\n在OpenAI CFO Sarah Friar的博客中，Sarah Friar披露过OpenAI的收入结构，其中企业订阅服务占比约60%、API（针对开发者）占比约25%、C端消费者服务占比约15%。\n\n这意味着，OpenAI在赚钱这方面，目前依然要和Anthropic等竞对正面开战。\n\n提高C端用户的付费率，已经成为OpenAI的当务之急。据公开信息和OpenAI官网内容显示，2026年OpenAI会重点降低获客成本，提升客户留存和付费转化率。\n\n但考虑到OpenAI参与的\"星门\"需要持续投入，以及英伟达等算力厂商2026年大概率的涨价策略，OpenAI的成本压力只能更高。\n\n换言之，它确实急需通过融资\"续命\"+\"造血\"，但这形成了一个\"莫比乌斯环\"：为了扩大C端和企业级用户量级付费率，OpenA需要通过投入更多算力资源去提高模型能力；但如果想短时间内尽快\"堆积\"更高算力，便需要尽快投入巨资抢占资源优势（或完善基础设施）；但资本方已经不是AI圈跑马圈地的时代，正在聚焦于AI产品的商业化。\n\n所以OpenAI解散了Mission Alignment，招入了OpenClaw（Clawdbot、Moltbot）之父Peter Steinberger（负责Agent），这些动作本质上都是为了尽快完善产品迭代、打通商业化路径。\n\n一个危机信号来自竞对。\n\n在完成G轮300亿美元融资前后，Anthropic在2月5日和2月17日分别发布了Claude Opus 4.6旗舰模型（Humanity's Last Exam测试53.1%，达到全球第一）、Claude Sonnet 4.6中端模型（以1/5旗舰价格实现近乎99%旗舰性能）。近几日，硅谷AI舆论圈几乎被Anthropic相关热度话题淹没。\n\n而早在2025年7月美国知名风险投资机构Menlo Ventures发布的《2025企业AI报告》显示Anthropic在2025年便以32%企业级LLM支出占比超越OpenAI（25%），成为B端第一大模型厂商。\n\n这意味着，OpenAI在战略上已经被逼到了墙角：它必须尽快拿出一款颠覆性的产品。\n\n当然，这个产品大概率不能是另一款\"谄媚\"的GPT-4o。\n\n本文来自虎嗅，原文链接：https://www.huxiu.com/article/4835859.html?f=wyxwapp"
  },
  {
    "source": "WebProNews",
    "company": "OpenAI",
    "title": "OpenAI's Growth Gambit: Inside Sam Altman's Push to Reclaim Momentum as ChatGPT Hits a Pivotal Inflection Point",
    "date": "2026-02-09T17:07:14Z",
    "url": "https://www.webpronews.com/openais-growth-gambit-inside-sam-altmans-push-to-reclaim-momentum-as-chatgpt-hits-a-pivotal-inflection-point/",
    "content": "In a candid internal message to staff, OpenAI Chief Executive Sam Altman declared that ChatGPT has returned to \"exceeding 10% monthly growth,\" a milestone that signals renewed momentum for the artificial intelligence company as it navigates an increasingly competitive market and prepares for what could be the largest private funding round in technology history. The disclosure, shared with employees in recent days, comes at a moment when OpenAI finds itself fighting on multiple fronts -- defending its technological lead against a surging Anthropic, preparing to introduce advertising into its flagship product for the first time, and racing to close a funding round that could value the company at roughly $300 billion.\n\nThe growth figure is significant not merely as a vanity metric but as a critical data point for investors who are being asked to write checks of unprecedented size. OpenAI is in the final stages of securing approximately $40 billion in new funding, which would bring its total capital raised to roughly $100 billion -- a staggering sum that underscores both the enormous promise and the enormous appetite of large-scale AI development. As CNBC reported, Altman's internal communication was designed to rally the troops at a time when competitive pressures have intensified and the stakes for execution have never been higher.\n\nThe funding trajectory OpenAI has charted is without precedent in the history of private technology companies. The roughly $40 billion round, led by SoftBank with participation from other major institutional investors, would cement OpenAI's position as the most heavily capitalized startup ever. But that capital comes with expectations -- expectations of sustained user growth, expanding revenue, and a clear path to the kind of returns that justify a valuation approaching $300 billion. Altman's decision to share the 10% monthly growth figure with staff suggests an awareness that internal confidence is as important as external fundraising narratives. When employees see the product gaining traction, they are more likely to stay focused and less likely to be lured away by competitors offering equity in rival ventures.\n\nThe financial picture at OpenAI has evolved dramatically over the past year. The company's annualized revenue has grown substantially, driven by the success of ChatGPT's subscription tiers -- including the $20-per-month Plus plan and the $200-per-month Pro offering -- as well as expanding enterprise contracts. But revenue growth alone doesn't tell the full story. The cost of training and running frontier AI models remains extraordinarily high, with OpenAI spending billions on compute infrastructure. This is precisely why the company needs the kind of capital infusion it is now pursuing, and why demonstrating robust user growth is essential to maintaining investor confidence.\n\nPerhaps the most urgent competitive threat facing OpenAI comes from Anthropic, the AI safety-focused company founded by former OpenAI executives Dario and Daniela Amodei. Anthropic's Claude model has made significant inroads among software developers, with its improved coding capabilities drawing praise from engineers who have found Claude's outputs to be more reliable and contextually aware for programming tasks. This is not a peripheral concern for OpenAI -- coding assistance represents one of the highest-value use cases for large language models, and losing ground in this domain could have cascading effects on enterprise adoption and developer mindshare.\n\nAltman's mention of an \"updated Chat model\" coming this week appears to be a direct response to Anthropic's gains. While the specifics of the model update remain closely guarded, industry observers expect improvements focused on reasoning capabilities, code generation accuracy, and the kind of multi-step problem solving that professional developers demand. The timing is deliberate: by shipping improvements on a rapid cadence, OpenAI aims to demonstrate that it can iterate faster than its competitors and that ChatGPT remains the most capable general-purpose AI assistant on the market. The internal message to staff carried an unmistakable tone of urgency -- this is a company that knows it cannot afford to rest on the laurels of being first to market.\n\nIn what may prove to be one of the most consequential business decisions in OpenAI's history, the company has begun testing advertisements within ChatGPT. As The Verge reported, OpenAI has started a pilot program to introduce ads into its free-tier ChatGPT experience, marking the first time the company has pursued advertising revenue in its core consumer product. The move represents a fundamental shift in OpenAI's business model, which has until now relied almost exclusively on subscription fees and enterprise licensing agreements.\n\nThe advertising rollout, which 9to5Mac confirmed began rolling out to users, raises profound questions about the future of AI-assisted search and the broader digital advertising ecosystem. If ChatGPT can serve contextually relevant ads within conversational AI interactions, it could capture a share of the hundreds of billions of dollars currently flowing to Google, Meta, and Amazon's advertising platforms. For OpenAI, the financial logic is compelling: with hundreds of millions of users on the free tier, even modest ad revenue per user could generate billions in annual income, helping to offset the enormous infrastructure costs associated with running large language models at scale.\n\nThe introduction of advertising into ChatGPT is not without risk. OpenAI has built its consumer brand on the promise of a helpful, unbiased AI assistant. Inserting commercial messages into that experience could erode user trust, particularly if ads are perceived as influencing the quality or objectivity of ChatGPT's responses. The company appears to be proceeding cautiously, initially limiting ads to specific contexts and clearly labeling sponsored content. But the history of digital advertising is littered with examples of companies that started with restrained ad placements only to gradually increase ad density as revenue pressures mounted.\n\nIndustry commentators have been quick to weigh in on the implications. On X, user @kimmonismus noted the significance of OpenAI's advertising move in the context of the company's broader strategic evolution, highlighting how the shift signals OpenAI's maturation from a research laboratory into a full-fledged consumer technology company. This transformation has been underway for some time -- OpenAI's controversial restructuring from a nonprofit to a capped-profit entity, and more recently its exploration of a full for-profit conversion, have all pointed in this direction. But advertising makes the commercial intent unmistakable in a way that subscription pricing alone did not.\n\nAltman's promise of an \"updated Chat model\" this week has set off a wave of speculation among AI researchers and industry analysts about what specific improvements OpenAI will deliver. The most likely areas of focus include enhanced reasoning and chain-of-thought capabilities, improved performance on coding benchmarks, better handling of long-context tasks, and refinements to the model's ability to follow complex, multi-part instructions. These are precisely the areas where Anthropic's Claude has been gaining ground, and OpenAI's model team -- led by some of the most accomplished machine learning researchers in the world -- has been under intense pressure to respond.\n\nThe cadence of model releases has become a competitive weapon in its own right. Unlike the early days of the AI boom, when a new model release was a major event that happened once or twice a year, the leading AI companies now ship improvements on a near-continuous basis. OpenAI has adopted an increasingly aggressive release schedule, pushing updates to GPT-4o and its successors at a pace that would have been unthinkable just two years ago. This rapid iteration cycle is enabled by the company's massive investment in compute infrastructure, including partnerships with Microsoft Azure and its own custom hardware initiatives. For enterprise customers evaluating which AI platform to standardize on, the frequency and quality of model updates is a critical factor in their decision-making.\n\nOpenAI's challenges extend well beyond the head-to-head competition with Anthropic. Google's Gemini models continue to improve, benefiting from the search giant's unmatched data assets and its deep integration with the Android ecosystem and Google Workspace. Meta's open-source Llama models have created a thriving ecosystem of fine-tuned variants that are increasingly competitive with proprietary offerings for specific use cases. And a new generation of Chinese AI companies, including DeepSeek, has demonstrated that frontier-level capabilities can be achieved at a fraction of the cost that Western companies have assumed necessary.\n\nThe DeepSeek phenomenon, in particular, has sent shockwaves through the AI industry. The Chinese company's ability to produce models that rival GPT-4-class performance using reportedly far less compute has raised uncomfortable questions about whether the massive capital expenditures being made by OpenAI, Google, and others are truly necessary -- or whether there are more efficient paths to artificial general intelligence. For OpenAI, which is asking investors to commit $40 billion on the premise that scale is destiny, the DeepSeek challenge strikes at the heart of its strategic thesis. Altman has pushed back on the notion that OpenAI is overspending, arguing that the company's infrastructure investments will pay dividends as AI capabilities continue to advance and new applications emerge.\n\nWhile consumer growth metrics like the 10% monthly figure grab headlines, OpenAI's long-term financial viability depends heavily on its enterprise business. ChatGPT Enterprise and the company's API platform serve thousands of businesses, from Fortune 500 corporations to fast-growing startups. Enterprise customers typically generate far more revenue per user than consumers, and they tend to be stickier -- once a company has integrated an AI platform into its workflows, switching costs create a powerful retention mechanism. OpenAI has been investing heavily in enterprise features, including enhanced security controls, custom model fine-tuning, and administrative tools that make it easier for large organizations to deploy AI at scale.\n\nThe enterprise push is also where OpenAI's partnership with Microsoft becomes most strategically important. Microsoft has integrated OpenAI's models into its Copilot products across the Office 365 suite, GitHub, and Azure, creating distribution channels that no other AI company can match. This partnership gives OpenAI access to Microsoft's vast enterprise customer base, while Microsoft benefits from having the most advanced AI models embedded in its productivity tools. The symbiotic relationship has been a key competitive advantage for both companies, though it has also created tensions -- particularly as Microsoft has occasionally pursued its own AI initiatives that appear to compete with OpenAI's direct offerings.\n\nAltman's specific mention of 10% monthly growth deserves careful parsing. In the context of a product that already has hundreds of millions of monthly users, 10% month-over-month growth is extraordinary -- if sustained, it implies a doubling of the user base roughly every seven months. The \"back to\" phrasing in Altman's message is equally telling, suggesting that there was a period when growth had slowed below this threshold. This is not unusual for consumer technology products, which often experience growth plateaus as they saturate their initial target markets. The fact that growth has reaccelerated suggests that OpenAI's recent product improvements, expanded feature set, and growing brand awareness are successfully pulling in new user cohorts.\n\nFor the investors participating in the $40 billion funding round, this growth trajectory is essential to the investment thesis. At a valuation approaching $300 billion, OpenAI is being priced not on its current revenue but on its potential to become one of the most important technology platforms of the coming decade. The comparison that many investors draw is to the early days of Google or the iPhone -- moments when a new technology paradigm was emerging and the companies that established early dominance went on to generate hundreds of billions in annual revenue. Whether OpenAI can fulfill that promise remains an open question, but the 10% monthly growth figure provides at least one data point suggesting the trajectory is intact.\n\nAs OpenAI prepares to ship its updated model and expand its advertising program, the company stands at a pivotal moment in its evolution. The decisions it makes in the coming months -- about model architecture, business model, competitive positioning, and organizational structure -- will shape the trajectory of the entire AI industry. Altman's internal message to staff was more than a pep talk; it was a strategic signal that OpenAI intends to compete aggressively on every front, from consumer growth to enterprise adoption to the fundamental science of artificial intelligence.\n\nThe challenges are formidable. Anthropic is gaining ground in coding and enterprise applications. Google has unmatched distribution and data advantages. Open-source alternatives are becoming increasingly viable. Regulators in the United States and Europe are circling, with new AI governance frameworks that could impose significant compliance costs. And the sheer financial burden of operating at the frontier of AI research means that even with $100 billion in total funding, OpenAI cannot afford strategic missteps. But if the 10% monthly growth figure is any indication, the company's core product continues to resonate with users in a way that few technology products ever have. The next chapter of the AI revolution is being written in real time, and OpenAI clearly intends to hold the pen."
  },
  {
    "source": "tmtpost.com",
    "company": "OpenAI",
    "title": "Anthropic遭遇OpenAI贴身肉搏，上市路多了个坎-钛媒体官方网站",
    "date": "2026-01-29T12:54:45Z",
    "url": "https://www.tmtpost.com/7860246.html",
    "content": "Prism到Codex，OpenAI对Anthropic核心腹地的精准打击。\n\nAnthropic的CEO达里奥·阿莫迪这两天执笔了一篇2万字的长文，他写到\"人类即将获得几乎难以想象的力量，而我们的社会、政治和技术体系是否具备驾驭这种力量的成熟度，目前还非常不明朗。\"\n\n阿莫迪认为，最多再过几年，AI会在几乎所有方面都将优于人类，而且这是无可避免的。\n\n与此同时他和联合创始人做出了一个在硅谷几乎前所未有的承诺，他们将捐出80%的个人财富用于慈善事业，Anthropic的员工也承诺捐出公司股份。\n\n无巧不成书，刚刚做出慈善承诺的阿莫迪，就受到了来自OpenAI的产品组合拳。\n\n放在以往，这并不会引起Anthropic多么大的反应。但现如今不同，此时此刻，正是Anthropic和OpenAI的融资竞赛。\n\n两家全球最顶尖的AI公司同时发起融资，额度均为上百亿美元，且这笔融资对于Anthropic来说，还是他们上市前最后一笔融资，不能由得半点马虎。\n\nAnthropic唯一的选择就是接招，要么加速融资，要么加速产品迭代。\n\n然而OpenAI这次的攻势并非大范围火力覆盖，而是对Anthropic最核心的业务发起精准打击，以至于留给Anthropic证明自己的时间窗口正在急剧收窄。\n\n2026年1月27日，OpenAI正式推出了Prism，这是一个由GPT-5.2驱动的AI原生科研工作空间。\n\nPrism是一个深度整合了LaTeX编辑、文献检索、公式管理、协作评审于一体的云端科研平台。\n\n它的核心价值在于将科研写作过程中原本碎片化的工具链整合到单一工作流中，让GPT-5.2能够在完整的论文上下文中工作，理解公式、引用、图表以及整体结构之间的关联关系。\n\n写过理工科论文的一定体会过，过去你需要先在word里写，然后要切换到Overleaf编LaTeX 公式，再打开Zotero管理参考文献，接下来要花好几个小时画图表。\n\n而现在，Prism一个平台就可以搞定上述所有流程，还引入ChatGPT，通过AI来获取信息。\n\nPrism建立在OpenAI收购云端LaTeX平台Crixet的基础之上。\n\n通过将Crixet成熟的技术与GPT-5.2的推理能力深度融合，Prism实现了多项突破性功能。\n\n不只是这样，它还能直接从arXiv等学术平台检索相关文献，并根据论文语境自动生成参考文献列表。更重要的是，它支持无限数量的项目和协作者，完全免费向所有ChatGPT个人账户用户开放。\n\nOpenAI科学副总裁凯文·威尔（Kevin Weil）在发布会上做出了一个大胆的预言：\"我认为2026年之于AI与科学，将如同2025年之于AI与软件工程。\"\n\n科研和AI编程是OpenAI今年的大方向。\n\n说到AI编程，目前OpenAI正处于Codex发布月，会在1月24日到2月24日期间持续发布Codex相关产品。\n\n其实从2025年下半年开始，OpenAI就一直在持续强化其开发者工具生态。Codex已经从最初的独立API演变为一个完整的开发者平台，包括Codex CLI命令行工具、Codex Web云端自主编码代理以及IDE集成插件。\n\n根据OpenAI的官方消息，Codex的使用量自2025年8月以来，日均消息量增长10倍，每周处理数万亿个token。\n\n与此同时，OpenAI内部工程师使用Codex的占比，也从7月的50%升到了100%，每周合并PR数量增加70%、\n\n相当于绝大多数用Codex编写的代码，都通过了质量验证，并且这些代码都实际应用到了产品开发之中。\n\nOpenAI还通过开放API的方式，让Codex模型能够集成到Cursor、Windsurf、Factory和GitHub等主流开发工具中，形成了一个相对完整的生态系统。\n\nOpenAI正在从\"大而全\"的通用AI工具，向垂直领域的深度整合转型。\n\nCodex针对软件开发者，Prism针对科研人员，OpenAI的战略意图十分明确，要垂直化抢占高价值和专业用户市场。\n\n这种转变并非偶然，而是基于对市场需求的深刻洞察。通用型聊天机器人虽然用户基数庞大，但用户粘性和付费意愿相对较低。\n\n相比之下，专业领域的用户对工具的依赖度更高，付费能力也更强，更容易形成稳定的商业模式。\n\n从产品发布的节奏来看，OpenAI显然经过了精心策划。Prism的推出时机选择在1月底，正值学术界新学期开始，许多科研项目进入启动阶段。\n\nCodex发布月横跨整个2月，恰好覆盖了企业年度预算确定和Q1技术采购决策高峰期。\n\n更值得关注的是，OpenAI在推出这些垂直化产品时，并没有采取高价策略。\n\n这种策略的目的很明确，先通过免费或低价快速占领市场，培养用户习惯，然后再通过企业版和高级功能实现商业化。\n\n当我们把视角拉远就会发现，OpenAI推出Prism和强化Codex生态，其战略矛头直指Anthropic最核心的竞争优势，深度智力工作场景。\n\n长期以来，AI圈内存在一种默认共识，ChatGPT适合大众日常使用和闲聊，Claude更适合写代码、读论文和做科研。\n\nAnthropic凭借更严谨的逻辑推理以及\"少废话\"的风格，牢牢占据了科研人员、程序员和深度内容创作者的心智。\n\n许多开发者在社区分享使用心得时，都会提到Claude在处理复杂任务时的表现更加稳定，输出的内容更加精准，不会像ChatGPT那样经常出现\"谄媚\"或\"过度发挥\"的情况。\n\nAnthropic曾在去年10月推出\"Claude for Life Sciences\"，这是一个专门为医疗、科研领域优化模型。\n\n1月12日，Anthropic又为这条产品线带来了多项更新，深度集成了Medidata、ClinicalTrials.gov、OpenTargets等平台，能够自动化临床试验操作和监管提交流程。\n\n多家制药公司和研究机构已经开始使用Claude来加速药物研发流程，从文献综述到实验方案设计，再到监管文件准备，Claude都能提供实质性的帮助。\n\n因此，Prism几乎是贴身肉搏，OpenAI就差明着布告说自己是冲着Anthropic去的。\n\nOpenAI不希望在\"AI for Science\"这个未来最重要的增长点上输给Anthropic。\n\n科研领域的市场规模虽然不如消费级应用，但其战略价值不容小觑。\n\n掌握了科研工具的话语权，就意味着能够影响知识生产的方式，进而影响整个社会的创新能力。\n\n更重要的是，科研用户往往是技术的早期采用者和意见领袖，他们的选择会对其他用户群体产生示范效应。\n\nAnthropic之前的杀手锏是Artifacts功能，它把AI从\"一问一答\"的聊天机器人变成了能实时生成代码、预览文档、管理上下文的\"生产力伙伴\"。\n\n用户可以在对话过程中直接看到代码运行结果，可以实时修改和调试，整个工作流程变得非常流畅。\n\n这种体验上的优势，让Claude在开发者群体中建立了良好的口碑。OpenAI虽然推出了Canvas作为回应，但并未完全改变局面。\n\n但这次不一样，Prism集成了科研写作、数据分析和推理协作，直接拆解了Anthropic在长程任务上的优势。以及接下来一个月内数款Codex产品。\n\nOpenAI想用这些产品，向高价值用户传递这么一个信息\"Claude能做的我们ChatGPT能做，Claude做不了的我们ChatGPT也能做。\"\n\n这种全方位的竞争姿态，给Anthropic带来了巨大的压力。过去，Anthropic可以专注于做好自己擅长的事情，在细分领域建立优势。但现在，OpenAI开始在每个细分领域都推出针对性的产品，Anthropic的生存空间被不断压缩。\n\nOpenAI入局，让这场战争从\"对话框\"升级到了\"工作流\"。\n\n过去的竞争主要集中在模型能力上，谁的回答更准确、更流畅、更符合用户期望。现在的竞争则转向了产品化能力，谁能把AI更好地嵌入到用户的实际工作流程中，谁就能赢得市场。\n\n这种转变对Anthropic来说是个挑战，因为OpenAI在产品化和生态建设方面有更多的资源和经验。\n\n在\"严谨性\"这个Anthropic引以为傲的领域，OpenAI也在发起挑战。\n\nAnthropic一直标榜\"HHH\"原则，即Helpful、Honest、Harmless，在安全性及减少幻觉方面口碑较好，这对科研用户至关重要。\n\n没有人愿意在论文中引用一个AI编造的文献，也没有人愿意基于AI生成的错误代码进行开发。\n\n但OpenAI试图在\"严谨性\"上反超Claude，至少从Prism上来看是这样的。这等于是在攻击Anthropic\"最安全、最可信\"的品牌形象。\n\n此外，OpenAI很可能会开始跟Anthropic打价格战。\n\nAnthropic目前一个非常大的痛点就是太贵了。Claude Code成本非常高，重度用户一天单哪怕是最高级的订阅也经常出现达到上限的情况，因此被迫需要开多个账号才能满足日常需求。\n\n资本市场正在见证AI领域有史以来最激烈的融资竞赛。\n\n2026年1月，Anthropic将其最新一轮融资目标从100亿美元大幅提高至200亿美元，融资后估值预计达到3500亿美元。这个估值相比2025年9月的1830亿美元，在短短四个月内几乎翻倍。\n\n这轮融资由新加坡主权财富基金GIC和CoatueManagement领投，红杉资本、微软、英伟达等豪华阵容参与。\n\n红杉资本同时也是OpenAI的投资方，这种\"脚踏两只船\"的投资策略，在硅谷并不罕见，但也反映出资本对AI赛道的谨慎态度，不愿意把鸡蛋放在一个篮子里。\n\n虽然Anthropic融得数额大，但是他们的生存压力同样也挺大的。\n\n该公司2026年的营收目标从150亿美元上调至180亿美元，对比2025年的47亿美元营收，增长近四倍。2027年更是预期营收约550亿美元，这个增长速度在整个科技行业都算是疯狂的。\n\n然而，Anthropic将盈利时间从2027年推迟至2028年，比预期晚了一年。这意味着公司在未来两年内仍然需要持续烧钱，资金主要用于Claude5模型训练和全球算力基础设施扩建，这些都是烧钱的无底洞。\n\n随着模型规模的扩大和训练数据的增加，所需的算力呈指数级增长，这就导致训练大型语言模型的成本正在快速上升。\n\nAnthropic之前曾表示，要在美国投入500亿美元建设数据中心，这个数字听起来惊人，但在当前的AI竞赛中，这可能只是起步价。\n\n数据中心的建设不仅需要巨额资金，还需要大量的电力供应和冷却系统，这些基础设施的建设周期往往需要数年时间。\n\n在这个过程中，Anthropic要一直保持足够的资金支撑，否则就可能面临资金链断裂的风险。\n\n实际上OpenAI这边的融资规模更加惊人。\n\n1月28日传出消息，软银正在向OpenAI追加300亿美元投资，而且这还只是阶段性目标，长期只会投入更多。\n\n在此之前，软银曾在2025年12月投了OpenAI410亿美元。\n\n但是这笔融资对于目前的OpenAI来说还远远不够，他们正在寻求总计高达1000亿美元的融资，估值可能达到8300亿美元。\n\n亚马逊和英伟达也在参与这轮融资的讨论。\n\n这个融资规模如果成功，将创下科技行业的历史纪录。\n\n之所以Anthropic和OpenAI都在扩大融资，其原因很简单，两家公司都希望能在2026或者2027年上市。\n\n本来一帆风顺的Anthropic，突然和OpenAI开始碰撞，那么将会大大阻碍其融资乃至上市的进度，因为这会直接降低其市场份额和预期收入。\n\n投资者在评估一家公司的价值时，不仅看其当前的表现，更看其未来的增长潜力。\n\n如果市场认为Anthropic在与OpenAI的竞争中处于下风，其估值就会受到影响。这种影响不仅体现在融资估值上，更会影响到未来的IPO定价。\n\nAnthropic已经聘请知名律所WilsonSonsini启动IPO准备工作，并与多家投资银行进行初步沟通，最快可能在2026年下半年上市。\n\nWilsonSonsini是硅谷最负盛名的律所之一，曾经协助谷歌和LinkedIn等科技巨头完成上市。\n\n摆在Anthropic前只有两条路。\n\n第一条路是在OpenAI占领这些垂直市场之前完成上市，锁定估值，否则可能面临估值大幅缩水的风险。\n\n上市能够为公司带来大量资金，也能为早期投资者提供退出渠道。但上市也意味着公司将面临更严格的监管和信息披露要求，每个季度的财务表现都会受到市场的审视。\n\n第二条路是通过产品正面击败OpenAI。\n\n这条路更加艰难，但如果成功，回报也更大。\n\nAnthropic需要在产品功能、用户体验、生态建设等多个方面全面超越OpenAI，才能在竞争中占据优势。\n\n这需要大量的研发投入和时间，但时间恰恰是Anthropic最缺乏的资源。在快速变化的AI市场中，每一天的延迟都可能意味着市场份额的流失。\n\n从融资节奏来看，Anthropic正在加速。从2024年3月的35亿美元融资，到9月的130亿美元融资，再到现在的200亿美元融资目标，融资规模在不断扩大，融资间隔在不断缩短。\n\n这种加速度反映出公司对资金的迫切需求，也反映出市场对AI赛道的持续看好。\n\n但融资只是手段，不是目的。最终决定胜负的，还是产品和技术。无论融到多少钱，如果产品无法满足用户需求，市场份额就会流失。\n\n在当前的AI竞赛中，技术迭代速度极快，今天的领先优势可能明天就会被超越。Anthropic和OpenAI都明白这个道理。\n\n这场竞赛的结果，也将在很大程度上决定AI行业未来的格局。"
  },
  {
    "source": "凤凰网（凤凰新媒体）",
    "company": "OpenAI",
    "title": "Anthropic遭遇OpenAI贴身肉搏，上市路多了个坎",
    "date": "2026-01-29T09:22:24Z",
    "url": "https://tech.ifeng.com/c/8qJPWRkchfU",
    "content": "Anthropic的CEO达里奥·阿莫迪这两天执笔了一篇2万字的长文，他写到\"人类即将获得几乎难以想象的力量，而我们的社会、政治和技术体系是否具备驾驭这种力量的成熟度，目前还非常不明朗。\"\n\n阿莫迪认为，最多再过几年，AI会在几乎所有方面都将优于人类，而且这是无可避免的。\n\n与此同时他和联合创始人做出了一个在硅谷几乎前所未有的承诺，他们将捐出80%的个人财富用于慈善事业，Anthropic的员工也承诺捐出公司股份。\n\n无巧不成书，刚刚做出慈善承诺的阿莫迪，就受到了来自OpenAI的产品组合拳。\n\n放在以往，这并不会引起Anthropic多么大的反应。但现如今不同，此时此刻，正是Anthropic和OpenAI的融资竞赛。\n\n两家全球最顶尖的AI公司同时发起融资，额度均为上百亿美元，且这笔融资对于Anthropic来说，还是他们上市前最后一笔融资，不能由得半点马虎。\n\nAnthropic唯一的选择就是接招，要么加速融资，要么加速产品迭代。\n\n然而OpenAI这次的攻势并非大范围火力覆盖，而是对Anthropic最核心的业务发起精准打击，以至于留给Anthropic证明自己的时间窗口正在急剧收窄。\n\n01\n\nOpenAI系列产品发布\n\n2026年1月27日，OpenAI正式推出了Prism，这是一个由GPT-5.2驱动的AI原生科研工作空间。\n\nPrism是一个深度整合了LaTeX编辑、文献检索、公式管理、协作评审于一体的云端科研平台。\n\n它的核心价值在于将科研写作过程中原本碎片化的工具链整合到单一工作流中，让GPT-5.2能够在完整的论文上下文中工作，理解公式、引用、图表以及整体结构之间的关联关系。\n\n写过理工科论文的一定体会过，过去你需要先在word里写，然后要切换到Overleaf编LaTeX 公式，再打开Zotero管理参考文献，接下来要花好几个小时画图表。\n\n而现在，Prism一个平台就可以搞定上述所有流程，还引入ChatGPT，通过AI来获取信息。\n\nPrism建立在OpenAI收购云端LaTeX平台Crixet的基础之上。\n\n通过将Crixet成熟的技术与GPT-5.2的推理能力深度融合，Prism实现了多项突破性功能。\n\n不只是这样，它还能直接从arXiv等学术平台检索相关文献，并根据论文语境自动生成参考文献列表。更重要的是，它支持无限数量的项目和协作者，完全免费向所有ChatGPT个人账户用户开放。\n\nOpenAI科学副总裁凯文·威尔（Kevin Weil）在发布会上做出了一个大胆的预言：\"我认为2026年之于AI与科学，将如同2025年之于AI与软件工程。\"\n\n科研和AI编程是OpenAI今年的大方向。\n\n说到AI编程，目前OpenAI正处于Codex发布月，会在1月24日到2月24日期间持续发布Codex相关产品。\n\n其实从2025年下半年开始，OpenAI就一直在持续强化其开发者工具生态。Codex已经从最初的独立API演变为一个完整的开发者平台，包括Codex CLI命令行工具、Codex Web云端自主编码代理以及IDE集成插件。\n\n根据OpenAI的官方消息，Codex的使用量自2025年8月以来，日均消息量增长10倍，每周处理数万亿个token。\n\n与此同时，OpenAI内部工程师使用Codex的占比，也从7月的50%升到了100%，每周合并PR数量增加70%、\n\n相当于绝大多数用Codex编写的代码，都通过了质量验证，并且这些代码都实际应用到了产品开发之中。\n\nOpenAI还通过开放API的方式，让Codex模型能够集成到Cursor、Windsurf、Factory和GitHub等主流开发工具中，形成了一个相对完整的生态系统。\n\nOpenAI正在从\"大而全\"的通用AI工具，向垂直领域的深度整合转型。\n\nCodex针对软件开发者，Prism针对科研人员，OpenAI的战略意图十分明确，要垂直化抢占高价值和专业用户市场。\n\n这种转变并非偶然，而是基于对市场需求的深刻洞察。通用型聊天机器人虽然用户基数庞大，但用户粘性和付费意愿相对较低。\n\n相比之下，专业领域的用户对工具的依赖度更高，付费能力也更强，更容易形成稳定的商业模式。\n\n从产品发布的节奏来看，OpenAI显然经过了精心策划。Prism的推出时机选择在1月底，正值学术界新学期开始，许多科研项目进入启动阶段。\n\nCodex发布月横跨整个2月，恰好覆盖了企业年度预算确定和Q1技术采购决策高峰期。\n\n更值得关注的是，OpenAI在推出这些垂直化产品时，并没有采取高价策略。\n\n这种策略的目的很明确，先通过免费或低价快速占领市场，培养用户习惯，然后再通过企业版和高级功能实现商业化。\n\n02\n\n瞄准Anthropic，贴身肉搏\n\n当我们把视角拉远就会发现，OpenAI推出Prism和强化Codex生态，其战略矛头直指Anthropic最核心的竞争优势，深度智力工作场景。\n\n长期以来，AI圈内存在一种默认共识，ChatGPT适合大众日常使用和闲聊，Claude更适合写代码、读论文和做科研。\n\nAnthropic凭借更严谨的逻辑推理以及\"少废话\"的风格，牢牢占据了科研人员、程序员和深度内容创作者的心智。\n\n许多开发者在社区分享使用心得时，都会提到Claude在处理复杂任务时的表现更加稳定，输出的内容更加精准，不会像ChatGPT那样经常出现\"谄媚\"或\"过度发挥\"的情况。\n\nAnthropic曾在去年10月推出\"Claude for Life Sciences\"，这是一个专门为医疗、科研领域优化模型。\n\n1月12日，Anthropic又为这条产品线带来了多项更新，深度集成了Medidata、ClinicalTrials.gov、OpenTargets等平台，能够自动化临床试验操作和监管提交流程。\n\n多家制药公司和研究机构已经开始使用Claude来加速药物研发流程，从文献综述到实验方案设计，再到监管文件准备，Claude都能提供实质性的帮助。\n\n因此，Prism几乎是贴身肉搏，OpenAI就差明着布告说自己是冲着Anthropic去的。\n\nOpenAI不希望在\"AI for Science\"这个未来最重要的增长点上输给Anthropic。\n\n科研领域的市场规模虽然不如消费级应用，但其战略价值不容小觑。\n\n掌握了科研工具的话语权，就意味着能够影响知识生产的方式，进而影响整个社会的创新能力。\n\n更重要的是，科研用户往往是技术的早期采用者和意见领袖，他们的选择会对其他用户群体产生示范效应。\n\nAnthropic之前的杀手锏是Artifacts功能，它把AI从\"一问一答\"的聊天机器人变成了能实时生成代码、预览文档、管理上下文的\"生产力伙伴\"。\n\n用户可以在对话过程中直接看到代码运行结果，可以实时修改和调试，整个工作流程变得非常流畅。\n\n这种体验上的优势，让Claude在开发者群体中建立了良好的口碑。OpenAI虽然推出了Canvas作为回应，但并未完全改变局面。\n\n但这次不一样，Prism集成了科研写作、数据分析和推理协作，直接拆解了Anthropic在长程任务上的优势。以及接下来一个月内数款Codex产品。\n\nOpenAI想用这些产品，向高价值用户传递这么一个信息\"Claude能做的我们ChatGPT能做，Claude做不了的我们ChatGPT也能做。\"\n\n这种全方位的竞争姿态，给Anthropic带来了巨大的压力。过去，Anthropic可以专注于做好自己擅长的事情，在细分领域建立优势。但现在，OpenAI开始在每个细分领域都推出针对性的产品，Anthropic的生存空间被不断压缩。\n\nOpenAI入局，让这场战争从\"对话框\"升级到了\"工作流\"。\n\n过去的竞争主要集中在模型能力上，谁的回答更准确、更流畅、更符合用户期望。现在的竞争则转向了产品化能力，谁能把AI更好地嵌入到用户的实际工作流程中，谁就能赢得市场。\n\n这种转变对Anthropic来说是个挑战，因为OpenAI在产品化和生态建设方面有更多的资源和经验。\n\n在\"严谨性\"这个Anthropic引以为傲的领域，OpenAI也在发起挑战。\n\nAnthropic一直标榜\"HHH\"原则，即Helpful、Honest、Harmless，在安全性及减少幻觉方面口碑较好，这对科研用户至关重要。\n\n没有人愿意在论文中引用一个AI编造的文献，也没有人愿意基于AI生成的错误代码进行开发。\n\n但OpenAI试图在\"严谨性\"上反超Claude，至少从Prism上来看是这样的。这等于是在攻击Anthropic\"最安全、最可信\"的品牌形象。\n\n此外，OpenAI很可能会开始跟Anthropic打价格战。\n\nAnthropic目前一个非常大的痛点就是太贵了。Claude Code成本非常高，重度用户一天单哪怕是最高级的订阅也经常出现达到上限的情况，因此被迫需要开多个账号才能满足日常需求。\n\n03\n\n融资数额突破历史\n\n资本市场正在见证AI领域有史以来最激烈的融资竞赛。\n\n2026年1月，Anthropic将其最新一轮融资目标从100亿美元大幅提高至200亿美元，融资后估值预计达到3500亿美元。这个估值相比2025年9月的1830亿美元，在短短四个月内几乎翻倍。\n\n这轮融资由新加坡主权财富基金GIC和CoatueManagement领投，红杉资本、微软、英伟达等豪华阵容参与。\n\n红杉资本同时也是OpenAI的投资方，这种\"脚踏两只船\"的投资策略，在硅谷并不罕见，但也反映出资本对AI赛道的谨慎态度，不愿意把鸡蛋放在一个篮子里。\n\n虽然Anthropic融得数额大，但是他们的生存压力同样也挺大的。\n\n该公司2026年的营收目标从150亿美元上调至180亿美元，对比2025年的47亿美元营收，增长近四倍。2027年更是预期营收约550亿美元，这个增长速度在整个科技行业都算是疯狂的。\n\n然而，Anthropic将盈利时间从2027年推迟至2028年，比预期晚了一年。这意味着公司在未来两年内仍然需要持续烧钱，资金主要用于Claude5模型训练和全球算力基础设施扩建，这些都是烧钱的无底洞。\n\n随着模型规模的扩大和训练数据的增加，所需的算力呈指数级增长，这就导致训练大型语言模型的成本正在快速上升。\n\nAnthropic之前曾表示，要在美国投入500亿美元建设数据中心，这个数字听起来惊人，但在当前的AI竞赛中，这可能只是起步价。\n\n数据中心的建设不仅需要巨额资金，还需要大量的电力供应和冷却系统，这些基础设施的建设周期往往需要数年时间。\n\n在这个过程中，Anthropic要一直保持足够的资金支撑，否则就可能面临资金链断裂的风险。\n\n实际上OpenAI这边的融资规模更加惊人。\n\n1月28日传出消息，软银正在向OpenAI追加300亿美元投资，而且这还只是阶段性目标，长期只会投入更多。\n\n在此之前，软银曾在2025年12月投了OpenAI410亿美元。\n\n但是这笔融资对于目前的OpenAI来说还远远不够，他们正在寻求总计高达1000亿美元的融资，估值可能达到8300亿美元。\n\n亚马逊和英伟达也在参与这轮融资的讨论。\n\n这个融资规模如果成功，将创下科技行业的历史纪录。\n\n之所以Anthropic和OpenAI都在扩大融资，其原因很简单，两家公司都希望能在2026或者2027年上市。\n\n本来一帆风顺的Anthropic，突然和OpenAI开始碰撞，那么将会大大阻碍其融资乃至上市的进度，因为这会直接降低其市场份额和预期收入。\n\n投资者在评估一家公司的价值时，不仅看其当前的表现，更看其未来的增长潜力。\n\n如果市场认为Anthropic在与OpenAI的竞争中处于下风，其估值就会受到影响。这种影响不仅体现在融资估值上，更会影响到未来的IPO定价。\n\nAnthropic已经聘请知名律所WilsonSonsini启动IPO准备工作，并与多家投资银行进行初步沟通，最快可能在2026年下半年上市。\n\nWilsonSonsini是硅谷最负盛名的律所之一，曾经协助谷歌和LinkedIn等科技巨头完成上市。\n\n摆在Anthropic前只有两条路。\n\n第一条路是在OpenAI占领这些垂直市场之前完成上市，锁定估值，否则可能面临估值大幅缩水的风险。\n\n上市能够为公司带来大量资金，也能为早期投资者提供退出渠道。但上市也意味着公司将面临更严格的监管和信息披露要求，每个季度的财务表现都会受到市场的审视。\n\n第二条路是通过产品正面击败OpenAI。\n\n这条路更加艰难，但如果成功，回报也更大。\n\nAnthropic需要在产品功能、用户体验、生态建设等多个方面全面超越OpenAI，才能在竞争中占据优势。\n\n这需要大量的研发投入和时间，但时间恰恰是Anthropic最缺乏的资源。在快速变化的AI市场中，每一天的延迟都可能意味着市场份额的流失。\n\n从融资节奏来看，Anthropic正在加速。从2024年3月的35亿美元融资，到9月的130亿美元融资，再到现在的200亿美元融资目标，融资规模在不断扩大，融资间隔在不断缩短。\n\n这种加速度反映出公司对资金的迫切需求，也反映出市场对AI赛道的持续看好。\n\n但融资只是手段，不是目的。最终决定胜负的，还是产品和技术。无论融到多少钱，如果产品无法满足用户需求，市场份额就会流失。\n\n在当前的AI竞赛中，技术迭代速度极快，今天的领先优势可能明天就会被超越。Anthropic和OpenAI都明白这个道理。\n\n这场竞赛的结果，也将在很大程度上决定AI行业未来的格局。"
  },
  {
    "source": "cnBeta.COM",
    "company": "OpenAI",
    "title": "狙击Anthropic？OpenAI布局企业市场的动作与野心",
    "date": "2026-01-24T18:48:17Z",
    "url": "http://m.cnbeta.com.tw/view/1546926.htm",
    "content": "过去一年，OpenAI 推出的多款产品均聚焦消费端市场，既有社交类应用，也有计划于今年晚些时候发布的人工智能硬件设备。不过就在上周，该公司首席执行官山姆・奥特曼在旧金山召集迪士尼首席执行官鲍勃・艾格等一众企业高管出席晚宴，传递出一个明确信号：OpenAI 正认真布局企业客户市场。\n\nOpenAI 首席财务官 萨拉・弗莱尔\n\n据一位知情人士透露，在这场觥筹交错的豪华多道菜晚宴上，奥特曼向与会者表示，OpenAI 可成为企业人工智能需求的 \"一站式服务商\"，其产品矩阵覆盖聊天机器人 ChatGPT、代码生成工具 Codex，以及各类工作流自动化模型。\n\n另一位知情人士表示，这场晚宴的核心目的，是提前向外界透露 OpenAI 专为大型企业打造的新服务。目前该服务的具体细节尚未披露，但其核心目标是助力企业客户完成大规模人工智能转型。通常而言，这类转型意味着企业需彻底改造现有技术架构，将人工智能全面融入客户服务、老旧应用代码重构、企业数据管理等各类业务场景。\n\n这款新服务还致力于整合企业的人工智能应用工作。其可能的实现方式是，将 OpenAI 现有的多元产品打包整合，从而帮助企业客户更便捷地追踪相关开支。目前，OpenAI 为企业客户提供的服务包括不同版本的 ChatGPT、付费高级功能（如先进模型使用权、无限量消息发送权限），以及通过应用程序编程接口（API）调用模型等。\n\n部分晚宴与会者敏锐地察觉到了此番动作背后的潜台词：奥特曼意在将客户从主要竞争对手 Anthropic 那里争取过来。过去一个月，Anthropic 推出的代码生成工具 Claude Code 和办公自动化工具 Cowork 已在商界掀起广泛关注。\n\n两家公司公开的财务数据显示，Anthropic 去年曾对外披露，其通过 API 向企业销售人工智能模型所产生的收入，预计将超过 OpenAI。这一成绩颇为引人注目，要知道 OpenAI 在该市场的布局要领先 Anthropic 数年之久。\n\n尽管如此，OpenAI 的企业业务规模仍大概率超过 Anthropic。原因在于，许多企业客户除了通过 API 购买其模型服务外，还会付费订阅 ChatGPT。但不少大型客户表示，在企业高管圈层中，Anthropic 已被视为企业级人工智能服务的首选供应商。这一口碑的形成，很大程度上得益于 Anthropic 并未在消费端功能上投入过多精力，而是始终专注于企业市场。\n\n反观 OpenAI，部分大客户指出，该公司仍在以惊人的速度推出各类消费级产品，其企业战略似乎仍处于探索和调整阶段。过去数年，OpenAI 的主要增长逻辑是，依靠 ChatGPT 个人用户的口碑传播，吸引其所属企业的关注，进而与企业签订大额订阅合同。然而，向企业客户销售大额合同的流程往往耗时漫长，通常需要在数月内与企业不同部门的高管进行多轮洽谈，而这些高管早已习惯了各类商务宴请和礼遇。\n\n知情人士透露，OpenAI 希望吸引更多像数据平台公司 Databricks、财务软件公司 Intuit 这样的客户 -- -- 这类客户会主动找上门来，直接采购其产品。\n\n上周，世界经济论坛在瑞士达沃斯举行期间，两位 OpenAI 高管 -- -- 首席运营官布拉德・莱特卡普和首席营收官丹尼斯・德雷瑟（曾任协同办公软件 Slack 首席执行官）也在当地积极接洽潜在企业客户，一位知情人士证实了这一消息。与此同时，OpenAI 于本周三在旧金山总部向员工宣布，公司新近从思维机器实验室（Thinking Machines Lab）聘请的顶尖人工智能研究员巴雷特・佐夫，将负责牵头企业级产品的研发工作。\n\nAnthropic 的竞争优势\n\nAnthropic 去年推出的 Claude Code，是其近期在企业市场取得成功的核心驱动力。这款工具具备代码编写和优化功能。一家每月在 OpenAI 和 Anthropic 平台均投入数百万美元的初创企业首席执行官表示，对于部分客户而言，Claude Code 对生产效率的提升作用，远超员工日常用于通用办公和信息检索的聊天机器人。此外，非技术岗位员工也开始使用 Claude Code 及其衍生工具 Cowork，处理各类办公软件相关的非编程任务。\n\nAnthropic 产品的市场热度，给 OpenAI 带来了双重压力：一方面需要对标优化竞品性能，另一方面还要开发大型企业客户青睐的新功能。与此同时，OpenAI 部分面向知识型工作的产品（如用于电子表格创建和编辑的智能代理工具），其市场表现并未达到预期。\n\n一位与 OpenAI 合作的咨询顾问透露，OpenAI 领导层在与合作咨询公司的会议中，提前透露了 Codex 工具的升级计划。他们表示，新版 Codex 的性能和功能很快将超越 Claude Code。目前，OpenAI 也在为 ChatGPT 新增协作功能及其他办公场景功能。这一动向自然逃不过微软的眼睛 -- -- 作为 OpenAI 的重要合作伙伴，微软正加紧升级其办公软件中的人工智能功能。\n\n上述咨询顾问指出，OpenAI 正考虑推出一款新工具，帮助客户更直观地量化使用其人工智能产品所获得的经济效益。这类工具的落地，将成为 OpenAI 拿下大额企业订单的关键所在。\n\n此外，OpenAI 也在针对医疗等特定行业推出定制化功能。奥特曼于本周五在社交平台 X 发文称，公司正为 Codex 工具开发面向网络安全等领域的专属功能 -- -- 而网络安全恰好也是 Anthropic 的重点布局方向。\n\n知情人士表示，过去一年，OpenAI 对客户销售模式进行了重组：摒弃了原先 \"多人负责多款产品\" 的销售模式，改为由专属销售代表对接客户，为其提供一站式的多产品销售服务。\n\n该知情人士还透露，OpenAI 正在分析企业客户使用其人工智能产品的价值高点，并计划推出工具帮助客户量化这些经济效益。合作咨询顾问强调，这类工具的推出，将是 OpenAI 持续斩获大额企业订单的核心关键。\n\n据知情人士披露，OpenAI 已与 7 家客户签订了价值至少 1 亿美元的多年期合作协议；另有 6 家客户签署的合同金额不低于 7500 万美元，且这些合同在续约时，金额有望增长至 1 亿美元乃至更高。\n\n相比之下，Anthropic 曾在去年 12 月宣布，其麾下至少有 9 家客户的年消费额超过 1 亿美元，其中就包括微软 -- -- 按照合作计划，微软每年在 Anthropic 模型上的投入将高达 5 亿美元。\n\n上述初创企业首席执行官及 OpenAI 合作咨询顾问均表示，在企业客户合同条款的设计上，Anthropic 似乎也已领先于 OpenAI。具体而言，Anthropic 允许客户预先承诺一定量的 API 调用额度，以此换取更优惠的定价。而部分 OpenAI 客户曾私下抱怨，他们未能获得如此灵活的合同条款。\n\n不过，OpenAI 已于去年对合同条款进行了优化，提升了灵活性。知情人士表示，目前公司会向企业客户提供先进模型和工具的使用额度，客户购买的额度越多，对应的每席位单价也就越低。\n\n值得注意的是，OpenAI 的部分业务推进并不顺利。例如，该公司去年将智能代理工具的营收预期削减了一半，预计 2025 年相关收入为 14 亿美元。知情人士透露，2024 年 7 月，OpenAI 为 ChatGPT 订阅用户推出了智能代理功能 ChatGPT Agent，支持用户创建和编辑电子表格及演示文稿，但该功能的实际表现未能达到部分内部考核指标，比如未能实现 \"覆盖 10% 的 ChatGPT 周活跃用户\" 这一目标。\n\n尽管遭遇此类挫折，OpenAI 并未动摇 -- -- 这家公司长期以来的愿景，就是同时在消费端和企业端市场站稳脚跟。奥特曼曾将 ChatGPT 形容为 \"助力工作的超级智能私人助理\"。例如在一年前，OpenAI 就曾展示过一款销售领域的人工智能代理原型，该工具能够自动筛选销售线索，并判断哪些线索具备跟进价值。\n\n目前，OpenAI 约 40% 的营收来自企业客户。公司首席财务官萨拉・弗莱尔本周三在达沃斯论坛上表示，预计到今年年底，企业客户贡献的营收占比将提升至 50%。奥特曼于本周四在社交平台 X 发文称，仅过去一个月，OpenAI 通过 API 销售就新增了 10 亿美元的年化收入。他还特别指出，这一数据打破了外界对公司的固有印象：\"很多人觉得我们只有 ChatGPT。\""
  },
  {
    "source": "DNyuz",
    "company": "OpenAI",
    "title": "Can OpenAI make the numbers meet? It's a trillion-dollar question.",
    "date": "2026-02-10T13:44:49Z",
    "url": "https://dnyuz.com/2026/02/10/can-openai-make-the-numbers-meet-its-a-trillion-dollar-question/",
    "content": "Last November, OpenAI investor Brad Gerstner pressed Sam Altman on a podcast about how a company with $13 billion in revenue could commit to $1.4 trillion in spending. Altman bristled.\n\n\"If you want to sell your shares, I'll find you a buyer,\" he said. \"Enough.\"\n\nThree months later, OpenAI is aiming to raise $100 billion in its latest funding round -- a sign that, even amid mounting questions, Altman can find buyers.\n\nAmazon, SoftBank, and Nvidia are all reportedly considering investments that could run into the tens of billions. It would be the largest fundraise in history, surpassing the last recordholder: the company's March 2025 fundraising round of $40 billion. And there are strong signals that it's weighing an IPO for later this year.\n\nFor all that momentum, the chatbot pioneer can't shake the central concern: It is spending money it doesn't have, at a scale that could overwhelm its backers if revenue doesn't offset its costs.\n\nThe urgency is mounting. On Monday, OpenAI began testing ads inside ChatGPT for free and low-paying users in the US, a sign it's looking for more ways to monetize. Rival Anthropic seized the moment, spending millions on Super Bowl ads the day before mocking the decision.\n\nOpenAI's view is that scale itself will become an advantage, one large enough to overwhelm competitors and cement OpenAI's position at the center of the AI industry.\n\nWhether it can make good on that all-important bet is the trillion-dollar question -- and one that will be in front of millions of investors if the company goes public this year.\n\nInside Business stories reveal the inner workings of companies from Silicon Valley to Wall Street that are shaping our world today. Sign up for the newsletter.\n\nFor some investors, the uncertainty is reason enough to stay away.\n\n\"The company is not yet profitable and doesn't have a plausible pathway to near-term profitability,\" Charles Jaskel, founder of secondaries firm New Vintage Partners, told Business Insider. He wouldn't invest in this round if given the chance, he said.\n\n\"Markets change, and there is no guarantee that a technological edge, especially in this AI-driven end of the market, will endure.\"\n\nOthers have been more blunt. George Noble, a former Fidelity portfolio manager, likened OpenAI to a \"cash incinerator\" on X. Sebastian Mallaby, a senior fellow at the Council on Foreign Relations, wrote in a New York Times op-ed in mid-January that OpenAI will run out of money in the next 18 months.\n\nOpenAI's losses will total $143 billion between 2024 and 2029, the \"largest startup losses in history,\" Deutsche Bank analysts wrote in a December 4 note. HSBC researchers said in a late November report that they expect OpenAI to have a $207 billion shortfall by 2030, even when modeling for significant boosts in revenue.\n\nIn September, The Information reported that OpenAI was telling investors it would burn $115 billion of cash by 2029, more than three times the company's previous estimate.\n\nIts largest corporate partner is feeling the pressure. Last month, Microsoft said nearly half of its cloud computing backlog was tied to OpenAI. The software giant's reliance on OpenAI helped wipe $440 billion from Microsoft's market value amid concerns that Altman's company may not deliver on its obligations.\n\nAt the center of OpenAI's spending is compute: the processing power required to train and run AI models. It has signed agreements for more than 30 gigawatts of capacity in the coming years -- nearly a third of what JLL estimates the entire industry used last year.\n\n\"You should expect OpenAI to spend trillions of dollars on data center construction in the not very distant future,\" Altman reportedly told journalists at a dinner in August.\n\nFor OpenAI and those investing in it, that spending isn't reckless -- it's necessary.\n\nIn a January blog post, chief financial officer Sarah Friar said the company's relative lack of computing power had been holding back monetization. OpenAI had about 1.9 gigawatts at the end of the year, far short of what it needs to serve demand, she said.\n\n\"Compute is the scarcest resource in AI,\" Friar wrote. \"Three years ago, we relied on a single compute provider. Today, we are working with providers across a diversified ecosystem. That shift gives us resilience and, critically, compute certainty.\"\n\nSome investors say that the headline spending figures overstate OpenAI's true exposure.\n\nEthan Choi, an OpenAI investor at Khosla Ventures, wrote on X last month that of the $1.4 trillion in obligations cited by Altman, about $600 billion would be spent directly by OpenAI. The remaining $800 billion, he said, would be covered by partners building the data centers OpenAI will rely on.\n\nChoi told Business Insider that the $600 billion figure is less daunting than it sounds -- spread over a decade, it could be matched by the revenue OpenAI expects to generate over the same period.\n\nChoi noted that OpenAI and its rival Anthropic have shown they can generate about $10 billion in annual revenue for each gigawatt of computing power. OpenAI should have about 14 gigawatts online in three years or so, Choi said, adding that the figure was his estimate based on public information. That means $140 billion in potential annual revenue.\n\nChoi trusts Altman and Friar on how to deploy capital. They \"have the most data in front of them,\" he said. \"Whenever there is a big wave like this, there are always naysayers, or folks who don't see the big picture. I think we just need to step back and think about how we can apply AI to our lives, and it's endless.\"\n\nChatGPT's launch was a watershed for consumer technology, hitting 100 million users in two months to achieve the fastest consumer adoption of any product in history. For a time, OpenAI seemed unstoppable.\n\nIt went on one of the most impressive growth runs in business history. The company hit $20 billion in annual revenue in roughly three years, torrid growth even in the blitzscaling world of tech.\n\nRevenue is only half the equation.\n\n\"It basically did what Google did in seven years, in two,\" Rob Enderle, an independent technology analyst, told Business Insider. \"So that's great, except for the problem of their capex.\"\n\n\"The infrastructure they're buying is undergoing massive advancements, with performance doubling, tripling, quadrupling,\" he said. \"You can't build a data center fast enough to ensure that it's not going to be obsolete by the time you finish it.\"\n\nOne way to fund that burn: Go public. Multiple outlets, including Business Insider, have reported there are signs OpenAI is aiming to IPO later this year. That would give OpenAI access to a far deeper pool of capital than private markets can offer, but it also means quarterly earnings calls, public scrutiny of its losses, and increased pressure to show a path to profitability.\n\nA competitive market for IPOs adds to the pressure. Last week, Elon Musk merged xAI with SpaceX, a company widely expected to go public this year. Reporting also suggests Anthropic, OpenAI's closest competitor, is eyeing going public by the end of the year.\n\nThere are also questions about focus and competition. Over the past year, OpenAI has shifted between expansion and retrenchment.\n\nLast May, OpenAI named Instacart chief Fidji Simo as CEO of applications, signaling how serious the company is about expanding beyond its core chatbot. In December, Altman called a \"Code Red\" in an internal memo, telling employees the company had spread itself too thin and needed to refocus on improving ChatGPT.\n\nTwo months later, the company is pushing deeper into search and ecommerce with a new checkout feature, considering adult content, working on a personal device with Apple's former design chief, and rolling out ads -- something the \"Code Red\" memo explicitly hit pause on. It's also exploring robotics and designing its own chips.\n\nMeanwhile, rivals are closing in. On the consumer side, Google's Gemini has been steadily eating into ChatGPT's lead, according to data from Apptopia. On the enterprise side, Anthropic has carved out a strong position, particularly with software developers and businesses looking for specialized AI coding tools.\n\nIt's no longer clear that ChatGPT's consumer dominance alone will be enough to generate the kind of returns OpenAI's valuation demands.\n\nAndrej Karpathy, an OpenAI founder who is now building his own startup, fawned over Anthropic's Claude on X.\n\n\"This is easily the biggest change to my basic coding workflow in ~2 decades of programming and it happened over the course of a few weeks,\" he wrote. \"I'd expect something similar to be happening to well into double digit percent of engineers out there.\"\n\nOpenAI's version, Codex, has not received the same level of adulation.\n\nThere's also competition on the model approach. Open-source models from China and elsewhere are delivering rapid performance gains at a fraction of the cost.\n\n\"Our biggest concern is competitive intensity,\" Kyle Qi, an investor at Llama Ventures, which backs Anthropic, Thinking Machines Lab, and xAI, told Business Insider. \"Google and leading Chinese LLMs are rapidly catching up and, in some areas, already outperforming on cost, latency, and specific benchmarks. That convergence compresses OpenAI's technological moat at a time when expectations for durable dominance are priced in.\"\n\nJeremy Abelson, the founder and CEO at Irving Investors, which has investment exposure to OpenAI, told Business Insider that \"OpenAI needs to spend a large amount of money on capex now because key competitors like Google are investing in-line to larger capex at this point to remain at the top of the LLM leaderboard.\"\n\nHe added that if the Altman-led company doesn't keep ahead of the innovation curve, \"it risks decreasing market share and losing relevance.\"\n\nUntil fundraising is finished, it's unclear what OpenAI's valuation will be, but estimates reported by other news outlets range from $750 billion to $830 billion. Whether its ultimate valuation makes sense may depend less on the brilliance of OpenAI's models than on the discipline of its capital allocation. The margin for error is thin.\n\n\"At this valuation, a lot of things need to go right,\" said Steve Brotman, managing director and founding partner at Alpha Partners, who said he would pass on OpenAI at this stage for less risky opportunities. \"Use the product if you enjoy it. That doesn't mean you need to buy the stock.\"\n\nAltman himself has been upfront on the stakes.\n\n\"Someone is going to lose a phenomenal amount of money,\" he said at the August dinner. \"And a lot of people are going to make a phenomenal amount of money.\""
  },
  {
    "source": "Business Insider",
    "company": "OpenAI",
    "title": "Everyone is wondering about OpenAI's path to profitability. Here's what the experts think.",
    "date": "2026-02-10T10:13:32Z",
    "url": "https://www.businessinsider.com/openai-profitability-analyst-investor-opinions-funding-ipo-2026-2",
    "content": "Last November, OpenAI investor Brad Gerstner pressed Sam Altman on a podcast about how a company with $13 billion in revenue could commit to $1.4 trillion in spending. Altman bristled.\n\n\"If you want to sell your shares, I'll find you a buyer,\" he said. \"Enough.\"\n\nThree months later, OpenAI is aiming to raise $100 billion in its latest funding round -- a sign that, even amid mounting questions, Altman can find buyers.\n\nAmazon, SoftBank, and Nvidia are all reportedly considering investments that could run into the tens of billions. It would be the largest fundraise in history, surpassing the last recordholder: the company's March 2025 fundraising round of $40 billion. And there are strong signals that it's weighing an IPO for later this year.\n\nFor all that momentum, the chatbot pioneer can't shake the central concern: It is spending money it doesn't have, at a scale that could overwhelm its backers if revenue doesn't offset its costs.\n\nThe urgency is mounting. On Monday, OpenAI began testing ads inside ChatGPT for free and low-paying users in the US, a sign it's looking for more ways to monetize. Rival Anthropic seized the moment, spending millions on Super Bowl ads the day before mocking the decision.\n\nOpenAI's view is that scale itself will become an advantage, one large enough to overwhelm competitors and cement OpenAI's position at the center of the AI industry.\n\nWhether it can make good on that all-important bet is the trillion-dollar question -- and one that will be in front of millions of investors if the company goes public this year.\n\nFor some investors, the uncertainty is reason enough to stay away.\n\n\"The company is not yet profitable and doesn't have a plausible pathway to near-term profitability,\" Charles Jaskel, founder of secondaries firm New Vintage Partners, told Business Insider. He wouldn't invest in this round if given the chance, he said.\n\n\"Markets change, and there is no guarantee that a technological edge, especially in this AI-driven end of the market, will endure.\"\n\nOthers have been more blunt. George Noble, a former Fidelity portfolio manager, likened OpenAI to a \"cash incinerator\" on X. Sebastian Mallaby, a senior fellow at the Council on Foreign Relations, wrote in a New York Times op-ed in mid-January that OpenAI will run out of money in the next 18 months.\n\nOpenAI's losses will total $143 billion between 2024 and 2029, the \"largest startup losses in history,\" Deutsche Bank analysts wrote in a December 4 note. HSBC researchers said in a late November report that they expect OpenAI to have a $207 billion shortfall by 2030, even when modeling for significant boosts in revenue.\n\nIn September, The Information reported that OpenAI was telling investors it would burn $115 billion of cash by 2029, more than three times the company's previous estimate.\n\nIts largest corporate partner is feeling the pressure. Last month, Microsoft said nearly half of its cloud computing backlog was tied to OpenAI. The software giant's reliance on OpenAI helped wipe $440 billion from Microsoft's market value amid concerns that Altman's company may not deliver on its obligations.\n\nAt the center of OpenAI's spending is compute: the processing power required to train and run AI models. It has signed agreements for more than 30 gigawatts of capacity in the coming years -- nearly a third of what JLL estimates the entire industry used last year.\n\n\"You should expect OpenAI to spend trillions of dollars on data center construction in the not very distant future,\" Altman reportedly told journalists at a dinner in August.\n\nFor OpenAI and those investing in it, that spending isn't reckless -- it's necessary.\n\nIn a January blog post, chief financial officer Sarah Friar said the company's relative lack of computing power had been holding back monetization. OpenAI had about 1.9 gigawatts at the end of the year, far short of what it needs to serve demand, she said.\n\n\"Compute is the scarcest resource in AI,\" Friar wrote. \"Three years ago, we relied on a single compute provider. Today, we are working with providers across a diversified ecosystem. That shift gives us resilience and, critically, compute certainty.\"\n\nSome investors say that the headline spending figures overstate OpenAI's true exposure.\n\nEthan Choi, an OpenAI investor at Khosla Ventures, wrote on X last month that of the $1.4 trillion in obligations cited by Altman, about $600 billion would be spent directly by OpenAI. The remaining $800 billion, he said, would be covered by partners building the data centers OpenAI will rely on.\n\nChoi told Business Insider that the $600 billion figure is less daunting than it sounds -- spread over a decade, it could be matched by the revenue OpenAI expects to generate over the same period.\n\nChoi noted that OpenAI and its rival Anthropic have shown they can generate about $10 billion in annual revenue for each gigawatt of computing power. OpenAI should have about 14 gigawatts online in three years or so, Choi said, adding that the figure was his estimate based on public information. That means $140 billion in potential annual revenue.\n\nChoi trusts Altman and Friar on how to deploy capital. They \"have the most data in front of them,\" he said. \"Whenever there is a big wave like this, there are always naysayers, or folks who don't see the big picture. I think we just need to step back and think about how we can apply AI to our lives, and it's endless.\"\n\nChatGPT's launch was a watershed for consumer technology, hitting 100 million users in two months to achieve the fastest consumer adoption of any product in history. For a time, OpenAI seemed unstoppable.\n\nIt went on one of the most impressive growth runs in business history. The company hit $20 billion in annual revenue in roughly three years, torrid growth even in the blitzscaling world of tech.\n\nRevenue is only half the equation.\n\n\"It basically did what Google did in seven years, in two,\" Rob Enderle, an independent technology analyst, told Business Insider. \"So that's great, except for the problem of their capex.\"\n\n\"The infrastructure they're buying is undergoing massive advancements, with performance doubling, tripling, quadrupling,\" he said. \"You can't build a data center fast enough to ensure that it's not going to be obsolete by the time you finish it.\"\n\nOne way to fund that burn: Go public. Multiple outlets, including Business Insider, have reported there are signs OpenAI is aiming to IPO later this year. That would give OpenAI access to a far deeper pool of capital than private markets can offer, but it also means quarterly earnings calls, public scrutiny of its losses, and increased pressure to show a path to profitability.\n\nA competitive market for IPOs adds to the pressure. Last week, Elon Musk merged xAI with SpaceX, a company widely expected to go public this year. Reporting also suggests Anthropic, OpenAI's closest competitor, is eyeing going public by the end of the year.\n\nThere are also questions about focus and competition. Over the past year, OpenAI has shifted between expansion and retrenchment.\n\nLast May, OpenAI named Instacart chief Fidji Simo as CEO of applications, signaling how serious the company is about expanding beyond its core chatbot. In December, Altman called a \"Code Red\" in an internal memo, telling employees the company had spread itself too thin and needed to refocus on improving ChatGPT.\n\nTwo months later, the company is pushing deeper into search and ecommerce with a new checkout feature, considering adult content, working on a personal device with Apple's former design chief, and rolling out ads -- something the \"Code Red\" memo explicitly hit pause on. It's also exploring robotics and designing its own chips.\n\nMeanwhile, rivals are closing in. On the consumer side, Google's Gemini has been steadily eating into ChatGPT's lead, according to data from Apptopia. On the enterprise side, Anthropic has carved out a strong position, particularly with software developers and businesses looking for specialized AI coding tools.\n\nIt's no longer clear that ChatGPT's consumer dominance alone will be enough to generate the kind of returns OpenAI's valuation demands.\n\nAndrej Karpathy, an OpenAI founder who is now building his own startup, fawned over Anthropic's Claude on X.\n\n\"This is easily the biggest change to my basic coding workflow in ~2 decades of programming and it happened over the course of a few weeks,\" he wrote. \"I'd expect something similar to be happening to well into double digit percent of engineers out there.\"\n\nOpenAI's version, Codex, has not received the same level of adulation.\n\nThere's also competition on the model approach. Open-source models from China and elsewhere are delivering rapid performance gains at a fraction of the cost.\n\n\"Our biggest concern is competitive intensity,\" Kyle Qi, an investor at Llama Ventures, which backs Anthropic, Thinking Machines Lab, and xAI, told Business Insider. \"Google and leading Chinese LLMs are rapidly catching up and, in some areas, already outperforming on cost, latency, and specific benchmarks. That convergence compresses OpenAI's technological moat at a time when expectations for durable dominance are priced in.\"\n\nJeremy Abelson, the founder and CEO at Irving Investors, which has investment exposure to OpenAI, told Business Insider that \"OpenAI needs to spend a large amount of money on capex now because key competitors like Google are investing in-line to larger capex at this point to remain at the top of the LLM leaderboard.\"\n\nHe added that if the Altman-led company doesn't keep ahead of the innovation curve, \"it risks decreasing market share and losing relevance.\"\n\nUntil fundraising is finished, it's unclear what OpenAI's valuation will be, but estimates reported by other news outlets range from $750 billion to $830 billion. Whether its ultimate valuation makes sense may depend less on the brilliance of OpenAI's models than on the discipline of its capital allocation. The margin for error is thin.\n\n\"At this valuation, a lot of things need to go right,\" said Steve Brotman, managing director and founding partner at Alpha Partners, who said he would pass on OpenAI at this stage for less risky opportunities. \"Use the product if you enjoy it. That doesn't mean you need to buy the stock.\"\n\nAltman himself has been upfront on the stakes.\n\n\"Someone is going to lose a phenomenal amount of money,\" he said at the August dinner. \"And a lot of people are going to make a phenomenal amount of money.\""
  },
  {
    "source": "驱动之家",
    "company": "OpenAI",
    "title": "硅谷不相信忠诚！AI行业玩成NBA 科学家爽拿\"转会费",
    "date": "2026-02-09T15:27:50Z",
    "url": "https://news.mydrivers.com/1/1103/1103326.htm",
    "content": "\"过去，创始人对自己的公司忠心耿耿。如今，只要价钱合适，任何人都可以被挖走。\"\n\n前几天，WIRED的标题吓到了我 -- --\n\n硅谷的忠诚已死！\n\n仔细一瞅，说的很有道理：历数2025年年中到现在，硅谷已经发生了至少三起重大的\"收购式招聘\"事件 -- --\n\n2025年6月，Meta向Scale AI投资143亿美元，挖走其联合创始人Alexandr Wang；\n\n2025年7月，谷歌斥资24亿美元获得Windsurf的技术授权，将其联合创始人Varun Mohan及研究团队并入 DeepMind；\n\n2025年12月，英伟达与Groq达成价值200亿美元的授权协议，将其核心推理技术、 创始人兼CEO Jonathan Ross 及多名核心高管统统打包带走。\n\n前不久，OpenAI又从Mira Murati那里挖回来几名离职加入Thinking Machines Lab的研究人员，更别提小扎一直想从OpenAI挖人，而微软也带走过超过二十名谷歌DeepMind的研究人员。\n\n巨头的\"天价抢人\"和人才的\"反复横跳\"仿佛已经成为了常态，不得不令人感叹：难道硅谷只剩下\"雇佣兵\"了吗？\n\nAI汹涌，硅谷都是\"雇佣兵\"\n\n首先，要论\"忠不忠诚\"，也得分两种情况：\"主动跳槽\"和\"被动跳槽\"。\n\n第一种情况是研究员主动跳槽，这背后的动机有很多：比如高薪诱惑、尖端资源的吸引、追求更有前景的技术和产品等等。\n\n第二种比较常见的情况是公司被收购，人也得跟着挪位置。这就不得不提到硅谷各大AI巨头现在最流行的玩法：Acqui-hire（人才收购），买人才送公司。\n\n咱们一个一个来唠。\n\n谷歌研究员投奔OpenAI：我们是来搞ChatGPT的\n\nAI行业比较有代表性的「挖人」事件，最早可以追溯到2023年初。\n\n很多人或许还记得，在OpenAI正式发布ChatGPT前后，有消息传出多名谷歌大脑研究员加入OpenAI。\n\n这一趋势在ChatGPT发布前就开始了，据The Information报道，在ChatGPT问世前，OpenAI悄悄雇用了至少5位前AI谷歌研究员，他们在调整和准备ChatGPT中发挥了关键作用。\n\n当时有不少言论调侃：谷歌人才正是ChatGPT能够迅速推出的秘密武器。\n\n2023年1月之后，又有至少4名谷歌大脑成员加入OpenAI。还有两位老哥激动地在X上发帖：\n\n他们是Jason Wei和Hyung Won Chung，分别在谷歌大脑效力2年、3年，研究方向均涉猎大语言模型。Jason Wei还是思维链（chain-of-thought，CoT）的最早一作。\n\n两人的推文中，都提到了\"Excited\"\"Can't wait\"的字眼，可以说激动之情溢于言表。\n\n当然，最新动向是2025年8月，两人又组团跳槽去了Meta的超级智能实验室。\n\n当时的谷歌人才为啥纷纷拥抱OpenAI呢？原因不难理解，一方面是ChatGPT当时空前的影响力，它基于GPT-3.5打造，积攒了OpenAI从GPT一路迭代至InstructGPT的技术经验与实力，一经上线便迅速席卷全球。\n\n2023年1月，在微软的支持下，ChatGPT接入了Bing、Office、GitHub、Azure等微软\"全家桶\"。此外，微软还宣布向OpenAI追加数十亿美元投资。ChatGPT仅上线一个多月，ChatGPT就把OpenAI的估值从200亿美元推高到290亿。\n\n另一方面则是谷歌Bard的失利。2023年2月，谷歌发布了基于谷歌LaMDA大模型的下一代对话AI系统Bard -- -- 这是Gemini的前身，也是一个被要求「一百天内打造出的与 ChatGPT 抗衡的产品。」\n\n2023年2月8日，谷歌举行 Bard 人工智能演示直播。在演示视频中，Bard 回答詹姆斯・韦伯太空望远镜时出现事实性错误，导致 Alphabet 股价下跌近 9%， 市值蒸发了约 1000 亿美元。\n\n谷歌许多员工对于Bard的仓促推出并不满意，内部留言板 Memegen 上的一篇帖子写道：\n\n「Bard 的发布和裁员都太仓促、草率和短视了，请恢复长远眼光。」\n\n在这种情况下，部分研究员转投对家OpenAI的怀抱，选择眼下炙手可热的ChatGPT，并不奇怪。\n\nMeta天价挖人，奥特曼：不需要唯利是图的雇佣兵\n\n风水轮流转，2025年，轮到Meta挖OpenAI的人了。\n\n在组建Meta超级智能实验室期间，扎克伯格开出了四年内高达 3 亿美元的薪酬方案，其中第一年的总薪酬就超过 1 亿美元。\n\n虽然扎克伯格声称这些极端报价仅限于\"少数领导职位\"，但还是高得有点离谱了。并且Meta是冲着OpenAI来的，向OpenAI的员工发出了至少10 份如此高额的报价。\n\n其价码堪比顶级球星转会费。\n\n此外，扎克伯格还豪迈放话：不必担心GPU短缺！\n\n这对于AI研究员来说相当有吸引力，因为获取尖端芯片的竞争非常激烈，而且会直接影响研究的最终结果。\n\n不用说，这一招也是针对OpenAI。此前据相关人士透露，OpenAI的部分研究人员曾抱怨奥特曼承诺为员工提供 GPU，但最终却感觉这个承诺没有兑现。\n\n凭借高薪和尖端GPU的诱惑，扎克伯格成功从OpenAI挖走了多名研究员。\n\n在超级智能实验室（MSL）正式亮相之时，Meta披露了「挖人」的完整名单，包含来自OpenAI、Anthropic 和 Google 等竞争对手的11位 AI 顶尖人才：\n\n·多位 GPT-4o 和 GPT-4.1 的核心成员：如 Shengjia Zhao、Jiahui Yu、Shuchao Bi、Hongyu Ren；\n\n·来自 Anthropic 的高级工程师 Joel Pobar，此前曾在 Meta 任职11年；\n\n·DeepMind 的 Jack Rae 和 Pei Sun，曾负责 Gemini 模型和多模态推理系统；\n\n·OpenAI 语音与图像模型的重要推动者 Huiwen Chang、Ji Lin 等。这些人才曾是 OpenAI 和 Anthropic 等机构的核心技术骨干，主导过 GPT 系列、Gemini 系列等主流模型的关键技术领域。\n\n据外媒报道，Meta 在宣布暂停招聘前，为其 AI 团队招募了超过50人。对此，OpenAI首席研究员Mark Chen表示：\"感觉就像有人闯入我们家偷了东西。\"\n\n硅谷顶级风险投资公司Kleiner Perkins董事会主席John Doerr曾将硅谷企业划分为两类，一类奉行\"雇佣兵文化\"，核心目标是追逐利润；另一类秉持\"传教士精神\"，在追求商业成功的同时更注重创造社会价值。\n\nSam Altman在OpenAI内部信中引用了这一比喻。他怒斥Meta的挖人行径，称其\"令人反感\"，并评价被挖走的员工是\"唯利是图的雇佣兵\"。\n\n\"当然会有一些雇佣兵，但传教士将会打败雇佣兵。\"\n\nAltman 还在信中强调，留在 OpenAI 才是那些希望构建通用人工智能（AGI）研究者的正确选择，并暗示公司正在重新评估整个研究团队的薪酬结构。\n\n\"Meta 确实招到了一些优秀的人，但整体来看，他们并没有挖到那些顶尖人才，还得一路向下寻找；他们已经尝试招募很久了，我都记不清他们试图从我们这里挖走多少人去当他们的首席科学家。......我为整个行业的使命感感到骄傲，当然总会有一些唯利是图的人。\"\n\n天价薪酬买不来员工的\"忠诚\"\n\n尽管Meta给出了天价薪酬，但似乎也买不来员工的\"忠诚\"。\n\n2025年8月，MSL 成立还不到两个月，就迎来了一波关键员工离职潮，其中不乏核心老将和高薪新秀。有两名研究员入职 Meta 还不满 1 个月，就重返老东家 OpenAI 。\n\n这两位研究员是Avi Verma和Ethan Knight。Avi曾在 Tesla 担任高级机器学习工程师，于2024年6月入职OpenAI ，被小扎以天价薪酬挖来 Meta 不到一个月后，就重返 OpenAI。\n\nEthan则曾在 OpenAI 实习，后加入马斯克的 xAI，再被 Meta 挖走，但很快也回到了 OpenAI。\n\n另外还有几名Meta资深员工选择跳槽到OpenAI或Antropic。\n\n其中比较重量级的是Chaya Nayak，她在 Meta 工作近 9 年，曾担任生成式 AI 产品管理总监，负责选举透明度项目、数据共享计划，并参与 Llama 大语言模型开发；2025年8月宣布加入 OpenAI，负责特别项目。\n\n还有Afroz Mohiuddin，他曾在谷歌工作 14 年，担任软件工程师，24年加入 Meta，随后又跳槽并加入了OpenAI 技术团队。\n\n另外还有一位在Meta工作了12年的资深工程师Bert Maher，他曾参与开发 PyTorch（全球广泛使用的 AI 训练工具）和 Triton（优化 AI 模型运行的编程语言与编译器）。2025年8月官宣加入Anthropic，并表示自己将致力于让 Claude \"跑得更快\"。\n\n读到这里，大家会发现，这些研究员大多在谷歌、OpenAI、xAI、Anthropic等公司之间流转，每个人的简历上都有好几家巨头的title。\n\n由于没有竞业协议，硅谷AI巨头之间的人才流动相当频繁且常见，而且各家的AI技术进展可以很快被追平，几乎没有核心机密可言。\n\n但相比之下，OpenAI或Anthropic这种有明确使命感的公司会更有人才粘性 -- -- 也就是所谓的\"忠诚度\"。\n\n风险投资公司SignalFire表示，Anthropic的工程师招聘速度是其流失速度的2.68倍，OpenAI的这一比例为2.18，Meta为2.07，而谷歌仅为1.17。\n\nAcqui-hire：硅谷大公司「挖人」的流行策略\n\n除了天价挖人，硅谷AI巨头还有一种常见做法是Acqui-hire（人才收购），以收购另一家公司的方式获取其人才，而非产品或服务。\n\n通过这种策略，收购方可以快速获得一支才华横溢、拥有特定技能且运转良好的人才团队，还可以避免全面并购的反垄断审查。对于需要极高人才密度来支撑技术竞赛的AI巨头来说，这是一个非常理想的选择。\n\n但在硅谷流行的\"人才收购\"中，也发生过不少创始人\"不忠诚\"的事情。其中最有名的就是谷歌收购Windsurf一事。\n\n2025年7月，谷歌截胡了OpenAI收购Windsurf的交易计划，花24亿美元挖走了Windsurf的CEO Varun Mohan、联合创始人Douglas Chen以及核心研发团队，并入谷歌DeepMind团队。\n\n由于谷歌只拿走了核心团队和部分技术授权，公司本体和剩下的250多名员工都被留在了原地。这让Windsurf瞬间从一家明星独角兽成为一具\"空壳\"，剩余员工手里的股权期权也面临归零风险。\n\n当时有不少人怒斥两位创始人\"背刺\"员工、抛弃团队，风险投资家Vinod Khosla还公开表示未来不会再与他们合作。\n\n部分员工表示感到被抛弃和背叛，甚至有员工因股权价值几乎清零而遭受重大经济损失。尽管后续Cognition公司收购了Windsurf剩余资产，并承诺保障员工权益，但创始人的行为仍被视为违背了创业圈\"共患难\"的基本共识。\n\n过去一两年，或要人才，或要技术，或是两者都要 -- -- 类似的收购案例在OpenAI、微软、谷歌、亚马逊等几大巨头之间层出不穷。\n\n大家熟知的诸多明星初创公司，如InflectionAI、AdeptAI、CharacterAI、ScaleAI、Windsurf......也几乎都以这样的方式被巨头并入。\n\n甚至，现在还出现了一种\"反向收购\"的趋势：\n\n即初创公司积极布局，以便被大型科技公司纯粹为了人才而收购。他们不是为了销售产品而开发产品，而是为了打造大型科技公司无法抗拒的团队。\n\n好家伙，真是道高一尺，魔高一丈啊。\n\n\"反向收购\"的玩法是这样的：\n\n第一步，初创公司积极地从顶尖人工智能项目（斯坦福大学、麻省理工学院、DeepMind）中招聘人才；\n\n第二步，发表令人印象深刻的研究论文来建立信誉；\n\n第三步，他们筹集少量资金以维持运营。\n\n最后，等待AI巨头们意识到他们的人才优势。\n\n这种\"先雇佣后收购\"的模式催生了业内人士所谓的\"光环效应\"，即\"雇佣并授权\" -- --\n\n企业通过构建交易结构，既能雇佣关键人才，又能获得技术的非独家授权，从而规避反垄断审查和全面收购带来的种种复杂问题。\n\n而一些初创公司对这种策略毫不掩饰。他们将自己定位为\"人才密度型公司\"，押注团队的集体智慧比可能开发的任何产品都更有价值。\n\n为什么大家都热衷于跳槽\n\n第一，当然是因为钱多。\n\n比如Meta开出的天价薪酬。小扎曾经亲自致电24岁的AI研究员Matt Deitke，初始报价为四年1.25亿美元。\n\n在被拒绝后，Meta迅速将报价翻倍至四年2.5亿美元，其中1亿美元可在第一年兑现。最终，Meta的疯狂加码让他选择接受了邀约。\n\n但跳槽的原因并不只是致富那么简单。\n\n普林斯顿大学计算机科学研究员、Mozilla 高级研究员Sayash Kapoor表示，现在科技行业正在发生一种\"文化变革\"：员工们开始担心长期效力于一家公司或机构。\n\n早在2010和2010 年代那个充满理想主义的年代，科技公司的雇主们理所当然地认为员工至少会工作到四年期满，原因是那时他们的股票期权通常会到期。\n\n而许多早期联合创始人及员工也真心相信公司所宣称的使命，并希望能够参与其中，为实现这些使命贡献力量。\n\n但在AI时代，一切都变了。随着技术的快速发展，留在原地带来的机会成本可能更高。\n\n不仅普通员工会衡量继续留在当前公司的局限性，创业公司的创始人也变得更加\"务实\"。比如对于Windsurf创始人来说，他们可能已经权衡过，如果加入像谷歌这样资源丰富的公司，影响力会更大。\n\n除了寻求更大的发展，AI时代更快的技术变革也导致了人才流动的加速。WIRED记者Steven Levy表示：\"在AI创业公司工作一年，相当于在另一个科技时代为创业公司工作五年。\"\n\n这样的迭代速度使团队可以在较短的时间内推出数百万用户使用的全新产品，可能让员工自我感觉良好 -- -- 认为自己已经磨练出了足够的技能，可以迎接更大的挑战。\n\n而Kapoor还提到，学术界也出现了类似现象。过去五年里，越来越多的计算机科学博士生离开博士项目，转而进入业界工作。\n\n但创业公司一言不合\"被收购\"如此之流行，让投资人也坐不住了。风险投资公司Striker Venture Partners 的创始人 Max Gazor 表示，他们现在比以往任何时候都更加注重考察创始团队的\"化学反应和凝聚力\"。\n\n他还指出，越来越多的交易中加入了\"保护性条款，要求在进行重大知识产权许可或其他类似操作时必须获得董事会的批准\"。\n\n\"抢人大战\"的风还是吹到了国内\n\nAI行业的\"抢人大战\"不止发生在国外几大巨头之间，国内互联网公司对AI人才的争夺也相当激烈，并且瞄准了OpenAI、谷歌Deepmind等顶级实验室的研究员。\n\n比如，腾讯从OpenAI挖来了年仅28岁的研究员姚顺雨，命其出任\"CEO/总裁办公室\"首席AI科学家，兼任AI Infra部与大语言模型部负责人，向总裁刘炽平、技术工程事业群总裁卢山双线汇报，成为混元研发核心掌舵人。\n\n随后，前Sea AI Lab研究员庞天宇也加盟腾讯，担任腾讯混元多模态强化学习首席研究员及技术负责人，主要负责强化学习前沿算法探索。他曾在ICML、NeurIPS、ICLR等顶级AI会议发表多篇论文，谷歌学术引用超15000次。\n\n2024年10月，字节从阿里挖来了前通义技术负责人周畅。周畅曾担任阿里巴巴通义千问大模型的技术负责人，主导开发了2021年发布的M6多模态预训练模型。\n\n同样是字节，2025年2月挖来了前谷歌DeepMind研究副总裁吴永辉，负责 Seed 部门大模型理论基础研究。吴永辉曾领导大型前沿AI研发团队，构建支撑大模型训练与推理的底层AI基础设施。\n\n2025年12月，前谷歌DeepMind研究员潘欣也宣布加入美团。潘欣曾在谷歌推动TensorFlow动态图模式开发。回国后，他先后在百度、腾讯与字节跳动任职。\n\n在百度期间，他主导了PaddlePaddle平台的性能优化工作；在腾讯，他负责构建了\"无量\"深度学习框架；在字节跳动，他担任视觉大模型AI平台负责人，专注于AIGC与视觉大模型平台的研发。\n\n加入美团后，潘欣又迅速主导了美团自研大模型LongCat系列的落地应用，特别是LongCat App的开发。\n\n相比互联网时代的\"一人一司\"，国内AI人才的流动速度也是相当快。同一个人的履历上，可能会同时出现数家横跨国内外的顶级实验室、AI巨头或互联网大厂。\n\n看来，也不完全是硅谷的忠诚已死。高薪和高流动性，本身就是AI行业的特性。\n\n在AI领域，一位顶尖人才的突破可能将模型训练成本降低10%，或使模型性能提高20%，这种影响直接转化为数亿美元的价值创造或成本节约。\n\n而这样的人才又相当稀缺，专注于AI和机器学习领域的招聘公司Razoroo的AI招聘总监Aaron Sines曾表示：\"全球真正具备经验、能够开发和部署基础模型的人大概只有一千人，最多两千。\"\n\n在这种情况下，公司会将顶级AI人才视为战略资产，与知识产权甚至整个业务单元同等重要；而大佬们处于一个\"哪哪都能赚钱\"的买方市场中，他们的反复横跳似乎也就不足为奇了。"
  },
  {
    "source": "新浪财经",
    "company": "OpenAI",
    "title": "狙击Anthropic？OpenAI布局企业市场的动作与野心",
    "date": "2026-01-24T18:17:26Z",
    "url": "https://finance.sina.com.cn/world/2026-01-25/doc-inhimsun0831827.shtml",
    "content": "过去一年，OpenAI 推出的多款产品均聚焦消费端市场，既有社交类应用，也有计划于今年晚些时候发布的人工智能硬件设备。不过就在上周，该公司首席执行官山姆・奥特曼在旧金山召集迪士尼首席执行官鲍勃・艾格等一众企业高管出席晚宴，传递出一个明确信号：OpenAI 正认真布局企业客户市场。\n\n据一位知情人士透露，在这场觥筹交错的豪华多道菜晚宴上，奥特曼向与会者表示，OpenAI 可成为企业人工智能需求的 \"一站式服务商\"，其产品矩阵覆盖聊天机器人 ChatGPT、代码生成工具 Codex，以及各类工作流自动化模型。\n\n另一位知情人士表示，这场晚宴的核心目的，是提前向外界透露 OpenAI 专为大型企业打造的新服务。目前该服务的具体细节尚未披露，但其核心目标是助力企业客户完成大规模人工智能转型。通常而言，这类转型意味着企业需彻底改造现有技术架构，将人工智能全面融入客户服务、老旧应用代码重构、企业数据管理等各类业务场景。\n\n这款新服务还致力于整合企业的人工智能应用工作。其可能的实现方式是，将 OpenAI 现有的多元产品打包整合，从而帮助企业客户更便捷地追踪相关开支。目前，OpenAI 为企业客户提供的服务包括不同版本的 ChatGPT、付费高级功能（如先进模型使用权、无限量消息发送权限），以及通过应用程序编程接口（API）调用模型等。\n\n部分晚宴与会者敏锐地察觉到了此番动作背后的潜台词：奥特曼意在将客户从主要竞争对手 Anthropic 那里争取过来。过去一个月，Anthropic 推出的代码生成工具 Claude Code 和办公自动化工具 Cowork 已在商界掀起广泛关注。\n\n两家公司公开的财务数据显示，Anthropic 去年曾对外披露，其通过 API 向企业销售人工智能模型所产生的收入，预计将超过 OpenAI。这一成绩颇为引人注目，要知道 OpenAI 在该市场的布局要领先 Anthropic 数年之久。\n\n尽管如此，OpenAI 的企业业务规模仍大概率超过 Anthropic。原因在于，许多企业客户除了通过 API 购买其模型服务外，还会付费订阅 ChatGPT。但不少大型客户表示，在企业高管圈层中，Anthropic 已被视为企业级人工智能服务的首选供应商。这一口碑的形成，很大程度上得益于 Anthropic 并未在消费端功能上投入过多精力，而是始终专注于企业市场。\n\n反观 OpenAI，部分大客户指出，该公司仍在以惊人的速度推出各类消费级产品，其企业战略似乎仍处于探索和调整阶段。过去数年，OpenAI 的主要增长逻辑是，依靠 ChatGPT 个人用户的口碑传播，吸引其所属企业的关注，进而与企业签订大额订阅合同。然而，向企业客户销售大额合同的流程往往耗时漫长，通常需要在数月内与企业不同部门的高管进行多轮洽谈，而这些高管早已习惯了各类商务宴请和礼遇。\n\n知情人士透露，OpenAI 希望吸引更多像数据平台公司 Databricks、财务软件公司 Intuit 这样的客户 -- -- 这类客户会主动找上门来，直接采购其产品。\n\n上周，世界经济论坛在瑞士达沃斯举行期间，两位 OpenAI 高管 -- -- 首席运营官布拉德・莱特卡普和首席营收官丹尼斯・德雷瑟（曾任协同办公软件 Slack 首席执行官）也在当地积极接洽潜在企业客户，一位知情人士证实了这一消息。与此同时，OpenAI 于本周三在旧金山总部向员工宣布，公司新近从思维机器实验室（Thinking Machines Lab）聘请的顶尖人工智能研究员巴雷特・佐夫，将负责牵头企业级产品的研发工作。\n\nAnthropic 的竞争优势\n\nAnthropic 去年推出的 Claude Code，是其近期在企业市场取得成功的核心驱动力。这款工具具备代码编写和优化功能。一家每月在 OpenAI 和 Anthropic 平台均投入数百万美元的初创企业首席执行官表示，对于部分客户而言，Claude Code 对生产效率的提升作用，远超员工日常用于通用办公和信息检索的聊天机器人。此外，非技术岗位员工也开始使用 Claude Code 及其衍生工具 Cowork，处理各类办公软件相关的非编程任务。\n\nAnthropic 产品的市场热度，给 OpenAI 带来了双重压力：一方面需要对标优化竞品性能，另一方面还要开发大型企业客户青睐的新功能。与此同时，OpenAI 部分面向知识型工作的产品（如用于电子表格创建和编辑的智能代理工具），其市场表现并未达到预期。\n\n一位与 OpenAI 合作的咨询顾问透露，OpenAI 领导层在与合作咨询公司的会议中，提前透露了 Codex 工具的升级计划。他们表示，新版 Codex 的性能和功能很快将超越 Claude Code。目前，OpenAI 也在为 ChatGPT 新增协作功能及其他办公场景功能。这一动向自然逃不过微软的眼睛 -- -- 作为 OpenAI 的重要合作伙伴，微软正加紧升级其办公软件中的人工智能功能。\n\n上述咨询顾问指出，OpenAI 正考虑推出一款新工具，帮助客户更直观地量化使用其人工智能产品所获得的经济效益。这类工具的落地，将成为 OpenAI 拿下大额企业订单的关键所在。\n\n此外，OpenAI 也在针对医疗等特定行业推出定制化功能。奥特曼于本周五在社交平台 X 发文称，公司正为 Codex 工具开发面向网络安全等领域的专属功能 -- -- 而网络安全恰好也是 Anthropic 的重点布局方向。\n\n知情人士表示，过去一年，OpenAI 对客户销售模式进行了重组：摒弃了原先 \"多人负责多款产品\" 的销售模式，改为由专属销售代表对接客户，为其提供一站式的多产品销售服务。\n\n该知情人士还透露，OpenAI 正在分析企业客户使用其人工智能产品的价值高点，并计划推出工具帮助客户量化这些经济效益。合作咨询顾问强调，这类工具的推出，将是 OpenAI 持续斩获大额企业订单的核心关键。\n\n据知情人士披露，OpenAI 已与 7 家客户签订了价值至少 1 亿美元的多年期合作协议；另有 6 家客户签署的合同金额不低于 7500 万美元，且这些合同在续约时，金额有望增长至 1 亿美元乃至更高。\n\n相比之下，Anthropic 曾在去年 12 月宣布，其麾下至少有 9 家客户的年消费额超过 1 亿美元，其中就包括微软 -- -- 按照合作计划，微软每年在 Anthropic 模型上的投入将高达 5 亿美元。\n\n上述初创企业首席执行官及 OpenAI 合作咨询顾问均表示，在企业客户合同条款的设计上，Anthropic 似乎也已领先于 OpenAI。具体而言，Anthropic 允许客户预先承诺一定量的 API 调用额度，以此换取更优惠的定价。而部分 OpenAI 客户曾私下抱怨，他们未能获得如此灵活的合同条款。\n\n不过，OpenAI 已于去年对合同条款进行了优化，提升了灵活性。知情人士表示，目前公司会向企业客户提供先进模型和工具的使用额度，客户购买的额度越多，对应的每席位单价也就越低。\n\n值得注意的是，OpenAI 的部分业务推进并不顺利。例如，该公司去年将智能代理工具的营收预期削减了一半，预计 2025 年相关收入为 14 亿美元。知情人士透露，2024 年 7 月，OpenAI 为 ChatGPT 订阅用户推出了智能代理功能 ChatGPT Agent，支持用户创建和编辑电子表格及演示文稿，但该功能的实际表现未能达到部分内部考核指标，比如未能实现 \"覆盖 10% 的 ChatGPT 周活跃用户\" 这一目标。\n\n尽管遭遇此类挫折，OpenAI 并未动摇 -- -- 这家公司长期以来的愿景，就是同时在消费端和企业端市场站稳脚跟。奥特曼曾将 ChatGPT 形容为 \"助力工作的超级智能私人助理\"。例如在一年前，OpenAI 就曾展示过一款销售领域的人工智能代理原型，该工具能够自动筛选销售线索，并判断哪些线索具备跟进价值。\n\n目前，OpenAI 约 40% 的营收来自企业客户。公司首席财务官萨拉・弗莱尔本周三在达沃斯论坛上表示，预计到今年年底，企业客户贡献的营收占比将提升至 50%。奥特曼于本周四在社交平台 X 发文称，仅过去一个月，OpenAI 通过 API 销售就新增了 10 亿美元的年化收入。他还特别指出，这一数据打破了外界对公司的固有印象：\"很多人觉得我们只有 ChatGPT。\""
  },
  {
    "source": "Kashmir Observer",
    "company": "OpenAI",
    "title": "OpenAI Drops 'Safely,' Tests Who AI Really Serves - Kashmir Observer",
    "date": "2026-02-15T11:51:04Z",
    "url": "https://kashmirobserver.net/2026/02/15/openai-drops-safely-tests-who-ai-really-serves/",
    "content": "OpenAI, the maker of the most popular AI chatbot, used to say it aimed to build artificial intelligence that \"safely benefits humanity, unconstrained by a need to generate financial return,\" according to its 2023 mission statement. But the ChatGPT maker seems to no longer have the same emphasis on doing so \"safely.\"\n\nWhile reviewing its latest IRS disclosure form, which was released in November 2025 and covers 2024, I noticed OpenAI had removed \"safely\" from its mission statement, among other changes. That change in wording coincided with its transformation from a nonprofit organisation into a business increasingly focused on profits.\n\nOpenAI currently faces several lawsuits related to its products' safety, making this change newsworthy. Many of the plaintiffs suing the AI company allege psychological manipulation, wrongful death and assisted suicide, while others have filed negligence claims.\n\nAs a scholar of nonprofit accountability and the governance of social enterprises, I see the deletion of the word \"safely\" from its mission statement as a significant shift that has largely gone unreported - outside highly specialized outlets.\n\nAnd I believe OpenAI's makeover is a test case for how we, as a society, oversee the work of organisations that have the potential to both provide enormous benefits and do catastrophic harm.\n\nTracing OpenAI's origins\n\nOpenAI, which also makes the Sora video artificial intelligence app, was founded as a nonprofit scientific research lab in 2015. Its original purpose was to benefit society by making its findings public and royalty-free rather than to make money.\n\nTo raise the money that developing its AI models would require, OpenAI, under the leadership of CEO Sam Altman, created a for-profit subsidiary in 2019. Microsoft initially invested USD 1 billion in this venture; by 2024 that sum had topped USD 13 billion.\n\nIn exchange, Microsoft was promised a portion of future profits, capped at 100 times its initial investment. But the software giant didn't get a seat on OpenAI's nonprofit board - meaning it lacked the power to help steer the AI venture it was funding.\n\nA subsequent round of funding in late 2024, which raised USD 6.6 billion from multiple investors, came with a catch: that the funding would become debt unless OpenAI converted to a more traditional for-profit business in which investors could own shares, without any caps on profits, and possibly occupy board seats.\n\nEstablishing a new structure\n\nIn October 2025, OpenAI reached an agreement with the attorneys general of California and Delaware to become a more traditional for-profit company.\n\nUnder the new arrangement, OpenAI was split into two entities: a nonprofit foundation and a for-profit business.\n\nThe restructured nonprofit, the OpenAI Foundation, owns about one-fourth of the stock in a new for-profit public benefit corporation, the OpenAI Group. Both are headquartered in California but incorporated in Delaware.\n\nA public benefit corporation is a business that must consider interests beyond shareholders, such as those of society and the environment, and it must issue an annual benefit report to its shareholders and the public. However, it is up to the board to decide how to weigh those interests and what to report in terms of the benefits and harms caused by the company.\n\nThe new structure is described in a memorandum of understanding signed in October 2025 by OpenAI and the California attorney general, and endorsed by the Delaware attorney general.\n\nMany business media outlets heralded the move, predicting that it would usher in more investment. Two months later, SoftBank, a Japanese conglomerate, finalised a USD 41 billion investment in OpenAI.\n\nChanging its mission statement\n\nMost charities must file forms annually with the Internal Revenue Service with details about their missions, activities and financial status to show that they qualify for tax-exempt status. Because the IRS makes the forms public, they have become a way for nonprofits to signal their missions to the world.\n\nIn its forms for 2022, and 2023, OpenAI said its mission was \"to build general-purpose artificial intelligence (AI) that safely benefits humanity, unconstrained by a need to generate financial return.\"\n\nThat mission statement has changed, as of OpenAI's 990 form for 2024 - which the company filed with the IRS in late 2025. It became \"to ensure that artificial general intelligence benefits all of humanity.\"\n\nOpenAI had dropped its commitment to safety from its mission statement - along with a commitment to being \"unconstrained\" by a need to make money for investors. According to Platformer, a tech media outlet, it has also disbanded its \"mission alignment\" team.\n\nIn my view, these changes explicitly signal that OpenAI is making its profits a higher priority than the safety of its products.\n\nTo be sure, OpenAI continues to mention safety when it discusses its mission. \"We view this mission as the most important challenge of our time,\" it states on its website. \"It requires simultaneously advancing AI's capability, safety, and positive impact in the world.\"\n\nRevising its legal governance structure\n\nNonprofit boards are responsible for key decisions and upholding their organisation's mission.\n\nUnlike private companies, board members of tax-exempt charitable nonprofits cannot personally enrich themselves by taking a share of earnings.\n\nIn cases where a nonprofit owns a for-profit business, as OpenAI did with its previous structure, investors can take a cut of profits - but they typically do not get a seat on the board or have an opportunity to elect board members, because that would be seen as a conflict of interest.\n\nThe OpenAI Foundation now has a 26 per cent stake in OpenAI Group. In effect, that means that the nonprofit board has given up nearly three-quarters of its control over the company. Software giant Microsoft owns a slightly larger stake - 27 per cent of OpenAI's stock - due to its USD 13.8 billion investment in the AI company to date. OpenAI's employees and its other investors own the rest of the shares.\n\nSteps that might help keep people safe\n\nSeveral conditions in the OpenAI restructuring memo are designed to promote safety, including:\n\nA safety and security committee on the OpenAI Foundation board has the authority to \"require mitigation measures\" that could potentially include the halting of a release of new OpenAI products based on assessments of their risks.\n\nThe for-profit OpenAI Group has its own board, which must consider only OpenAI's mission - rather than financial issues - regarding safety and security issues.\n\nThe OpenAI Foundation's nonprofit board gets to appoint all members of the OpenAI Group's for-profit board.\n\nBut given that neither the mission of the foundation nor of the OpenAI group explicitly alludes to safety, it will be hard to hold their boards accountable for it.\n\nFurthermore, since all but one board member currently serve on both boards, it is hard to see how they might oversee themselves. And the memorandum signed by the California attorney general doesn't indicate whether he was aware of the removal of any reference to safety from the mission statement.\n\nIdentifying other paths OpenAI could have taken\n\nThere are alternative models that I believe would serve the public interest better than this one.\n\nWhen Health Net, a California nonprofit health maintenance organisation, converted to a for-profit insurance company in 1992, regulators required that 80 per cent of its equity be transferred to another nonprofit health foundation. Unlike with OpenAI, the foundation had majority control after the transformation.\n\nA coalition of California nonprofits has argued that the attorney general should require OpenAI to transfer all of its assets to an independent nonprofit.\n\nAnother example is The Philadelphia Inquirer. The Pennsylvania newspaper became a for-profit public benefit corporation in 2016. It belongs to the Lenfest Institute, a nonprofit.\n\nThis structure allows Philadelphia's biggest newspaper to attract investment without compromising its purpose - journalism serving the needs of its local communities. It's become a model for potentially transforming the local news industry.\n\nAt this point, I believe that the public bears the burden of two governance failures. One is that OpenAI's board has apparently abandoned its mission of safety. And the other is that the attorneys general of California and Delaware have let that happen."
  },
  {
    "source": "@businessline",
    "company": "OpenAI",
    "title": "OpenAI has deleted the word 'safely' from its mission - and its new structure is a test for whether AI serves society or shareholders",
    "date": "2026-02-15T03:47:55Z",
    "url": "https://www.thehindubusinessline.com/info-tech/openai-has-deleted-the-word-safely-from-its-mission-and-its-new-structure-is-a-test-for-whether-ai-serves-society-or-shareholders/article70634505.ece",
    "content": "OpenAI, the maker of the most popular AI chatbot, used to say it aimed to build artificial intelligence that \"safely benefits humanity, unconstrained by a need to generate financial return,\" according to its 2023 mission statement. But the ChatGPT maker seems to no longer have the same emphasis on doing so \"safely.\"\n\nWhile reviewing its latest IRS disclosure form, which was released in November 2025 and covers 2024, I noticed OpenAI had removed \"safely\" from its mission statement, among other changes. That change in wording coincided with its transformation from a nonprofit organisation into a business increasingly focused on profits.\n\nOpenAI currently faces several lawsuits related to its products' safety, making this change newsworthy. Many of the plaintiffs suing the AI company allege psychological manipulation, wrongful death and assisted suicide, while others have filed negligence claims.\n\nAs a scholar of nonprofit accountability and the governance of social enterprises, I see the deletion of the word \"safely\" from its mission statement as a significant shift that has largely gone unreported - outside highly specialised outlets.\n\nAnd I believe OpenAI's makeover is a test case for how we, as a society, oversee the work of organisations that have the potential to both provide enormous benefits and do catastrophic harm.\n\nTracing OpenAI's origins\n\nOpenAI, which also makes the Sora video artificial intelligence app, was founded as a nonprofit scientific research lab in 2015. Its original purpose was to benefit society by making its findings public and royalty-free rather than to make money.\n\nTo raise the money that developing its AI models would require, OpenAI, under the leadership of CEO Sam Altman, created a for-profit subsidiary in 2019. Microsoft initially invested $1 billion in this venture; by 2024 that sum had topped $13 billion.\n\nIn exchange, Microsoft was promised a portion of future profits, capped at 100 times its initial investment. But the software giant didn't get a seat on OpenAI's nonprofit board - meaning it lacked the power to help steer the AI venture it was funding.\n\nA subsequent round of funding in late 2024, which raised $6.6 billion from multiple investors, came with a catch: that the funding would become debt unless OpenAI converted to a more traditional for-profit business in which investors could own shares, without any caps on profits, and possibly occupy board seats.\n\nEstablishing a new structure\n\nIn October 2025, OpenAI reached an agreement with the attorneys general of California and Delaware to become a more traditional for-profit company.\n\nUnder the new arrangement, OpenAI was split into two entities: a nonprofit foundation and a for-profit business.\n\nThe restructured nonprofit, the OpenAI Foundation, owns about one-fourth of the stock in a new for-profit public benefit corporation, the OpenAI Group. Both are headquartered in California but incorporated in Delaware.\n\nA public benefit corporation is a business that must consider interests beyond shareholders, such as those of society and the environment, and it must issue an annual benefit report to its shareholders and the public. However, it is up to the board to decide how to weigh those interests and what to report in terms of the benefits and harms caused by the company.\n\nThe new structure is described in a memorandum of understanding signed in October 2025 by OpenAI and the California attorney general, and endorsed by the Delaware attorney general.\n\nMany business media outlets heralded the move, predicting that it would usher in more investment. Two months later, SoftBank, a Japanese conglomerate, finalised a $41 billion investment in OpenAI.\n\nChanging its mission statement\n\nMost charities must file forms annually with the Internal Revenue Service with details about their missions, activities and financial status to show that they qualify for tax-exempt status. Because the IRS makes the forms public, they have become a way for nonprofits to signal their missions to the world.\n\nIn its forms for 2022, and 2023, OpenAI said its mission was \"to build general-purpose artificial intelligence (AI) that safely benefits humanity, unconstrained by a need to generate financial return.\"\n\nThat mission statement has changed, as of OpenAI's 990 form for 2024 - which the company filed with the IRS in late 2025. It became \"to ensure that artificial general intelligence benefits all of humanity.\"\n\nOpenAI had dropped its commitment to safety from its mission statement - along with a commitment to being \"unconstrained\" by a need to make money for investors. According to Platformer, a tech media outlet, it has also disbanded its \"mission alignment\" team.\n\nIn my view, these changes explicitly signal that OpenAI is making its profits a higher priority than the safety of its products.\n\nTo be sure, OpenAI continues to mention safety when it discusses its mission. \"We view this mission as the most important challenge of our time,\" it states on its website. \"It requires simultaneously advancing AI's capability, safety, and positive impact in the world.\"\n\nRevising its legal governance structure\n\nNonprofit boards are responsible for key decisions and upholding their organisation's mission.\n\nUnlike private companies, board members of tax-exempt charitable nonprofits cannot personally enrich themselves by taking a share of earnings.\n\nIn cases where a nonprofit owns a for-profit business, as OpenAI did with its previous structure, investors can take a cut of profits - but they typically do not get a seat on the board or have an opportunity to elect board members, because that would be seen as a conflict of interest.\n\nThe OpenAI Foundation now has a 26 per cent stake in OpenAI Group. In effect, that means that the nonprofit board has given up nearly three-quarters of its control over the company. Software giant Microsoft owns a slightly larger stake - 27 per cent of OpenAI's stock - due to its $13.8 billion investment in the AI company to date. OpenAI's employees and its other investors own the rest of the shares.\n\nSteps that might help keep people safe\n\nSeveral conditions in the OpenAI restructuring memo are designed to promote safety, including:\n\nA safety and security committee on the OpenAI Foundation board has the authority to \"require mitigation measures\" that could potentially include the halting of a release of new OpenAI products based on assessments of their risks.\n\nThe for-profit OpenAI Group has its own board, which must consider only OpenAI's mission - rather than financial issues - regarding safety and security issues.\n\nThe OpenAI Foundation's nonprofit board gets to appoint all members of the OpenAI Group's for-profit board.\n\nBut given that neither the mission of the foundation nor of the OpenAI group explicitly alludes to safety, it will be hard to hold their boards accountable for it.\n\nFurthermore, since all but one board member currently serve on both boards, it is hard to see how they might oversee themselves. And the memorandum signed by the California attorney general doesn't indicate whether he was aware of the removal of any reference to safety from the mission statement.\n\nIdentifying other paths OpenAI could have taken\n\nThere are alternative models that I believe would serve the public interest better than this one.\n\nWhen Health Net, a California nonprofit health maintenance organisation, converted to a for-profit insurance company in 1992, regulators required that 80 per cent of its equity be transferred to another nonprofit health foundation. Unlike with OpenAI, the foundation had majority control after the transformation.\n\nA coalition of California nonprofits has argued that the attorney general should require OpenAI to transfer all of its assets to an independent nonprofit.\n\nAnother example is The Philadelphia Inquirer. The Pennsylvania newspaper became a for-profit public benefit corporation in 2016. It belongs to the Lenfest Institute, a nonprofit.\n\nThis structure allows Philadelphia's biggest newspaper to attract investment without compromising its purpose - journalism serving the needs of its local communities. It's become a model for potentially transforming the local news industry.\n\nAt this point, I believe that the public bears the burden of two governance failures. One is that OpenAI's board has apparently abandoned its mission of safety. And the other is that the attorneys general of California and Delaware have let that happen.\n\nBy Alnoor Ebrahim, Tufts University\n\nComments\n\nPublished on February 15, 2026\n\nREAD MORE"
  },
  {
    "source": "NewsDrum",
    "company": "OpenAI",
    "title": "OpenAI has deleted the word 'safely' from its mission - and its new structure is a test for whether AI serves society or shareholders",
    "date": "2026-02-15T02:54:35Z",
    "url": "https://www.newsdrum.in/international/openai-has-deleted-the-word-safely-from-its-mission-and-its-new-structure-is-a-test-for-whether-ai-serves-society-or-shareholders-11107719",
    "content": "Massachusetts, Feb 15 (The Conversation) OpenAI, the maker of the most popular AI chatbot, used to say it aimed to build artificial intelligence that \"safely benefits humanity, unconstrained by a need to generate financial return,\" according to its 2023 mission statement. But the ChatGPT maker seems to no longer have the same emphasis on doing so \"safely.\" While reviewing its latest IRS disclosure form, which was released in November 2025 and covers 2024, I noticed OpenAI had removed \"safely\" from its mission statement, among other changes. That change in wording coincided with its transformation from a nonprofit organisation into a business increasingly focused on profits.\n\nOpenAI currently faces several lawsuits related to its products' safety, making this change newsworthy. Many of the plaintiffs suing the AI company allege psychological manipulation, wrongful death and assisted suicide, while others have filed negligence claims.\n\nAs a scholar of nonprofit accountability and the governance of social enterprises, I see the deletion of the word \"safely\" from its mission statement as a significant shift that has largely gone unreported - outside highly specialized outlets.\n\nAnd I believe OpenAI's makeover is a test case for how we, as a society, oversee the work of organisations that have the potential to both provide enormous benefits and do catastrophic harm.\n\nTracing OpenAI's origins ---------------------------- OpenAI, which also makes the Sora video artificial intelligence app, was founded as a nonprofit scientific research lab in 2015. Its original purpose was to benefit society by making its findings public and royalty-free rather than to make money.\n\nTo raise the money that developing its AI models would require, OpenAI, under the leadership of CEO Sam Altman, created a for-profit subsidiary in 2019. Microsoft initially invested USD 1 billion in this venture; by 2024 that sum had topped USD 13 billion.\n\nIn exchange, Microsoft was promised a portion of future profits, capped at 100 times its initial investment. But the software giant didn't get a seat on OpenAI's nonprofit board - meaning it lacked the power to help steer the AI venture it was funding.\n\nA subsequent round of funding in late 2024, which raised USD 6.6 billion from multiple investors, came with a catch: that the funding would become debt unless OpenAI converted to a more traditional for-profit business in which investors could own shares, without any caps on profits, and possibly occupy board seats.\n\nEstablishing a new structure ------------------------------- In October 2025, OpenAI reached an agreement with the attorneys general of California and Delaware to become a more traditional for-profit company.\n\nUnder the new arrangement, OpenAI was split into two entities: a nonprofit foundation and a for-profit business.\n\nThe restructured nonprofit, the OpenAI Foundation, owns about one-fourth of the stock in a new for-profit public benefit corporation, the OpenAI Group. Both are headquartered in California but incorporated in Delaware.\n\nA public benefit corporation is a business that must consider interests beyond shareholders, such as those of society and the environment, and it must issue an annual benefit report to its shareholders and the public. However, it is up to the board to decide how to weigh those interests and what to report in terms of the benefits and harms caused by the company.\n\nThe new structure is described in a memorandum of understanding signed in October 2025 by OpenAI and the California attorney general, and endorsed by the Delaware attorney general.\n\nMany business media outlets heralded the move, predicting that it would usher in more investment. Two months later, SoftBank, a Japanese conglomerate, finalised a USD 41 billion investment in OpenAI.\n\nChanging its mission statement ----------------------------------- Most charities must file forms annually with the Internal Revenue Service with details about their missions, activities and financial status to show that they qualify for tax-exempt status. Because the IRS makes the forms public, they have become a way for nonprofits to signal their missions to the world.\n\nIn its forms for 2022, and 2023, OpenAI said its mission was \"to build general-purpose artificial intelligence (AI) that safely benefits humanity, unconstrained by a need to generate financial return.\" That mission statement has changed, as of OpenAI's 990 form for 2024 - which the company filed with the IRS in late 2025. It became \"to ensure that artificial general intelligence benefits all of humanity.\" OpenAI had dropped its commitment to safety from its mission statement - along with a commitment to being \"unconstrained\" by a need to make money for investors. According to Platformer, a tech media outlet, it has also disbanded its \"mission alignment\" team.\n\nIn my view, these changes explicitly signal that OpenAI is making its profits a higher priority than the safety of its products.\n\nTo be sure, OpenAI continues to mention safety when it discusses its mission. \"We view this mission as the most important challenge of our time,\" it states on its website. \"It requires simultaneously advancing AI's capability, safety, and positive impact in the world.\" Revising its legal governance structure -------------------------------------------- Nonprofit boards are responsible for key decisions and upholding their organisation's mission.\n\nUnlike private companies, board members of tax-exempt charitable nonprofits cannot personally enrich themselves by taking a share of earnings.\n\nIn cases where a nonprofit owns a for-profit business, as OpenAI did with its previous structure, investors can take a cut of profits - but they typically do not get a seat on the board or have an opportunity to elect board members, because that would be seen as a conflict of interest.\n\nThe OpenAI Foundation now has a 26 per cent stake in OpenAI Group. In effect, that means that the nonprofit board has given up nearly three-quarters of its control over the company. Software giant Microsoft owns a slightly larger stake - 27 per cent of OpenAI's stock - due to its USD 13.8 billion investment in the AI company to date. OpenAI's employees and its other investors own the rest of the shares.\n\nSteps that might help keep people safe -------------------------------------------- Several conditions in the OpenAI restructuring memo are designed to promote safety, including: A safety and security committee on the OpenAI Foundation board has the authority to \"require mitigation measures\" that could potentially include the halting of a release of new OpenAI products based on assessments of their risks.\n\nThe for-profit OpenAI Group has its own board, which must consider only OpenAI's mission - rather than financial issues - regarding safety and security issues.\n\nThe OpenAI Foundation's nonprofit board gets to appoint all members of the OpenAI Group's for-profit board.\n\nBut given that neither the mission of the foundation nor of the OpenAI group explicitly alludes to safety, it will be hard to hold their boards accountable for it.\n\nFurthermore, since all but one board member currently serve on both boards, it is hard to see how they might oversee themselves. And the memorandum signed by the California attorney general doesn't indicate whether he was aware of the removal of any reference to safety from the mission statement.\n\nIdentifying other paths OpenAI could have taken -------------------------------------------------------- There are alternative models that I believe would serve the public interest better than this one.\n\nWhen Health Net, a California nonprofit health maintenance organisation, converted to a for-profit insurance company in 1992, regulators required that 80 per cent of its equity be transferred to another nonprofit health foundation. Unlike with OpenAI, the foundation had majority control after the transformation.\n\nA coalition of California nonprofits has argued that the attorney general should require OpenAI to transfer all of its assets to an independent nonprofit.\n\nAnother example is The Philadelphia Inquirer. The Pennsylvania newspaper became a for-profit public benefit corporation in 2016. It belongs to the Lenfest Institute, a nonprofit.\n\nThis structure allows Philadelphia's biggest newspaper to attract investment without compromising its purpose - journalism serving the needs of its local communities. It's become a model for potentially transforming the local news industry.\n\nAt this point, I believe that the public bears the burden of two governance failures. One is that OpenAI's board has apparently abandoned its mission of safety. And the other is that the attorneys general of California and Delaware have let that happen. (The Conversation) GRS GRS"
  },
  {
    "source": "WebProNews",
    "company": "OpenAI",
    "title": "Microsoft's Delicate Dance: How Satya Nadella's Empire Is Navigating the OpenAI Agent Threat From Within",
    "date": "2026-02-07T16:32:01Z",
    "url": "https://www.webpronews.com/microsofts-delicate-dance-how-satya-nadellas-empire-is-navigating-the-openai-agent-threat-from-within/",
    "content": "For years, Microsoft's $13 billion bet on OpenAI has been heralded as one of the shrewdest strategic moves in modern technology. The partnership gave Microsoft early access to the most advanced artificial intelligence models on the planet, supercharging its cloud business and breathing new life into its enterprise software suite. But now, as OpenAI rapidly evolves from a model provider into a full-fledged enterprise software company, the very alliance that propelled Microsoft to new heights is beginning to show unmistakable signs of strain.\n\nThe latest flashpoint emerged when OpenAI unveiled its new enterprise agent product -- a tool designed to automate complex business workflows and interact directly with corporate customers. The move places OpenAI squarely in competition with Microsoft's own burgeoning AI agent offerings, which the Redmond giant has been building into its Copilot suite and Azure platform. According to The Information, Microsoft's sales leadership has been forced to directly address the potential rivalry, marking a notable escalation in what had previously been a carefully managed coexistence.\n\nMicrosoft's sales chief has responded to growing internal anxiety about OpenAI's new agent product by attempting to frame the situation as complementary rather than competitive. As reported by The Information, the response came amid questions from Microsoft's sprawling global sales force, many of whom are now encountering OpenAI representatives in the same customer meetings and deal cycles. The tension is palpable: salespeople who once positioned OpenAI's technology as a core differentiator for Azure are now being asked to compete against that same technology packaged under OpenAI's own brand.\n\nThe internal communications suggest that Microsoft leadership is keenly aware of the optics and the operational reality. The company has invested enormous resources into building AI agents through its Copilot platform -- tools that can handle tasks ranging from customer service automation to complex data analysis within enterprise environments. OpenAI's entrance into the same territory with its own agent product represents not just a competitive challenge but a philosophical one: Can a company simultaneously be your most important technology partner and your most dangerous rival?\n\nThe emergence of AI agents as a central battleground in enterprise technology has been one of the defining trends of 2025. Unlike traditional chatbots or copilots that assist humans with tasks, agents are designed to operate autonomously -- executing multi-step workflows, making decisions, and interacting with software systems on behalf of users. Microsoft CEO Satya Nadella has repeatedly described agents as the next major paradigm shift in computing, comparing their potential impact to the rise of mobile apps or cloud computing itself.\n\nMicrosoft has been aggressively building out its agent capabilities. At its Build developer conference and through successive product announcements, the company has introduced agent-building tools within Copilot Studio, integrated autonomous agents into Dynamics 365, and positioned Azure as the premier platform for enterprises looking to deploy AI agents at scale. The company's vision is expansive: a world where every business process, from supply chain management to human resources, is mediated by intelligent agents operating within the Microsoft ecosystem.\n\nOpenAI, meanwhile, has been on its own transformation journey. Once content to operate primarily as an API provider -- selling access to its GPT models to developers and enterprises through Microsoft's Azure infrastructure -- the Sam Altman-led company has increasingly moved up the stack. The launch of ChatGPT Enterprise was an early signal. The introduction of custom GPTs and the GPT Store pushed further into application territory. Now, with a dedicated enterprise agent product, OpenAI is making an unmistakable play to own the customer relationship directly, rather than ceding that ground to Microsoft.\n\nThis evolution has been driven by both strategic ambition and financial necessity. OpenAI's costs are staggering -- the company spends billions annually on compute, much of it purchased from Microsoft's Azure cloud. To justify its reported valuation of over $300 billion and to build a sustainable business, OpenAI needs high-margin enterprise revenue streams that go beyond API access fees. Selling agent products directly to corporations, with their promise of recurring revenue and deep organizational integration, represents exactly the kind of business OpenAI needs to build. The problem, of course, is that this is precisely the business Microsoft has been building for decades.\n\nThe response from Microsoft's sales organization, as detailed by The Information, reflects a carefully calibrated strategy. Rather than openly acknowledging a competitive threat, Microsoft's leadership has emphasized the breadth and depth of its enterprise relationships, its integrated product ecosystem, and the security and compliance capabilities that large organizations require. The implicit message to the sales force: OpenAI may have impressive technology, but Microsoft has the enterprise DNA, the customer relationships, and the full-stack platform that businesses need.\n\nThis is not an unfamiliar playbook for Microsoft. The company has long operated in a world of \"coopetition\" -- simultaneously partnering and competing with companies like SAP, Oracle, Salesforce, and Amazon. What makes the OpenAI dynamic different is the sheer depth of the entanglement. Microsoft doesn't just partner with OpenAI; it is OpenAI's largest investor, its primary cloud provider, and the exclusive commercial reseller of its models to many enterprise customers. Unwinding or even significantly restructuring this relationship would be enormously complex and potentially value-destructive for both parties.\n\nFor enterprise customers, the emerging rivalry creates both opportunity and confusion. On one hand, competition between Microsoft and OpenAI could drive faster innovation, better products, and more competitive pricing in the AI agent space. On the other hand, CIOs and technology leaders now face a bewildering set of choices: Do they build their agent strategies on Microsoft's Copilot platform, which offers deep integration with the Office 365 and Azure tools they already use? Or do they go directly to OpenAI, betting that the company's cutting-edge models and rapid innovation cycle will deliver superior results?\n\nSeveral large enterprises have reportedly begun hedging their bets, engaging with both Microsoft and OpenAI simultaneously while also evaluating alternatives from Google, Anthropic, and a growing constellation of startups. The risk of vendor lock-in is particularly acute in the agent space, where these tools become deeply embedded in business processes and are difficult to replace once deployed. Microsoft's advantage here is significant -- its enterprise software already sits at the heart of most large organizations, giving it a natural distribution channel that OpenAI cannot easily replicate.\n\nThe financial dimensions of the Microsoft-OpenAI relationship add another layer of complexity. Under the terms of their partnership, Microsoft receives a significant share of OpenAI's revenue until its investment is recouped, and it earns cloud computing fees from the massive Azure consumption that OpenAI's training and inference workloads generate. In theory, even if OpenAI's enterprise agent product succeeds wildly, Microsoft benefits financially through these mechanisms.\n\nBut theory and practice diverge when it comes to strategic control. If OpenAI builds direct relationships with enterprise customers and those customers begin to view OpenAI -- rather than Microsoft -- as their primary AI vendor, the long-term implications for Microsoft's cloud and software businesses could be profound. Azure's growth story has been significantly powered by AI workloads, and a substantial portion of that growth is tied to OpenAI's models. If enterprises begin accessing those models through OpenAI's own products rather than through Azure, Microsoft's cloud revenue growth could decelerate, even as OpenAI's business expands.\n\nIndustry observers have noted that the Microsoft-OpenAI dynamic increasingly resembles the fraught relationships that have historically defined -- and sometimes destroyed -- major technology partnerships. The IBM-Microsoft partnership of the 1980s, which gave birth to the PC revolution before collapsing under the weight of competing ambitions, is a frequently cited parallel. More recently, the Apple-Google relationship around search and mobile has demonstrated how two companies can maintain a mutually beneficial partnership even as they compete fiercely in adjacent markets.\n\nFor now, both Microsoft and OpenAI appear committed to maintaining the partnership, even as the competitive surface area between them expands. Microsoft continues to integrate OpenAI's latest models into its products, and OpenAI continues to rely on Azure for its computing infrastructure. But the emergence of OpenAI's enterprise agent product -- and the pointed response it has elicited from Microsoft's sales leadership -- suggests that the era of uncomplicated partnership is over. What replaces it will be one of the most consequential business dynamics in the technology industry for years to come, shaping not just the fortunes of two companies but the trajectory of artificial intelligence adoption across the global economy.\n\nAs one industry veteran noted, the real question is not whether Microsoft and OpenAI will compete -- that is already happening. The question is whether they can compete without destroying the partnership that made both of them AI superpowers in the first place."
  },
  {
    "source": "凤凰网（凤凰新媒体）",
    "company": "OpenAI",
    "title": "2000亿！孙正义要投出AI史上最大单笔融资",
    "date": "2026-01-28T15:39:47Z",
    "url": "https://tech.ifeng.com/c/8qI9aEms1g8",
    "content": "编译｜万贵霞\n\n编辑｜云鹏\n\n智东西1月28日消息，据彭博社和《华尔街日报》最新披露，软银集团正与OpenAI洽谈一笔最高达300亿美元（约合人民币2083.8亿元）的追加投资。\n\n外媒提到，如果这笔交易最终落地，这将成为全球AI领域迄今规模最大的单笔融资之一，也意味着孙正义正在把几乎所有筹码压向OpenAI。\n\n消息传出后，软银昨天的股价一度飙升8.8%，随后涨幅收窄，今天在东京交易时段上涨约3.7%。\n\n软银最新股价（来源：谷歌财经）\n\n与此同时，OpenAI也在产品层面持续加速，OpenAI昨天发布了一款面向科研人员的全新AI工具Prism，试图将ChatGPT从\"通用助手\"升级为\"科学发现的基础设施\"。\n\n昨天，OpenAI发布上线Prism工具的公告（图源：OpenAI官网）\n\n日本股票策略师Amir Anvarzadeh说，\"孙正义显然已经孤注一掷，把所有筹码都押在了ChatGPT上。\"\n\n一、300亿美元追加投资，孙正义快要把OpenAI变成\"第二个Arm\"\n\n根据多位知情人士的说法，软银目前正在评估向OpenAI追加投资至多300亿美元的可能性，相关谈判仍在进行中，最终金额和条款尚未敲定。\n\n如果这笔交易完成，软银在OpenAI的持股比例将进一步上升。去年12月，软银刚刚向OpenAI投资225亿美元（约合人民币1562.8亿元），持股比例提升至约11%，一跃成为OpenAI最大的外部股东之一。\n\n据昨天《华尔街日报》报道，这轮追加投资是OpenAI更大规模融资计划的一部分，该公司正试图从全球投资者手中筹集500亿（约合人民币3473.0亿元）至1000亿美元（约合人民币6945.9亿元）资金，目标估值高达7500亿至8300亿美元（约合人民币5.2万亿至5.8万亿元）。\n\nOpenAI正在考虑进行首次公开募股（IPO），并计划从中东主权财富基金（Middle Eastern sovereign-wealth funds）和其他风险投资基金筹集资金。该公司现有投资者包括Thrive Capital、Khosla Ventures和阿联酋基金MGX。\n\n对软银而言，这不仅是一笔财务投资，更是一次战略押注。过去几年，孙正义反复强调\"AI将重塑一切产业\"，而OpenAI被他视为最有可能站在这场变革核心的公司。\n\n为了筹集足够资金持续加码OpenAI，软银正在进行一系列激进的资产调整。\n\n《华尔街日报》报道中提到，为了筹集资金投资OpenAI，软银已将其持有的英伟达的股份以58亿美元（约合人民币402.9亿元）的价格出售。同时，软银减持了T-Mobile等资产，并利用Arm股票进行融资操作。\n\n此前，软银还暂停了对美国数据中心运营商Switch的收购谈判，将资金集中投入AI相关项目。\n\n过去一年里，软银在AI和自动化领域的投入明显提速：斥资65亿美元（约合人民币451.5亿元）收购美国芯片设计公司Ampere Computing；以54亿美元（约合人民币375.1亿元）收购ABB（ASEA Brown Boveri）的机器人业务；持续加码OpenAI。\n\n软银之前的投资情况（图源：软银官网）\n\n此外，标普全球（S&P Global）评级机构已经发出警告称，软银在AI领域的激进投资，叠加Arm股价波动，正对软银的信用评级构成压力。如果OpenAI的估值在软银资产结构中占比过高，可能进一步放大投资组合风险。\n\n彭博社的行业研究员提到，如果软银集团向OpenAI追加投资300亿美元，其BB+信用评级可能面临下行压力。在计入尚未完成的交易、并假设软银当前持有的OpenAI股权被重新估值上调的情况下，软银的贷款价值比（LTV）可能触及35%的评级下调触发线。\n\n为将披露口径下的LTV控制在25%以下，该交易很可能需要软银通过出售资产及动用保证金贷款等方式筹集至少150亿美元（约合人民币1041.9亿元）资金。\n\n与此同时，软银的投资组合风险将进一步上升。按1月27日Arm股价计算，OpenAI的估值有可能超过Arm，成为软银最大的单一持股，其占软银整体资产价值的比重或将超过30%。\n\n日本股票策略师Amir Anvarzadeh说，随着谷歌Gemini、Anthropic等模型快速追赶，ChatGPT的领先优势正受到更大挑战。\n\n\"从竞争角度看，OpenAI的前景已经不像一年前那样毫无悬念。\"但他同时提到，\"孙正义显然已经孤注一掷，把所有筹码都押在了ChatGPT上。\"\n\nOpenAI自身也面临着一个现实问题：烧钱速度极快，模型训练需要巨额算力、推理成本持续攀升、顶尖研究人员的争夺愈发激烈。\n\n二、Prism亮相：OpenAI把AI推进\"科研工作流\"\n\n就在融资消息发酵的同时，OpenAI在产品层面也释放出一个重要信号。\n\n1月27日，OpenAI正式推出了一款名为Prism的免费工具，定位为面向科学家的AI工具，任何拥有ChatGPT账户的用户均可免费使用。\n\nPrism工具界面（图源：OpenAI官网）\n\n与传统聊天界面不同，Prism更像是一个\"AI增强型科研写作与协作平台\"：基于GPT-5.2模型、原生支持LaTeX、可用于论文写作、修改、文献检索、支持多人协作，能将手绘草图快速转化为规范图表。\n\nOpenAI官方说，Prism并不是要取代科学家，而是加速他们的工作流程。公司高管将其类比为科研领域的\"Cursor或Windsurf\"，即深度嵌入工作流、显著提升效率的工具。\n\n现在，AI的发展越来越面向垂直场景应用。\n\nOpenAI及其竞争对手，如谷歌的DeepMind和Anthropic的Claude，越来越专注于AI在科学和医疗保健领域的应用，从利用该技术帮助指导新药研究到审查个人医疗数据。\n\nOpenAI将Prism推向科研领域并非偶然，该公司数据显示，ChatGPT平均每周会收到840万条与高等科学和数学相关的消息，而且这一数字预计在2025年将增长47%。\n\nOpenAI科学副总裁Kevin Weil直言：\"我认为2026年对AI和科学来说，就像2025年对AI和软件工程一样。\"在他看来，AI辅助科研正站在一个类似\"代码助手爆发前夜\"的时间点。\n\n在数学和统计学领域，这种趋势已经开始显现。外媒Tech Crunch提到，去年12月发表的一篇统计学论文《关于最大似然估计的学习曲线单调性（On Learning-Curve Monotonicity for Maximum Likelihood Estimators）》，该论文利用GPT-5.2 Pro模型，完成了对核心公理的新证明，而人类研究人员仅负责提示和验证模型的工作。\n\n该论文详情（图源：康奈尔大学官网）\n\n这类案例正在不断强化OpenAI的一个判断：AI将成为科学发现的重要参与者，而不仅是工具。\n\n结语：从ChatGPT到\"AI基础设施\"，OpenAI的终局想象\n\n把融资与产品放在一起看，会发现OpenAI正在沿着一条非常明确的路线前进。\n\n一方面，OpenAI通过巨额融资，确保模型规模、算力和人才储备的持续领先；另一方面，OpenAI通过Prism等工具，把AI深度嵌入科研、教育、企业等高价值工作流中。\n\n而软银，用大把的钱砸向OpenAI，赌的是AI重构产业的终极可能性。"
  },
  {
    "source": "Geeky Gadgets",
    "company": "OpenAI",
    "title": "OpenAI Targets Growth with $8 ChatGPT Plan, Ad-Supported Tiers & 2026 Revenue Push",
    "date": "2026-01-22T14:29:31Z",
    "url": "https://www.geeky-gadgets.com/openai-2026-future-outlook/",
    "content": "What does it take to stay ahead in one of the most competitive industries on the planet? In this overview, Matthew Berman explores how OpenAI's latest moves are reshaping the AI landscape and solidifying its position as a market leader. From unveiling an $8/month subscription plan to introducing advertising into ChatGPT, OpenAI is making bold, calculated decisions that could redefine how we interact with artificial intelligence. But these moves aren't without controversy, balancing accessibility with monetization has sparked debate among users and industry experts alike. With projections to hit a staggering $20 billion in revenue by 2026, OpenAI's strategy is as ambitious as it is innovative, and the stakes couldn't be higher.\n\nThis breakdown offers a closer look at the genius behind OpenAI's latest initiatives, including its strategic partnerships and tiered pricing models. You'll discover how the company is tackling critical challenges like GPU shortages while expanding its global reach with offerings like ChatGPT Go. Whether you're curious about how advertising might transform AI interactions or intrigued by OpenAI's plans for consumer hardware, this guide will unpack the implications of these bold moves. As the AI race intensifies, OpenAI's ability to innovate while staying user-focused raises a compelling question: Is this the blueprint for the future of technology?\n\nOpenAI's Strategic Moves\n\nImpressive Revenue Growth and Strategic Investments\n\nOpenAI's financial trajectory demonstrates significant momentum, with the company projected to reach $20 billion in revenue by 2026. This represents a threefold year-over-year increase, driven by the widespread adoption of its AI models and a diversified product portfolio. However, this rapid growth is not without its challenges. A critical bottleneck lies in the limited availability of GPUs, which are essential for training and deploying advanced AI systems.\n\nTo address this constraint, OpenAI has secured a $10 billion agreement with Cerebras to access specialized AI chips. This partnership ensures the computational resources necessary to sustain its innovation pipeline and meet operational demands. By proactively resolving this issue, OpenAI is positioning itself to maintain its leadership in a rapidly evolving and resource-intensive market.\n\nExpanding Accessibility with Tiered Subscription Plans\n\nOpenAI has introduced a new $8/month subscription plan, \"ChatGPT Go,\" designed to make its services more accessible to users in regions with lower purchasing power. This plan provides access to GPT-5.2 Instant, a smaller and faster model with enhanced features compared to the free tier. While this initiative may initially operate at a financial loss, it represents a strategic investment in building long-term user loyalty and expanding OpenAI's global footprint.\n\nThe introduction of ChatGPT Go complements OpenAI's existing Pro and Enterprise subscription tiers, creating a tiered pricing structure that caters to diverse market segments. This approach not only broadens accessibility but also reinforces OpenAI's reputation as a user-focused innovator. By offering tailored solutions for different user groups, OpenAI is strengthening its position in the global AI market.\n\nOpenAI Just Made a Genius Move!\n\nHere are more guides from our previous articles and guides related to OpenAI that you may find helpful.\n\nAdvertising: A New Revenue Stream\n\nOpenAI has ventured into advertising by integrating ads into its free and $8/month subscription tiers. These ads are designed to be clearly labeled and kept separate from ChatGPT's responses, making sure transparency and maintaining user trust. By using user data, OpenAI aims to deliver personalized and relevant advertisements, unlocking a significant new revenue stream.\n\nHigher-tier subscriptions, such as Pro and Enterprise plans, will remain ad-free, preserving an uninterrupted experience for premium users. This dual approach allows OpenAI to monetize its services effectively while maintaining value for its most dedicated customers. The integration of advertising represents a calculated move to diversify revenue streams without compromising user satisfaction.\n\nCompeting in a Crowded Market\n\nDespite its leadership in AI adoption, OpenAI faces intense competition from established players like Google and emerging companies such as Anthropic. Google's Gemini models and Anthropic's coding-focused APIs present formidable challenges, each offering distinct strengths. OpenAI's revenue mix reflects its diversified strategy, with consumer subscriptions contributing 55-60%, enterprise solutions accounting for 25-30%, and API/developer platforms making up 15-20%.\n\nTo stay ahead, OpenAI employs a bundling strategy similar to Microsoft's, integrating products and services to appeal to enterprise users. This approach not only strengthens its market position but also highlights its adaptability in a competitive environment. OpenAI's ability to innovate and strategically invest in its offerings will be critical in maintaining its edge in the crowded AI landscape.\n\nDriving Innovation with a Long-Term Vision\n\nOpenAI's long-term strategy revolves around creating a \"flywheel effect,\" where improved models drive adoption, generating revenue to fund further innovation. By using user data, OpenAI aims to enhance personalization and improve its models, fostering user retention and market lock-in. This cyclical approach ensures sustained growth and innovation.\n\nLooking to the future, OpenAI plans to introduce a consumer hardware device, potentially competing with smartphones. This move could further integrate AI into everyday life, expanding the company's influence and reach. Such initiatives demonstrate OpenAI's commitment to shaping the future of technology and embedding AI into the fabric of daily life.\n\nBalancing Growth and Profitability\n\nWhile OpenAI's revenue growth is impressive, the company is likely operating at a loss due to the high costs associated with running advanced AI models. These losses, however, are viewed as strategic investments aimed at capturing market share and achieving long-term profitability. OpenAI's willingness to prioritize innovation and growth over immediate financial returns underscores its commitment to maintaining its leadership in the AI industry.\n\nCompetitor Strategies: A Comparative View\n\nOpenAI's competitors are employing distinct strategies to carve out their niches in the AI market. Anthropic focuses on API-based revenue and excels in coding applications, while Google uses its diversified revenue streams, hardware integration, and vast user base to pose a formidable challenge. OpenAI's ability to differentiate itself through innovation, strategic investments, and user-centric solutions will be critical in maintaining its competitive edge.\n\nStrategic Communication and Market Perception\n\nThe timing of OpenAI's announcements, ads, the $8 subscription plan, and revenue growth, was carefully orchestrated to project strength and stability. This strategic communication counters potential concerns about financial struggles and reinforces OpenAI's position as a market leader. By demonstrating transparency and a clear vision, OpenAI aims to build trust and confidence among users, investors, and partners.\n\nShaping the Future of AI\n\nOpenAI's recent initiatives reflect a calculated effort to dominate the AI market by balancing accessibility, innovation, and monetization. From addressing GPU constraints to introducing affordable subscription plans and integrating advertising, the company is positioning itself for sustained growth. As competition intensifies, OpenAI's focus on long-term vision, strategic investments, and user-centric innovation will be pivotal in maintaining its leadership in the rapidly evolving AI landscape."
  },
  {
    "source": "mint",
    "company": "OpenAI",
    "title": "OpenAI has deleted the word 'safely' from its mission - and its new structure is a test for whether AI serves society or shareholders | Mint",
    "date": "2026-02-15T03:00:45Z",
    "url": "https://www.livemint.com/technology/openai-has-deleted-the-word-safely-from-its-mission-and-its-new-structure-is-a-test-for-whether-ai-serves-society-or-shareholders-11771123905407.html",
    "content": "Massachusetts, Feb 15 (The Conversation) OpenAI, the maker of the most popular AI chatbot, used to say it aimed to build artificial intelligence that \"safely benefits humanity, unconstrained by a need to generate financial return,\" according to its 2023 mission statement. But the ChatGPT maker seems to no longer have the same emphasis on doing so \"safely.\"\n\nWhile reviewing its latest IRS disclosure form, which was released in November 2025 and covers 2024, I noticed OpenAI had removed \"safely\" from its mission statement, among other changes. That change in wording coincided with its transformation from a nonprofit organisation into a business increasingly focused on profits.\n\nOpenAI currently faces several lawsuits related to its products' safety, making this change newsworthy. Many of the plaintiffs suing the AI company allege psychological manipulation, wrongful death and assisted suicide, while others have filed negligence claims.\n\nAs a scholar of nonprofit accountability and the governance of social enterprises, I see the deletion of the word \"safely\" from its mission statement as a significant shift that has largely gone unreported - outside highly specialized outlets.\n\nAnd I believe OpenAI's makeover is a test case for how we, as a society, oversee the work of organisations that have the potential to both provide enormous benefits and do catastrophic harm.\n\nOpenAI, which also makes the Sora video artificial intelligence app, was founded as a nonprofit scientific research lab in 2015. Its original purpose was to benefit society by making its findings public and royalty-free rather than to make money.\n\nTo raise the money that developing its AI models would require, OpenAI, under the leadership of CEO Sam Altman, created a for-profit subsidiary in 2019. Microsoft initially invested USD 1 billion in this venture; by 2024 that sum had topped USD 13 billion.\n\nIn exchange, Microsoft was promised a portion of future profits, capped at 100 times its initial investment. But the software giant didn't get a seat on OpenAI's nonprofit board - meaning it lacked the power to help steer the AI venture it was funding.\n\nA subsequent round of funding in late 2024, which raised USD 6.6 billion from multiple investors, came with a catch: that the funding would become debt unless OpenAI converted to a more traditional for-profit business in which investors could own shares, without any caps on profits, and possibly occupy board seats.\n\nIn October 2025, OpenAI reached an agreement with the attorneys general of California and Delaware to become a more traditional for-profit company.\n\nUnder the new arrangement, OpenAI was split into two entities: a nonprofit foundation and a for-profit business.\n\nThe restructured nonprofit, the OpenAI Foundation, owns about one-fourth of the stock in a new for-profit public benefit corporation, the OpenAI Group. Both are headquartered in California but incorporated in Delaware.\n\nA public benefit corporation is a business that must consider interests beyond shareholders, such as those of society and the environment, and it must issue an annual benefit report to its shareholders and the public. However, it is up to the board to decide how to weigh those interests and what to report in terms of the benefits and harms caused by the company.\n\nThe new structure is described in a memorandum of understanding signed in October 2025 by OpenAI and the California attorney general, and endorsed by the Delaware attorney general.\n\nMany business media outlets heralded the move, predicting that it would usher in more investment. Two months later, SoftBank, a Japanese conglomerate, finalised a USD 41 billion investment in OpenAI.\n\nMost charities must file forms annually with the Internal Revenue Service with details about their missions, activities and financial status to show that they qualify for tax-exempt status. Because the IRS makes the forms public, they have become a way for nonprofits to signal their missions to the world.\n\nIn its forms for 2022, and 2023, OpenAI said its mission was \"to build general-purpose artificial intelligence (AI) that safely benefits humanity, unconstrained by a need to generate financial return.\"\n\nThat mission statement has changed, as of OpenAI's 990 form for 2024 - which the company filed with the IRS in late 2025. It became \"to ensure that artificial general intelligence benefits all of humanity.\"\n\nOpenAI had dropped its commitment to safety from its mission statement - along with a commitment to being \"unconstrained\" by a need to make money for investors. According to Platformer, a tech media outlet, it has also disbanded its \"mission alignment\" team.\n\nIn my view, these changes explicitly signal that OpenAI is making its profits a higher priority than the safety of its products.\n\nTo be sure, OpenAI continues to mention safety when it discusses its mission. \"We view this mission as the most important challenge of our time,\" it states on its website. \"It requires simultaneously advancing AI's capability, safety, and positive impact in the world.\"\n\nNonprofit boards are responsible for key decisions and upholding their organisation's mission.\n\nUnlike private companies, board members of tax-exempt charitable nonprofits cannot personally enrich themselves by taking a share of earnings.\n\nIn cases where a nonprofit owns a for-profit business, as OpenAI did with its previous structure, investors can take a cut of profits - but they typically do not get a seat on the board or have an opportunity to elect board members, because that would be seen as a conflict of interest.\n\nThe OpenAI Foundation now has a 26 per cent stake in OpenAI Group. In effect, that means that the nonprofit board has given up nearly three-quarters of its control over the company. Software giant Microsoft owns a slightly larger stake - 27 per cent of OpenAI's stock - due to its USD 13.8 billion investment in the AI company to date. OpenAI's employees and its other investors own the rest of the shares.\n\nSeveral conditions in the OpenAI restructuring memo are designed to promote safety, including:\n\nA safety and security committee on the OpenAI Foundation board has the authority to \"require mitigation measures\" that could potentially include the halting of a release of new OpenAI products based on assessments of their risks.\n\nThe for-profit OpenAI Group has its own board, which must consider only OpenAI's mission - rather than financial issues - regarding safety and security issues.\n\nThe OpenAI Foundation's nonprofit board gets to appoint all members of the OpenAI Group's for-profit board.\n\nBut given that neither the mission of the foundation nor of the OpenAI group explicitly alludes to safety, it will be hard to hold their boards accountable for it.\n\nFurthermore, since all but one board member currently serve on both boards, it is hard to see how they might oversee themselves. And the memorandum signed by the California attorney general doesn't indicate whether he was aware of the removal of any reference to safety from the mission statement.\n\nIdentifying other paths OpenAI could have taken\n\n--------------------------------------------------------\n\nThere are alternative models that I believe would serve the public interest better than this one.\n\nWhen Health Net, a California nonprofit health maintenance organisation, converted to a for-profit insurance company in 1992, regulators required that 80 per cent of its equity be transferred to another nonprofit health foundation. Unlike with OpenAI, the foundation had majority control after the transformation.\n\nA coalition of California nonprofits has argued that the attorney general should require OpenAI to transfer all of its assets to an independent nonprofit.\n\nAnother example is The Philadelphia Inquirer. The Pennsylvania newspaper became a for-profit public benefit corporation in 2016. It belongs to the Lenfest Institute, a nonprofit.\n\nThis structure allows Philadelphia's biggest newspaper to attract investment without compromising its purpose - journalism serving the needs of its local communities. It's become a model for potentially transforming the local news industry.\n\nAt this point, I believe that the public bears the burden of two governance failures. One is that OpenAI's board has apparently abandoned its mission of safety. And the other is that the attorneys general of California and Delaware have let that happen. (The Conversation) GRS"
  },
  {
    "source": "WinBuzzer",
    "company": "OpenAI",
    "title": "OpenAI Accuses Elon Musk's xAI of Destroying Evidence in Court Fight",
    "date": "2026-02-03T21:48:00Z",
    "url": "https://winbuzzer.com/2026/02/03/openai-accuses-musks-xai-of-destroying-evidence-in-court-fig-xcxwbn/",
    "content": "Ongoing Conflict: The dispute is part of multiple legal battles between Musk and OpenAI, with another billion-dollar lawsuit heading to jury trial in April 2026.\n\nOpenAI accused Elon Musk's xAI of systematically destroying evidence using auto-delete messaging tools in a court filing on Monday. The accusations escalate a legal battle already tilting against Musk after a federal judge signaled she may dismiss xAI's lawsuit.\n\nThe claims add a new dimension to the bitter dispute between the ChatGPT maker and the AI startup founded by OpenAI's co-founder, with OpenAI asserting xAI failed to turn over internal documents needed to support its allegations.\n\nOpenAI's filing contends xAI routed communications about every aspect of its business through ephemeral messaging tools designed to auto-delete messages after a set period. The ChatGPT maker contends xAI deployed these tools even as they knew they were planning to sue OpenAI. This potentially destroyed evidence crucial to litigation.\n\nOpenAI is asking the court to stop xAI from using ephemeral communication tools going forward and to appoint a neutral legal expert to review xAI's preservation practices. The allegations land as xAI faces a separate legal setback.\n\nU.S. District Judge Rita Lin signaled her tentative view is to grant OpenAI's motion to dismiss xAI's lawsuit on January 31. Oral arguments are scheduled for February 3, 2026.\n\nJudge Lin stated xAI did not plausibly allege OpenAI acquired or encouraged theft of trade secrets. In a four-page filing, she wrote it was not plausible to infer that OpenAI used xAI's trade secrets.\n\nShe also found it implausible that former xAI employees used them after joining OpenAI.\n\nJudge Lin also indicated she may dismiss xAI's unfair competition claim. She wrote that xAI's allegations:\n\n\"all focus on poaching in service of acquiring xAI's trade secrets and do not identify any other reason why the hiring of those employees was anticompetitive\"\n\nThe judicial skepticism stems from allegations first filed in September 2025. xAI sued OpenAI, accusing it of hiring xAI employees to obtain confidential information related to the Grok chatbot.\n\nThe lawsuit named three individuals who left xAI for OpenAI. This included Xuechen Li, a former xAI engineer who had worked on core AI systems.\n\nOpenAI filed a motion to dismiss the case in the U.S. District Court for the Northern District of California. OpenAI characterized the lawsuit as a \"campaign to harass a competitor with unfounded legal claims\" in its response.\n\nxAI was seeking injunctive relief and damages, hoping to prevent further employee departures and recover losses from alleged trade secret theft.\n\nCountering xAI's claims, OpenAI argues employees freely chose to join OpenAI based on mission alignment.\n\nOpenAI maintains that xAI claims Grok is more advanced than OpenAI's models yet alleges OpenAI stole trade secrets. OpenAI highlights this contradiction as evidence the lawsuit lacks coherence.\n\nOpenAI also contends xAI employees are departing under Musk's leadership and joining OpenAI to advance the mission. The company argues this reflects dissatisfaction with xAI's workplace culture rather than any improper recruitment.\n\nOpenAI filed a countersuit against Musk in a related case, further intensifying the legal conflict between the former collaborators.\n\nTo understand the animosity driving this dispute, one must look back to the pair's shared history. Musk co-founded OpenAI in 2015 as a nonprofit AI research organization alongside Sam Altman and other tech leaders.\n\nHe stepped down from OpenAI's board in 2018 amid disagreements over the organization's direction.\n\nThe relationship soured after OpenAI transitioned from nonprofit to a for-profit structure. Musk has repeatedly criticized this as a betrayal of the organization's founding mission.\n\nMusk founded xAI in 2023 as a direct competitor to OpenAI. He positioned his startup as an alternative to what he views as OpenAI's commercial pivot and Microsoft partnership.\n\nMusk previously filed a separate lawsuit challenging OpenAI's for-profit conversion. He is separately suing OpenAI for becoming a for-profit entity, arguing the transformation violated the organization's original nonprofit charter.\n\nLooking forward, the xAI versus OpenAI dispute is part of multiple ongoing legal battles between Musk and OpenAI. In a separate billion-dollar lawsuit heading to jury trial, Musk is seeking up to $134.5 billion in damages from OpenAI and Microsoft over OpenAI's conversion to a for-profit company. Jury selection is scheduled for April 27, 2026.\n\nDespite the multiple fronts, the case is overseen by Judge Jacqueline Scott Corley in the Northern District of California, while Judge Lin handles the trade secrets matter. Legal observers have expressed skepticism about xAI's legal strategy.\n\nThey note the claims appear to lack insufficient factual support for allegations of coordinated trade secret theft. The evidence destruction allegations may complicate xAI's position further if the court finds the company failed to meet its own preservation obligations while accusing OpenAI of anticompetitive conduct."
  },
  {
    "source": "Techloy",
    "company": "OpenAI",
    "title": "The Elon Musk vs Sam Altman OpenAI Feud Explained: Timeline of the $134B Lawsuit",
    "date": "2026-01-22T22:09:01Z",
    "url": "https://www.techloy.com/the-elon-musk-vs-sam-altman-openai-feud-explained-timeline-of-the-134b-lawsuit/",
    "content": "Almost 800 million people use ChatGPT every week. Most have no idea the tool they rely on was born from a partnership that's now part of a $134 billion lawsuit, or that the feud between some of its original founders could reshape who controls AI development for the next decade.\n\nElon Musk and Sam Altman are two of 11 people who co-founded OpenAI in 2015 with a shared mission: build safe artificial intelligence that benefits humanity, not corporate profits.\n\nMore than ten years later, their relationship has gone sour. The latest exchange happened Monday night on X. After a post was made on X claimed that ChatGPT was linked to nine deaths, including five suicides, Musk replied saying, \"Don't let your loved ones use ChatGPT.\"\n\nAltman fired back hours later, accusing Musk of releasing autopilot on Tesla when \"it was far from a safe thing for Tesla to have released.\"\n\nHere is a complete timeline of how their relationship went sour:\n\nJune 2015: The Vision\n\nSam Altman emailed Elon Musk with a proposal. \"The mission would be to create the first general AI and use it for individual empowerment -- i.e., the distributed version of the future that seems the safest,\" Altman wrote. Small elite team, five-person leadership including both of them, compensation separated from work to avoid conflicts. Musk agreed to all of it.\n\nNovember 2015: Early Cracks\n\nMusk raised concerns about structure before OpenAI even launched. \"The YC stock along with a salary from the nonprofit muddies the alignment of incentives,\" he wrote to Altman. \"Probably better to have a standard C corp with a parallel nonprofit.\" The email would later be used as Musk as evidence that he suspected OpenAI's nonprofit model wouldn't last.\n\nNovember 2015: First Funding\n\nWhen the founders planned to announce $100 million in funding, Musk pushed back. \"We need to go with a much bigger number than $100M to avoid sounding hopeless,\" he wrote. \"I think we should say that we are starting with a $1B funding commitment... I will cover whatever anyone else doesn't provide.\"\n\nDecember 2015: Naming and Launch\n\nGreg Brockman another co-founder and current president of OpenAI suggested \"Cogito\" and \"Consider\" for the new company. Musk rejected both. \"Consider sounds a bit nannyish and self-righteous,\" he wrote, then proposed \"OpenAI.\" By December 11, OpenAI launched publicly as a nonprofit with an announced $1 billion in commitments from Musk, Altman, Peter Thiel, and others. Court documents later revealed most of that billion never materialized. Musk provided nearly all early capital -- between $38 million and $44 million by 2018.\n\n2016: Competition Fears\n\nBy 2016, worries about competition began to swell inside the company. Musk's emails showed deep anxiety about Google DeepMind. \"Deepmind is causing me extreme mental stress,\" he wrote. \"If they win, it will be really bad news with their one mind to rule the world philosophy.\" He worried DeepMind c0-founder, Demis Hassabis, \"could create an AGI dictatorship.\"\n\nOn January 2, 2016, another OpenAI co-founder, Ilya Sutskever sent Musk an email that would become central to the lawsuit. \"As we get closer to building AI, it will make sense to start being less open,\" Sutskever wrote. \"The Open in openAI means that everyone should benefit from the fruits of AI after it's built, but it's totally OK to not share the science...\" Musk replied: \"Yup.\" The exchange suggests OpenAI's leaders planned from the start to become less transparent as they approached AGI.\n\n2017: The Breaking Point\n\nComputing costs escalated. OpenAI realized it would need billions annually, far more than the nonprofit could raise. By late 2017, the founder began to speak about the need for a for-profit structure. They fought over control.\n\nMusk proposed creating a for-profit with himself as CEO, holding majority equity and initial board control. When negotiations stalled, he withheld promised funding. Reid Hoffman, one of the founders of LinkedIn and a Silicon Valley mainstay stepped in with donations to cover salaries.\n\nBrockman and Sutskever sent Altman an email in September expressing concerns about both Altman's judgment and Musk's intentions. Though Musk claimed not to want control of the final AGI, they wrote, \"the negotiations revealed that absolute control was crucial to him.\" They feared the proposed structure would allow Musk to become \"a dictator in the company, should he decide to become one.\"\n\nLater that month, Musk registered \"Open Artificial Intelligence Technologies, Inc.\" as a public benefit corporation -- preparing his own AI venture.\n\nAfter tense back-and-forth, Musk sent the email that ended the partnership. \"Guys, I've had enough. This is the final straw. Either go do something on your own or continue with OpenAI as a nonprofit. I will no longer fund OpenAI until you have made a firm commitment to stay or I'm just being a fool who is essentially providing free funding for you to create a startup. Discussions are over.\"\n\nBrockman later noted in communications that shifting to for-profit without Musk \"would be morally bankrupt.\"\n\nFebruary 2018: Exit and Warning\n\nIn early 2018, Musk suggested OpenAI should \"attach to Tesla as its cash cow,\" calling it \"the only path that could even hope to hold a candle to Google.\" When co-founders rejected terms that gave Musk control, he resigned from the board in February. The official reason: potential conflicts with Tesla's self-driving AI work.\n\nBehind closed doors, Musk told the team OpenAI's \"probability of success was zero\" and that he planned to build an AGI competitor within Tesla. \"When he left in late February 2018, he told our team he was supportive of us finding our own path to raising billions of dollars,\" OpenAI later stated of Musk at the time.\n\nIn August, Altman sent Musk drafts of plans to shift OpenAI to for-profit. Musk didn't respond.\n\nBy December, Musk sent a final warning: \"Even raising several hundred million won't be enough. This needs billions per year immediately or forget it.\"\n\n2019: The Pivot\n\nOpenAI created a \"capped-profit\" subsidiary with the nonprofit maintaining control. Microsoft invested $1 billion. Altman became CEO. The structure attracted major funding while theoretically preserving the mission. Musk later alleged this moment betrayed everything OpenAI stood for.\n\nNovember 2022: ChatGPT Changes Everything\n\nOpenAI released ChatGPT publicly. The chatbot hit 100 million users in under two months. Musk tweeted: \"ChatGPT is scary good. We are not far from dangerously strong AI.\" Despite concerns, the comment acknowledged OpenAI's achievement.\n\nWithin weeks, Musk began criticizing the shift from open-source to closed models and the deepening Microsoft relationship.\n\n2023: Private Pain, Public Competition\n\nEarly in the year, private emails between the two men showed Altman still trying to salvage the relationship. He called Musk a \"hero\" but expressed hurt over public attacks. Musk responded that \"the fate of civilization is at stake.\"\n\nIn July, Musk launched xAI as a direct OpenAI competitor. The stated goal: \"understand the universe.\" The break was now official.\n\nWhen OpenAI's board briefly fired Altman in November, then reinstated him after employee revolt, Musk mocked it as the \"OpenAI Telenovela\" and demanded transparency.\n\nMarch 2024: The Lawsuit Begins\n\nMusk filed in San Francisco state court, alleging breach of contract and fiduciary duty. He claimed OpenAI abandoned its founding mission and had become a de facto Microsoft subsidiary.\n\nDays later, OpenAI published Musk's old emails, including his push for for-profit structure and the Tesla merger idea. \"We're sad that it's come to this with someone whom we've deeply admired -- someone who inspired us to aim higher, then told us we would fail, started a competitor, and then sued us when we started making meaningful progress towards OpenAI's mission without him,\" the company wrote.\n\nJune 2024: Strategic Withdrawal\n\nMusk dropped the lawsuit without explanation. Legal observers were puzzled.\n\nWhen Apple announced iOS integration with ChatGPT that same month, Musk threatened to ban Apple devices at his companies. \"If Apple integrates OpenAI at the OS level, then Apple devices will be banned at my companies,\" he wrote. \"That is an unacceptable security violation.\"\n\nAugust 2024: Federal Court and Public Barbs\n\nMusk refiled in federal court with stronger allegations: fraud, breach of fiduciary duty, RICO violations. The case accused OpenAI and Microsoft of racketeering and antitrust violations.\n\nOctober 2024: The Transformation Complete\n\nOpenAI finished recapitalization. The nonprofit retained control, but its for-profit arm -- valued at $135 billion with Microsoft's investment -- represented the full transformation Musk had warned against.\n\nNovember 2024: The Case Survives\n\nU.S. District Judge Yvonne Gonzalez Rogers rejected OpenAI and Microsoft's third dismissal attempt. She pointed to \"ample evidence supporting Musk's claims\" and internal communications showing leadership \"saying one thing publicly and planning something completely different privately.\" The case would go to trial.\n\nFebruary 2025: The $97.4 Billion Bid\n\nMusk led a consortium offering $97.4 billion to buy OpenAI's nonprofit. Altman rejected it publicly within an hour: \"[N]o thank you but we will buy twitter for $9.74 billion if you want\" -- implying X was worth a fraction of the $44 billion Musk paid in 2022.\n\nMusk responded with one word: \"swindler.\"\n\nThe exchange got personal. Altman called Musk \"not a happy person\" operating from \"insecurity.\" In April, Musk said powerful AI shouldn't be controlled by someone \"not trustworthy\" like Altman.\n\nAugust 2025: App store rankings dispute\n\nOn August 11, Musk claimed Apple was manipulating App Store rankings to favour OpenAI. Altman shot back: \"This is a remarkable claim given what I have heard alleged that Elon does to manipulate X to benefit himself and his own companies and harm his competitors and people he doesn't like.\"\n\nMusk replied: \"You got 3M views on your bullshit post, you liar, far more than I've received on many of mine, despite me having 50 times your follower count!\"\n\nAltman challenged: \"Will you sign an affidavit that you have never directed changes to the X algorithm in a way that has hurt your competitors or helped your own companies? i will apologize if so.\"\n\nBy September, Altman said in interviews he once admired Musk but now views him differently. In November, they fought over a Tesla Roadster refund, with Musk accusing Altman of \"stealing a nonprofit.\"\n\nJanuary 2026: Trial Set, Stakes Clear\n\nOn January 8, Judge Rogers scheduled trial for March 30, 2026. \"Part of this is about whether a jury believes the people who will testify and whether they are credible,\" she said in court.\n\nMusk is seeking $79 billion to $134 billion in damages. His expert witness calculated that OpenAI earned $65.5 billion to $109.4 billion in wrongful gains while Microsoft gained $13.3 billion to $25.1 billion from their partnership. The logic: just as early startup investors realize gains far exceeding initial investments, Musk's $38 million plus reputation, network, and strategic guidance entitled him to substantial returns from a company now valued at $500 billion.\n\nOpenAI warned investors to expect \"deliberately outlandish, attention-grabbing claims\" from Musk. \"We believe this case is worth no more than the $38M that Elon donated -- though that is not a guarantee,\" the company wrote.\n\nOn January 17, Musk posted he was eager for trial. \"Can't wait to start the trial. The discovery and testimony will blow your mind.\"\n\nJanuary 20: The Safety War\n\nMonday night's exchange brought safety concerns front and centre.\n\nMusk quoted a post alleging ChatGPT had been linked to nine deaths and warned against using it.\n\n\"Don't let your loved ones use ChatGPT,\" Musk posted.\n\nSeven families filed wrongful death lawsuits against OpenAI in November 2025 -- four addressing ChatGPT's alleged role in suicides, three claiming it reinforced harmful delusions. Internal estimates show 1.2 million of ChatGPT's 800 million weekly users discuss suicide with the chatbot each week.\n\nAltman defended OpenAI's responsibility while pivoting to Autopilot. Tesla's system has been linked to several crashes, including over two dozen fatal incidents, according to U.S. National Highway Traffic Safety Administration investigations. Many involved drivers who believed Autopilot was fully autonomous.\n\n\"Sometimes you complain about ChatGPT being too restrictive, and then in cases like this you claim it's too relaxed. Almost a billion people use it and some of them may be in very fragile mental states. We will continue to do our best to get this right and we feel huge responsibility to do the best we can, but these are tragic and complicated situations that deserve to be treated with respect,\" Altman said.\n\nAltman also referenced Grok, Musk's xAI chatbot, which faces regulatory scrutiny in Europe, India, and Malaysia after studies found nearly three-quarters of prompts for nonconsensual sexualized images went to Grok.\n\n\"Apparently more than 50 people have died from crashes related to Autopilot. I only ever rode in a car using it once, some time ago, but my first thought was that it was far from a safe thing for Tesla to have released. I won't even start on some of the Grok decisions,\" he posted.\n\nThe March 30 trial will force both to defend their safety records publicly."
  },
  {
    "source": "Trending Topics",
    "company": "OpenAI",
    "title": "Ziemlich beste Feinde: Wir analysieren den Fight Elon Musk vs. Sam Altman",
    "date": "2026-01-22T07:46:57Z",
    "url": "https://www.trendingtopics.eu/ziemlich-beste-feinde-wir-analysieren-den-fight-elon-musk-vs-sam-altman/",
    "content": "Aus Datenschutz-Gründen ist dieser Inhalt ausgeblendet. Die Einbettung von externen Inhalten kann in den Datenschutz-Einstellungen aktiviert werden:\n\nAb 27. April 2026 stehen sich der reichste Mann der Welt und das wertvollste KI-Startup vor Gericht gegenüber. Elon Musk fordert von OpenAI und CEO Sam Altman Schadensersatz in Höhe von bis zu 134 Milliarden Dollar. Der Vorwurf: Betrug durch Aufgabe der ursprünglichen gemeinnützigen Mission. Was als altruistisches Forschungsprojekt begann, entwickelte sich zu einem der wertvollsten Tech-Unternehmen der Welt (geplanter Börsengang mit über 1 Billion Dollar Bewertung).\n\nHier sind die zehn entscheidenden Knackpunkte im Rechtsstreit - analysiert von den AI Talk Hosts ⁠⁠⁠Jakob Steinschaden⁠⁠⁠ (Trending Topics, ⁠⁠⁠newsrooms⁠⁠⁠) und ⁠⁠⁠Clemens Wasner⁠⁠⁠ (⁠⁠⁠enliteAI⁠⁠⁠, ⁠⁠⁠AI Austria⁠⁠⁠).\n\nDer Kernvorwurf von Musks Klage lautet, dass OpenAI systematisch von seiner gemeinnützigen Mission abgewichen ist, sobald Musk das Unternehmen verlassen hatte. Die Klageschrift argumentiert, dass man 2015 gemeinsam angetreten sei, um Googles drohende KI-Monopolstellung zu verhindern und künstliche Intelligenz der gesamten Menschheit zugänglich zu machen. Diese altruistische Gründungsmission habe OpenAI von allen anderen Organisationen unterschieden.\n\nDoch nach Musks Ausstieg 2018 sei die Transformation zu einem gewinnorientierten Unternehmen eingeleitet worden. 2019 wurde eine For-Profit-Tochtergesellschaft gegründet, die zwar noch unter Kontrolle des Non-Profits stand, aber den Grundstein für die spätere kommerzielle Ausrichtung legte. Musk sieht darin ein klassisches Beispiel für das Vorgaukeln falscher Tatsachen: Man habe Spenden unter dem Vorwand eines gemeinnützigen Zwecks eingesammelt, nur um später die Struktur so umzubauen, dass frühe Unterstützer leer ausgehen, während neue Investoren profitieren.\n\nEin besonders brisantes Beweisstück in Musks Klage sind die persönlichen Tagebucheinträge von Greg Brockman, dem langjährigen CTO von OpenAI. Brockman, der zuvor Engineering Lead bei Stripe war und deutlich weniger als Musk in OpenAI investiert hatte, führte offenbar detaillierte Aufzeichnungen über interne Überlegungen. In diesen Notizen findet sich eine bemerkenswert offene Passage über die Pläne zur Kommerzialisierung:\n\n\"Our plan: it would be nice to be making the billions. We've been thinking that maybe we should just flip to a for profit. Making the money for us sounds great and all.\"\n\nDiese Aussage steht in krassem Widerspruch zu den öffentlichen Verlautbarungen von OpenAI, die noch 2024 von \"The Greater Good\" und dem Wohl der gesamten Menschheit sprachen. Die Tagebucheinträge legen nahe, dass die Transformation zum gewinnorientierten Unternehmen keine notwendige Anpassung an veränderte Umstände war, sondern von Anfang an als attraktive Option betrachtet wurde. Für Musks Anwälte ist dies ein klarer Beleg dafür, dass die Non-Profit-Struktur möglicherweise nie ernst gemeint war.\n\nNoch aufschlussreicher ist ein weiterer Eintrag in Brockmans Tagebuch, der zeigt, dass sich die Führung von OpenAI der ethischen Problematik ihrer geplanten Umstrukturierung durchaus bewusst war. Brockman schrieb:\n\n\"It'd be wrong to steal the non-profit from Musk, to convert to a b-corp without him. That'd be pretty morally bankrupt.\"\n\nDieser Eintrag ist deshalb so bedeutsam, weil er belegt, dass die Verantwortlichen intern genau wussten, dass eine Umwandlung in eine gewinnorientierte Struktur ohne Musk moralisch fragwürdig wäre. Interessanterweise ist genau dieser Schritt dann 2024 versucht worden: OpenAI kündigte die Umwandlung in eine Public Benefit Company an, bei der die Non-Profit-Stiftung zunächst komplett leer ausgehen sollte. Erst massiver öffentlicher Druck führte dazu, dass der Non-Profit-Arm doch noch etwa 30 Prozent Anteile am For-Profit-Unternehmen erhielt. Die Tatsache, dass dieser Move jahrelang in internen Dokumenten diskutiert wurde, spricht laut Musk dafür, dass die Umstrukturierung von langer Hand geplant war.\n\nEin weiterer zentraler Vorwurf betrifft das Timing von Sam Altmans Gesprächen mit Microsoft. Laut Klageschrift wandte sich Altman bereits wenige Wochen nach Musks Ausstieg 2018 an Microsoft, um über ein neues \"commercial venture\" zu sprechen. Dies ist insofern problematisch, als Musk zuvor wiederholt gesagt worden sein soll, dass OpenAI niemals kommerziell werden würde.\n\nDie Chronologie legt nahe, dass Altman möglicherweise nur auf Musks Abgang wartete, um die kommerzielle Transformation einzuleiten. Microsoft investierte in den folgenden Jahren insgesamt etwa 13 Milliarden Dollar in OpenAI und erhielt dafür weitreichende Rechte an der Technologie. Für Musk ist dies der Beweis, dass man ihm gegenüber die wahren Absichten verschleiert hatte: Hätte er gewusst, dass OpenAI kurz nach seinem Ausstieg zu einem kommerziellen Unternehmen werden würde, hätte er seine Anteile niemals aufgegeben.\n\nOpenAI kontert in seiner Verteidigungsschrift mit einem interessanten Gegenvorwurf: Musk habe OpenAI von Anfang an nur als Finanzierungsvehikel für seine Mars-Kolonisierungspläne betrachtet. Laut internen Dokumenten, die OpenAI vorgelegt hat, forderte Musk im September 2017 die absolute Kontrolle über ein mögliches For-Profit-Unternehmen. Seine Begründung: Er benötige das Geld für sein Ziel, eine sich selbst erhaltende Stadt auf dem Mars zu errichten, wofür er etwa 80 Milliarden Dollar veranschlagte. Die anderen Gründer lehnten dies ab, weil sie nicht wollten, dass OpenAI zu einem Tochterunternehmen von Musks Mars-Projekt wird. OpenAI argumentiert, dass Musk selbst derjenige war, der OpenAI kommerzialisieren wollte, allerdings unter seiner alleinigen Kontrolle. Als dies scheiterte, habe er das Projekt verlassen und versuche nun nachträglich, sich seinen Anteil zu sichern.\n\nEin weiterer Streitpunkt betrifft Musks Plan, OpenAI in Tesla zu integrieren. Laut OpenAIs Darstellung wollte Musk die KI-Technologie nutzen, um Teslas Ambitionen im Bereich autonomes Fahren und Robotik voranzutreiben. Die Vision: OpenAI würde das \"Gehirn\" nicht nur für selbstfahrende Autos liefern, sondern auch für Teslas Roboter-Projekt Optimus. Sam Altman und die anderen Gründer lehnten diesen Plan ab, weil sie OpenAI als eigenständiges Unternehmen erhalten wollten, nicht als Zulieferer für Tesla. Diese Weigerung könnte ein entscheidender Grund für Musks Ausstieg gewesen sein. OpenAI wirft Musk vor, dass er nun versuche, durch die Klage das zu erreichen, was ihm damals verwehrt wurde: Kontrolle über die KI-Technologie für seine eigenen kommerziellen Zwecke, insbesondere für sein konkurrierendes KI-Startup xAI.\n\nOpenAI stützt seine Verteidigung auch auf E-Mails, die Musk 2018 verschickte, kurz nachdem er OpenAI verlassen hatte. In diesen Nachrichten äußerte er sich äußerst skeptisch über die Erfolgsaussichten des Unternehmens. Wörtlich schrieb er, die Erfolgswahrscheinlichkeit von OpenAI liege bei null Prozent, nicht einmal bei einem Prozent. Er würde zwar wünschen, dass das Projekt erfolgreich werde, glaube aber nicht daran.\n\nDiese Aussagen sind deshalb relevant, weil sie Musks heutige Position untergraben: Wie kann jemand, der öffentlich erklärt hat, nicht an den Erfolg zu glauben und deshalb nicht weiter investieren zu wollen, Jahre später behaupten, er sei um seinen rechtmäßigen Anteil betrogen worden? OpenAI argumentiert, dass Musk freiwillig ausgestiegen sei, weil er das Projekt für aussichtslos hielt. Erst als OpenAI mit ChatGPT zum wertvollsten KI-Startup der Welt wurde, habe er seine Meinung geändert und versuche nun, nachträglich Ansprüche geltend zu machen.\n\nOpenAI wirft Musk vor, dass die aktuelle Klage bereits der vierte Versuch sei, das Unternehmen zu torpedieren. Die Klage wurde zunächst in Kalifornien eingereicht, dann zurückgezogen und im August 2024 auf Bundesebene erneut eingereicht. OpenAI sieht darin ein Muster: Musk wolle das Unternehmen systematisch ausbremsen, um sein eigenes KI-Startup xAI zu begünstigen. xAI konkurriert zwar nicht direkt mit OpenAI (xAI fokussiert auf Robotik und autonomes Fahren, OpenAI auf Sprachmodelle), aber beide Unternehmen buhlen um dieselben Talente, Rechenressourcen und Investoren. Der Zeitpunkt der Klage, kurz vor dem geplanten Börsengang von OpenAI, erscheint strategisch gewählt: Ein langwieriger Rechtsstreit könnte Investoren abschrecken und die Bewertung drücken. Gleichzeitig würde ein Erfolg der Klage Musk einen erheblichen Anteil an einem Unternehmen sichern, das er selbst für gescheitert erklärt hatte.\n\nIm Zentrum des rechtlichen Streits steht die ungewöhnliche Unternehmensstruktur von OpenAI. 2019 wurde eine For-Profit-Tochtergesellschaft gegründet, die unter Kontrolle des Non-Profits stand. 2023 kam es zum \"Blip\", als Sam Altman für drei Tage gefeuert wurde, weil das Aufsichtsgremium (hauptsächlich mit Non-Profit-Vertretern besetzt) ihm Täuschung vorwarf. Ende 2024 kündigte OpenAI an, eine Public Benefit Company zu werden, wobei der Non-Profit-Arm zunächst leer ausgehen sollte.\n\nNach öffentlichem Druck wurde die Struktur 2025 so angepasst, dass die Non-Profit-Stiftung etwa 30 Prozent am For-Profit-Unternehmen hält. Die rechtliche Frage lautet: Kann ein Unternehmen, das als gemeinnützige Organisation Spenden eingesammelt hat, einfach in eine gewinnorientierte Struktur umgewandelt werden, ohne dass frühe Unterstützer angemessen entschädigt werden? In den USA ist dies grundsätzlich nicht zulässig, weshalb Musks Klage durchaus Erfolgsaussichten hat.\n\nUnabhängig vom Ausgang des Prozesses hat der Rechtsstreit bereits erheblichen Schaden angerichtet. OpenAI hat in der KI-Community massiv an Vertrauen verloren. Während das Unternehmen früher als Vorreiter für verantwortungsvolle KI-Entwicklung galt, arbeiten die meisten ernsthaften Forscher in diesem Bereich heute bei Anthropic oder Google DeepMind. OpenAI fokussiert sich fast ausschließlich auf kommerzielle Anwendungen, während echte Grundlagenforschung (etwa in Drug Discovery oder Alignment Research) anderswo stattfindet.\n\nPrediction Markets in den USA geben Musk eine 60-prozentige Chance, den Prozess zu gewinnen, wobei die tatsächliche Schadensersatzsumme weit unter den geforderten 134 Milliarden Dollar liegen dürfte. Beobachter vergleichen OpenAIs Entwicklung mit Meta nach dem Cambridge-Analytica-Skandal: ein rasanter Reputationsverlust, von dem sich das Unternehmen möglicherweise nie vollständig erholen wird. Die großen Gewinner des Streits sind vermutlich Google, Anthropic und andere Wettbewerber, die in Ruhe Marktanteile gewinnen können, während sich OpenAI und Musk vor Gericht zerfleischen."
  },
  {
    "source": "WinBuzzer",
    "company": "OpenAI",
    "title": "OpenAI-Nvidia $100B Deal is Now 'On Ice'",
    "date": "2026-02-01T23:43:07Z",
    "url": "https://winbuzzer.com/2026/02/02/openai-nvidia-100b-deal-on-ice-xcxwbn/",
    "content": "Market Impact: Nvidia's stock jumped nearly four percent when the deal was announced despite the agreement carrying no assurance of proceeding.\n\nThe $100 billion partnership between OpenAI and Nvidia announced in September is now 'on ice,' with both companies rethinking the deal's future, The Wall Street Journal reported Friday.\n\nAccording to people familiar with the matter, the two sides are reconsidering their collaboration. Latest discussions focus on an equity investment of tens of billions of dollars as part of OpenAI's current funding round.\n\nLatest discussions include an equity investment of tens of billions of dollars as part of OpenAI's current funding round, marking a substantial reduction from the original commitment. Talks about some form of deal between the companies continue, though the scope has changed considerably.\n\nAn OpenAI spokesperson said the teams are \"actively working through details\" of the partnership.Nvidia reaffirmed its decade-long partnership with OpenAI in a statement, emphasizing continued collaboration between the two companies.\n\nThe shift from the infrastructure commitment to tens of billions in equity investment indicates both companies are reassessing the optimal structure for their partnership.\n\nThis repositioning suggests Nvidia may prefer direct ownership stakes over progressive infrastructure financing, reflecting Nvidia's earlier interest in investing in OpenAI. Meanwhile, OpenAI gains flexibility to diversify its hardware partnerships without committing exclusively to Nvidia's ecosystem.\n\nTo understand how dramatically the deal has changed, it's worth revisiting the original announcement. When announced in September, the deal represented one of the tech industry's largest infrastructure projects.\n\nNvidia pledged up to $100 billion in funding for OpenAI, with the companies signing a letter of intent for a strategic partnership to deploy a minimum of 10 gigawatts of NVIDIA systems for OpenAI's AI infrastructure expansion.\n\nNvidia's investment would go directly toward building data center and power capacity through a progressive structure tied to OpenAI's deployment of each gigawatt of new AI infrastructure.\n\nGreg Brockman, OpenAI's cofounder and president, expressed enthusiasm for the collaboration at the time.\n\n\"We've been working closely with NVIDIA since the early days of OpenAI. We're excited to deploy 10 gigawatts of compute with NVIDIA to push back the frontier of intelligence and scale the benefits of this technology to everyone.\"\n\nJensen Huang described the 10-gigawatt deployment as \"the next leap forward\" for AI infrastructure when announcing the deal. Sam Altman emphasized compute infrastructure as foundational to the future economy in the same announcement.\n\nThe 10-gigawatt commitment far exceeds any previous AI infrastructure project, creating deployment challenges that may have contributed to the deal's current uncertainty. Securing this level of compute capacity would position OpenAI to maintain its lead in foundation model development.\n\nHowever, for Nvidia, the progressive investment structure tied revenue directly to OpenAI's ability to execute on aggressive infrastructure timelines, introducing execution risk into what initially appeared to be guaranteed hardware sales.\n\nUnder the original plan, the first gigawatt of new AI infrastructure was scheduled to come online in the second half of 2026 using Nvidia's Vera Rubin platform. OpenAI would designate Nvidia as its preferred strategic compute and networking partner for its AI factory expansion.\n\nBoth companies committed to co-optimizing hardware and software development roadmaps, aligning OpenAI's model and infrastructure software innovations with Nvidia's GPU and networking products.\n\nDespite the ambitious public announcement, cracks in the foundation appeared almost immediately.\n\nNvidia CEO Jensen Huang told industry partners that the original agreement was non-binding, and the deal faced skepticism from the start. Nvidia CFO Colette Kress confirmed in December that no final agreement had been reached. Nvidia's $500 billion revenue forecast didn't include any potential revenue from the OpenAI deal.\n\nOpenAI's infrastructure needs were substantial. The company had more than 700 million weekly active users as of September 2025, driving considerable compute requirements.\n\nThe non-binding nature of the agreement signaled from the outset that Nvidia maintained flexibility to reassess the partnership. Nvidia's exclusion of the deal from its $500 billion revenue forecast indicates the company did not treat the commitment as a firm financial obligation, despite the public announcement's scale. This conservative accounting approach proved prescient as the deal entered its current uncertain phase.\n\nBeyond the structural warning signs, external observers raised red flags about the deal's unusual financing arrangement. Jensen Huang called the deal 'the largest computing project in history' when it was announced, but concerns about its structure emerged quickly. Nvidia would invest in $10 billion tranches as each gigawatt came online, with OpenAI using the cash to buy Nvidia's chips, creating a self-reinforcing deal structure.\n\nStacy Rasgon of Bernstein noted the action would \"clearly fuel 'circular' concerns\" when the deal was first announced. Jay Goldberg of Seaport Global Securities characterized Nvidia's investment strategy as \"bubble-like behavior.\"\n\nHuang dismissed the circular financing criticism as \"completely ridiculous\" regarding Nvidia's CoreWeave investment.\n\nBehind the scenes, tensions were building. Jensen Huang privately criticized OpenAI's business approach as showing a \"lack of discipline.\" Nvidia believes that OpenAI's competitors, notably Anthropic and Google, are becoming much more competitive, potentially reducing OpenAI's strategic value as a partner.\n\nThe circular financing structure created a scenario where Nvidia's investment would immediately return as hardware revenue, raising questions about whether the deal represented genuine capital deployment or a mechanism to inflate both companies' valuations.\n\nHuang's private criticism of OpenAI's discipline suggests Nvidia's concerns extend beyond deal structure to fundamental questions about OpenAI's execution capability and competitive positioning against rivals that may offer more attractive partnership terms.\n\nWhile the Nvidia partnership falters, OpenAI is pursuing alternative funding sources to support its ambitious expansion plans. The company is now seeking funding at an $830 billion valuation, though the source of that capital remains uncertain with the Nvidia deal stalled.\n\nAmazon is in talks to invest up to $50 billion in OpenAI, while Microsoft remains OpenAI's largest backer with approximately $13 billion invested and a 49% stake.\n\nMeanwhile, Nvidia's partnership strategy extends beyond OpenAI. Prior to the OpenAI announcement, the company pledged a $5 billion investment into Intel. The OpenAI partnership builds on ongoing alliances with Microsoft, Oracle, SoftBank, and Stargate partners.\n\nOpenAI's pursuit of Amazon investment while the Nvidia deal stalls indicates the company is diversifying its infrastructure funding sources rather than depending on a single large-scale commitment. This strategy positions OpenAI to reduce dependence on any single supplier.\n\nFor Nvidia, maintaining partnerships across multiple AI leaders creates competitive advantages by ensuring the company remains the preferred chip provider regardless of which foundation model company gains market dominance.\n\nNvidia disclosed in a prior 10-Q filing that its deal with OpenAI is a \"non-binding\" agreement and that there is no assurance it will proceed. In September 2025, the companies announced a letter of intent to build a minimum of 10 gigawatts of computing power, roughly equivalent to the output of ten typical nuclear power plants.\n\nUnder the plan, Nvidia would invest the full amount to help finance the project, while OpenAI would lease the chips. Nvidia's share price jumped nearly four percent on the news of the September announcement.\n\nThe 4% share price jump following the September announcement demonstrates how ambitious AI infrastructure deals can move markets, even when the underlying agreements lack binding commitments. Despite this market enthusiasm, the disclosure in Nvidia's 10-Q filing that the deal carried no assurance of proceeding reveals a gap between public presentation and contractual reality that investors may not have fully appreciated at announcement.\n\nAs OpenAI plans to go public in late 2026, the uncertainty around the Nvidia deal raises questions about the company's ability to secure infrastructure funding necessary for supporting its 700 million weekly active user base and AGI development ambitions. Whether that funding comes from Nvidia or alternative sources remains uncertain."
  },
  {
    "source": "Techreport",
    "company": "OpenAI",
    "title": "ChatGPT and Advertising: Concern Over Regulation & Governance",
    "date": "2026-01-28T15:33:41Z",
    "url": "https://techreport.com/news/chatgpts-move-into-advertising-raises-concerns-about-ai-regulation-and-data-governance/",
    "content": "New rules needed: Governments may need to adapt or create policies to govern transparency, user protection, and accountability for conversational AI advertising.\n\nIt seems like OpenAI's CEO, Sam Altman, has a new favorite sport: backpedaling. At an event at Harvard University in May 2024, he said that using advertising in ChatGPT would be a 'last resort' and that 'ads plus AI is sort of uniquely unsettling.'\n\nWell, things have since taken a rather drastic turn. On Friday, January 16, OpenAI announced that it's testing impression-based advertising in ChatGPT with US users, and that this new feature will roll out to the free and Go tiers of the platform around February. This all points to a shift in how companies are monetizing generative AI, and it definitely raises concerns around transparency, data governance, and regulatory compliance.\n\nAnd with any change comes questions:\n\n* What does this mean for me as an everyday ChatGPT user?\n\n* Is the large language model (LLM) going to use our data to serve personalized ads?\n\n* How will we know what is an ad and what isn't?\n\nLet's unpack what ChatGPT ads mean for users wanting to protect their sensitive information and lawmakers who may now need to adjust policies and governance around conversational AI.\n\nHow ChatGPT Advertising Will Actually Work\n\nBefore getting into regulation and data governance, it's good to understand what OpenAI is rolling out and, importantly, what it isn't. For now, OpenAI says it's testing ads with a limited pool of US advertisers, each committing less than $1M.\n\nSource: OpenAI\n\nThis isn't a full-scale marketplace, but a controlled pilot that OpenAI designed to see how advertising fits into a conversational interface. Unlike performance-based ads, OpenAI is charging advertisers on a pay-per-impression basis, which means advertisers pay when ChatGPT shows an ad, not when a user clicks or buys anything.\n\nFrom OpenAI's perspective, this guarantees them revenue even if users ignore ads completely, which is a safer bet than experimenting with a brand-new ad format.\n\nAds will appear at the bottom of the ChatGPT interface, not within the answers themselves. And OpenAI will clearly label them and keep them visually separate from conversations.\n\nSource: OpenAI\n\nAlthough OpenAI is designing ChatGPT ads to display below conversations (at least for now), even that separation introduces new questions regulators may not have expected to answer. In its announcement, the company stated that 'Ads do not influence the answers ChatGPT gives you.' And it said that 'Answers are optimized based on what's most helpful to you. Ads are always separate and clearly labeled.'\n\nAds won't be shown to Plus, Pro, or Enterprise users, which basically indicates that OpenAI is using this as a way to subsidize free and low-cost access to ChatGPT.\n\nWhy OpenAI Is Turning To Ads Now\n\nAds have popped up in numerous places for years, streaming services, social media, and web browsing being just a few, but why has OpenAI trialed this shift now?\n\n* OpenAI says advertising helps keep ChatGPT accessible for free and low-cost users.\n\n* Subscription revenue isn't enough to cover the cost of running and scaling large AI models.\n\n* The company reportedly lost around $8B to operational costs in the first half of 2025, with only 5% of users paying.\n\n* The massive spending commitments on data centers and chips are driving up costs for the company.\n\nOpenAI has framed advertising as a way to keep ChatGPT widely accessible. In its announcement, the company said the move would allow 'more people to benefit from our tools with fewer usage limits or without having to pay.'\n\nOkay, so that explanation may make a bit of sense, but it's also incomplete.\n\nAccording to reporting by the Financial Times, OpenAI lost around $8B in the first six months of 2025 in operating expenses. Even though they have roughly 800M users, only about 5%, around 40M, are paid subscribers. At the same time, OpenAI now has about $14T in spending commitments tied to data centers, chips, and other infrastructure to scale its models.\n\nIt's fair to say that running one of the world's most widely used AI systems is expensive, and subscription revenue alone clearly isn't covering the bill. So, this means advertising looks less like a philosophical compromise and more like a financial necessity.\n\nSpeaking shortly after the announcement at the World Economic Forum (WEF) in Davos, OpenAI CFO Sarah Friar defended this move by framing it as an access issue rather than a profit grab. 'Our mission is artificial general intelligence for the benefit of humanity,' she said, 'not for the benefit of humanity who can pay.'\n\nIn this framing, it seems the company is using advertising to fund its growth without locking advanced AI behind a paywall, even if it complicates trust, privacy, and governance in the process.\n\nAdvertising Inside Conversations Changes The Data Equation\n\nLet's face it, advertising on websites and social media platforms is nothing new. Advertising inside a conversational AI system is. The worrying thing about this development is that ChatGPT is often responding to prompts that contain personal, emotional, or sensitive information.\n\nRemember that trend when everyone was showing off how they were using ChatGPT as a therapist? That's the concern here: a data governance issue.\n\nSource: Reddit\n\nOpenAI says ad targeting will be contextual, which means the topic of conversation will trigger ads rather than personal data. For example, someone researching a holiday destination might see travel-related ads, while a user asking about productivity might see a sponsored service.\n\nContextual targeting may be privacy-friendly, but conversational context is far richer than a search query or article headline. This is because prompts inside ChatGPT can reveal intent, uncertainty, and vulnerability in ways that traditional advertising platforms don't see.\n\nOpenAI has firmly stated that it will not share user conversations with advertisers. But even if they don't do this, questions remain about what data OpenAI processes internally to decide which ads show up, how long that data is retained, and who has access to it.\n\nSource: OpenAI\n\nConversational AI doesn't know what users are interested in, like traditional digital advertising, but it often knows why. This makes advertising decisions far more sensitive and potentially powerful. So far, OpenAI hasn't fully explained what information it will use to determine relevance, which is a big gap in transparency.\n\nSafeguards, Controls, And OpenAI's Promises\n\nThere are a couple of safeguards OpenAI is putting into place. But are they enough? OpenAI won't show ads to accounts where the user is under 18, and it will exclude ads from appearing near sensitive topics like health and mental health.\n\nChatGPT determines if a user is under 18 either because the user has said so or because the system predicts it.\n\nSource: OpenAI on X\n\nUsers will also be able to see why OpenAI is showing them an ad, dismiss it, and provide feedback. These features may seem a little like what we already see on social media platforms. But the difference is emotional context.\n\nSeeing an ad next to a Facebook post is one thing. Seeing one after asking AI for advice about your career, finances, or well-being is quite another. OpenAI insists that ChatGPT's responses will always be 'driven by what's objectively useful, never by advertising.'\n\nWhether users continue to believe that as ads become more familiar is an open question.\n\nWhy Existing Regulations Don't Quite Fit\n\nLet's dig into the regulatory perspective on all of this. Right now, ChatGPT ads are in a grey zone. Lawmakers originally wrote most advertising and digital governance frameworks with feeds, timelines, and websites in mind, not conversational systems that respond in natural language and build ongoing context with users.\n\n* The EU's Digital Services Act (DSA) focuses a lot on advertising transparency and disclosure. While ChatGPT's ads are labeled, the DSA doesn't fully account for how conversational interfaces shape trust and perception.\n\nSource: European Commission\n\n* The Digital Markets Act (DMA) is all about gatekeeping power and competition. If conversational AI becomes a primary interface for accessing information, questions around preferential treatment and commercial influence will become harder to ignore.\n\n* The EU AI Act is another regulation lawmakers may need to re-examine. The law requires platforms to clearly identify AI-generated content and advertising to prevent manipulation.\n\nOpenAI somewhat aligns with these requirements, but it's likely regulators will still need to consider whether disclosure alone is enough when ads appear inside individual and highly personalized dialogue dialogue rather than alongside content made for wide audiences.\n\nThe Risk of What Comes Next\n\nThere's an even bigger concern to consider here: how these ads may evolve.\n\nRight now, OpenAI is saying there'll be strict separation, contextual targeting only, and user control. But once their advertising turns into a serious income stream, the platform may shift to looser protocols.\n\nThe internal use of data to optimize the relevance of ads and measure their effectiveness could expand, even if OpenAI never sells the data externally.\n\nA Regulatory Reckoning For Conversational AI\n\nAdvertising inside ChatGPT may help fund the next phase of AI development, but it's also going to force regulators to confront what could be an entirely new reality. Governments may need to rethink how transparency, consent, and accountability will work when ads appear inside conversations rather than beside content.\n\nThat could mean new policies specifically drawn up for conversational systems, or significant changes to existing frameworks. What's clear right now is that advertising has pushed generative AI into regulatory territory that no one has fully mapped yet.\n\nWhether OpenAI's promises hold and whether regulators move quickly enough to keep up will shape how commercialized and trusted conversational AI becomes.\n\nWhat Happens Next for ChatGPT Ads And AI Regulation?\n\nFor now, OpenAI's advertising experiment is relatively small, and they're certainly wrapping it in assurances. Ads are limited, clearly labeled, and framed as a way to keep ChatGPT accessible rather than to squeeze users for data.\n\nBut that careful positioning may not last forever, especially if ads become essential to funding AI infrastructure at scale. Regulators are likely to watch this rollout closely, as conversational AI advertising sits a little awkwardly between existing frameworks, and that will likely trigger policy reviews, clarifications, or entirely new rules.\n\nWhy? Because lawmakers didn't build current advertising rules for systems that talk. Regulators now need to decide how advertising law applies when the interface itself feels human.\n\nIf ads inside AI conversations become the norm, lawmakers will need to move fast. Otherwise, the rules governing AI monetization may be one step behind the tech that's about to reshape how people ask, learn, and decide."
  },
  {
    "source": "k.sina.com.cn",
    "company": "OpenAI",
    "title": "OpenAI与微软关系紧张，八个月谈判陷入僵局",
    "date": "2026-01-27T14:44:05Z",
    "url": "https://k.sina.com.cn/article_7879848900_1d5acf3c401902p1jy.html",
    "content": "划重点：\n\n* 围绕AI合作关系的未来，OpenAI与微软之间的紧张关系持续升温。\n\n* 据知情人士透露，OpenAI高管已考虑指控微软存在垄断行为。\n\n* 双方目前在OpenAI转型为公共利益公司后微软的持股比例上存在分歧。\n\n6月17日消息，据知情人士透露，OpenAI正在推进公司重组计划，并就相关协议条款与其最大外部股东微软进行重新谈判。然而，双方谈判已持续八个月，仍未取得实质进展，且过程中不断出现新的分歧，使得OpenAI内部对微软的不满情绪加剧，甚至已开始讨论是否向监管机构提起反垄断投诉。\n\n股权谈判卡壳，OpenAI重组与融资双重受阻\n\n根据现有协议，OpenAI重组旗下营利性部门的计划需获得微软批准。OpenAI希望微软在重组后持有盈利部门约33%的股权，并以此为交换，放弃对未来利润的分成权。但微软则要求更高比例的股权，作为其继续支持OpenAI的条件。微软此前承诺向OpenAI投入超过130亿美元资金，并通过其持股安排，最多可获得约1200亿美元的未来利润分成。\n\n重组的时限也给OpenAI带来巨大压力。根据与软银的协议，OpenAI必须在年底前完成重组，否则将面临损失约200亿美元融资的风险。该笔融资计划总额为400亿美元，分两阶段实施：第一阶段100亿美元已到账，第二阶段300亿美元预计于12月完成。其中软银计划出资约225亿美元，其余75亿美元需引入新投资者。但如果重组失败，软银可能将投资总额缩减至200亿美元。\n\n谈判僵局愈发激化。据悉，OpenAI高层近期已讨论一项被视为\"极端手段\"（nuclear option）的备选方案：指控微软在合作期间存在垄断行为，并寻求联邦监管机构对双方合同条款进行审查，以判断是否违反反垄断法，甚至可能配合发起一场公关攻势。事实上，美国联邦贸易委员会（FTC）早在2024年就对微软展开了广泛的反垄断调查，并曾审查其对OpenAI的投资及其他大型科技公司在AI领域的资本布局。\n\n如果OpenAI最终选择对微软发起反垄断指控，这将对双方长达六年的合作关系造成巨大冲击。这一合作长期以来被视为科技史上最成功的合作之一，微软在OpenAI早期提供了关键技术资源和平台支持。然而，随着双方在AI基础模型领域逐渐转为竞争关系，原本的合作框架也面临重构的压力，谈判破裂的风险正在不断上升。\n\n云托管、知识产权与AGI：深层裂痕全面爆发\n\n在与微软的谈判中，OpenAI还试图修改一项关键条款：现有协议赋予微软在其Azure云平台上独家托管OpenAI模型的权利。OpenAI希望打破这一限制，与其他云服务提供商合作，以拓展客户基础并获取更多算力资源。知情人士称，OpenAI正积极寻求引入谷歌云服务，以满足其日益增长的计算需求。这一转向不仅反映了OpenAI对多元化算力的迫切需求，也标志着其算力策略的一次重大调整。\n\n与此同时，OpenAI还计划以约30亿美元收购AI代码生成初创公司Windsurf，并希望这笔交易能够豁免于现有与微软签订的\"知识产权可访问\"协议条款。OpenAI担心，一旦Windsurf纳入该协议，其与微软旗下的GitHub Copilot存在的直接竞争关系，可能暴露出技术敏感性并限制其战略部署。\n\n根据当前协议，微软不仅拥有独家通过Azure销售OpenAI软件产品的权利，还对OpenAI的核心技术享有优先访问权，并被指定为唯一算力提供商。尽管去年微软曾放宽这一限制，允许OpenAI启动其主导的数据中心项目\"星际之门\"（Stargate），但其对OpenAI关键资源的掌控仍具有决定性影响。\n\n据接近谈判的人士透露，微软尚未接受OpenAI提出的修改条款，并希望从中换取其他重大让步，例如延长其对OpenAI知识产权的使用权期限。目前协议规定，该使用权将持续至2030年。\n\n双方的矛盾也与竞争态势加剧密切相关。如今，OpenAI与微软已不再是单一的合作关系，在从消费者聊天机器人到企业AI平台等多个核心产品线中，双方均已展开正面竞争。2023年，微软CEO萨提亚·纳德拉聘请了DeepMind联合创始人穆斯塔法·苏莱曼（Mustafa Suleyman），秘密启动了微软自有的大模型研发项目，进一步加剧了OpenAI的警觉。\n\n更深层的分歧，则围绕\"通用人工智能\"（AGI）的定义与触发机制展开。根据协议，一旦OpenAI宣布其模型达到AGI水平，微软与OpenAI的现有合作条款将自动终止。因此，微软希望即使OpenAI达成AGI，仍能继续获得相关技术授权。问题在于，AGI在业内尚无统一标准：部分科技高管认为AGI已迫近现实，另一些则坚持认为它仍遥不可及，甚至可能永远无法实现。他们更倾向于将AI视为一个长期、渐进演化的过程，而非突然跃迁的技术奇点。\n\n这一切使得OpenAI与微软的合作基础正在迅速动摇。从资源绑定、市场分工到技术路线的根本分歧，双方原本被视为业界标杆的伙伴关系，正处在复杂且充满变数的重塑过程之中。\n\n本文来自\"腾讯科技\"，作者：无忌，36氪经授权发布。"
  },
  {
    "source": "k.sina.com.cn",
    "company": "OpenAI",
    "title": "烧6000亿算力，冲2800亿营收！OpenAI豪赌2030",
    "date": "2026-02-21T09:36:18Z",
    "url": "https://k.sina.com.cn/article_5952915720_162d2490806703clce.html",
    "content": "【新智元导读】刚刚，OpenAI 2030路线图曝出，奥特曼要用6000亿美元总算力支出，赌2800亿美元年营收。超9亿日活用户、8500亿美元估值，史上最疯狂的科技豪赌，底牌已经掀开。\n\n从1.4万亿美元到6000亿美元，OpenAI这头烧钱巨兽，终于开始向投资人的钱包「妥协」了。\n\n2025年11月，OpenAI CEO奥特曼曾公开表示，承诺在未来8年内投资约1.4万亿美元用于AI基础设施建设，主要聚焦于数据中心和计算资源扩展。\n\n我们预计今年的年化经常性收入将超过200亿美元，并在2030年增长至数千亿美元。我们正在规划未来8年约1.4万亿美元的投入承诺。\n\n这项承诺之所以远超以往科技投资规模，一方面是因为OpenAI的收入增长，在2025年创造了131亿美元的收入，并且计划到2030年增长至数千亿美元。\n\n另一方面，是为了满足用户AI需求的爆炸式增长（如模型训练和推理）所带来的庞大资源消耗。\n\n然而，如今这台狂飙突进的烧钱巨兽已会了量入为出。\n\nOpenAI刚刚告诉投资者，计划2030年前将总算力支出控制在约6000亿美元；与此同时，也给出了2030年总收入超过2800亿美元的预期，其中消费者业务和企业业务的贡献几乎各占一半。\n\n知情人士称，这一调整是OpenAI对外界担忧其扩张雄心是否与潜在收入相匹配所做出的回应，旨在保持支出安排与预期收入增长更加直接对应。\n\n千亿融资与「循环融资」质疑\n\n2025年，OpenAI创造了131亿美元的收入，超越了原本100亿美元的内部目标。\n\n与此同时，他们全年消耗了80亿美元现金，也低于此前预计的90亿美元。\n\n然而，这却是一场不断加码、无法回头的豪赌。\n\n为了支撑起2030年2800亿美元的惊人营收目标，以及高达6000亿美元的算力总支出，OpenAI必须疯狂吸金。\n\n于是，一场规模可能超过1000亿美元的巨额融资正在暗流涌动。\n\n知情人士称，OpenAI正在敲定一轮规模可能超过1000亿美元的巨额融资，其中约90%的资金将来自战略投资者：\n\n英伟达正在讨论豪掷最多300亿美元入局，软银和亚马逊也已拿到入场券，这轮输血极可能将OpenAI的投前估值推高至7300亿美元，包括新资金后总估值可能达8500亿美元，高于最初预期的8300亿美元。\n\n但「循环融资」「供应商融资」的质疑也不绝于耳。\n\n拿了英伟达的钱，转头又去买英伟达的算力卡。如果这条脆弱链条上的某一个环节断裂，会不会引发波及整个OpenAI甚至整个科技生态的连环雪崩？\n\n奥特曼并不完全同意这种质疑，而是展现出一个掌控千亿美金帝国操盘手的理性：\n\n我不同意前半部分的担忧，但同意后半部分。必须有新的收入进入这个系统。公司之间可以相互投资、相互采购，但如果没有新增收入，那才是真正的问题。\n\n在他的逻辑里，公司之间相互投资、相互采购完全不是问题，真正的问题在于，必须有源源不断的新收入血液流入这个系统。\n\n只要新增收入还在高速增长，只要整个生态系统还有真金白银在支撑这些庞大的基础设施建设，这场游戏就能继续。\n\n现在收入增长非常快，不仅是我们，整个生态系统也是如此，大量收入正在流入以支撑这些基础设施建设，所以我并不担心这一点。\n\n但奥特曼也坦言了自己的焦虑，当前供应链极度紧张，如果技术栈中的某一层出现故障，比如无法获得某些关键技术或芯片，确实可能产生连锁反应，而这正是他所担忧的地方。\n\n9亿用户的变现焦虑\n\n面对Google和Anthropic的重兵压境，ChatGPT在去年秋季一度遭遇了增长放缓的瓶颈，OpenAI也被迫进入红色警报（code red），全员死磕ChatGPT的改进。\n\n绝地反击的成效也是显著的。\n\n如今，ChatGPT的每周活跃用户已超过9亿人，相比去年10月的8亿人，实现了惊人的跃升，日活和周活双双重回历史巅峰。\n\n2025年12月ChatGPT的每周活跃用户已超过9亿\n\n其中，印度市场的每周活跃用户达1亿人，成为OpenAI的最大市场之一。\n\nCodex在2月5日活跃用户超过100万\n\n不仅如此，OpenAI编程利器Codex也拿下了超过150万的周活用户，较2026年初增长三倍以上，这给了OpenAI硬刚劲敌Anthropic旗下Claude Code的重要筹码。\n\n在这些光鲜的数据背后，赚钱也成了OpenAI的第一优先级。\n\n因此，OpenAI开始试水AI广告。\n\n但此举却招致用户广泛反弹，主要集中在隐私侵犯、用户体验等负面影响上，认为这是奥特曼违背早期承诺（他曾在2024年表示讨厌广告），是从非盈利理想向纯商业模式的堕落。\n\n研究员Zoë Hitzig甚至在广告测试当天辞职，并撰文警告OpenAI不要重蹈Facebook的覆辙。\n\n除了用户和内部员工的反对，OpenAI的广告计划，还遭到了死对头Anthropic的「狙击」，砸下数千万美元，在超级碗做广告嘲讽OpenAI「把广告带进AI」。\n\n奥特曼立刻在推文中怒斥Anthropic虚伪，指责对方服务高端用户并封杀OpenAI这样的竞争者。\n\n双方的冷战，还从超级碗转移到刚刚举行的新德里AI峰会上，奥特曼与Anthropic CEO Dario Amodei同台而拒绝握手，成了AI圈一大热梗。\n\n迫于重重压力，奥特曼表示，目前OpenAI广告业务仍非常早期，因此非常谨慎，还在探索最合适的广告形式。\n\n他个人比较喜欢一类Instagram风格的科技广告，人们可以通过这些广告发现一些原本不会注意到、但可能真正喜欢的新产品，这也是奥特曼推出ChatGPT广告的一个努力方向。\n\n我们还在测试各种不同的形式，比如广告应该在什么位置、以什么方式出现，主要是这些战术层面的尝试。\n\n奥特曼称，未来计划将广告推广到美国以外的市场，但目前还没有明确的时间表。\n\n除了广告外，奥特曼还率先在印度市场推出了极低成本ChatGPT Go的「Go计划」。\n\n有报道称，ChatGPT的广告将在美国的免费用户和Go版本用户中测试推广。\n\n此外，OpenAI还在加速与印度的塔塔合作推进Stargate算力基础设施的落地。\n\n首阶段将部署100兆瓦（MW）AI数据中心容量，未来有望扩展至1吉瓦（GW）级别。\n\n无论是企业端的跑马圈地，还是下沉市场的低价抢滩，OpenAI的动作都在释放一个强烈的信号，他们渴求每一滴能够滋养这台机器的商业活水。\n\n硬件入口与造芯暗战\n\n除了大模型，OpenAI还在向硬件入口与底层算力渗透。\n\n在硬件方面，他们拉来了苹果前设计教父Jony Ive。\n\nJony Ive\n\n2025年5月，OpenAI以65亿美元收购了Jony Ive的硬件初创公司io Products，进军AI硬件领域。\n\n这一合作被视为OpenAI向无屏、环境式AI平台的重大布局。\n\n目前，OpenAI的硬件团队超过200人，由Jony Ive的LoveFrom设计工作室主导设计，计划中的首款设备是智能音箱，最早将于2027年2月推出。\n\n此外，OpenAI还计划开发智能眼镜、智能灯等产品。\n\n奥特曼清楚，AI的一个强大之处在于它能够处理大量上下文信息。一个越来越了解你的AI，可以为你提供越来越好的产品和服务。\n\n因此，他希望打造一种硬件设备，使其能够在虚拟世界和现实世界中都很好地实现这一点。\n\n在算力底座上，OpenAI不再把身家性命全部押在一家芯片供应商身上，开始学习Google的「TPU加外采」的战略，推出自己的多元化半导体战略：\n\n一边继续大举采购英伟达的芯片，一边与Cerebras等在快速推理领域崭露头角的新兴公司紧密合作；同时，也会开发自己的推理芯片。\n\n我们预计需要一个比最初设想更丰富的解决方案组合。\n\n不仅是巨头之间的肉搏，更有新锐力量频频「掀桌子」，全球大模型产业正处于一场剧烈大洗牌的前夜。\n\nDeepMind创始人Demis Hassabis近期更是直言，中国在模型能力上仅落后几个月。最近几周，中国在视频模型和具身智能领域连续抛出重磅成果。\n\n奥特曼坦言中国科技公司在整个技术栈上的进展非常惊人，速度极快。在某些领域已经逼近了技术的最前沿。\n\n中国科技公司在整个技术栈上，不仅在AI而是在多个领域的进展，都非常惊人。\n\n在这样的大震荡下，外界对OpenAI高达7300亿美元投前估值的拷问从未停止。但奥特曼对这一估值仍非常自信，他认为这由市场决定，他个人觉得「非常合理」。\n\n在这场残酷的洗牌中，盈利被奥特曼战略性地后置了。\n\n他坚信，只要单位经济效益保持健康，OpenAI就应该毫不犹豫地继续加速扩张，而盈利更像是一种自然而然的结果。\n\n我们目前正以极快的速度增长。只要单位经济效益保持健康，我认为就应该继续加速扩张。在我们认为合适的时候，自然会实现盈利。\n\n6000亿美元的算力豪赌，2800亿美元的营收目标，8500亿美元的估值，9亿周活用户......只要供应链不断裂，只要新增收入还持续涌入，OpenAI这台烧钱巨兽就不会停下。\n\n未来，奥特曼的新挑战，不是OpenAI敢不敢烧钱，而是它能不能赚钱。\n\n参考资料：\n\nhttps://www.cnbc.com/2026/02/20/openai-resets-spend-expectations-targets-around-600-billion-by-2030.html"
  },
  {
    "source": "ThePrint",
    "company": "OpenAI",
    "title": "OpenAI has deleted the word 'safely' from its mission - and its new structure is a test for whether AI serves society or shareholders",
    "date": "2026-02-15T03:50:57Z",
    "url": "https://theprint.in/world/openai-has-deleted-the-word-safely-from-its-mission-and-its-new-structure-is-a-test-for-whether-ai-serves-society-or-shareholders/2855258/",
    "content": "As a scholar of nonprofit accountability and the governance of social enterprises, I see the deletion of the word \"safely\" from its mission statement as a significant shift that has largely gone unreported - outside highly specialized outlets.\n\nAnd I believe OpenAI's makeover is a test case for how we, as a society, oversee the work of organisations that have the potential to both provide enormous benefits and do catastrophic harm.\n\nTracing OpenAI's origins -- -- -- -- -- -- -- -- -- - OpenAI, which also makes the Sora video artificial intelligence app, was founded as a nonprofit scientific research lab in 2015. Its original purpose was to benefit society by making its findings public and royalty-free rather than to make money.\n\nTo raise the money that developing its AI models would require, OpenAI, under the leadership of CEO Sam Altman, created a for-profit subsidiary in 2019. Microsoft initially invested USD 1 billion in this venture; by 2024 that sum had topped USD 13 billion.\n\nIn exchange, Microsoft was promised a portion of future profits, capped at 100 times its initial investment. But the software giant didn't get a seat on OpenAI's nonprofit board - meaning it lacked the power to help steer the AI venture it was funding.\n\nA subsequent round of funding in late 2024, which raised USD 6.6 billion from multiple investors, came with a catch: that the funding would become debt unless OpenAI converted to a more traditional for-profit business in which investors could own shares, without any caps on profits, and possibly occupy board seats.\n\nEstablishing a new structure -- -- -- -- -- -- -- -- -- -- - In October 2025, OpenAI reached an agreement with the attorneys general of California and Delaware to become a more traditional for-profit company.\n\nUnder the new arrangement, OpenAI was split into two entities: a nonprofit foundation and a for-profit business.\n\nThe restructured nonprofit, the OpenAI Foundation, owns about one-fourth of the stock in a new for-profit public benefit corporation, the OpenAI Group. Both are headquartered in California but incorporated in Delaware.\n\nA public benefit corporation is a business that must consider interests beyond shareholders, such as those of society and the environment, and it must issue an annual benefit report to its shareholders and the public. However, it is up to the board to decide how to weigh those interests and what to report in terms of the benefits and harms caused by the company.\n\nThe new structure is described in a memorandum of understanding signed in October 2025 by OpenAI and the California attorney general, and endorsed by the Delaware attorney general.\n\nMany business media outlets heralded the move, predicting that it would usher in more investment. Two months later, SoftBank, a Japanese conglomerate, finalised a USD 41 billion investment in OpenAI.\n\nChanging its mission statement -- -- -- -- -- -- -- -- -- -- -- - Most charities must file forms annually with the Internal Revenue Service with details about their missions, activities and financial status to show that they qualify for tax-exempt status. Because the IRS makes the forms public, they have become a way for nonprofits to signal their missions to the world.\n\nIn its forms for 2022, and 2023, OpenAI said its mission was \"to build general-purpose artificial intelligence (AI) that safely benefits humanity, unconstrained by a need to generate financial return.\" That mission statement has changed, as of OpenAI's 990 form for 2024 - which the company filed with the IRS in late 2025. It became \"to ensure that artificial general intelligence benefits all of humanity.\" OpenAI had dropped its commitment to safety from its mission statement - along with a commitment to being \"unconstrained\" by a need to make money for investors. According to Platformer, a tech media outlet, it has also disbanded its \"mission alignment\" team.\n\nIn my view, these changes explicitly signal that OpenAI is making its profits a higher priority than the safety of its products.\n\nTo be sure, OpenAI continues to mention safety when it discusses its mission. \"We view this mission as the most important challenge of our time,\" it states on its website. \"It requires simultaneously advancing AI's capability, safety, and positive impact in the world.\" Revising its legal governance structure -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Nonprofit boards are responsible for key decisions and upholding their organisation's mission.\n\nUnlike private companies, board members of tax-exempt charitable nonprofits cannot personally enrich themselves by taking a share of earnings.\n\nIn cases where a nonprofit owns a for-profit business, as OpenAI did with its previous structure, investors can take a cut of profits - but they typically do not get a seat on the board or have an opportunity to elect board members, because that would be seen as a conflict of interest.\n\nThe OpenAI Foundation now has a 26 per cent stake in OpenAI Group. In effect, that means that the nonprofit board has given up nearly three-quarters of its control over the company. Software giant Microsoft owns a slightly larger stake - 27 per cent of OpenAI's stock - due to its USD 13.8 billion investment in the AI company to date. OpenAI's employees and its other investors own the rest of the shares.\n\nSteps that might help keep people safe -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Several conditions in the OpenAI restructuring memo are designed to promote safety, including: A safety and security committee on the OpenAI Foundation board has the authority to \"require mitigation measures\" that could potentially include the halting of a release of new OpenAI products based on assessments of their risks.\n\nThe for-profit OpenAI Group has its own board, which must consider only OpenAI's mission - rather than financial issues - regarding safety and security issues.\n\nThe OpenAI Foundation's nonprofit board gets to appoint all members of the OpenAI Group's for-profit board.\n\nBut given that neither the mission of the foundation nor of the OpenAI group explicitly alludes to safety, it will be hard to hold their boards accountable for it.\n\nFurthermore, since all but one board member currently serve on both boards, it is hard to see how they might oversee themselves. And the memorandum signed by the California attorney general doesn't indicate whether he was aware of the removal of any reference to safety from the mission statement.\n\nIdentifying other paths OpenAI could have taken -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - There are alternative models that I believe would serve the public interest better than this one.\n\nWhen Health Net, a California nonprofit health maintenance organisation, converted to a for-profit insurance company in 1992, regulators required that 80 per cent of its equity be transferred to another nonprofit health foundation. Unlike with OpenAI, the foundation had majority control after the transformation.\n\nA coalition of California nonprofits has argued that the attorney general should require OpenAI to transfer all of its assets to an independent nonprofit.\n\nAnother example is The Philadelphia Inquirer. The Pennsylvania newspaper became a for-profit public benefit corporation in 2016. It belongs to the Lenfest Institute, a nonprofit.\n\nThis structure allows Philadelphia's biggest newspaper to attract investment without compromising its purpose - journalism serving the needs of its local communities. It's become a model for potentially transforming the local news industry.\n\nAt this point, I believe that the public bears the burden of two governance failures. One is that OpenAI's board has apparently abandoned its mission of safety. And the other is that the attorneys general of California and Delaware have let that happen. (The Conversation) GRS GRS"
  },
  {
    "source": "k.sina.com.cn",
    "company": "Anthropic",
    "title": "Anthropic正取代OpenAI，成为中国AI界的白月光",
    "date": "2026-02-09T02:45:10Z",
    "url": "https://k.sina.com.cn/article_5952915720_162d2490806703ap4q.html",
    "content": "（来源：硅星人）\n\n作者 ｜ 郭海惟\n\n邮箱 ｜ guohaiwei@pingwest.com\n\n直到OpenAI发布GPT3.5的第3年后，人们才好像恍然意识到：\n\nAGI 的 A 其实有可能是Anthropic。\n\n这是我在与某个AI 开发者聊天时临时脑子里冒出来的一个想法。因为他说在过去一周的时间里，他与Anthropic相处的日子已经远远超过自己的老婆孩子和狗（其中狗对此最为不满）。对于他来说，这个世界的一切\"虚拟形态\"都是可以被取代的，Windows和MacOS、iPhone和Android、Steam和Playstation、罗永浩和贾国龙，一切都不重要，只要Anthropic在，日子就可以正常过下去。\n\n而对于这样一个重度 Anthropic 信徒来说，AGI 更真实含义则可能是：\n\nAnthropic Given Intelligence。\n\n只要你身处AI行业，就大概率会感受到这种我想已经可以称之为\"Anthropic Fever\"的东西。它像是另一种形式的\"温水煮青蛙\"，当人们反应过来的时候，才发现Anthropic好像已经占领了自己的工作电脑和微信公众号订阅流。\n\n\"Anthropic Fever\"在中国的AGI世界里蔓延，不仅仅只局限于开发者。我们此前报道过而在那场被一些人称作\"开源四杰\"的圆桌论坛里，被提及最多的公司名字已经不是OpenAI了。\n\n以智谱GLM模型公众号发布的圆桌环节精修实录计算，Anthropic和Claude总计至少被提及了27次，而OpenAI和GPT合计20次、Gemini只有两次。\n\n其中，而身为前OpenAI研究员的姚顺雨，则成为了全场最爱Anthropic的人 -- --\n\n因为他一个人就提了16次。\n\nAnthropic也在看着中国AGI\n\n1\n\n当我们谈论Anthropic的时候，我们到底在谈论什么？\n\n在硅谷的基模四巨头里，Anthropic的业务布局是最简单的，但它背后被赋予的含义却好像是最复杂的。而且正如\"一千个读者有一千个哈姆雷特\"，一万个AI参与者心目中大概也有一万个Anthropic。\n\n2024年7月，Anthropic的头号粉丝、硅谷老牌投资机构Menlo Venture宣布与其合作设立一个总计1亿美元的Anthology Fund，用于支持包括Anthropic生态在内的全球AI初创公司的发展。与OpenAI、谷歌的初创投资基金不同，Anthology Fund虽然有Anthropic的官方支持，但所有筹集资金全部来自于Menlo。\n\n那时Menlo Venture的合伙人Matt Murphy就对媒体表示：\n\nAnthology Fund的灵感来自于iFund。\n\niFund是苹果与风险投资公司 Kleiner Perkins合作成立于的初创基金，用于扶持IOS生态的开发者，同样也是由专业投资机构而非生态发起者主导的基金。只是iFund成立于iPhone发布后的第二年，2008；而Anthology则成立于GPT3.5发布后的第二年，2024。\n\nMatt Murphy想要暗示Anthropic的生态是下一个超越IOS的OS。因为他说iFund与Anthology的最大区别是 -- -- AI的发展远远比iPhone更快。\n\n在2024年，最被认为像苹果的AI公司还是OpenAI。奥特曼想要构建一个\"垂直一体\"的帝国，OpenAI发布了包括GPTS在内的一系列的生态工具，秘密研发自己的芯片，半遮半掩自己的硬件计划，还从苹果挖来了一堆软硬件工程师。\n\n然而在过去的一年时间里，人们发现Menlo可能是对的。因为在AGI语境中，Apple和IOS生态可能真的是两回事情。\n\n根据Menlo的市场数据显示，Anthropic在企业级LLM API市场和Coding市场中都占据了统治性的地位。\n\n其中，在2025年中旬，Anthropic在Coding市场中的占有率高达42%，刚好是OpenAI的两倍。\n\n而在企业级LLM API市场上，Anthropic在2025年底的市占率则达到了40%。而OpenAI则从2023年50%的市占率下滑到了27%，而且趋势似乎并没有逆转的意思。\n\n在两年前，OpenAI好像是不可战胜的。而至少在企业级市场上，OpenAI已经有点\"起大早赶晚集\"的意思了。\n\n而今天AGI世界中，大量重要的概念都正在来自于Anthropic，比如MCP、Skills、Artifacts、Constitution，甚至是最近爆火的clawdbot（即后来的Moltbot），也是来自于claude的\"谐音梗\"。\n\n人们可以找到无数种理由来解释Anthropic的成功：\n\nOpenAI的支持者会说，这是来自于巨头早期对API市场不够重视 -- --\n\n在顶级入口的叙事下，API往往被错误地视作智能延伸的毛细血管，而非需要去认真重构的能力。与此同时，Anthropic倾其所有将业务押注在了API叙事上，所以在局部市场中取得了局部的成功。\n\n但Coding以及今天相当大一部分的API场景其实是没有忠诚度的，OpenAI只要推出更强大的模型能力，战局就会从根本上扭转。而GPT-5.3 Codex其实就是为今年OpenAI局部反攻的开始，因为Codex最近终于在Coding的benchmark上超过了Opus。\n\n而Anthropic的支持者会认为这来自于一种独特的\"品味\" -- --\n\n在商业战场中，能力领先本来就是护城河本身。丰田之所以能击败福特，不是因为丰田重新发明了汽车，而是丰田掌握了一种独特的精益生产方式，并且不断迭代自己的工业开发和生产效率。\n\nAnthropic同样构建了属于自己的体系和对产品的审美。\n\n在过去很长的一段时间里，这家公司是基模四巨头中的绝对异类。Gemini的定位叙事与OpenAI高度重合，X.ai力图讲一个物理世界的新故事，但在入口重要性认同上与前两者高度一致。只有Anthropic似乎根本不关心\"超级入口\"能力，只是专心做生产力生态和它的一系列衍生品。\n\n在四个头部公司里，Anthropic对免费用户最为苛刻，这家公司本质上是将免费用户视为\"债务\"而非\"资产\"；在端内的实时搜索能力方面最为保守；在语言表达习惯上，也绝不讨好迎合用户。\n\nAnthropic也是头部基模厂里唯一没有做多模态生成的公司，更不碰全模态的产品线。\n\n此外，它过去也是唯一长期通过购买AWS等云厂商算力来支持训练和推理的基模公司。通过购买的云服务而非自建数据中心，从而减缓现金流失。直到最近资本市场开始纷纷想把钞票塞进Anthropic账户的时候，它才终于开始筹划自建大型算力中心。\n\n这种极其专注的业务姿态，让它得以从巨头中找到了自己的路。\n\nAnthropic构建的高安全叙事的Constitutional AI，非常符合企业级用户的采购审美；\n\nClaude长期耕耘的长下文能力、逻辑推理一致性等能力，又切中了大量生产力场景的需求；\n\n它推出的MCP协议、不断探索的Computer Use场景交互，又为接下来的Agent交互时代打下了基础。\n\n而且随着Anthropic在企业级市场大杀四方。人们也慢慢发现，API场景也并不意味着\"低护城河\"。\n\n因为模型一旦被深度嵌入到一些工作流里时，它一旦跑顺畅了，智能上限的重要性则会逐步退位给模型与工作流耦合后的稳定性，再加上企业IT主管往往本能地厌恶更换服务商 -- -- 这也是为什么Anthropic为什么会为生态提供了大量过去的模型版本的原因。而对旧版本依赖的客户，往往是最稳定的用户。\n\n随着AI的能力越跑越快，它与生产关系相互嵌入的程度几乎必然会不断加深，谁能跑得更前，谁就会占据更好的生态位置。换言之，高护城河的API场景会越来越多。\n\n当Opus最新模型发布后，有华尔街分析师直接将其称之为\"SaaSpocalypse\"，即SaaS+Apocalypse（末日）。\n\n一方面，Claude Cowork原本就在加剧市场对于 AI 颠覆 SaaS 的恐慌情绪；另一方面，Opus超长的百万级上下文能力，足以吞噬掉海量的企业内部数据，且Anthropic同时还发布的Claude系列法律等行业自动化插件。最终共同导致大量的SaaS股票在市场上被恐慌性抛售。\n\n所以对于普通用户来说，Anthropic是一个趁手的工具。它能提升生产力，加速项目周期，可玩性丰富。\n\n而对于投资者来说，Anthropic代表了AGI竞争中一种独特的商业审美。它用最少的钱，撬动了最大的估值水平，力图在最短的时间率先实现PMF。\n\n对于AI观察者来说，Anthropic代表了一种AI的哲学观念，克制、安全、缓慢。Claude新宪法为所有人介绍了一种让AI进行道德对齐的范式，对模型场景能力的专注与成功，又让打榜这件事情显得有些滑稽而愚蠢。\n\n而综合以上所有：\n\nOpenAI们的本质更像是在用互联网的思维去构建AI帝国，创造一个服务人的AI生态，是一个相对存量的市场。而Anthropic的本质是不断创造为AI和Agent服务的工具，建造一个以Agent为主体的新世界。\n\n奥特曼也在最近回应Anthropic超级碗广告的推文中侧面应证了这种公司站位差距：\n\n他说OpenAI力图服务普通人，而Anthropic只希望服务\"富人\"。\n\n因此，奥特曼相当于承认广告植入背后是两家公司不同商业模式带来的不同结果。但人们已经能看出奥特曼在舆论战中正处于相对被动的一方。毕竟去年在超级碗投广告的还是OpenAI。\n\n然而，当Anthropic的成功飘到大洋\"此岸\"。它可能又会成为另一种叙事，一种可以重新给投资人和员工徐徐道来的故事 -- -- 尽管这些故事的侧面其实也都不尽相同。\n\n1\n\n当OpenAI不再成为所有人的故事\n\n再次回到姚顺雨的那个论坛上，其实当中参会的不少人，都曾经与OpenAI的名号有着一些纠缠：\n\n但今天其中不少人都成为了Anthropic的追随者。\n\n比如姚顺雨，作为全场提及Anthropic名字最多的嘉宾，他实际在用Anthropic解释三件事情：\n\n第一，刷分不那么重要，能力才重要。你看，Anthropic就不爱刷分，但不妨碍用户们爱它。\n\n第二，它说Anthropic\"基本上不做什么创新\"，\"模型预训练变大了，然后老老实实把RL做好\"。\n\n第三，对创业者来说，To B 比 To C 更难。因为Anthropic的模型能力和收入正相关，模型能力带来用户支出的增长。所以Anthropic证明，B端要做好模型能力上限，而C端的情况可能更复杂。\n\n杨植麟的演讲主要围绕着预训练的效率革命，公开提出\"大而美\"的口号。\n\n在演讲最后，杨植麟强调\"Taste\"的能力。他说\"做模型的过程本质上是在创造一种世界观\"。而智能不会像水电一样是相同的，而是如人一般迥异的。未来taste空间会越来越多，模型会有更多新的taste出来。\n\n而效率和Taste刚好是Anthropic的重要标签。\n\n在去年最后一天，Kimi发的内部信里，杨植麟公开表示下一阶段要以\"超越Anthropic 等前沿公司成为世界领先的AGI公司\" -- -- 不是OpenAI，也不是Gemini或者Grok，而是以Anthropic作为了直接对标的代表。\n\n而超越之路，其实也如上所述：更好的预训练+垂直整合模型训练与Agent的Taste。\n\n唐杰背后的智谱，被主持人李广密直接称作\"走了Anthropic这条路线\"的公司。\n\n它在产品矩阵和技术研发思路上，与Anthropic最相似，对Coding和Agent都下了重注。用唐杰的话说，他们\"运气好Bet了Coding\"，（此前）\"把所有的精力放在了Coding上\"。而唐杰在当天的演讲中表示，智谱还要进一步去探索Agent的生态能力 -- --\n\n有点摸着Anthropic过河的意思了。\n\n除了智谱外，阶跃等公司也在不断试水Agent相关产品。而MiniMax，骆轶航老师专门写了一篇文章来论证 ，因为他们共享了一种类似的技术理念与审美。\n\n这种价值对标迁移的背后，也是整个中国AGI战场的变换。\n\n一方面，OpenAI的叙事的确在\"老去\"。\n\n无论是谁，他们在讨论Anthropic的时候确实也都在表达相似的技术理念和商业审美：\n\n当AI进入下半场的时候，真实场景的能力比刷分更重要、实实在在的价值会超越悬浮的商业叙事、Agent的能力会绕过庞大的入口，以及无论在哪里，人们终究会为了更好的生产效率付费。\n\n而对标Anthropic\"替代\"会吸引更多的专业用户加入，属于天然更高性价比的叙事策略。\n\n另一方面，对于不少人来说，中国OpenAI的叙事也事实上在\"远去\"。\n\n因为中国OpenAI似乎已经有了自己的答案（如果我们真的有自己的OpenAI）的话 -- -- 字节在前，阿里紧追，DeepSeek则保留\"银子弹\"的鬼魅。\n\n相比于以上三者，其余每个人其实都有自己的难题要解。创业公司无法支持一个AI入口的超级战争；腾讯有超级入口，但模型能力还有待追赶。\n\n但对于很多人来说，中国OpenAI的叙事却已经不再是资产，而是成为了负债，从\"解药\"变成\"毒药\"。所有人都必须要在心里回答一句 -- -- 我们该如何面对中国OpenAI的竞争。\n\n而大洋彼岸的Anthropic便给了所有人最好的示范，也就成了许多人对中国OpenAI问题的解药。\n\n尤其在春节AI大战如火如荼的当下，人人拥抱Anthropic，这何尝又不是一种对某种中国版OpenAI式叙事的反叛呢？\n\n如果说Anthropic是对OpenAI采取了一种系统且深刻的反叛，那么中国AI不同的人，其实都怀揣着各自不同的\"Anthropic\"，这里既有对宏大愿景的兴奋，也有战场迁移的无奈。但可以肯定的是，未来Anthropic主导的生产力叙事将成为接下来一段时间的新故事，一针新的估值与产业兴奋剂，而无人将可以真正幸免于Anthropic叙事（包括OpenAI们）。\n\n点个\"爱心\"，再走 吧"
  },
  {
    "source": "The Atlantic",
    "company": "Anthropic",
    "title": "Anthropic Is at War With Itself",
    "date": "2026-01-28T19:06:04Z",
    "url": "https://www.theatlantic.com/technology/2026/01/anthropic-is-at-war-with-itself/684892/",
    "content": "The rapidly growing AI company can't quite bring itself to slow down.\n\nThese are not the words you want to hear when it comes to human extinction, but I was hearing them: \"Things are moving uncomfortably fast.\" I was sitting in a conference room with Sam Bowman, a safety researcher at Anthropic. Worth $183 billion at the latest estimate, the AI firm has every incentive to speed things up, ship more products, and develop more advanced chatbots to stay competitive with the likes of OpenAI, Google, and the industry's other giants. But Anthropic is at odds with itself -- thinking deeply, even anxiously, about seemingly every decision.\n\nAnthropic has positioned itself as the AI industry's superego: the firm that speaks with the most authority about the big questions surrounding the technology, while rival companies develop advertisements and affiliate shopping links (a difference that Anthropic's CEO, Dario Amodei, was eager to call out during an interview in Davos last week). On Monday, Amodei published a lengthy essay, \"The Adolescence of Technology,\" about the \"civilizational concerns\" posed by what he calls \"powerful AI\" -- the very technology his firm is developing. The essay has a particular focus on democracy, national security, and the economy. \"Given the horror we're seeing in Minnesota, its emphasis on the importance of preserving democratic values and rights at home is particularly relevant,\" Amodei posted on X, making him one of very few tech leaders to make a public statement against the Trump administration's recent actions.\n\nThis rhetoric, of course, serves as good branding -- a way for Anthropic to stand out in a competitive industry. But having spent a long time following the company and, recently, speaking with many of its employees and executives, including Amodei, I can say that Anthropic is at least consistent. It messages about the ethical issues surrounding AI constantly, and it appears unusually focused on user safety. Bowman's job, for example, is to vet Anthropic's products before they're released into the world, making sure that they will not spew, say, white-supremacist talking points; push users into delusional crises; or generate nonconsensual porn.\n\nSo far, the effort seems to be working: Unlike other popular chatbots, including OpenAI's ChatGPT and Elon Musk's Grok, Anthropic's bot, Claude, has not had any major public blowups despite being as advanced as, and by some measures more advanced than, the rest of the field. (That may be in part because its chatbot does not generate images and has a smaller user base than some rival products.) But although Anthropic has so far dodged the various scandals that have plagued other large language models, the company has not inspired much faith that such problems will be avoided forever. When I met Bowman last summer, the company had recently divulged that, in experimental settings, versions of Claude had demonstrated the ability to blackmail users and assist them when they ask about making bioweapons. But the company has pushed its models onward anyway, and now says that Claude writes a good chunk -- and in some instances all -- of its own code.\n\nAnthropic publishes white papers about the terrifying things it has made Claude capable of (\"How LLMs Could Be Insider Threats,\" \"From Shortcuts to Sabotage\"), and raises these issues to politicians. OpenAI CEO Sam Altman and other AI executives also have long spoken in broad, aggrandizing terms about AI's destructive potential, often to their own benefit. But those competitors have released junky TikTok clones and slop generators. Today, Anthropic's only major consumer product other than its chatbot is Claude Code, a powerful tool that promises to automate all kinds of work, but is nonetheless targeted to a relatively small audience of developers and coders.\n\nThe company's discretion has resulted in a corporate culture that doesn't always make much sense. Anthropic comes across as more sincerely committed to safety than its competitors, but it is also moving full speed toward building tools that it acknowledges could be horrifically dangerous. The firm seems eager for a chance to stand out. But what does Anthropic really stand for?\n\nFounded in 2021 by seven people who splintered off from OpenAI, Anthropic is full of staff and executives who come across as deeply, almost pathologically earnest. I sat in on a meeting of Anthropic's Societal Impacts team, a small group dedicated to studying how AI affects work, education, and more. This was a brainstorming session: The team wanted to see if it could develop AI models that work better with people than alone, which, the group reasoned, could help prevent or slow job loss. A researcher spoke up. He pressed the team to consider that, in the very near future, AI models might just be better than humans at everything. \"Basically, we're cooked,\" he said. In which case, this meeting was nothing more than a \"lovely thought exercise.\" The group agreed this was possible. Then it moved on.\n\nThe researcher referred to his brief, existential interruption as \"classic Anthropic.\" Hyperrational thought experiments, forceful debates on whether AI could be shaped for the better, an unshakable belief in technological progress -- these are classic Anthropic qualities. They trickle down from the top. A few weeks after the Societal Impacts meeting, I wanted to see what Amodei himself thought about all of this. If Altman is the AI boom's great salesman and Demis Hassabis, the CEO of Google DeepMind and a Nobel laureate, its scientist, then Amodei is the closest the industry has to a philosopher. He is also responsible for some of the technical research that made ChatGPT possible. \"Whenever I say 'AI,' people think about the thing they're using today,\" Amodei told me, hands clasped and perched atop his head. \"That's almost never where my mind is. My mind is almost always at: We're releasing a new version every three months. Where are we gonna be eight versions from now? In two years?\"\n\nWhen he was at OpenAI, Amodei wrote an internal document called \"The Big Blob of Compute.\" It laid out his belief that AI models improve as a function of the resources put into them. More power, more data, more chips, better AI. That belief now animates the entire industry. Such unwavering faith in AI progress is perhaps Anthropic's defining feature. The company has hired a \"model welfare\" researcher to study whether Claude can experience suffering or is conscious. The Societal Impacts team has set up a miniature, AI-run vending machine in the firm's cafeteria to study whether the technology could autonomously operate a small business selling snacks and trinkets. Claude selects inventory, sets prices, and requests refills, while humans just restock the shelves. Welcome to the singularity.\n\nAmodei and the rest of the group founded Anthropic partly because of disagreements over how to prepare the world for AI. Amodei is especially worried about job displacement, telling me that AI could erase a large portion of white-collar jobs within five years; he dedicated an entire section of \"The Adolescence of Technology\" to the danger that the AI boom might accumulate tremendous wealth primarily to firms such as his own.\n\nEven with this and other gloomy forecasts of his, Amodei has bristled at the notion that he and his firm are \"doomers\" -- that their primary motivation is preventing AI from wiping out a large number of jobs or lives. \"I tend to be fairly optimistic,\" he told me. In addition to \"The Adolescence of Technology,\" Amodei has published a 14,000-word manifesto called \"Machines of Loving Grace\" that comprehensively details a utopian vision for his technology: eliminating almost all disease, lifting billions out of poverty, doubling human lifespan. There is not a hint of irony; the essay envisions people being \"literally moved to tears\" by the majesty of AI's accomplishments. Amodei's employees cited it to me in conversation numerous times. Meanwhile, Altman trolls on X, and Musk seems to exist in a continuum of AI slop and conspiracy theories.\n\nWhen Anthropic launched Claude, in 2023, the bot's distinguishing feature was a \"Constitution\" that the model was trained on detailing how it should behave; last week, Anthropic revamped the document into a 22,000-word treatise on how to make Claude a moral and sincere actor. Claude, the constitution's authors write, has the ability to foster emotional dependence, design bioweapons, and manipulate its users, so it's Anthropic's responsibility to instill upright character in Claude to avoid these outcomes. \"Once we decide to create Claude, even inaction is a kind of action,\" they write. No other firm had, or has, any truly comparable document.\n\nAmodei says he wants rival companies to act in ways he believes are more responsible -- and thinks Anthropic's commercial success will pressure them to do so. Several of Anthropic's major AI-safety initiatives and research advances have indeed been adopted by top competitors, such as its approach to preventing the use of AI to build bioweapons. And OpenAI has shared a \"Model Spec,\" its far more streamlined and pragmatic answer to Anthropic's constitution -- which contains no talk of ChatGPT's \"character\" or \"preserving important societal structures.\" (OpenAI has a corporate partnership with The Atlantic.)\n\nAll of this helps Anthropic's bottom line, of course: The emphasis on responsibility is \"very attractive to large enterprise businesses which are also quite safety-, brand-conscious,\" Daniela Amodei, Anthropic's president (and Dario's sister), told me from a sweaty conference room in Anthropic's old headquarters in 2024. Nearly two years later, Anthropic controls 40 percent of the enterprise-AI market. The Amodeis hopes their commercial success will pressure competitors to more aggressively prioritize safety as well.\n\nThat said, it's not always clear that these efforts to spark a \"race to the top\" -- another phrase of Amodei's that his employees invoke constantly -- have been successful. Anthropic's research established AI sycophancy as an issue well before \"AI psychosis\" emerged, yet AI psychosis still became something that many people apparently suffer from. Amodei recognizes that his own products aren't perfect, either. \"I absolutely do not want to warrant and guarantee that we will never have these problems,\" he said. Several independent AI researchers, including some who have partnered with Anthropic to test Claude for various risks, told me that although Anthropic appears more committed to AI safety than its competitors, that's a low bar.\n\nAnthropic's mode is generally to publish information about AI models and wait for the world to make the hard calls about how to control or regulate them. The main regulatory proposal of Jack Clark, a co-founder of Anthropic and its head of policy, is that governments establish \"transparency\" requirements, or some sort of mandated reporting about what internal tests reveal about AI products. But the company is particular about what it deems worth publishing. The firm does not, for instance, share much about its AI-training data or carbon footprint. When I asked Clark about how much information remains hidden -- particularly in terms of how Anthropic's AI tools are actually developed -- he argued that transparency into how AI models are produced isn't all that important. (Some of that information is also, presumably, proprietary.) Rather, Clark told me, the outcomes of the technology are what matter.\n\nThere is a \"well-established norm that whatever goes on inside a factory is by and large left up to the innovator that's built that factory, but you care a lot about what comes out of the factory,\" he said, explaining why he believes that AI companies sharing information about how their products are made matters less than reporting what they can do. Typically the government \"reaches inside\" the factory, he said, only when something in the output -- say, heavy metals -- raises cause for concern. Never mind the long history of regulation dictating what goes on inside factories -- emergency exits in clothing factories, cleanliness standards in meatpacking facilities, and so on. (Clark did note that laws sometimes need to change, and that they haven't yet adapted to AI.)\n\nHe brought up Wall Street, of all examples, to make his point. Lawmakers \"thought they had transparency into financial systems,\" he said -- that banks and hedge funds and so on were giving reliable reports on their dealings. \"Then the financial crash happened,\" regulators realized that transparency was inadequate and gameable, and Congress changed the law. (President Trump then changed much of it back.) In the long run, Clark seemed to feel, this was the system working as it should. But his comparison also raises the possibility that before anybody can figure out how to get the AI boom right, something must go horribly wrong.\n\nIn mid-September, Anthropic cybersecurity experts detected unusual activity among a group of Claude users. They came to suspect that it was a major, AI-enabled Chinese cyberespionage campaign -- an attempt by foreign actors to use Claude to automate the theft of sensitive information. Anthropic promptly shut the operation down, published a report, and sent Logan Graham, who heads a team at the company that evaluates advanced uses of AI, to explain the situation to Congress.\n\nIn theory, this sequence represented Anthropic's philosophy at work: Detect risks posed by AI and warn the public. But the incident also underscored how unpredictable, and uncontrollable, the environment really is. Months before the Chinese hack, Graham told me that he felt \"pretty good\" about the precautions the company had taken around cyberthreats.\n\nNobody can foresee all of the ways any AI product might be used, for good or ill, but that's exactly why Anthropic's sanctimony can seem silly. For all Amodei's warnings about the possible harms of automation, Anthropic's bots themselves are among the products that may take away jobs; many consider Claude the best AI at coding, for instance. After one of my visits to Anthropic's offices, I went to an event for software engineers a few blocks away at which founders gave talks about products developed with Anthropic software. Someone demonstrated a tool that could automate outreach for job recruitment -- leading one attendee to exclaim, with apparent glee, \"This is going to destroy an entire industry!\"\n\nWhen I asked several Anthropic employees if they'd want to slow down the AI boom in an ideal world, none seemed to have ever seriously considered the question; it was too far-fetched a possibility, even for them. Joshua Batson, an interpretability researcher at Anthropic -- he studies the labyrinthine inner workings of AI models -- told me that it would be nice if the industry could go half as fast. Jared Kaplan, a co-founder of Anthropic and the firm's chief science officer, told me he'd prefer it if AGI, or artificial general intelligence, arrived in 2032 rather than, say, 2028; Bowman, the safety researcher, said he thought slowing down for just a couple of months might be enough. Everyone seemed to believe, though, that AI-safety research itself could eventually be automated with Claude -- and once that happens, they reasoned, their tests could keep up with the AI's exponentially improving capabilities.\n\nLike so many others in the industry, the employees I spoke with also contended that neither Anthropic nor any other AI company could actually slow development down. \"The world gets to make this decision, not companies,\" Clark told me, seated cross-legged on his chair, and \"the system of capital markets says, Go faster.\" So they are. Anthropic is reportedly fundraising at a $350 billion valuation, and its advertisements litter Instagram and big-city billboards. This month, the company launched a version of its Claude Code product geared toward non-software engineers called Claude Cowork. And in July, as first reported in Wired, Amodei wrote an internal memo to employees that Anthropic would seek investments from the United Arab Emirates and Qatar, which, in his words, would likely enrich \"dictators.\" Warnings about the dangers of authoritarian AI have been central in Anthropic's public messaging; \"Machines of Loving Grace\" includes dire descriptions of the threat of \"authoritarian\" AI.\n\nWhen I brought this up to Amodei, he cut me off. \"We never made a commitment not to seek funding from the Middle East,\" he said. \"One of the traps you can fall into when you're doing a good job running a responsible company is every decision that you make\" can be \"interpreted as a moral commitment.\" There was no \"pressing need\" to seek Middle Eastern funding before, and doing so entailed \"complexities,\" he said. I took his implication to be that the intensive capital demands of the AI race now made such investments a necessity. Still, such investors, Amodei said, wouldn't have any control over his firm. A few days after we spoke, Anthropic announced the Qatar Investment Authority as a \"significant\" investor in a new fundraising round.\n\nIf you zoom out enough, and perhaps not even all that far, Anthropic stands for the same things that OpenAI, Google, Meta, and anyone else in the AI race do: to build fantastically powerful chatbots and use them to transform the world and beat the competition. Across the company, the belief in AI's potential is messianic. AI \"presents one of the only technologies\" that gets us out of the challenges ahead for humanity, Clark told me: climate change, aging populations, resource contention, authoritarianism, war. Without AI, he said, there will be more and more \"Mad Max-like swaths of the world.\"\n\nTrenton Bricken, who works on AI safety at Anthropic, took this notion to an even greater extreme: He would ideally want the AI industry to slow down, but \"every year that we stall, there are lots of people suffering who otherwise would not,\" he told me, referring to the possibility that AI will eventually cure diseases and achieve everything else outlined in \"Machines of Loving Grace.\" His colleague Sholto Douglas claimed that such a delay \"comes at the cost of millions of lives.\"\n\nPerhaps the greatest confusion at Anthropic is between theory and practice -- the idea of safe AI versus the speed necessary to win the AI race. A corporate culture built around deep thought experiments and genuine disagreements about the future also has to sell AI. In the company's view, these ends are complementary; better for it to responsibly usher in the AI future than Elon Musk or China. But that's also a convenient way to justify an any-means-necessary approach to progress. I thought of that automated vending machine that the company had set up in its office. Claude ran the business into the ground in only a month through a string of very poor pricing and stocking decisions. But none of those really mattered: Anthropic had placed the machine next to all the free snacks in the office canteen.\n\nWhen I asked Amodei recently about how he could justify the breakneck pace given the concerns he has over safety, he expressed total confidence in his staff -- and also floated a new idea. Perhaps, he suggested, Claude will become so intelligent in the very near future that the bot will enable something radical: \"Maybe at some point in 2027, what we want to do is just slow things down,\" he said, and let the models fix themselves. \"For just a few months.\""
  },
  {
    "source": "Ed Zitron's Where's Your Ed At",
    "company": "Anthropic",
    "title": "Premium: The Hater's Guide to Anthropic",
    "date": "2026-02-21T15:35:49Z",
    "url": "https://www.wheresyoured.at/premium-the-haters-guide-to-anthropic/",
    "content": "In May 2021, Dario Amodei and a crew of other former OpenAI researchers formed Anthropic and dedicated themselves to building the single-most-annoying Large Language Model company of all time.\n\nPardon me, sorry, I mean safest, because that's the reason that Amodei and his crew claimed was why they left OpenAI:\n\nDario Amodei: Yeah. So there was a group of us within OpenAI, that in the wake of making GPT-2 and GPT-3, had a kind of very strong focus belief in two things. I think even more so than most people there. One was the idea that if you pour more compute into these models, they'll get better and better and that there's almost no end to this. I think this is much more widely accepted now. But, you know, I think we were among the first believers in it. And the second was the idea that you needed something in addition to just scaling the models up, which is alignment or safety. You don't tell the models what their values are just by pouring more compute into them. And so there were a set of people who believed in those two ideas. We really trusted each other and wanted to work together. And so we went off and started our own company with that idea in mind.\n\nI'm also being a little sarcastic. Anthropic, a \"public benefit corporation\" (a company that is quasi-legally required to sometimes sort of focus on goals that aren't profit driven, and in this case, one that chose to incorporate in Delaware as opposed to California, where it would have actual obligations), is the only meaningful competitor to OpenAI, one that went from (allegedly) making about $116 million in March 2025 to making $1.16 billion in February 2026, in the very same month it raised $30 billion from thirty-seven different investors, including a \"partial\" investment from NVIDIA and Microsoft announced in November 2025 that was meant to be \"up to\" $15 billion.\n\nAnthropic's models regularly dominate the various LLM model leaderboards, and its Claude Code command-line interface tool (IE: a terminal you type stuff into) has become quite popular with developers who either claim it writes every single line of their code, or that it's vaguely useful in some situations.\n\nCEO Dario Amodei predicted last March that in six months AI would be writing 90% of code, and when that didn't happen, he simply made the same prediction again in January, because, and I do not say this lightly, Dario Amodei is full of shit.\n\nYou see, Anthropic has, for the best part of five years, been framing itself as the trustworthy, safe alternative to OpenAI, focusing more on its paid offerings and selling to businesses (realizing that the software sales cycle usually focuses on dimwitted c-suite executives rather than those who actually use the products), as opposed to building a giant, expensive free product that lots of people use but almost nobody pays for.\n\nAnthropic, separately, has avoided following OpenAI in making gimmicky (and horrendously expensive) image and video generation tools, which I assume is partly due to the cost, but also because neither of those things are likely something that an enterprise actually cares about.\n\nAnthropic also caught on early to the idea that coding was the one use case that Large Language Models fit naturally:\n\nAnthropic has held the lead in coding LLMs since the launch of June 2024's Claude Sonnet 3.5, and as a story from The Information from December 2024 explained, this terrified OpenAI:\n\nEarlier this fall, OpenAI leaders got a shock when they saw the performance of Anthropic's artificial intelligence model for automating computer programming tasks, which had gained an edge on OpenAI's models, according to its own internal benchmarks. AI for coding is one of OpenAI's strong suits and one of the main reasons why millions of people subscribe to its chatbot, ChatGPT.\n\nOpenAI leaders were already on edge after Cursor, a startup OpenAI funded last year, in July made Anthropic's Claude model the default for Cursor's AI coding assistant instead of OpenAI's models, as it had previously done, according to an OpenAI employee. In a podcast in October, Cursor co-founder Aman Sanger called the latest version of Anthropic's model, Claude 3.5 Sonnet, the \"net best\" for coding in part because of its superior understanding of what customers ask it to do.\n\nCursor would, of course, eventually go on to become its own business, raising $3.2 billion in 2025 to compete with Claude Code, a product made by Anthropic, which Cursor pays to offer its models through its AI coding product. Cursor is Anthropic's largest customer, with the second being Microsoft's Github Copilot. I have heard from multiple sources that Cursor is spending more than 100% of its revenue on API calls, with the majority going to Anthropic and OpenAI, both of whom now compete with Cursor.\n\nAnthropic sold itself as the stable, thoughtful, safety-oriented AI lab, with Amodei himself saying in an August 2023 interview that he purposefully avoided the limelight:\n\nDwarkesh Patel (01:56:14 - 01:56:26):\n\nYou've been less public than the CEOs of other AI companies. You're not posting on Twitter, you're not doing a lot of podcasts except for this one. What gives? Why are you off the radar?\n\nDario Amodei (01:56:26 - 01:58:03):\n\nI aspire to this and I'm proud of this. If people think of me as boring and low profile, this is actually kind of what I want. I've just seen cases with a number of people I've worked with, where attaching your incentives very strongly to the approval or cheering of a crowd can destroy your mind, and in some cases, it can destroy your soul.\n\nI've deliberately tried to be a little bit low profile because I want to defend my ability to think about things intellectually in a way that's different from other people and isn't tinged by the approval of other people. I've seen cases of folks who are deep learning skeptics, and they become known as deep learning skeptics on Twitter. And then even as it starts to become clear to me, they've sort of changed their mind. This is their thing on Twitter, and they can't change their Twitter persona and so forth and so on.\n\nI don't really like the trend of personalizing companies. The whole cage match between CEOs approach. I think it distracts people from the actual merits and concerns of the company in question. I want people to think in terms of the nameless, bureaucratic institution and its incentives more than they think in terms of me. Everyone wants a friendly face, but actually, friendly faces can be misleading.\n\nA couple of months later in October 2023, Amodei joined The Logan Bartlett show, saying that he \"didn't like the term AGI\" because, and I shit you not, \"...because we're closer to the kinds of things that AGI is pointing at,\" making it \"no longer a useful term.\" He said that there was a \"future point\" where a model could \"build dyson spheres around the sun and calculate the meaning of life,\" before rambling incoherently and suggesting that these things were both very close and far away at the same time. He also predicted that \"no sooner than 2025, maybe 2026\" that AI would \"really invent new science.\"\n\nThis was all part of Anthropic's use of well-meaning language to tell a story that said \"you should be scared\" and \"only Anthropic will save you.\" In July 2023, Amodei spoke before a senate committee about AI oversight and regulation, starting sensible (IE: if AI does become powerful, we should have regulations to mitigate those problems) and eventually veering aggressively into marketing slop:\n\nThe medium-term risks are where I would most like to draw the subcommittee's attention. Simply put, a straightforward extrapolation of the pace of progress suggests that, in 2-3 years, AI systems may facilitate extraordinary insights in broad swaths of many science and engineering disciplines. This will cause a revolution in technology and scientific discovery, but also greatly widen the set of people who can wreak havoc. In particular, I am concerned that AI systems could be misused on a grand scale in the domains of cybersecurity, nuclear technology, chemistry, and especially biology.\n\nThis is Amodei's favourite marketing trick -- using a vague timeline (2-3 years) to suggest that something vaguely bad that's also good for Anthropic is just around the corner, but managed correctly, could also be good for society (a revolution in technology and science! But also, havoc!). Only Dario has the answers (regulations that start with \"securing the AI supply chain\" meaning \"please stop China from competing\").\n\nIn retrospect, this was the most honest that he'd ever be. In 2024, Amodei would quickly learn that he loved personalizing companies, and that destroying his soul fucking rocked.\n\nIn October 2024, Amodei put out a 15,000-word-long blog -- ugh, AI is coming for my job! -- where he'd say that Anthropic needed to \"avoid the perception of propaganda\" while also saying that \"as early as 2026 (but there are also ways it could take much longer),\" AI would be smarter than a Nobel Prize winner, autonomously able to complete weeks-long tasks, and be the equivalent of a \"country of geniuses in a datacenter.\"\n\nThis piece, like all of his proclamations, had two goals: generating media coverage and investment. Amodei is a deeply dishonest man, couching \"predictions\" based on nothing in terms like \"maybe,\" \"possibly,\" or \"as early as,\" knowing that the media will simply ignore those words and report what he says as a wise, evidence-based fact.\n\nAmodei (and by extension Anthropic) nakedly manipulates the media by having them repeat these things without analysis or counterpoints -- such as that \"AI could surpass almost all humans at almost everything shortly after 2027 (which I'll get back to in a bit).\" He knows that these things aren't true. He knows he doesn't have any proof. And he knows that nobody will ask, and that his bullshit will make for a sexy traffic-grabbing headline.\n\nTo be clear, that statement was made three months after Amodei's essay said that AI labs needed to avoid \"the perception of propaganda.\" Amodei is a con artist that knows he can't sell Anthropic's products by explaining what they actually do, and everybody is falling for it.\n\nAnd, almost always, these predictions match up with Anthropic's endless fundraising. On September 23, 2024, The Information reported that Anthropic was raising a round at a $30-$40 billion valuation, and on October 12 2024, Amodei pooped out Machines of Loving Grace with the express position that he and Anthropic \"had not talked that much about powerful AI's upsides.\"\n\nA month later on November 22, 2024, Anthropic would raise another $4 billion from Amazon, a couple of weeks after doing a five-hour-long interview with Lex Fridman in which he'd say that \"someday AI would be better at everything.\"\n\nOn November 27, 2024, Amodei would do a fireside chat at Eric Newcomer's Cerebral Valley AI Summit where he'd say that in 2025, 2026, or 2027 (yes, he was that vague), AI could be as \"good as a Nobel Prize winner, polymathic across many fields,\" and have \"agency [to] act on its own for hours or days,\" the latter of which deliberately laid foundation for one of Anthropic's greatest lies: that AI can \"work uninterrupted\" for periods of time, leaving the reader or listener to fill in the (unsaid) gap of \"...and actually create useful stuff.\"\n\nAmodei crested 2024 with an interview with the Financial Times, and let slip what I believe will eventually become Anthropic's version of WeWork's Community-Adjusted EBITDA, by which I mean \"a way to lie and suggest profitability when a company isn't profitable\":\n\nLet's just take a hypothetical company. Let's say you train a model in 2023. The model costs $100mn dollars. And, then, in 2024, that model generates, say, $300mn of revenue. Then, in 2024, you train the next model, which costs $1bn. And that model isn't done yet, or it gets released near the end of 2024. Then, of course, it doesn't generate revenue until 2025.\n\nSo, if you ask \"is the company profitable in 2024\", well, you made $300mn and you spent $1bn, so it doesn't look profitable. If you ask, was each model profitable? Well, the 2023 model cost $100mn and generated several hundred million in revenue. So, the 2023 model is a profitable proposition.\n\nThese numbers are not Anthropic numbers. But what I'm saying here is: the cost of the models is going up, but the revenue of each model is going up and there's a mismatch in time because models are deployed substantially later than they're trained.\n\nYeah man, if a company made $300 million in revenue and spent $1 billion. No amount of DarioMath about how a model \"costs this much and makes this much revenue\" changes the fact that profitability is when a company makes more money than it spends.\n\nOn January 5, 2025, Forbes would report that Anthropic was working on a $60 billion round that would make Amodei, his sister Daniela, and five other cofounders billionaires.\n\nAnyway, as I said at Davos on January 21, 2025, Amodei said that he was \"more confident than ever\" that we're \"very close\" to \"powerful capabilities,\" defined as \"systems that are better than almost all humans at almost all terms,\" citing his long, boring essay. A day later, Anthropic would raise another $1 billion from Google.\n\nOn January 27, 2025, he'd tell Economist editor-in-chief Zanny Minton Beddoes that AI would get \"as good and eventually better\" at thinking as human beings, and that the ceiling of what models could do was \"well above humans.\"\n\nOn February 18, 2025, he'd tell Beddoes that we'd get a model \"...that can do everything a human can do at the level of a Nobel laureate across many fields\" by 2026 or 2027, and that we're \"on the eve of something that has great challenges\" that would \"upend the balance of power\" because we'd have \"10 million people smarter than any human alive...\" oh god, I'm not fucking writing it out. I'm sorry. It's always the same shit. The models are people, we're so scared.\n\nOn February 28, 2025, Amodei would join the New York Times' Hard Fork, saying that he wanted to \"slow down authoritarians,\" and that \"public officials and leaders at companies\" would \"look back at this period [where humanity would become a \"post-powerful AI society that co-exists with powerful intelligences]\" and \"feel like a fool,\" and that that was the number one goal of these people. Amodei would also add that he had been in the field for 10 years -- something he loves to say! -- and that there was a 70-80% chance that we will \"get a very large number of AI systems that are much smarter than humans at almost everything\" before the end of the decade.\n\nThree days later, Anthropic would raise $3.5 billion at a $61.5 billion valuation.\n\nBeneath the hype, Anthropic is, like OpenAI, a company making LLMs that can generate code and text, and that can interpret data from images and videos, all while burning billions of dollars and having no path to profitability. Per The Information, Anthropic made $4.5 billion in revenue and lost $5.2 billion generating it, and based on my own reporting from last year, costs appear to scale linearly above revenue.\n\nSome will argue that the majority of Anthropic's losses ($4.1 billion) were from training, and I think it's time we had a chat about what \"training\" means, especially as Anthropic plans to spend $100 billion on it in the next four years. Per my piece from last week:\n\nWhile most people know about pretraining -- the shoving of large amounts of data into a model (this is a simplification I realize) -- in reality a lot of the current spate of models use post-training, which covers everything from small tweaks to model behavior to full-blown reinforcement learning where experts reward or punish particular responses to prompts.\n\nTo be clear, all of this is well-known and documented, but the nomenclature of \"training\" suggests that it might stop one day, versus the truth: training costs are increasing dramatically, and \"training\" covers anything from training new models to bug fixes on existing ones. And, more fundamentally, it's an ongoing cost -- something that's an essential and unavoidable cost of doing business.\n\nIn an interview on the Dwarkesh Podcast, Amodei even admitted that if you \"never train another model\" you \"don't have any demand because you'll fall behind.\" Training is opex, and should be part of gross margins.\n\nIt's time we had an honest conversation about Anthropic.\n\nDespite its positioning as the trustworthy, \"nice\" AI lab, Anthropic is as big, ugly and wasteful as OpenAI, and Dario Amodei is an even bigger bullshit artist than Sam Altman. It burns just as much of its revenue on inference (59%, or $2.79 billion on $4.5 billion of revenue, versus OpenAI's 62%, or $2.5 billion on $4.3 billion of revenue in the first half of 2025, if you use The Information's numbers), and shows no sign of any \"efficiency\" or \"cost-cutting.\"\n\nWorse still, Anthropic continually abuses its users through varying rate limits to juice revenues and user numbers -- along with Amodei's gas-leak-esque proclamations -- to mislead the media, the general public, and investors about the financial condition of the company.\n\nBased on an analysis of many users' actual token burn on Claude Code, I believe Anthropic is burning anywhere from $3 to $20 to make $1, and that the product that users are using (and the media is raving about) is not one that Anthropic can actually support long-term.\n\nI also see signs that Amodei himself is playing fast and loose with financial metrics in a way that will blow up in his face if Anthropic ever files its paperwork to go public. In simpler terms, Anthropic's alleged \"38% gross margins\" are, if we are to believe Amodei's own words, not the result of \"revenue minus COGS\" but \"how much a model costs and how much revenue it's generated.\"\n\nAnthropic is also making promises it can't keep. It's promising to spend $30 billion on Microsoft Azure (and an additional \"up to one gigawatt\"), \"tens of billions\" on Google Cloud, $21 billion on Google TPUs with Broadcom, \"$50 billion on American infrastructure,\" as much as $3 billion on Hut8's data center in Louisiana, and an unknowable (yet likely in the billions) amount of money with Amazon Web Services. Not to worry, Dario also adds that if you're off by a couple of years on your projections of revenue and ability to pay for compute, it'll be \"ruinous.\"\n\nI think that he's right. Anthropic cannot afford to pay its bills, as the ruinous costs of training -- which will never, ever stop -- and inference will always outpace whatever spikes of revenue it can garner through media campaigns built on deception, fear-mongering, and an exploitation of reporters unwilling to ask or think about the hard questions.\n\nI see no difference between OpenAI's endless bullshit non-existent deal announcements and what Anthropic has done in the last few months. Anthropic is as craven and deceptive as OpenAI, and Dario Amodei is as willing a con artist as Altman, and I believe is desperately jealous of his success.\n\nAnd after hours and hours of listening to Amodei talk, I think he is one of the most annoying, vacuous, bloviating fuckwits in tech history. He rambles endlessly, stutters more based on how big a lie he's telling, and will say anything and everything to get on TV and say noxious, fantastical, intentionally-manipulative bullshit to people who should know better but never seem to learn. He stammers, he blithers, he rambles, he continually veers between \"this is about to happen\" and \"actually it's far away\" so that nobody can say he's a liar, but that's exactly what I call a person who intentionally deceives people, even if they couch their lies in \"maybes\" and \"possiblies.\"\n\nDario Amodei fucking sucks, and it's time to stop pretending otherwise. Anthropic has no more soul or ethics than OpenAI -- it's just done a far better job of conning people into believing otherwise.\n\nThis is the Hater's Guide To Anthropic, or \"DarioWare: Get It Together.\""
  },
  {
    "source": "WebProNews",
    "company": "Anthropic",
    "title": "Anthropic's Paradox: How the AI Safety Champion Struggles With Its Own Contradictions",
    "date": "2026-02-01T09:17:43Z",
    "url": "https://www.webpronews.com/anthropics-paradox-how-the-ai-safety-champion-struggles-with-its-own-contradictions/",
    "content": "In a sparse conference room in San Francisco's Mission District, the phrase that escaped from a senior researcher's lips carried the weight of existential dread: \"Things are moving uncomfortably fast.\" This wasn't a casual observation about product development cycles or market competition. It was an admission about the pace of artificial intelligence advancement and humanity's potential obsolescence -- coming from inside Anthropic, the company that has positioned itself as artificial intelligence's conscience.\n\nFounded by former OpenAI executives Dario and Daniela Amodei in 2021, Anthropic has cultivated an identity distinct from its Silicon Valley peers. While competitors race to deploy increasingly powerful models, Anthropic has wrapped itself in the mantle of safety-first development, promising a more measured approach to creating artificial general intelligence. The company's Claude chatbot competes directly with ChatGPT and Google's Gemini, but Anthropic insists its true differentiator lies not in speed or capability, but in caution and constitutional AI principles designed to align systems with human values.\n\nYet beneath this carefully constructed image, Anthropic finds itself trapped in a fundamental contradiction. The company must simultaneously advance AI capabilities fast enough to remain commercially viable while maintaining the rigorous safety standards that justify its existence. It must attract the venture capital and corporate partnerships necessary to fund billion-dollar compute clusters while warning about existential risks that could make those investments worthless. It must compete for the same elite talent as OpenAI and Google DeepMind while arguing that the breakneck pace of development at those companies courts catastrophe.\n\nThe Safety Paradox: Moving Fast While Preaching Caution\n\nThe tension became impossible to ignore when Anthropic announced its expansion plans in San Francisco. According to SFGate, the company is dramatically increasing its physical footprint in the city, adding hundreds of thousands of square feet to accommodate rapid headcount growth. This expansion signals ambitions that extend far beyond a cautious research lab. Anthropic is building the infrastructure of a company preparing to compete at the highest levels of the AI industry -- a posture that sits uneasily with its public positioning as the sector's prudent alternative.\n\nThe Atlantic's investigation reveals an organization grappling with internal contradictions at every level. Employees describe a workplace culture split between two competing imperatives: the mission-driven researchers who joined specifically because of Anthropic's safety commitments, and the pragmatists who recognize that without commercial success, the company's safety research becomes academically interesting but practically irrelevant. \"You can't influence the direction of AI development from the sidelines,\" one engineer told The Atlantic, capturing the utilitarian logic that justifies aggressive growth. \"If we don't build it, someone else will -- and they won't care about safety.\"\n\nThis reasoning has become a familiar refrain in AI circles, a kind of prisoner's dilemma that traps even safety-conscious actors in an accelerating race. But critics argue it represents a fundamental betrayal of Anthropic's founding principles. If the company's response to dangerous AI development is to develop AI just as quickly while adding a veneer of safety research, has it actually changed anything? Or has it simply provided ethical cover for the same reckless acceleration it claims to oppose?\n\nDario Amodei's Contradictory Warnings\n\nAt the center of these contradictions stands CEO Dario Amodei, whose public statements have drawn increasing scrutiny for their internal inconsistencies. An essay published by Transformer News systematically dismantles Amodei's recent warnings about AI risk, arguing that his dire predictions about existential threats don't align with his company's aggressive development timeline. If Amodei genuinely believes we're approaching artificial general intelligence within years and that such systems pose catastrophic risks, the essay argues, why is Anthropic racing to build them?\n\nThe analysis points to a pattern in which Amodei issues increasingly alarming warnings about AI capabilities and risks while simultaneously announcing new, more powerful models and expanding Anthropic's commercial partnerships. In public appearances, he speaks gravely about the potential for AI systems to escape human control or be weaponized by bad actors. In earnings calls and investor presentations, he touts Anthropic's technological achievements and growth trajectory. These dual narratives serve different audiences but create a credibility problem: Which Dario Amodei should we believe?\n\nThis contradiction extends to Anthropic's approach to model capabilities. The company has published research on \"constitutional AI\" designed to make systems more aligned with human values, and it emphasizes testing and red-teaming before releases. Yet it continues to push the boundaries of what its models can do, releasing increasingly capable versions of Claude that can process longer contexts, handle more complex reasoning tasks, and interface with external tools -- precisely the kind of capability expansion that safety researchers warn could lead to unexpected emergent behaviors.\n\nThe Commercial Imperative and Its Discontents\n\nAnthropic's commercial partnerships reveal the depth of its entanglement with the very dynamics it claims to resist. The company has secured billions in funding from Google, Amazon, and other tech giants who view AI as the next platform war. These investors didn't write massive checks to fund a cautious research lab; they expect competitive products, rapid iteration, and market share gains. According to Newcomer, this financial reality has created internal tensions as researchers watch safety considerations take a backseat to shipping deadlines and feature parity with competitors.\n\nThe pressure manifests in subtle ways that accumulate over time. Safety reviews that might have taken weeks get compressed into days. Features that researchers flag as potentially problematic get released anyway, with the justification that competitors already offer similar capabilities. The \"constitutional AI\" principles that supposedly guide development become more aspirational than operational, honored in the breach as much as the observance. Employees who raise concerns find themselves marginalized or reassured that safety will be prioritized \"once we achieve market stability\" -- a milestone that perpetually recedes into the future.\n\nThis dynamic isn't unique to Anthropic. Every AI lab faces similar pressures, caught between the long-term imperative of safe development and the short-term necessity of commercial survival. But Anthropic's situation is particularly acute because the company has staked its identity and market positioning on being different. When OpenAI or Google DeepMind prioritize speed over caution, it's consistent with their stated goals. When Anthropic does the same thing, it undermines the core premise that justified its existence.\n\nThe Talent War and Cultural Erosion\n\nNowhere is Anthropic's contradiction more visible than in its approach to talent acquisition. The company competes for the same pool of elite AI researchers as its rivals, offering competitive compensation packages and the promise of working on cutting-edge problems. But it also tries to attract a different kind of employee: researchers motivated by safety concerns who might be uncomfortable with the culture at OpenAI or Google. This dual recruitment strategy creates a workforce with fundamentally different values and priorities.\n\nThe Atlantic's reporting describes a growing divide within Anthropic between \"safety idealists\" and \"capabilities pragmatists.\" The idealists joined because they believed in Anthropic's mission to develop AI safely, even if it meant moving more slowly than competitors. The pragmatists recognize that in the current environment, moving slowly means irrelevance -- and irrelevant companies don't influence how AI develops. These factions don't necessarily disagree about the importance of safety; they disagree about whether Anthropic's current approach actually promotes it.\n\nThe cultural tension has real consequences for how work gets done. Safety researchers describe feeling increasingly sidelined as product timelines accelerate. They watch as features they've flagged as potentially dangerous get shipped anyway, with assurances that monitoring systems will catch any problems. They see the gap between Anthropic's public messaging about safety-first development and the internal reality of constant pressure to match competitors' capabilities. Some have left the company, disillusioned by what they see as a betrayal of founding principles. Others stay, hoping to influence the trajectory from within while growing increasingly pessimistic about their ability to do so.\n\nThe Regulatory Gambit\n\nAnthropic has positioned itself as a friendly face for AI regulation, with executives regularly testifying before Congress and participating in White House convenings on AI safety. This engagement serves multiple purposes: it reinforces the company's identity as a responsible actor, it potentially shapes regulations in ways favorable to Anthropic's approach, and it provides a form of insurance against being caught off-guard by sudden regulatory changes. But it also creates another layer of contradiction.\n\nWhen Anthropic advocates for AI safety regulations, is it genuinely trying to slow down dangerous development, or is it attempting to use regulation as a competitive weapon against rivals? Skeptics note that the specific regulations Anthropic tends to support -- transparency requirements, safety testing protocols, alignment research mandates -- are areas where the company has already invested heavily. Imposing these requirements on all AI developers could turn Anthropic's safety investments from a competitive disadvantage into a regulatory moat. The company's safety research, in this view, isn't an alternative to the AI race but a different strategy for winning it.\n\nThis interpretation may be overly cynical, but it highlights the difficulty of disentangling genuine safety concerns from strategic positioning. Even if Anthropic's leaders sincerely believe in the regulations they propose, those regulations would also happen to benefit their company. And the fact that Anthropic continues to develop increasingly capable systems while advocating for regulations that might constrain such development suggests that the company doesn't expect these regulations to actually slow it down significantly. The regulatory engagement becomes another form of safety theater: visible, reassuring, and ultimately ineffective at addressing the core dynamics driving AI acceleration.\n\nThe Measurement Problem\n\nOne of the most fundamental challenges facing Anthropic is that we lack good ways to measure whether its approach actually makes AI safer. The company publishes research on alignment techniques, implements testing protocols, and describes its constitutional AI framework. But do these measures meaningfully reduce existential risk, or do they simply make dangerous development feel more responsible? Without clear metrics, Anthropic's safety claims become unfalsifiable -- and therefore potentially meaningless.\n\nThe difficulty of measurement creates space for motivated reasoning on all sides. Anthropic can point to its safety research and testing protocols as evidence of responsible development, even if those measures don't actually prevent the risks they're designed to address. Critics can argue that any AI development at the current pace is reckless, regardless of what safety measures accompany it. And observers can project their own beliefs onto Anthropic's work, seeing either a genuine attempt to solve an impossible problem or an elaborate justification for business as usual.\n\nThis ambiguity serves Anthropic's commercial interests even as it undermines its safety mission. Investors can fund the company while believing they're supporting responsible AI development. Customers can use Claude while feeling good about choosing a more ethical alternative to ChatGPT. Employees can work on capabilities research while telling themselves they're contributing to safety. Everyone gets to feel virtuous while the underlying dynamics -- the race to build more powerful AI systems as quickly as possible -- continue unchanged.\n\nThe Structural Trap\n\nUltimately, Anthropic's contradictions may be less about the company's specific choices than about the structural impossibility of its mission. The company is trying to compete in a winner-take-all market while advocating for caution, to raise billions from investors expecting exponential returns while warning about existential risks, to attract top talent with competitive compensation while asking them to move more slowly than they could elsewhere. These goals are fundamentally in tension, perhaps fundamentally incompatible.\n\nThe problem isn't that Anthropic's leaders are hypocrites or that its employees don't genuinely care about safety. The problem is that the current structure of AI development creates incentives that overwhelm individual intentions. As long as AI capabilities translate directly into commercial value, as long as investors expect rapid progress, as long as the competitive dynamics reward speed over caution, any company that actually prioritizes safety will be outcompeted and rendered irrelevant. Anthropic's contradictions aren't failures of will; they're the inevitable result of trying to operate within a system whose rules make its stated mission impossible.\n\nThis structural analysis suggests that Anthropic's real contribution may not be in actually developing safer AI, but in making the impossibility of that goal visible. By positioning itself as the safety-conscious alternative and then finding itself forced to make the same compromises as everyone else, Anthropic demonstrates that individual corporate responsibility cannot solve a collective action problem. If even the company most committed to AI safety can't actually slow down, perhaps we need to look beyond corporate self-regulation for solutions.\n\nThe Path Forward Remains Unclear\n\nAs Anthropic expands its San Francisco offices and scales up its operations, the company faces a choice about its identity. It can continue trying to balance commercial success with safety leadership, accepting the contradictions and compromises that entails. It can abandon its safety positioning and compete directly on capabilities, joining the race it once claimed to resist. Or it could take a radically different approach: using its position and resources to advocate for industry-wide coordination mechanisms that could actually slow development across all labs simultaneously.\n\nThe third option would require Anthropic to potentially sacrifice its commercial interests for its stated mission -- proposing regulations or coordination frameworks that would constrain its own development as much as competitors'. It would mean treating AI safety as a genuine collective action problem requiring industry-wide solutions rather than a competitive advantage to be exploited. And it would force the company to confront the possibility that its current approach, for all its good intentions, may be making the problem worse by providing ethical cover for continued acceleration.\n\nThere's little indication that Anthropic is prepared to take such a radical step. The company's expansion plans, funding rounds, and product roadmap all point toward conventional competition with a safety-focused brand. The internal tensions described by The Atlantic will likely continue, with safety-minded researchers growing increasingly frustrated while the company's commercial trajectory continues upward. The contradictions between Anthropic's warnings and its actions will become more glaring as AI capabilities advance and the risks become more concrete.\n\nWhat remains to be seen is whether these contradictions will ultimately matter. If Anthropic's safety research genuinely makes a difference, even at the margins, perhaps the compromises are justified. If the company's existence pushes competitors to take safety more seriously, even slightly, perhaps its conflicted position serves a purpose. But if Anthropic's primary effect is to make dangerous AI development seem more responsible without actually changing the underlying dynamics, then its contradictions aren't just internal tensions -- they're a warning about the inadequacy of our current approach to one of humanity's most consequential challenges.\n\nThe phrase that opened this article -- \"things are moving uncomfortably fast\" -- captures both the urgency and the paralysis that define Anthropic's situation. The company knows the pace of AI development may be dangerous. It has built its identity around addressing that danger. But it finds itself unable or unwilling to actually slow down, caught in the same accelerating dynamics as everyone else. Whether that makes Anthropic a tragic figure, a hypocritical one, or simply a realistic one depends on whether you believe any company could do better within the current system. The answer to that question will determine not just Anthropic's legacy, but possibly humanity's future."
  },
  {
    "source": "ansarpress.com",
    "company": "Anthropic",
    "title": "Tensions between the Pentagon and AI giant Anthropic reach a boiling point",
    "date": "2026-02-20T22:04:26Z",
    "url": "https://www.ansarpress.com/EN/42431",
    "content": "A Pentagon spokesperson told that it is reviewing its relationship with Anthropic.\n\nDefense Secretary Pete Hegseth speaks at Blue Origin in Cape Canaveral, Florida, on Feb. 2.Miguel J. Rodriguez Carrillo / AFP via Getty Images\n\nOver the last week, tensions between the Pentagon and artificial intelligence giant Anthropic have reached a boiling point.\n\nAnthropic, the creator of the Claude chatbot system and a frontier AI company with a defense contract worth up to $200 million, has built its brand around the promotion of AI safety, touting red lines the company says it won't cross.\n\nNow, the Pentagon appears to be pushing those boundaries.\n\nHints of a possible rift between Anthropic and the Defense Department, now rebranded the Department of War, began to intensify after The Wall Street Journal and Axios reported the use of Anthropic products in the operation to capture Venezuelan President Nicolás Maduro.\n\nIt is unclear how Anthropic's Claude was used.\n\nAnthropic has not raised or found any violations of its policies in the wake of the Maduro operation, according to two people familiar with the matter, who asked to remain anonymous in order to discuss sensitive topics. They said that the company has high visibility into how its AI tool Claude is used, such as in data analysis operations.\n\nAnthropic was the first AI company allowed to offer services on classified networks, via Palantir, which partnered with it in 2024. Palantir said in an announcement of the partnership that Claude could be used \"to support government operations such as processing vast amounts of complex data rapidly\" and \"helping U.S. officials to make more informed decisions in time-sensitive situations.\"\n\nPalantir is one of the military's favored data and software contractors, for example collecting data from space sensors to provide better strike targeting for soldiers. It has also attracted scrutiny for its work under the Trump administration and law enforcement agencies.\n\nThough Anthropic has maintained that it does not and will not allow its AI systems to be directly used in lethal autonomous weapons or for domestic surveillance, the reported use of its technology related to the Venezuela raid through the contract with Palantir allegedly raised concerns from an Anthropic employee.\n\nAnthropic CEO Dario Amodei, right, and Chief Product Officer Mike Krieger talk after unveiling Claude 4 during the Code with Claude conference May 22, 2025, in San Francisco.Don Feria / AP Content Services for Anthropic\n\nSemafor reported Tuesday that, during a routine meeting between Anthropic and Palantir, a Palantir executive was worried that an Anthropic employee did not seem to agree with how its systems might have been used in the operation, leading to \"a rupture in Anthropic's relationship with the Pentagon.\"\n\nA senior Pentagon official told NBC News that \"a senior executive from Anthropic communicated with a senior Palantir executive, inquiring as to whether their software was used for the Maduro raid.\"\n\nAccording to the Pentagon official, the Palantir executive \"was alarmed that the question was raised in such a way to imply that Anthropic might disapprove of their software being used during that raid.\"\n\nCiting the classified nature of military operations, an Anthropic spokesperson would neither confirm nor deny that its Claude chatbot systems had been used in the Maduro operation: \"We cannot comment on whether Claude, or any other AI model, was used for any specific operation, classified or otherwise,\" the spokesperson told NBC News in a statement.\n\nThe spokesperson pushed back on the idea that the incident had caused notable fallout, telling NBC News the company had not held out-of-the-ordinary discussions about Claude usage with partners or shared any mission-related disagreements with the military.\n\n\"Anthropic has not discussed the use of Claude for specific operations with the Department of War,\" the spokesperson said. \"We have also not discussed this with, or expressed concerns to, any industry partners outside of routine discussions on strictly technical matters.\"\n\nPalantir did not reply to a request for comment.\n\nThe core tension between Anthropic and the Defense Department appears to be rooted in a broader clash over the military's future use of Anthropic's systems. The Defense Department has recently emphasized its desire to be able to use all available AI systems for any purpose allowed by law, while Anthropic says it wants to maintain its own guardrails.\n\nChief spokesman for the Pentagon Sean Parnell told NBC News that \"The Department of War's relationship with Anthropic is being reviewed.\"\n\n\"Our nation requires that our partners be willing to help our warfighters win in any fight,\" he said in a statement.\n\n\"Ultimately, this is about our troops and the safety of the American people.\" On Tuesday, Undersecretary of Defense Emil Michael said that the department's negotiations with Anthropic had hit a snag over a disagreement over potential uses of its systems, according to CNBC.\n\nIn early January, Defense Secretary Pete Hegseth released a new AI strategy document that called for any contracts with AI companies to eliminate company-specific guardrails or constraints on how the military can use companies' AI systems, newly allowing \"any lawful use\" of AI for Defense Department purposes.\n\nThe document called for defense officials to incorporate this language into any Defense Department AI contract within 180 days, which would implicate Anthropic's dealings with the military.\n\nWhile Anthropic has broadly supported the use of its services for national security purposes, it has maintained that its systems not be used for domestic surveillance or in fully autonomous weapons.\n\nThe Defense Department has balked at Anthropic's insistence on these two issues and applied increasing pressure to the company.\n\n\"Claude is used for a wide variety of intelligence-related use cases across the government, including the Department of War, in line with our Usage Policy,\" the Anthropic spokesperson said. \"We are having productive conversations, in good faith, with the Department of War on how to continue that work and get these complex issues right.\"\n\nRelative to other AI companies, Anthropic has prioritized enterprise and national security applications of its AI systems. In August 2025, Anthropic formed a national security and public sector advisory council composed of former senior defense and intelligence officials and last week added Chris Lidell, President Donald Trump's former deputy chief of staff, to its board of directors.\n\nAnthropic has partnered with Palantir since late 2024 to provide U.S. defense and intelligence agencies with access to various Claude systems. At the time, Anthropic's head of sales and partnerships, Kate Earle Jensen, said the company was \"proud to be at the forefront of bringing responsible AI solutions to U.S. classified environments, enhancing analytical capabilities and operational efficiencies in vital government operations.\"\n\nAnthropic, along with other leading American AI companies such as OpenAI and Google, signed individual two-year contracts with the Defense Department in July 2025, each worth up to $200 million to help \"prototype frontier AI capabilities that advance U.S. national security.\"\n\n\"Anthropic is committed to using frontier AI in support of US national security,\" the Anthropic spokesperson told NBC News in a statement. \"We were the first frontier AI company to put our models on classified networks and the first to provide customized models for national security customers.\"\n\nAnthropic CEO Dario Amodei has routinely emphasized Anthropic's commitment to using its AI services for national security purposes. In an essay published in late January, Amodei wrote that \"democracies have a legitimate interest in some AI-powered military and geopolitical tools,\" and that \"we should arm democracies with AI, but we should do so carefully and within limits.\"\n\nMichael Horowitz, who led AI and emerging technology policy in the Pentagon and is now a professor of political science at the University of Pennsylvania, said that any concerns about use of Anthropic systems for active engagement in lethal autonomous weapons would likely be irrelevant to current negotiations given the type of systems Anthropic is developing.\n\n\"I would be surprised if Anthropic models were the right ones to use for lethal autonomous weapon systems right now, since the algorithms for that will be more bespoke than Claude's,\" Horowitz told NBC News.\n\n\"My sense is that Anthropic wants to increase the depth and scope of their work with the Pentagon. Based on what we know, this sounds like a dispute more over theoretical possibilities than real-world use cases on the table.\"\n\n_____________________________\n\nBy Jared Perlo and Gordon Lubold/NBC"
  },
  {
    "source": "凤凰网（凤凰新媒体）",
    "company": "Anthropic",
    "title": "Anthropic遭遇OpenAI贴身肉搏，上市路多了个坎",
    "date": "2026-01-29T09:22:24Z",
    "url": "https://tech.ifeng.com/c/8qJPWRkchfU",
    "content": "Anthropic的CEO达里奥·阿莫迪这两天执笔了一篇2万字的长文，他写到\"人类即将获得几乎难以想象的力量，而我们的社会、政治和技术体系是否具备驾驭这种力量的成熟度，目前还非常不明朗。\"\n\n阿莫迪认为，最多再过几年，AI会在几乎所有方面都将优于人类，而且这是无可避免的。\n\n与此同时他和联合创始人做出了一个在硅谷几乎前所未有的承诺，他们将捐出80%的个人财富用于慈善事业，Anthropic的员工也承诺捐出公司股份。\n\n无巧不成书，刚刚做出慈善承诺的阿莫迪，就受到了来自OpenAI的产品组合拳。\n\n放在以往，这并不会引起Anthropic多么大的反应。但现如今不同，此时此刻，正是Anthropic和OpenAI的融资竞赛。\n\n两家全球最顶尖的AI公司同时发起融资，额度均为上百亿美元，且这笔融资对于Anthropic来说，还是他们上市前最后一笔融资，不能由得半点马虎。\n\nAnthropic唯一的选择就是接招，要么加速融资，要么加速产品迭代。\n\n然而OpenAI这次的攻势并非大范围火力覆盖，而是对Anthropic最核心的业务发起精准打击，以至于留给Anthropic证明自己的时间窗口正在急剧收窄。\n\n01\n\nOpenAI系列产品发布\n\n2026年1月27日，OpenAI正式推出了Prism，这是一个由GPT-5.2驱动的AI原生科研工作空间。\n\nPrism是一个深度整合了LaTeX编辑、文献检索、公式管理、协作评审于一体的云端科研平台。\n\n它的核心价值在于将科研写作过程中原本碎片化的工具链整合到单一工作流中，让GPT-5.2能够在完整的论文上下文中工作，理解公式、引用、图表以及整体结构之间的关联关系。\n\n写过理工科论文的一定体会过，过去你需要先在word里写，然后要切换到Overleaf编LaTeX 公式，再打开Zotero管理参考文献，接下来要花好几个小时画图表。\n\n而现在，Prism一个平台就可以搞定上述所有流程，还引入ChatGPT，通过AI来获取信息。\n\nPrism建立在OpenAI收购云端LaTeX平台Crixet的基础之上。\n\n通过将Crixet成熟的技术与GPT-5.2的推理能力深度融合，Prism实现了多项突破性功能。\n\n不只是这样，它还能直接从arXiv等学术平台检索相关文献，并根据论文语境自动生成参考文献列表。更重要的是，它支持无限数量的项目和协作者，完全免费向所有ChatGPT个人账户用户开放。\n\nOpenAI科学副总裁凯文·威尔（Kevin Weil）在发布会上做出了一个大胆的预言：\"我认为2026年之于AI与科学，将如同2025年之于AI与软件工程。\"\n\n科研和AI编程是OpenAI今年的大方向。\n\n说到AI编程，目前OpenAI正处于Codex发布月，会在1月24日到2月24日期间持续发布Codex相关产品。\n\n其实从2025年下半年开始，OpenAI就一直在持续强化其开发者工具生态。Codex已经从最初的独立API演变为一个完整的开发者平台，包括Codex CLI命令行工具、Codex Web云端自主编码代理以及IDE集成插件。\n\n根据OpenAI的官方消息，Codex的使用量自2025年8月以来，日均消息量增长10倍，每周处理数万亿个token。\n\n与此同时，OpenAI内部工程师使用Codex的占比，也从7月的50%升到了100%，每周合并PR数量增加70%、\n\n相当于绝大多数用Codex编写的代码，都通过了质量验证，并且这些代码都实际应用到了产品开发之中。\n\nOpenAI还通过开放API的方式，让Codex模型能够集成到Cursor、Windsurf、Factory和GitHub等主流开发工具中，形成了一个相对完整的生态系统。\n\nOpenAI正在从\"大而全\"的通用AI工具，向垂直领域的深度整合转型。\n\nCodex针对软件开发者，Prism针对科研人员，OpenAI的战略意图十分明确，要垂直化抢占高价值和专业用户市场。\n\n这种转变并非偶然，而是基于对市场需求的深刻洞察。通用型聊天机器人虽然用户基数庞大，但用户粘性和付费意愿相对较低。\n\n相比之下，专业领域的用户对工具的依赖度更高，付费能力也更强，更容易形成稳定的商业模式。\n\n从产品发布的节奏来看，OpenAI显然经过了精心策划。Prism的推出时机选择在1月底，正值学术界新学期开始，许多科研项目进入启动阶段。\n\nCodex发布月横跨整个2月，恰好覆盖了企业年度预算确定和Q1技术采购决策高峰期。\n\n更值得关注的是，OpenAI在推出这些垂直化产品时，并没有采取高价策略。\n\n这种策略的目的很明确，先通过免费或低价快速占领市场，培养用户习惯，然后再通过企业版和高级功能实现商业化。\n\n02\n\n瞄准Anthropic，贴身肉搏\n\n当我们把视角拉远就会发现，OpenAI推出Prism和强化Codex生态，其战略矛头直指Anthropic最核心的竞争优势，深度智力工作场景。\n\n长期以来，AI圈内存在一种默认共识，ChatGPT适合大众日常使用和闲聊，Claude更适合写代码、读论文和做科研。\n\nAnthropic凭借更严谨的逻辑推理以及\"少废话\"的风格，牢牢占据了科研人员、程序员和深度内容创作者的心智。\n\n许多开发者在社区分享使用心得时，都会提到Claude在处理复杂任务时的表现更加稳定，输出的内容更加精准，不会像ChatGPT那样经常出现\"谄媚\"或\"过度发挥\"的情况。\n\nAnthropic曾在去年10月推出\"Claude for Life Sciences\"，这是一个专门为医疗、科研领域优化模型。\n\n1月12日，Anthropic又为这条产品线带来了多项更新，深度集成了Medidata、ClinicalTrials.gov、OpenTargets等平台，能够自动化临床试验操作和监管提交流程。\n\n多家制药公司和研究机构已经开始使用Claude来加速药物研发流程，从文献综述到实验方案设计，再到监管文件准备，Claude都能提供实质性的帮助。\n\n因此，Prism几乎是贴身肉搏，OpenAI就差明着布告说自己是冲着Anthropic去的。\n\nOpenAI不希望在\"AI for Science\"这个未来最重要的增长点上输给Anthropic。\n\n科研领域的市场规模虽然不如消费级应用，但其战略价值不容小觑。\n\n掌握了科研工具的话语权，就意味着能够影响知识生产的方式，进而影响整个社会的创新能力。\n\n更重要的是，科研用户往往是技术的早期采用者和意见领袖，他们的选择会对其他用户群体产生示范效应。\n\nAnthropic之前的杀手锏是Artifacts功能，它把AI从\"一问一答\"的聊天机器人变成了能实时生成代码、预览文档、管理上下文的\"生产力伙伴\"。\n\n用户可以在对话过程中直接看到代码运行结果，可以实时修改和调试，整个工作流程变得非常流畅。\n\n这种体验上的优势，让Claude在开发者群体中建立了良好的口碑。OpenAI虽然推出了Canvas作为回应，但并未完全改变局面。\n\n但这次不一样，Prism集成了科研写作、数据分析和推理协作，直接拆解了Anthropic在长程任务上的优势。以及接下来一个月内数款Codex产品。\n\nOpenAI想用这些产品，向高价值用户传递这么一个信息\"Claude能做的我们ChatGPT能做，Claude做不了的我们ChatGPT也能做。\"\n\n这种全方位的竞争姿态，给Anthropic带来了巨大的压力。过去，Anthropic可以专注于做好自己擅长的事情，在细分领域建立优势。但现在，OpenAI开始在每个细分领域都推出针对性的产品，Anthropic的生存空间被不断压缩。\n\nOpenAI入局，让这场战争从\"对话框\"升级到了\"工作流\"。\n\n过去的竞争主要集中在模型能力上，谁的回答更准确、更流畅、更符合用户期望。现在的竞争则转向了产品化能力，谁能把AI更好地嵌入到用户的实际工作流程中，谁就能赢得市场。\n\n这种转变对Anthropic来说是个挑战，因为OpenAI在产品化和生态建设方面有更多的资源和经验。\n\n在\"严谨性\"这个Anthropic引以为傲的领域，OpenAI也在发起挑战。\n\nAnthropic一直标榜\"HHH\"原则，即Helpful、Honest、Harmless，在安全性及减少幻觉方面口碑较好，这对科研用户至关重要。\n\n没有人愿意在论文中引用一个AI编造的文献，也没有人愿意基于AI生成的错误代码进行开发。\n\n但OpenAI试图在\"严谨性\"上反超Claude，至少从Prism上来看是这样的。这等于是在攻击Anthropic\"最安全、最可信\"的品牌形象。\n\n此外，OpenAI很可能会开始跟Anthropic打价格战。\n\nAnthropic目前一个非常大的痛点就是太贵了。Claude Code成本非常高，重度用户一天单哪怕是最高级的订阅也经常出现达到上限的情况，因此被迫需要开多个账号才能满足日常需求。\n\n03\n\n融资数额突破历史\n\n资本市场正在见证AI领域有史以来最激烈的融资竞赛。\n\n2026年1月，Anthropic将其最新一轮融资目标从100亿美元大幅提高至200亿美元，融资后估值预计达到3500亿美元。这个估值相比2025年9月的1830亿美元，在短短四个月内几乎翻倍。\n\n这轮融资由新加坡主权财富基金GIC和CoatueManagement领投，红杉资本、微软、英伟达等豪华阵容参与。\n\n红杉资本同时也是OpenAI的投资方，这种\"脚踏两只船\"的投资策略，在硅谷并不罕见，但也反映出资本对AI赛道的谨慎态度，不愿意把鸡蛋放在一个篮子里。\n\n虽然Anthropic融得数额大，但是他们的生存压力同样也挺大的。\n\n该公司2026年的营收目标从150亿美元上调至180亿美元，对比2025年的47亿美元营收，增长近四倍。2027年更是预期营收约550亿美元，这个增长速度在整个科技行业都算是疯狂的。\n\n然而，Anthropic将盈利时间从2027年推迟至2028年，比预期晚了一年。这意味着公司在未来两年内仍然需要持续烧钱，资金主要用于Claude5模型训练和全球算力基础设施扩建，这些都是烧钱的无底洞。\n\n随着模型规模的扩大和训练数据的增加，所需的算力呈指数级增长，这就导致训练大型语言模型的成本正在快速上升。\n\nAnthropic之前曾表示，要在美国投入500亿美元建设数据中心，这个数字听起来惊人，但在当前的AI竞赛中，这可能只是起步价。\n\n数据中心的建设不仅需要巨额资金，还需要大量的电力供应和冷却系统，这些基础设施的建设周期往往需要数年时间。\n\n在这个过程中，Anthropic要一直保持足够的资金支撑，否则就可能面临资金链断裂的风险。\n\n实际上OpenAI这边的融资规模更加惊人。\n\n1月28日传出消息，软银正在向OpenAI追加300亿美元投资，而且这还只是阶段性目标，长期只会投入更多。\n\n在此之前，软银曾在2025年12月投了OpenAI410亿美元。\n\n但是这笔融资对于目前的OpenAI来说还远远不够，他们正在寻求总计高达1000亿美元的融资，估值可能达到8300亿美元。\n\n亚马逊和英伟达也在参与这轮融资的讨论。\n\n这个融资规模如果成功，将创下科技行业的历史纪录。\n\n之所以Anthropic和OpenAI都在扩大融资，其原因很简单，两家公司都希望能在2026或者2027年上市。\n\n本来一帆风顺的Anthropic，突然和OpenAI开始碰撞，那么将会大大阻碍其融资乃至上市的进度，因为这会直接降低其市场份额和预期收入。\n\n投资者在评估一家公司的价值时，不仅看其当前的表现，更看其未来的增长潜力。\n\n如果市场认为Anthropic在与OpenAI的竞争中处于下风，其估值就会受到影响。这种影响不仅体现在融资估值上，更会影响到未来的IPO定价。\n\nAnthropic已经聘请知名律所WilsonSonsini启动IPO准备工作，并与多家投资银行进行初步沟通，最快可能在2026年下半年上市。\n\nWilsonSonsini是硅谷最负盛名的律所之一，曾经协助谷歌和LinkedIn等科技巨头完成上市。\n\n摆在Anthropic前只有两条路。\n\n第一条路是在OpenAI占领这些垂直市场之前完成上市，锁定估值，否则可能面临估值大幅缩水的风险。\n\n上市能够为公司带来大量资金，也能为早期投资者提供退出渠道。但上市也意味着公司将面临更严格的监管和信息披露要求，每个季度的财务表现都会受到市场的审视。\n\n第二条路是通过产品正面击败OpenAI。\n\n这条路更加艰难，但如果成功，回报也更大。\n\nAnthropic需要在产品功能、用户体验、生态建设等多个方面全面超越OpenAI，才能在竞争中占据优势。\n\n这需要大量的研发投入和时间，但时间恰恰是Anthropic最缺乏的资源。在快速变化的AI市场中，每一天的延迟都可能意味着市场份额的流失。\n\n从融资节奏来看，Anthropic正在加速。从2024年3月的35亿美元融资，到9月的130亿美元融资，再到现在的200亿美元融资目标，融资规模在不断扩大，融资间隔在不断缩短。\n\n这种加速度反映出公司对资金的迫切需求，也反映出市场对AI赛道的持续看好。\n\n但融资只是手段，不是目的。最终决定胜负的，还是产品和技术。无论融到多少钱，如果产品无法满足用户需求，市场份额就会流失。\n\n在当前的AI竞赛中，技术迭代速度极快，今天的领先优势可能明天就会被超越。Anthropic和OpenAI都明白这个道理。\n\n这场竞赛的结果，也将在很大程度上决定AI行业未来的格局。"
  },
  {
    "source": "tmtpost.com",
    "company": "Anthropic",
    "title": "Anthropic遭遇OpenAI贴身肉搏，上市路多了个坎-钛媒体官方网站",
    "date": "2026-01-29T12:54:45Z",
    "url": "https://www.tmtpost.com/7860246.html",
    "content": "Prism到Codex，OpenAI对Anthropic核心腹地的精准打击。\n\nAnthropic的CEO达里奥·阿莫迪这两天执笔了一篇2万字的长文，他写到\"人类即将获得几乎难以想象的力量，而我们的社会、政治和技术体系是否具备驾驭这种力量的成熟度，目前还非常不明朗。\"\n\n阿莫迪认为，最多再过几年，AI会在几乎所有方面都将优于人类，而且这是无可避免的。\n\n与此同时他和联合创始人做出了一个在硅谷几乎前所未有的承诺，他们将捐出80%的个人财富用于慈善事业，Anthropic的员工也承诺捐出公司股份。\n\n无巧不成书，刚刚做出慈善承诺的阿莫迪，就受到了来自OpenAI的产品组合拳。\n\n放在以往，这并不会引起Anthropic多么大的反应。但现如今不同，此时此刻，正是Anthropic和OpenAI的融资竞赛。\n\n两家全球最顶尖的AI公司同时发起融资，额度均为上百亿美元，且这笔融资对于Anthropic来说，还是他们上市前最后一笔融资，不能由得半点马虎。\n\nAnthropic唯一的选择就是接招，要么加速融资，要么加速产品迭代。\n\n然而OpenAI这次的攻势并非大范围火力覆盖，而是对Anthropic最核心的业务发起精准打击，以至于留给Anthropic证明自己的时间窗口正在急剧收窄。\n\n2026年1月27日，OpenAI正式推出了Prism，这是一个由GPT-5.2驱动的AI原生科研工作空间。\n\nPrism是一个深度整合了LaTeX编辑、文献检索、公式管理、协作评审于一体的云端科研平台。\n\n它的核心价值在于将科研写作过程中原本碎片化的工具链整合到单一工作流中，让GPT-5.2能够在完整的论文上下文中工作，理解公式、引用、图表以及整体结构之间的关联关系。\n\n写过理工科论文的一定体会过，过去你需要先在word里写，然后要切换到Overleaf编LaTeX 公式，再打开Zotero管理参考文献，接下来要花好几个小时画图表。\n\n而现在，Prism一个平台就可以搞定上述所有流程，还引入ChatGPT，通过AI来获取信息。\n\nPrism建立在OpenAI收购云端LaTeX平台Crixet的基础之上。\n\n通过将Crixet成熟的技术与GPT-5.2的推理能力深度融合，Prism实现了多项突破性功能。\n\n不只是这样，它还能直接从arXiv等学术平台检索相关文献，并根据论文语境自动生成参考文献列表。更重要的是，它支持无限数量的项目和协作者，完全免费向所有ChatGPT个人账户用户开放。\n\nOpenAI科学副总裁凯文·威尔（Kevin Weil）在发布会上做出了一个大胆的预言：\"我认为2026年之于AI与科学，将如同2025年之于AI与软件工程。\"\n\n科研和AI编程是OpenAI今年的大方向。\n\n说到AI编程，目前OpenAI正处于Codex发布月，会在1月24日到2月24日期间持续发布Codex相关产品。\n\n其实从2025年下半年开始，OpenAI就一直在持续强化其开发者工具生态。Codex已经从最初的独立API演变为一个完整的开发者平台，包括Codex CLI命令行工具、Codex Web云端自主编码代理以及IDE集成插件。\n\n根据OpenAI的官方消息，Codex的使用量自2025年8月以来，日均消息量增长10倍，每周处理数万亿个token。\n\n与此同时，OpenAI内部工程师使用Codex的占比，也从7月的50%升到了100%，每周合并PR数量增加70%、\n\n相当于绝大多数用Codex编写的代码，都通过了质量验证，并且这些代码都实际应用到了产品开发之中。\n\nOpenAI还通过开放API的方式，让Codex模型能够集成到Cursor、Windsurf、Factory和GitHub等主流开发工具中，形成了一个相对完整的生态系统。\n\nOpenAI正在从\"大而全\"的通用AI工具，向垂直领域的深度整合转型。\n\nCodex针对软件开发者，Prism针对科研人员，OpenAI的战略意图十分明确，要垂直化抢占高价值和专业用户市场。\n\n这种转变并非偶然，而是基于对市场需求的深刻洞察。通用型聊天机器人虽然用户基数庞大，但用户粘性和付费意愿相对较低。\n\n相比之下，专业领域的用户对工具的依赖度更高，付费能力也更强，更容易形成稳定的商业模式。\n\n从产品发布的节奏来看，OpenAI显然经过了精心策划。Prism的推出时机选择在1月底，正值学术界新学期开始，许多科研项目进入启动阶段。\n\nCodex发布月横跨整个2月，恰好覆盖了企业年度预算确定和Q1技术采购决策高峰期。\n\n更值得关注的是，OpenAI在推出这些垂直化产品时，并没有采取高价策略。\n\n这种策略的目的很明确，先通过免费或低价快速占领市场，培养用户习惯，然后再通过企业版和高级功能实现商业化。\n\n当我们把视角拉远就会发现，OpenAI推出Prism和强化Codex生态，其战略矛头直指Anthropic最核心的竞争优势，深度智力工作场景。\n\n长期以来，AI圈内存在一种默认共识，ChatGPT适合大众日常使用和闲聊，Claude更适合写代码、读论文和做科研。\n\nAnthropic凭借更严谨的逻辑推理以及\"少废话\"的风格，牢牢占据了科研人员、程序员和深度内容创作者的心智。\n\n许多开发者在社区分享使用心得时，都会提到Claude在处理复杂任务时的表现更加稳定，输出的内容更加精准，不会像ChatGPT那样经常出现\"谄媚\"或\"过度发挥\"的情况。\n\nAnthropic曾在去年10月推出\"Claude for Life Sciences\"，这是一个专门为医疗、科研领域优化模型。\n\n1月12日，Anthropic又为这条产品线带来了多项更新，深度集成了Medidata、ClinicalTrials.gov、OpenTargets等平台，能够自动化临床试验操作和监管提交流程。\n\n多家制药公司和研究机构已经开始使用Claude来加速药物研发流程，从文献综述到实验方案设计，再到监管文件准备，Claude都能提供实质性的帮助。\n\n因此，Prism几乎是贴身肉搏，OpenAI就差明着布告说自己是冲着Anthropic去的。\n\nOpenAI不希望在\"AI for Science\"这个未来最重要的增长点上输给Anthropic。\n\n科研领域的市场规模虽然不如消费级应用，但其战略价值不容小觑。\n\n掌握了科研工具的话语权，就意味着能够影响知识生产的方式，进而影响整个社会的创新能力。\n\n更重要的是，科研用户往往是技术的早期采用者和意见领袖，他们的选择会对其他用户群体产生示范效应。\n\nAnthropic之前的杀手锏是Artifacts功能，它把AI从\"一问一答\"的聊天机器人变成了能实时生成代码、预览文档、管理上下文的\"生产力伙伴\"。\n\n用户可以在对话过程中直接看到代码运行结果，可以实时修改和调试，整个工作流程变得非常流畅。\n\n这种体验上的优势，让Claude在开发者群体中建立了良好的口碑。OpenAI虽然推出了Canvas作为回应，但并未完全改变局面。\n\n但这次不一样，Prism集成了科研写作、数据分析和推理协作，直接拆解了Anthropic在长程任务上的优势。以及接下来一个月内数款Codex产品。\n\nOpenAI想用这些产品，向高价值用户传递这么一个信息\"Claude能做的我们ChatGPT能做，Claude做不了的我们ChatGPT也能做。\"\n\n这种全方位的竞争姿态，给Anthropic带来了巨大的压力。过去，Anthropic可以专注于做好自己擅长的事情，在细分领域建立优势。但现在，OpenAI开始在每个细分领域都推出针对性的产品，Anthropic的生存空间被不断压缩。\n\nOpenAI入局，让这场战争从\"对话框\"升级到了\"工作流\"。\n\n过去的竞争主要集中在模型能力上，谁的回答更准确、更流畅、更符合用户期望。现在的竞争则转向了产品化能力，谁能把AI更好地嵌入到用户的实际工作流程中，谁就能赢得市场。\n\n这种转变对Anthropic来说是个挑战，因为OpenAI在产品化和生态建设方面有更多的资源和经验。\n\n在\"严谨性\"这个Anthropic引以为傲的领域，OpenAI也在发起挑战。\n\nAnthropic一直标榜\"HHH\"原则，即Helpful、Honest、Harmless，在安全性及减少幻觉方面口碑较好，这对科研用户至关重要。\n\n没有人愿意在论文中引用一个AI编造的文献，也没有人愿意基于AI生成的错误代码进行开发。\n\n但OpenAI试图在\"严谨性\"上反超Claude，至少从Prism上来看是这样的。这等于是在攻击Anthropic\"最安全、最可信\"的品牌形象。\n\n此外，OpenAI很可能会开始跟Anthropic打价格战。\n\nAnthropic目前一个非常大的痛点就是太贵了。Claude Code成本非常高，重度用户一天单哪怕是最高级的订阅也经常出现达到上限的情况，因此被迫需要开多个账号才能满足日常需求。\n\n资本市场正在见证AI领域有史以来最激烈的融资竞赛。\n\n2026年1月，Anthropic将其最新一轮融资目标从100亿美元大幅提高至200亿美元，融资后估值预计达到3500亿美元。这个估值相比2025年9月的1830亿美元，在短短四个月内几乎翻倍。\n\n这轮融资由新加坡主权财富基金GIC和CoatueManagement领投，红杉资本、微软、英伟达等豪华阵容参与。\n\n红杉资本同时也是OpenAI的投资方，这种\"脚踏两只船\"的投资策略，在硅谷并不罕见，但也反映出资本对AI赛道的谨慎态度，不愿意把鸡蛋放在一个篮子里。\n\n虽然Anthropic融得数额大，但是他们的生存压力同样也挺大的。\n\n该公司2026年的营收目标从150亿美元上调至180亿美元，对比2025年的47亿美元营收，增长近四倍。2027年更是预期营收约550亿美元，这个增长速度在整个科技行业都算是疯狂的。\n\n然而，Anthropic将盈利时间从2027年推迟至2028年，比预期晚了一年。这意味着公司在未来两年内仍然需要持续烧钱，资金主要用于Claude5模型训练和全球算力基础设施扩建，这些都是烧钱的无底洞。\n\n随着模型规模的扩大和训练数据的增加，所需的算力呈指数级增长，这就导致训练大型语言模型的成本正在快速上升。\n\nAnthropic之前曾表示，要在美国投入500亿美元建设数据中心，这个数字听起来惊人，但在当前的AI竞赛中，这可能只是起步价。\n\n数据中心的建设不仅需要巨额资金，还需要大量的电力供应和冷却系统，这些基础设施的建设周期往往需要数年时间。\n\n在这个过程中，Anthropic要一直保持足够的资金支撑，否则就可能面临资金链断裂的风险。\n\n实际上OpenAI这边的融资规模更加惊人。\n\n1月28日传出消息，软银正在向OpenAI追加300亿美元投资，而且这还只是阶段性目标，长期只会投入更多。\n\n在此之前，软银曾在2025年12月投了OpenAI410亿美元。\n\n但是这笔融资对于目前的OpenAI来说还远远不够，他们正在寻求总计高达1000亿美元的融资，估值可能达到8300亿美元。\n\n亚马逊和英伟达也在参与这轮融资的讨论。\n\n这个融资规模如果成功，将创下科技行业的历史纪录。\n\n之所以Anthropic和OpenAI都在扩大融资，其原因很简单，两家公司都希望能在2026或者2027年上市。\n\n本来一帆风顺的Anthropic，突然和OpenAI开始碰撞，那么将会大大阻碍其融资乃至上市的进度，因为这会直接降低其市场份额和预期收入。\n\n投资者在评估一家公司的价值时，不仅看其当前的表现，更看其未来的增长潜力。\n\n如果市场认为Anthropic在与OpenAI的竞争中处于下风，其估值就会受到影响。这种影响不仅体现在融资估值上，更会影响到未来的IPO定价。\n\nAnthropic已经聘请知名律所WilsonSonsini启动IPO准备工作，并与多家投资银行进行初步沟通，最快可能在2026年下半年上市。\n\nWilsonSonsini是硅谷最负盛名的律所之一，曾经协助谷歌和LinkedIn等科技巨头完成上市。\n\n摆在Anthropic前只有两条路。\n\n第一条路是在OpenAI占领这些垂直市场之前完成上市，锁定估值，否则可能面临估值大幅缩水的风险。\n\n上市能够为公司带来大量资金，也能为早期投资者提供退出渠道。但上市也意味着公司将面临更严格的监管和信息披露要求，每个季度的财务表现都会受到市场的审视。\n\n第二条路是通过产品正面击败OpenAI。\n\n这条路更加艰难，但如果成功，回报也更大。\n\nAnthropic需要在产品功能、用户体验、生态建设等多个方面全面超越OpenAI，才能在竞争中占据优势。\n\n这需要大量的研发投入和时间，但时间恰恰是Anthropic最缺乏的资源。在快速变化的AI市场中，每一天的延迟都可能意味着市场份额的流失。\n\n从融资节奏来看，Anthropic正在加速。从2024年3月的35亿美元融资，到9月的130亿美元融资，再到现在的200亿美元融资目标，融资规模在不断扩大，融资间隔在不断缩短。\n\n这种加速度反映出公司对资金的迫切需求，也反映出市场对AI赛道的持续看好。\n\n但融资只是手段，不是目的。最终决定胜负的，还是产品和技术。无论融到多少钱，如果产品无法满足用户需求，市场份额就会流失。\n\n在当前的AI竞赛中，技术迭代速度极快，今天的领先优势可能明天就会被超越。Anthropic和OpenAI都明白这个道理。\n\n这场竞赛的结果，也将在很大程度上决定AI行业未来的格局。"
  },
  {
    "source": "Beritaja",
    "company": "Anthropic",
    "title": "Tensions Between The Pentagon And Ai Giant Anthropic Reach A Boiling Point",
    "date": "2026-02-20T18:14:44Z",
    "url": "https://beritaja.com/tensions-between-the-pentagon-and-ai-giant-anthropic-reach-a-boiling-point-beritaja-406230.html",
    "content": "BERITAJA is a International-focused news website dedicated to reporting current events and trending stories from across the country. We publish news coverage on local and national issues, politics, business, technology, and community developments. Content is curated and edited to ensure clarity and relevance for our readers.\n\nOver the past week, tensions betwixt the Pentagon and artificial intelligence elephantine Anthropic person reached a boiling point.\n\nAnthropic, the creator of the Claude chatbot strategy and a frontier AI institution pinch a defense statement worthy up to $200 million, has built its marque about the promotion of AI safety, touting reddish lines the institution says it won't cross.\n\nNow, the Pentagon appears to beryllium pushing those boundaries.\n\nHints of a imaginable rift betwixt Anthropic and the Defense Department, now rebranded the Department of War, began to intensify aft The Wall Street Journal and Axios reported the usage of Anthropic products successful the cognition to seizure Venezuelan President Nicolás Maduro.\n\nIt is unclear really Anthropic's Claude was used.\n\nAnthropic has not raised aliases recovered immoderate violations of its policies successful the aftermath of the Maduro operation, according to 2 group acquainted pinch the matter, who asked to stay anonymous successful bid to talk delicate topics. They said that the institution has precocious visibility into really its AI instrumentality Claude is used, specified arsenic successful information study operations.\n\nAnthropic was the first AI institution allowed to connection services connected classified networks, via Palantir, which collaborated pinch it successful 2024. Palantir said successful an announcement of the business that Claude could beryllium utilized \"to support authorities operations specified arsenic processing immense amounts of analyzable information rapidly\" and \"helping U.S. officials to make much informed decisions successful time-sensitive situations.\"\n\nPalantir is 1 of the military's favored information and software contractors, for illustration collecting information from space sensors to supply amended onslaught targeting for soldiers. It has besides attracted scrutiny for its activity nether the Trump management and rule enforcement agencies.\n\nThough Anthropic has maintained that it does not and will not let its AI systems to beryllium straight utilized successful lethal autonomous weapons or for home surveillance, the reported usage of its exertion related to the Venezuela raid done the statement pinch Palantir allegedly raised concerns from an Anthropic employee.\n\nSemafor reported Tuesday that, during a regular gathering betwixt Anthropic and Palantir, a Palantir executive was worried that an Anthropic worker did not look to work together pinch really its systems mightiness person been utilized successful the operation, starring to \"a rupture successful Anthropic's narration pinch the Pentagon.\"\n\nA elder Pentagon charismatic told Beritaja that \"a elder executive from Anthropic communicated pinch a elder Palantir executive, inquiring arsenic to whether their package was utilized for the Maduro raid.\"\n\nAccording to the Pentagon official, the Palantir executive \"was alarmed that the mobility was raised successful specified a measurement to connote that Anthropic mightiness disapprove of their package being utilized during that raid.\"\n\nCiting the classified quality of subject operations, an Anthropic spokesperson would neither corroborate nor contradict that its Claude chatbot systems had been utilized successful the Maduro operation: \"We cannot remark connected whether Claude, aliases immoderate different AI model, was utilized for immoderate circumstantial operation, classified aliases otherwise,\" the spokesperson told Beritaja successful a statement.\n\nThe spokesperson pushed backmost connected the thought that the incident had caused notable fallout, telling Beritaja the institution had not held out-of-the-ordinary discussions about Claude usage pinch partners aliases shared immoderate mission-related disagreements pinch the military.\n\n\"Anthropic has not discussed the usage of Claude for circumstantial operations pinch the Department of War,\" the spokesperson said. \"We person besides not discussed this with, aliases expressed concerns to, immoderate manufacture partners extracurricular of regular discussions connected strictly method matters.\"\n\nPalantir did not reply to a petition for comment.\n\nThe halfway hostility betwixt Anthropic and the Defense Department appears to beryllium rooted successful a broader conflict complete the military's early usage of Anthropic's systems. The Defense Department has precocious emphasized its desire to beryllium capable to usage each available AI systems for immoderate intent allowed by law, while Anthropic says it wants to support its ain guardrails.\n\nChief spokesperson for the Pentagon Sean Parnell told Beritaja that \"The Department of War's narration pinch Anthropic is being reviewed.\"\n\n\"Our federation requires that our partners beryllium consenting to thief our warfighters triumph successful immoderate fight,\" he said successful a statement.\n\n\"Ultimately, this is about our troops and the information of the American people.\" On Tuesday, Undersecretary of Defense Emil Michael said that the department's negotiations pinch Anthropic had deed a snag complete a disagreement complete imaginable uses of its systems, according to CNBC.\n\nIn early January, Defense Secretary Pete Hegseth released a caller AI strategy document that called for immoderate contracts pinch AI companies to destruct company-specific guardrails aliases constraints connected really the subject could usage companies' AI systems, recently allowing \"any lawful use\" of AI for Defense Department purposes.\n\nThe archive called for defense officials to incorporated this connection into immoderate Defense Department AI statement wrong 180 days, which would implicate Anthropic's dealings pinch the military.\n\nWhile Anthropic has broadly supported the usage of its services for nationalist information purposes, it has maintained that its systems not beryllium utilized for home surveillance aliases successful afloat autonomous weapons.\n\nThe Defense Department has balked astatine Anthropic's insistence connected these 2 issues and applied expanding unit to the company.\n\n\"Claude is utilized for a wide assortment of intelligence-related usage cases crossed the government, including the Department of War, successful statement pinch our Usage Policy,\" the Anthropic spokesperson said. \"We are having productive conversations, successful bully faith, pinch the Department of War connected really to proceed that activity and get these analyzable issues right.\"\n\nRelative to different AI companies, Anthropic has prioritized endeavor and nationalist information applications of its AI systems. In August 2025, Anthropic formed a nationalist information and nationalist assemblage advisory assembly composed of erstwhile elder defense and intelligence officials and last week added Chris Lidell, President Donald Trump's erstwhile lawman main of staff, to its committee of directors.\n\nAnthropic has collaborated pinch Palantir since precocious 2024 to supply U.S. defense and intelligence agencies pinch entree to various Claude systems. At the time, Anthropic's caput of income and partnerships, Kate Earle Jensen, said the institution was \"proud to beryllium astatine the forefront of bringing responsible AI solutions to U.S. classified environments, enhancing analytical capabilities and operational efficiencies successful captious authorities operations.\"\n\nAnthropic, on pinch different starring American AI companies specified arsenic OpenAI and Google, signed individual two-year contracts pinch the Defense Department successful July 2025, each worthy up to $200 cardinal to thief \"prototype frontier AI capabilities that beforehand U.S. nationalist security.\"\n\n\"Anthropic is committed to utilizing frontier AI successful support of US nationalist security,\" the Anthropic spokesperson told Beritaja successful a statement. \"We were the first frontier AI institution to put our models connected classified networks and the first to supply customized models for nationalist information customers.\"\n\nAnthropic CEO Dario Amodei has routinely emphasized Anthropic's committedness to utilizing its AI services for nationalist information purposes. In an effort published successful precocious January, Amodei wrote that \"democracies person a morganatic liking successful immoderate AI-powered subject and geopolitical tools,\" and that \"we should limb democracies pinch AI, but we should do truthful cautiously and wrong limits.\"\n\nMichael Horowitz, who led AI and emerging exertion argumentation successful the Pentagon and is now a professor of governmental subject astatine the University of Pennsylvania, said that immoderate concerns about usage of Anthropic systems for progressive engagement successful lethal autonomous weapons would apt beryllium irrelevant to existent negotiations fixed the type of systems Anthropic is developing.\n\n\"I would beryllium amazed if Anthropic models were the correct ones to usage for lethal autonomous limb systems correct now, since the algorithms for that will beryllium much bespoke than Claude's,\" Horowitz told Beritaja.\n\n\"My consciousness is that Anthropic wants to summation the extent and scope of their activity pinch the Pentagon. Based connected what we know, this sounds for illustration a conflict much complete theoretical possibilities than real-world usage cases connected the table.\""
  },
  {
    "source": "NBC News",
    "company": "Anthropic",
    "title": "Tensions between the Pentagon and AI giant Anthropic reach a boiling point",
    "date": "2026-02-20T17:49:14Z",
    "url": "https://www.nbcnews.com/tech/security/anthropic-ai-defense-war-venezuela-maduro-rcna259603",
    "content": "Over the last week, tensions between the Pentagon and artificial intelligence giant Anthropic have reached a boiling point.\n\nAnthropic, the creator of the Claude chatbot system and a frontier AI company with a defense contract worth up to $200 million, has built its brand around the promotion of AI safety, touting red lines the company says it won't cross.\n\nNow, the Pentagon appears to be pushing those boundaries.\n\nHints of a possible rift between Anthropic and the Defense Department, now rebranded the Department of War, began to intensify after The Wall Street Journal and Axios reported the use of Anthropic products in the operation to capture Venezuelan President Nicolás Maduro.\n\nIt is unclear how Anthropic's Claude was used.\n\nAnthropic has not raised or found any violations of its policies in the wake of the Maduro operation, according to two people familiar with the matter, who asked to remain anonymous in order to discuss sensitive topics. They said that the company has high visibility into how its AI tool Claude is used, such as in data analysis operations.\n\nAnthropic was the first AI company allowed to offer services on classified networks, via Palantir, which partnered with it in 2024. Palantir said in an announcement of the partnership that Claude could be used \"to support government operations such as processing vast amounts of complex data rapidly\" and \"helping U.S. officials to make more informed decisions in time-sensitive situations.\"\n\nPalantir is one of the military's favored data and software contractors, for example collecting data from space sensors to provide better strike targeting for soldiers. It has also attracted scrutiny for its work under the Trump administration and law enforcement agencies.\n\nThough Anthropic has maintained that it does not and will not allow its AI systems to be directly used in lethal autonomous weapons or for domestic surveillance, the reported use of its technology related to the Venezuela raid through the contract with Palantir allegedly raised concerns from an Anthropic employee.\n\nSemafor reported Tuesday that, during a routine meeting between Anthropic and Palantir, a Palantir executive was worried that an Anthropic employee did not seem to agree with how its systems might have been used in the operation, leading to \"a rupture in Anthropic's relationship with the Pentagon.\"\n\nA senior Pentagon official told NBC News that \"a senior executive from Anthropic communicated with a senior Palantir executive, inquiring as to whether their software was used for the Maduro raid.\"\n\nAccording to the Pentagon official, the Palantir executive \"was alarmed that the question was raised in such a way to imply that Anthropic might disapprove of their software being used during that raid.\"\n\nCiting the classified nature of military operations, an Anthropic spokesperson would neither confirm nor deny that its Claude chatbot systems had been used in the Maduro operation: \"We cannot comment on whether Claude, or any other AI model, was used for any specific operation, classified or otherwise,\" the spokesperson told NBC News in a statement.\n\nThe spokesperson pushed back on the idea that the incident had caused notable fallout, telling NBC News the company had not held out-of-the-ordinary discussions about Claude usage with partners or shared any mission-related disagreements with the military.\n\n\"Anthropic has not discussed the use of Claude for specific operations with the Department of War,\" the spokesperson said. \"We have also not discussed this with, or expressed concerns to, any industry partners outside of routine discussions on strictly technical matters.\"\n\nPalantir did not reply to a request for comment.\n\nThe core tension between Anthropic and the Defense Department appears to be rooted in a broader clash over the military's future use of Anthropic's systems. The Defense Department has recently emphasized its desire to be able to use all available AI systems for any purpose allowed by law, while Anthropic says it wants to maintain its own guardrails.\n\nChief spokesman for the Pentagon Sean Parnell told NBC News that \"The Department of War's relationship with Anthropic is being reviewed.\"\n\n\"Our nation requires that our partners be willing to help our warfighters win in any fight,\" he said in a statement.\n\n\"Ultimately, this is about our troops and the safety of the American people.\" On Tuesday, Undersecretary of Defense Emil Michael said that the department's negotiations with Anthropic had hit a snag over a disagreement over potential uses of its systems, according to CNBC.\n\nIn early January, Defense Secretary Pete Hegseth released a new AI strategy document that called for any contracts with AI companies to eliminate company-specific guardrails or constraints on how the military can use companies' AI systems, newly allowing \"any lawful use\" of AI for Defense Department purposes.\n\nThe document called for defense officials to incorporate this language into any Defense Department AI contract within 180 days, which would implicate Anthropic's dealings with the military.\n\nWhile Anthropic has broadly supported the use of its services for national security purposes, it has maintained that its systems not be used for domestic surveillance or in fully autonomous weapons.\n\nThe Defense Department has balked at Anthropic's insistence on these two issues and applied increasing pressure to the company.\n\n\"Claude is used for a wide variety of intelligence-related use cases across the government, including the Department of War, in line with our Usage Policy,\" the Anthropic spokesperson said. \"We are having productive conversations, in good faith, with the Department of War on how to continue that work and get these complex issues right.\"\n\nRelative to other AI companies, Anthropic has prioritized enterprise and national security applications of its AI systems. In August 2025, Anthropic formed a national security and public sector advisory council composed of former senior defense and intelligence officials and last week added Chris Lidell, President Donald Trump's former deputy chief of staff, to its board of directors.\n\nAnthropic has partnered with Palantir since late 2024 to provide U.S. defense and intelligence agencies with access to various Claude systems. At the time, Anthropic's head of sales and partnerships, Kate Earle Jensen, said the company was \"proud to be at the forefront of bringing responsible AI solutions to U.S. classified environments, enhancing analytical capabilities and operational efficiencies in vital government operations.\"\n\nAnthropic, along with other leading American AI companies such as OpenAI and Google, signed individual two-year contracts with the Defense Department in July 2025, each worth up to $200 million to help \"prototype frontier AI capabilities that advance U.S. national security.\"\n\n\"Anthropic is committed to using frontier AI in support of US national security,\" the Anthropic spokesperson told NBC News in a statement. \"We were the first frontier AI company to put our models on classified networks and the first to provide customized models for national security customers.\"\n\nAnthropic CEO Dario Amodei has routinely emphasized Anthropic's commitment to using its AI services for national security purposes. In an essay published in late January, Amodei wrote that \"democracies have a legitimate interest in some AI-powered military and geopolitical tools,\" and that \"we should arm democracies with AI, but we should do so carefully and within limits.\"\n\nMichael Horowitz, who led AI and emerging technology policy in the Pentagon and is now a professor of political science at the University of Pennsylvania, said that any concerns about use of Anthropic systems for active engagement in lethal autonomous weapons would likely be irrelevant to current negotiations given the type of systems Anthropic is developing.\n\n\"I would be surprised if Anthropic models were the right ones to use for lethal autonomous weapon systems right now, since the algorithms for that will be more bespoke than Claude's,\" Horowitz told NBC News.\n\n\"My sense is that Anthropic wants to increase the depth and scope of their work with the Pentagon. Based on what we know, this sounds like a dispute more over theoretical possibilities than real-world use cases on the table.\""
  },
  {
    "source": "MediaNama",
    "company": "Anthropic",
    "title": "Why Are Music Publishers Again Suing Anthropic for Copyright?",
    "date": "2026-01-29T12:54:21Z",
    "url": "https://www.medianama.com/2026/01/223-music-publishers-fresh-copyright-lawsuit-anthropic/",
    "content": "Building on the previous copyright lawsuit and a breakthrough judgment that revealed Anthropic was using copyrighted books for AI training, various global music publishers filed a new lawsuit against Anthropic, alleging it infringed on copyrighted content, including song lyrics, particularly through pirated means.\n\nThis is not the first time Anthropic has been dragged into copyright infringement cases. Earlier in September 2025, in the famous Anthropic vs. Bartz case, the company agreed to pay $1.5 billion in \"the largest publicly reported copyright recovery\" case. Interestingly, during the same court proceedings before the final settlement, the US court found that using purchased copyrighted works to train AI models is 'fair use' under the US Copyright Law.\n\nSeparately, several music publishers previously filed a separate copyright infringement case against Anthropic that the parties later resolved through an agreement. The fresh lawsuit, which also includes a few music publishers from the previous lawsuit, is a follow-up to the previously settled case.\n\nIn this new lawsuit, the music publishers say they tried to expand their earlier copyright case against Anthropic after Judge William Alsup's rulings in the case, popularly known as Anthropic Vs Bartz, which revealed Anthropic's illegal downloading from pirated shadow libraries.\n\n\"Until the revelations in those [Bartz vs Anthropic] opinions and filings, Publishers did not know that their works were being copied by Defendants from some of the most notorious pirated sources in the world,\" reads the lawsuit, referring to the piracy shadow libraries like LibGen (Library Genesis) and its mirror sites such as Z-library. Before evolving as Anna's Archive, Pirate Library Mirror (PiLiMi) replicated content from the banned Z-Library.\n\nWhen music labels sought to amend their previous complaint, Anthropic opposed it, arguing that the torrenting claims were unrelated to their copyright infringement case and would \"fundamentally transform\" the first case. So, the publishers say that they filed this separate lawsuit to address what they call \"willful infringement\" through the downloading and uploading of unauthorised copies of their works from massive piracy websites.\n\nReferring to the above-mentioned case, the publishers also noted they had already sued Anthropic in the earlier case over the alleged copying of their content to train certain Claude AI models. Despite their agreement to enforce safety guardrails to prevent their AI models from generating copyrighted content, they claim that Anthropic has continued to use their works on a much larger scale since then, leading to this second lawsuit over the same copyright infringement issues surrounding AI training and outputs.\n\nConcord Music Group, Universal Music, and others filed the lawsuit against Anthropic, its CEO, Dario Amodei, and co-founder Benjamin Mann. Along with allegations of copyright infringement at Anthropic, the lawsuit also claims that Amodei and Mann used the alleged pirated libraries while they were at OpenAI between 2019 and 2020, as revealed in the Bartz v. Anthropic court proceedings.\n\n\"From the very beginning, Anthropic has built its multibillion-dollar business on piracy,\" states the lawsuit, referring to the founders of Anthropic's alleged involvement with OpenAI.\n\nIn their new lawsuit, the music publishers asked the court to award statutory damages of up to $150,000 per infringed work. They also sought additional statutory damages of up to $25,000 per violation for the alleged removal or alteration of copyright management information from their original work.\n\n\"In total, Defendants torrented at least 5 million copies of pirated books from LibGen in 2021, and at least another 2 million copies of pirated books from PiLiMi in 2022,\" alleges the lawsuit. Additionally, it claims that they illegally downloaded a separate catalogue of bibliographic metadata for each collection, which included information on book title, author, and ISBN, a numeric commercial book identifier.\n\nThe publishers further requested an order requiring Anthropic to destroy all infringing copies of their works in its datasets under the court's supervision and to file a report on its compliance.\n\nMusic publishers say the industry depends on licensing and authorised deals to ensure songwriters and publishers are paid when their works are used or played. Publishers license songs in their catalogues, collect the revenue, and share it with the artists they represent. They say that these licensing arrangements can also extend to AI companies that can bring them additional revenue in exchange for their proprietary data for AI training.\n\nAfter initially suing them for copyright infringement, Universal Music Group and Udio, an AI music generator, announced a partnership following their settlement. Similarly, the recent Tips Music earnings call also revealed that its partnership with Warner Music Group involved AI training as part of NVIDIA's new music model, which it is developing in collaboration with Universal Music Group.\n\nParticularly regarding illegal downloading through peer-to-peer-based torrent networks, the lawsuit further said that by torrenting pirated books, Anthropic \"violated Publishers' exclusive right of reproduction.\" Explaining this claim, it said, \"to make matters worse, because of the two-way nature of the BitTorrent protocol, when Defendants downloaded copies of these pirated books via torrenting, they simultaneously uploaded to the public unauthorized copies of the same books, thereby infringing Publishers' exclusive right of distribution in these works and contributing to further infringement of Publishers' works as well.\"\n\n\"Despite its multibillion-dollar valuation, Anthropic refuses to pay a cent for the vast amounts of copyrighted content -- including Publishers' musical compositions -- it takes without permission or credit to build its business,\" claims the lawsuit, alleging the AI company of building a \"central library by copying and ingesting text from the internet and other sources.\"\n\nThe lawsuit states that Anthropic used BitTorrent to download and copy text from illegal pirate library websites. Torrenting is a peer-to-peer (P2P) file-sharing protocol that is \"infamously used for widespread unauthorised reproduction and distribution of copyrighted materials,\" as claimed by the lawsuit.\n\nCiting disclosures from the Bartz vs. Anthropic proceedings, the lawsuit said CEO Dario Amodei acknowledged that Anthropic had many legal options to obtain copyrighted works for AI training, but deliberately chose to obtain them illegally via torrenting because it was reportedly \"faster and free.\" He allegedly described the legal route as a \"practice/business slog.\" The lawsuit refers to another instance to illustrate Anthropic's approach to copyrighted content. When one Anthropic founder learned he could torrent additional copyrighted works from PiLiMi, he wrote to colleagues, \"Just in time!\" To which another employee allegedly replied, \"zlibrary, my beloved.\"\n\nThe publishers called LibGen and PiLiMi \"two of the largest and most infamous\" illegal libraries. \"These pirate libraries contain every genre of book imaginable, including songbooks, sheet music collections, and other books of song lyrics, containing copyrighted musical compositions owned and controlled by Publishers and others,\" states the complaint.\n\nIn addition to these allegations, internal documents unveiled at the recent legal filings revealed Project Panama is Anthropic's \"effort to destructively scan all the books in the world\", the Washington Post reported. It also revealed that Anthropic had \"spent tens of millions of dollars to acquire and slice the spines off millions of books.\"\n\nAfter Anthropic collects the vast text library that includes publishers' copyrighted works, it allegedly uses portions of that data to train its AI models through further unauthorised copying of the same. Publishers alleged that Anthropic \"cleans\" its training text but leaves infringing content, such as song lyrics, while using tools to remove copyright notices and other copyright management information that generally identify the copyright holders.\n\nThey also said Anthropic copies and processes the corpus in memory, breaking it into \"tokens\" for storage, and makes additional copies during finetuning and reinforcement learning based on human and AI feedback. As part of that process, publishers claimed Anthropic-directed human reviewers prompt and reward the model in ways that can involve outputs tied to publishers' lyrics.\n\nThe lawsuit said that as early as May 2021, senior Anthropic employees, including founders Benjamin Mann and Jared Kaplan, discussed using extraction tools to strip webpage footers, where copyright notices often appear, from training data.\n\nIn June 2021, they concluded that one tool, jusText, a Python-based boilerplate content removing tool, left too much \"useless junk,\" including copyright notice information, compared with alternatives like Readability and Newspaper. Mann also said he wanted the model to \"ignore the boilerplate.\" Publishers alleged that Anthropic chose another extraction tool, Newspaper3k, because it reportedly removed copyright owner names and notices more effectively.\n\n\"Because Newspaper [tool] removed Copyright Management Information more effectively, Anthropic purposefully decided to employ that tool to remove copyright notices and other Copyright Management Information from Publishers' lyrics and other copyrighted works,\" read the lawsuit.\n\n\"The datasets Anthropic has copied and filtered to train its Claude AI models include a well-known dataset called 'The Pile,' which includes countless unauthorised copies of Publishers' lyrics,\" claims the lawsuit. The now-deleted dataset, the Pile dataset, was also allegedly used by GPU chip-making company, NVIDIA.\n\nThe publishers claim that Anthropic continues to use The Pile to train its latest Claude models, which is drawn from several existing text sources, giving more weight to what it calls \"high-quality datasets.\"\n\nThese include Books3, a collection of hundreds of thousands of pirated books that allegedly contains many works with publishers' musical compositions, as well as the YouTube Subtitles dataset of human-written closed captions. For additional context, Books3 is the same dataset allegedly used by NVIDIA, Apple, Adobe, Meta, and Anthropic itself. In August 2023, after a legal complaint by a Denmark-based anti-piracy group, Rights Alliance, the Books3 dataset was removed from The Pile, which was also removed in the same year.\n\nThe new copyright lawsuit also said Anthropic uses the \"Common Crawl\" dataset for ongoing AI training. Publishers alleged that the dataset includes a large number of their copyrighted lyrics, scraped without permission from authorised sites such as MusixMatch, LyricFind and Genius. Common Crawl refers to the non-profit company that \"maintains a free, open repository of web crawl data that can be used by anyone.\""
  },
  {
    "source": "developpez.net",
    "company": "Anthropic",
    "title": "1",
    "date": "2026-01-25T05:02:30Z",
    "url": "https://www.developpez.net/forums/d2180624/general-developpement/algorithme-mathematiques/intelligence-artificielle/nouvelle-constitution-claude-d-anthropic-sois-serviable-honnete-ne-detruis-l-humanite/",
    "content": "Anthropic lance Claude Opus 4.5, son dernier modèle d'IA qui excelle dans le codage, l'utilisation d'ordinateurs et l'assistance aux utilisateurs pour les tâches professionnelles complexes, selon la société\n\nAnthropic a lancé Claude Opus 4.5, son dernier modèle d'intelligence artificielle (IA) de pointe conçu pour améliorer la productivité dans le domaine du codage, des tâches d'entreprise et de la recherche approfondie. S'appuyant sur ses prédécesseurs, Opus 4.5 promet des améliorations notables en termes d'efficacité et de polyvalence, en particulier pour les développeurs, les analystes financiers et les consultants. Cette sortie marque la troisième introduction majeure d'un modèle par Anthropic en deux mois, consolidant ainsi sa position dans le paysage en constante évolution de l'IA.\n\nAnthropic est une start-up spécialisée dans l'IA fondée en 2021 par un groupe d'anciens chercheurs et cadres d'OpenAI. La société est surtout connue pour avoir développé une famille de modèles d'IA appelée Claude. Elle attribue de nouveaux numéros aux modèles à mesure qu'ils évoluent au fil des générations, mais le plus grand modèle de la famille est généralement appelé Opus, le modèle de taille moyenne est appelé Sonnet et le plus petit modèle est Haiku. Le dernier modèle Opus, lancé par Anthropic en août, s'appelait Claude Opus 4.1. La start-up a par ailleurs dévoilé son modèle Claude Sonnet 4.5 fin septembre, suivi de son modèle Claude Haiku 4.5 en octobre.\n\nLa sortie de Claude Opus 4.5 intervient alors qu'Anthropic connaît une accélération rapide de son activité commerciale. Microsoft et Nvidia ont annoncé la semaine du 17 novembre 2025 des investissements de plusieurs milliards de dollars dans Anthropic, portant la valorisation du laboratoire d'IA à environ 350 milliards de dollars. La start-up prévoit également de quasiment tripler son chiffre d'affaires annualisé l'an prochain, portée par une demande croissante des entreprises. Cette dynamique confirme son ancrage sur le marché professionnel.\n\nLe lundi 24 novembre 2025, Anthropic a annoncé Claude Opus 4.5, son dernier modèle d'IA qui, selon la start-up, excelle dans le codage, l'utilisation d'ordinateurs et l'assistance aux utilisateurs dans des tâches d'entreprise complexes. Claude Opus 4.5 marque le troisième lancement majeur d'Anthropic en deux mois et constitue le dernier exemple en date du rythme effréné du développement dans le secteur de l'IA.\n\n\" Le montant que nous injectons sur le marché et les retours que nous en tirons me rendent incroyablement enthousiaste \", a déclaré Scott White, responsable produit pour Claude.ai chez Anthropic, lors d'une interview.\n\nEvaluation de Claude Opus 4.5\n\nSelon Scott White, les utilisateurs idéaux de Claude Opus 4.5 seront les développeurs de logiciels professionnels et les travailleurs du savoir tels que les analystes financiers, les consultants et les comptables. Les personnes qui \" ont envie de stimuler leur créativité, de créer de nouvelles choses et d'élargir leur champ d'action professionnel \" trouveront également ce modèle utile, a ajouté White.\n\nLe nouveau modèle est \" nettement meilleur \" pour les tâches quotidiennes telles que l'utilisation de feuilles de calcul et de diapositives et la conduite de recherches approfondies, a déclaré Anthropic dans un blog.\n\nClaude Opus 4.5 est également à la pointe de la technologie en matière de codage agentique, surpassant les modèles concurrents tels que Gemini 3 Pro de Google et GPT-5.1 d'OpenAI, selon SWE-bench Verified, un ensemble de tests qui mesure les capacités de codage logiciel d'un système d'IA.\n\nAnthropic a déclaré avoir testé Claude Opus 4.5 sur un examen difficile à faire à domicile qu'elle fait passer aux futurs candidats en ingénierie de la performance, et le modèle a obtenu un score supérieur à celui de tous les candidats humains.\n\n\" Nous soumettons les candidats potentiels en ingénierie de la performance à un examen à domicile réputé pour sa difficulté. Nous testons également de nouveaux modèles lors de cet examen afin d'établir une référence interne. Dans le délai imparti de deux heures, Claude Opus 4.5 a obtenu un score supérieur à celui de tous les candidats humains \" , a déclaré la société.\n\nLe test à domicile est conçu pour évaluer les compétences techniques et le jugement des candidats dans des conditions de pression temporelle. Il ne teste pas d'autres compétences essentielles que les candidats peuvent posséder, telles que la collaboration, la communication ou l'instinct acquis au fil des années. Mais ce résultat, où un modèle d'IA surpasse des candidats solides sur des compétences techniques importantes, soulève des questions sur la manière dont l'IA va transformer la profession d'ingénieur. Selon Anthropic, sa recherche sur les impacts sociétaux et l'avenir économique vise à comprendre ce type de changements dans de nombreux domaines.\n\nLe génie logiciel n'est pas le seul domaine dans lequel Claude Opus 4.5 s'est amélioré. Selon l'entreprise d'IA, les capacités du modèle Opus 4.5 sont globalement supérieures. Il disposerait de meilleures capacités visuelles, de raisonnement et mathématiques que ses prédécesseurs, et il serait, selon la start-up, à la pointe de la technologie dans de nombreux domaines :\n\nAnthropic a indiqué que les capacités du modèle dépassaient certaines des références utilisées dans ses tests. L'une de ces références courantes pour les capacités agentiques est le test τ2-bench, qui évalue les performances des agents dans des tâches réelles à plusieurs tours. Dans un scénario, les modèles devaient jouer le rôle d'agents de service aérien pour aider un client en détresse. Le critère de référence attendait des modèles qu'ils refusent toute modification d'une réservation en classe économique de base, car la compagnie aérienne n'autorise pas les changements pour cette catégorie de billets. Opus 4.5 a toutefois trouvé une solution pertinente (et légitime) au problème : il a d'abord surclassé la cabine, puis a modifié les vols.\n\nTechniquement, le benchmark a considéré cela comme un échec, car la manière dont Claude a aidé le client était imprévue. Mais, selon Anthropic, ce type de résolution créative des problèmes correspond exactement à ce qu'elle a entendu de la part de nos testeurs et de nos clients : \" c'est ce qui fait de Claude Opus 4.5 une avancée significative. \"\n\nL'entreprise précise que dans d'autres contextes, trouver des moyens astucieux de contourner les contraintes prévues pourrait être considéré comme du \" reward hacking \", c'est-à-dire lorsque les modèles \" contournent \" les règles ou les objectifs de manière imprévue.\n\nUn pas en avant en matière de sécurité\n\nComme indiqué dans le system card d'Anthropic, Claude Opus 4.5 est le modèle le plus robuste que l'entreprise d'IA a publié à ce jour et, selon elle, le modèle de pointe le mieux aligné parmi tous ceux développés par d'autres développeurs. Opus 4.5 s'inscrit dans la continuité de la tendance d'Anthropic vers des modèles plus sûrs et plus sécurisés :\n\nAnthropic a déclaré que ses clients utilisent souvent Claude pour des tâches critiques. \" Ils veulent être assurés que, face aux attaques malveillantes des pirates informatiques et des cybercriminels, Claude dispose de la formation et du \" bon sens \" nécessaires pour éviter les problèmes \", a indiqué la start-up.\n\nAvec Opus 4.5, Anthropic a réalisé des progrès considérables en matière de robustesse contre les attaques par injection de prompt, qui consistent à introduire des instructions trompeuses afin d'inciter le modèle à adopter un comportement nuisible. Selon l'entreprise, Opus 4.5 est plus difficile à tromper avec une injection de prompt que tout autre modèle de pointe dans l'industrie :\n\nNouveautés sur la plateforme de développement Claude\n\nAnthropic affirme qu'à mesure que les modèles gagnent en intelligence, ils peuvent résoudre des problèmes en moins d'étapes, ce qui se traduit par moins de retours en arrière, moins d'exploration redondante et moins de raisonnements verbeux. Selon la société, Claude Opus 4.5 utilise beaucoup moins de jetons que ses prédécesseurs pour obtenir des résultats similaires ou meilleurs.\n\nLa start-up souligne toutefois que différentes tâches nécessitent différents compromis et que parfois, les développeurs souhaitent qu'un modèle continue à réfléchir à un problème, tandis que d'autres fois, ils préfèrent quelque chose de plus agile. Anthropic affirme que grâce à son nouveau paramètre d'effort sur l'API Claude, les utilisateurs peuvent choisir de \"minimiser le temps et les dépenses ou de maximiser les capacités.\n\nSelon l'entreprise, lorsqu'il est réglé sur un niveau d'effort moyen, Opus 4.5 égale le meilleur score de Sonnet 4.5 sur SWE-bench Verified, mais utilise 76 % de jetons de sortie en moins. À son niveau d'effort maximal, Opus 4.5 dépasse les performances de Sonnet 4.5 de 4,3 points de pourcentage, tout en utilisant 48 % de jetons en moins.\n\nAnthropic explique en outre que grâce au contrôle des efforts, à la compression du contexte et à l'utilisation d'outils avancés, Claude Opus 4.5 \" fonctionne plus longtemps, accomplit davantage de tâches et nécessite moins d'intervention. \"\n\nLa société souligne que ses capacités de gestion du contexte et de la mémoire peuvent considérablement améliorer les performances des tâches agentiques. Elle indique également qu'Opus 4.5 est très efficace pour gérer une équipe de sous-agents, ce qui permet la construction de systèmes multi-agents complexes et bien coordonnés. Lors des tests réalisés par l'entreprise, la combinaison de toutes ces techniques a amélioré les performances du modèle lors d'une évaluation approfondie de la recherche de près de 15 points de pourcentage.\n\n\" Nous rendons notre plateforme de développement plus modulable au fil du temps. Nous voulons vous fournir les éléments de base nécessaires pour construire exactement ce dont vous avez besoin, tout en vous offrant un contrôle total sur l'efficacité, l'utilisation des outils et la gestion du contexte \", a déclaré la société.\n\nMises à jour des produits\n\nOutre le lancement du modèle Claude Opus 4.5, Anthropic a annoncé plusieurs mises à jour de la plateforme de développement Claude, de Claude Code et de ses applications grand public. De nouveaux outils sont disponibles pour les agents à exécution prolongée, ainsi que de nouvelles façons d'utiliser Claude dans Excel, Chrome et sur ordinateur de bureau.\n\nClaude Code bénéficie de deux mises à niveau avec Opus 4.5. Le mode Plan permet désormais d'élaborer des plans plus précis et de les exécuter de manière plus approfondie : Claude pose des questions de clarification au préalable, puis crée un fichier plan.md modifiable par l'utilisateur avant de l'exécuter.\n\nClaude Code est désormais également disponible dans l'application de bureau d'Anthropic, ce qui permet aux utilisateurs d'exécuter plusieurs sessions locales et distantes en parallèle : un agent peut corriger des bogues, un autre effectuer des recherches sur GitHub et un troisième mettre à jour des documents.\n\nPour les utilisateurs de l'application Claude, les longues conversations ne sont plus un obstacle : Claude résume automatiquement le contexte précédent si nécessaire, afin que les utilisateurs puissent poursuivre la conversation.\n\nClaude pour Chrome, qui permet à Claude de gérer des tâches dans les onglets du navigateur, est désormais disponible pour tous les utilisateurs Max. Claude pour Excel a été annoncé en octobre, et à partir de maintenant, l'accès à la version bêta pour tous les utilisateurs Max, Team et Enterprise a été étendu. Chacune de ces mises à jour tire parti des performances de pointe de Claude Opus 4.5 dans l'utilisation des ordinateurs, des feuilles de calcul et la gestion des tâches de longue durée.\n\nPour les utilisateurs de Claude et Claude Code ayant accès à Opus 4.5, Anthropic a supprimé les limites spécifiques à Opus. Pour les utilisateurs de Max et Team Premium, les limites d'utilisation globales ont été augmentées, ce qui signifie que l'utilisateur dispose à peu près du même nombre de jetons Opus qu'auparavant avec Sonnet. Anthropic indique avoir mis à jour les limites d'utilisation afin de permettre l'utilisation de Opus 4.5 dans le travail quotidien. Ces limites sont spécifiques à Opus 4.5 et à mesure que les futurs modèles le dépasseront, la start-up prévoit de mettre à jour les limites si nécessaire.\n\nDisponibilité et tarifs\n\nClaude Opus 4.5 est disponible dès aujourd'hui sur les applications, l'API et les trois principales plateformes cloud d'Anthropic. Il constituera le modèle par défaut des offres Pro, Max et Enterprise. Si vous êtes développeur, il vous suffit d'utiliser claude-opus-4-5-20251101 via l'API Claude. Le prix est désormais de 5 dollars/25 dollars par million de jetons d'entrée/sortie, ce qui rend les fonctionnalités d'Opus accessibles à encore plus d'utilisateurs, d'équipes et d'entreprises.\n\nAlors qu'Anthropic procède au lancement d'Opus 4.5, cette avancée intervient dans un contexte où les conséquences sociales de l'IA suscitent de vives préoccupations. Dario Amodei, le PDG d'Anthropic, a prévenu qu'une \" hécatombe des travailleurs du savoir \" se préparait et que la technologie pourrait faire disparaître jusqu'à la moitié des emplois de bureau débutants en cinq ans, avec un taux de chômage pouvant atteindre 20 %. Il affirme que l'IA dépassera progressivement les humains dans la plupart des tâches intellectuelles, y compris celles des cadres dirigeants.\n\nSource : Anthropic\n\nEt vous ?\n\nQuel est votre avis sur le sujet ?\n\nTrouvez-vous cette initiative d'Anthropic crédible ou pertinente ?\n\nTrouvez-vous les nouvelles fonctionnalités de Claude Opus 4.5 utiles et intéressantes ?\n\nAvez-vous déjà utilisé cet outil ou un outil similaire pour votre usage ou le développement d'applications, et si oui, qu'en pensez-vous ?\n\nVoir aussi :\n\nAnthropic lance Claude Opus 4.1 avec des améliorations en matière de codage, de raisonnement et de débogage pour répondre aux besoins en manipulation précise du code et en recherches avancées des utilisateurs\n\nAnthropic lance Claude Sonnet 4.5, son dernier modèle d'IA qui \" s'apparente davantage à un collègue \", Claude Sonnet 4.5 est plus petit que Claude Opus 4.1, mais serait plus intelligent\n\nAnthropic lance Claude Haiku 4.5, la variante la plus compacte de cette génération de LLM d'Anthropic, et promet des performances proches de celles du modèle d'IA GPT-5 d'OpenAI"
  },
  {
    "source": "WebProNews",
    "company": "Anthropic",
    "title": "The Paradox at the Heart of Anthropic: How AI Safety's Standard-Bearer Struggles With Its Own Contradictions",
    "date": "2026-02-02T13:11:49Z",
    "url": "https://www.webpronews.com/the-paradox-at-the-heart-of-anthropic-how-ai-safetys-standard-bearer-struggles-with-its-own-contradictions/",
    "content": "In a nondescript conference room in San Francisco, researchers at Anthropic are grappling with a question that sounds almost philosophical: How do you build artificial intelligence that could transform civilization while simultaneously ensuring it doesn't destroy humanity? For a company that has positioned itself as the artificial intelligence industry's moral compass, the answer is proving far more complicated than its founders anticipated.\n\n\"These are not the words you want to hear when it comes to human extinction, but I was hearing them: 'Things are moving uncomfortably fast,'\" reported The Atlantic in a recent deep dive into the company's internal tensions. The statement encapsulates the central dilemma facing Anthropic: how to maintain rigorous safety standards while competing in a market that rewards speed and scale above all else.\n\nFounded in 2021 by former OpenAI executives Dario and Daniela Amodei, Anthropic emerged with a clear mission statement that differentiated it from competitors. The company would prioritize AI safety research, develop interpretability tools to understand how neural networks make decisions, and refuse to cut corners in the race toward artificial general intelligence. This positioning attracted billions in funding from investors including Google, Spark Capital, and others who believed that responsible AI development could also be profitable AI development. Yet three years later, that thesis is being tested in ways that reveal fundamental contradictions in the company's operating model.\n\nThe Expansion Paradox: Growing While Preaching Caution\n\nThe physical manifestation of Anthropic's internal conflict is perhaps most visible in its real estate decisions. According to SF Gate, the company has been rapidly expanding its San Francisco office footprint, signing leases for additional space even as CEO Dario Amodei publicly warns about the existential risks posed by advanced AI systems. The company now occupies multiple floors in the city's South of Market district, with plans for further expansion to accommodate a workforce that has grown from dozens to hundreds in just over two years.\n\nThis aggressive growth trajectory sits uncomfortably alongside Anthropic's public positioning. The company has published extensive research on \"constitutional AI\" and mechanistic interpretability -- technical approaches designed to make AI systems more aligned with human values and more understandable to their creators. These research priorities require significant time and resources, yet they don't directly translate to the kind of rapid product iteration that generates revenue and justifies billion-dollar valuations. The tension between these competing demands has created what multiple current and former employees describe as a culture of cognitive dissonance.\n\nIndustry observers have noted that Anthropic's commercial product, Claude, has been released in increasingly powerful versions at a pace that rivals or exceeds competitors like OpenAI and Google. Each new model release represents months of training on massive datasets, requiring enormous computational resources and energy consumption. The company's partnership with Amazon Web Services, which includes a $4 billion investment, has provided the infrastructure necessary to train these large language models at scale. But it has also locked Anthropic into a commercial relationship that demands regular product updates and market-competitive capabilities.\n\nThe Amodei Contradiction: Warnings That Don't Align With Actions\n\nPerhaps no figure embodies Anthropic's contradictions more than CEO Dario Amodei himself. A former vice president of research at OpenAI, Amodei left that company in 2020 amid disagreements over safety protocols and the decision to accept a major investment from Microsoft. He founded Anthropic explicitly to create an alternative model -- one where safety research would be foundational rather than supplementary. Yet as Transformer News analyzed, there's a significant gap between Amodei's public warnings about AI risk and the company's actual operational decisions.\n\nIn essays and interviews, Amodei has painted vivid pictures of potential AI-driven catastrophes. He has discussed scenarios where advanced AI systems could be used to develop biological weapons, manipulate political systems at scale, or recursively self-improve beyond human control. These warnings have earned him credibility among AI safety advocates and helped position Anthropic as the responsible alternative to more commercially aggressive competitors. But critics point out that if Amodei truly believed these risks were imminent and severe, the company's behavior would look dramatically different.\n\n\"If you genuinely think there's a substantial probability that your work could lead to human extinction, the rational response isn't to do that work slightly more carefully than your competitors,\" one AI researcher told Transformer News. \"It's to stop doing that work entirely, or at minimum to advocate for regulatory frameworks that would constrain the entire industry.\" Instead, Anthropic has continued to push forward with increasingly capable models while advocating for voluntary safety standards that competitors can choose to ignore.\n\nThe Safety Theater Question: Substance Versus Optics\n\nThe gap between rhetoric and reality has led some critics to question whether Anthropic's safety focus represents genuine commitment or sophisticated marketing. Newcomer reported on what it termed \"Anthropic's throwdown on AI safety,\" highlighting the company's increasingly public stance on safety issues even as it races to maintain competitive parity with OpenAI's GPT-4 and Google's Gemini models.\n\nThe company's constitutional AI approach -- training models to follow a set of principles derived from documents like the Universal Declaration of Human Rights -- represents genuine technical innovation. Anthropic has published peer-reviewed research demonstrating that this methodology can reduce harmful outputs and make model behavior more predictable. However, the same models trained using these safety techniques are also being optimized for performance on benchmarks that measure general capabilities, coding ability, and reasoning skills. The dual mandate creates an inherent tension: safety features that constrain model behavior can also limit commercial viability.\n\nInternal documents reviewed by The Atlantic reveal that Anthropic employees have raised concerns about this tension in company meetings. Some researchers have questioned whether the pace of model releases leaves adequate time for safety testing. Others have pointed out that the company's responsible scaling policy -- which theoretically gates new model releases on passing certain safety evaluations -- has been modified multiple times to accommodate commercial pressures. In at least one instance, a model release proceeded despite some safety researchers recommending additional testing time.\n\nThe Market Forces That Shape Safety Decisions\n\nUnderstanding Anthropic's contradictions requires examining the market dynamics that shape all AI companies, regardless of their stated values. The artificial intelligence sector operates under what economists call a \"competitive race\" dynamic, where being second to market can mean becoming irrelevant. When OpenAI released GPT-4, it immediately captured enterprise customers and developer mindshare. Google's subsequent release of Gemini was widely seen as a response to prevent further market share loss. In this environment, even a company committed to safety faces enormous pressure to match competitors' capabilities and release timelines.\n\nThe financial structure of AI companies amplifies these pressures. Anthropic has raised billions in venture capital and strategic investments, creating obligations to investors who expect returns. While the company has emphasized that its corporate structure includes provisions designed to prioritize safety over profits, the practical reality is that without commercial success, there is no company to implement safety measures. This creates a recursive problem: safety research requires resources that come from commercial success, but commercial success requires competing effectively in a market that rewards speed over caution.\n\nThe talent market adds another dimension to this dynamic. Top AI researchers command salaries in the seven figures, and they have numerous employment options. Anthropic must compete for this talent not just with other AI companies but with major technology firms that can offer comparable compensation plus the resources of established organizations. Several former Anthropic employees have noted that the company's safety focus, while intellectually appealing, can feel constraining to researchers who want to push technical boundaries without the overhead of extensive safety protocols.\n\nThe Interpretability Promise and Its Limitations\n\nOne area where Anthropic has made substantive contributions is mechanistic interpretability -- research aimed at understanding how neural networks actually process information and make decisions. The company has published groundbreaking work identifying specific \"features\" within large language models that correspond to concepts like deception, sentiment, and topic categories. This research represents genuine scientific progress and could eventually enable more precise control over AI system behavior.\n\nHowever, even this research program reveals tensions in Anthropic's mission. Interpretability work is painstaking and slow, requiring months of analysis to understand even small components of large models. Meanwhile, the models themselves are growing exponentially larger and more complex. Claude 3, Anthropic's latest major release, contains hundreds of billions of parameters -- far more than can be thoroughly analyzed using current interpretability techniques. The company is essentially racing to understand systems that are growing faster than the understanding can keep pace.\n\nFurthermore, interpretability research doesn't directly address some of the most concerning AI risks. Understanding how a model processes information doesn't necessarily prevent it from being used for harmful purposes, nor does it solve alignment problems related to models pursuing unintended goals. Some AI safety researchers argue that interpretability, while valuable, may be receiving disproportionate attention because it's tractable and publishable, rather than because it addresses the most critical risks.\n\nThe Regulatory Gambit: Shaping Rules While Playing the Game\n\nAnthropic has positioned itself as a constructive voice in AI policy discussions, engaging with regulators in the United States, European Union, and United Kingdom. Company executives have testified before legislative bodies and participated in industry working groups developing voluntary safety standards. This engagement serves multiple purposes: it reinforces Anthropic's brand as the responsible AI company, it potentially shapes regulations in ways favorable to the company's approach, and it creates barriers to entry for smaller competitors who might struggle to meet compliance requirements.\n\nCritics note that Anthropic's policy positions tend to favor approaches that wouldn't significantly constrain its own operations. The company supports transparency requirements and safety testing protocols, but opposes measures that would slow development timelines or require government approval before deploying new models. This selective advocacy suggests that policy engagement may be as much about competitive positioning as genuine risk mitigation.\n\nThe company's relationship with government extends beyond advocacy. Anthropic has pursued contracts with defense and intelligence agencies, arguing that it's preferable for these organizations to use AI systems developed with safety considerations rather than alternatives. Yet this reasoning accepts the premise that advanced AI will be used for military and surveillance purposes -- a premise that conflicts with some interpretations of AI safety that emphasize avoiding dual-use technologies with catastrophic potential.\n\nThe Personnel Pipeline and Cultural Shifts\n\nAs Anthropic has grown from a small research organization to a company with hundreds of employees, its culture has inevitably shifted. Early employees describe a environment where safety considerations genuinely drove decision-making and researchers had significant autonomy to pursue long-term projects. More recent hires report a culture that feels increasingly similar to other technology companies, with quarterly objectives, product roadmaps, and pressure to ship features that satisfy enterprise customers.\n\nThe Atlantic's reporting revealed that some long-tenured employees have expressed concern about this cultural evolution. The company's hiring has accelerated dramatically, bringing in engineers and product managers from conventional technology companies who may not share the founding team's deep engagement with AI safety questions. While these hires bring valuable skills for building commercial products, they also shift the organizational center of gravity away from research and toward execution.\n\nTurnover among safety-focused researchers has been notable, though the company disputes characterizations of a mass exodus. Several prominent researchers have left to pursue academic positions or join newer organizations focused exclusively on safety research without commercial product obligations. Each departure removes institutional knowledge and potentially weakens the safety-focused faction within the company's internal debates.\n\nThe Existential Question: Can Safety and Speed Coexist?\n\nAt the core of Anthropic's contradictions lies a question that extends beyond any single company: Is it possible to develop transformatively powerful AI systems safely while operating in a competitive market? The company's struggles suggest that the answer may be no -- that the incentive structures of venture-funded technology companies are fundamentally incompatible with the caution that catastrophic risk mitigation requires.\n\nSome AI safety researchers argue for alternative organizational structures: government-funded research programs, international collaborations with no commercial objectives, or even moratoriums on certain types of AI development. Anthropic's model -- attempting to balance safety and commercial success -- may represent a compromise that satisfies neither goal adequately. The company isn't cautious enough to eliminate catastrophic risks, but its safety overhead makes it less competitive than rivals willing to move faster.\n\nThe broader AI industry watches Anthropic's experiment with mixed feelings. If the company succeeds in demonstrating that safety-focused development can be commercially viable, it could shift industry norms and prove that responsible AI isn't just possible but profitable. If it fails -- either by suffering a major safety incident or by losing market share to less cautious competitors -- it may demonstrate that market forces are incompatible with adequate safety measures, potentially strengthening arguments for regulatory intervention.\n\nThe Path Forward: Reconciling Mission and Market\n\nAnthropic faces difficult choices in the coming years. The company could double down on its safety focus, accepting slower development timelines and potentially reduced market share in exchange for more rigorous testing and research. This path would require convincing investors that long-term differentiation based on trust and reliability can justify near-term competitive disadvantages. Alternatively, the company could continue its current trajectory, maintaining safety rhetoric while matching competitors' pace -- a strategy that risks rendering its safety mission increasingly symbolic.\n\nA third option involves advocacy for industry-wide standards or regulations that would level the playing field by requiring all companies to adopt similar safety measures. This approach would eliminate Anthropic's competitive disadvantage while advancing its stated mission. However, it requires political capital and coordination across companies with divergent interests, and there's no guarantee that resulting regulations would be adequate to address genuine catastrophic risks.\n\nThe company's recent office expansion in San Francisco, as reported by SF Gate, suggests confidence in continued growth and a long-term presence in the AI industry. Whether that presence will be defined by genuine safety leadership or by increasingly strained attempts to reconcile contradictory objectives remains to be seen. For now, Anthropic exists in a state of productive tension -- uncomfortable, perhaps unsustainable, but undeniably influential in shaping conversations about what responsible AI development should look like.\n\nThe stakes extend far beyond one company's success or failure. If Anthropic's model proves unworkable, it may indicate that market-based approaches to AI safety are fundamentally inadequate, requiring more dramatic interventions to prevent potential catastrophes. The researchers who told The Atlantic that \"things are moving uncomfortably fast\" weren't just describing their employer's pace -- they were diagnosing a condition affecting the entire industry, one that no single company, however well-intentioned, may be able to cure on its own."
  },
  {
    "source": "VentureBeat",
    "company": "Anthropic",
    "title": "Anthropic's Claude Opus 4.6 brings 1M token context and 'agent teams' to take on OpenAI's Codex",
    "date": "2026-02-05T18:18:45Z",
    "url": "https://venturebeat.com/technology/anthropics-claude-opus-4-6-brings-1m-token-context-and-agent-teams-to-take",
    "content": "Anthropic on Thursday released Claude Opus 4.6, a major upgrade to its flagship artificial intelligence model that the company says plans more carefully, sustains longer autonomous workflows, and outperforms competitors including OpenAI's GPT-5.2 on key enterprise benchmarks -- a release that arrives at a tumultuous moment for the AI industry and global software markets.\n\nThe launch comes just three days after OpenAI released its own Codex desktop application in a direct challenge to Anthropic's Claude Code momentum, and amid a $285 billion rout in software and services stocks that investors attribute partly to fears that Anthropic's AI tools could disrupt established enterprise software businesses.\n\nFor the first time, Anthropic's Opus-class models will feature a 1 million token context window, allowing the AI to process and reason across vastly more information than previous versions. The company also introduced \"agent teams\" in Claude Code -- a research preview feature that enables multiple AI agents to work simultaneously on different aspects of a coding project, coordinating autonomously.\n\n\"We're focused on building the most capable, reliable, and safe AI systems,\" an Anthropic spokesperson told VentureBeat about the announcements. \"Opus 4.6 is even better at planning, helping solve the most complex coding tasks. And the new agent teams feature means users can split work across multiple agents -- one on the frontend, one on the API, one on the migration -- each owning its piece and coordinating directly with the others.\"\n\nThe release intensifies an already fierce competition between Anthropic and OpenAI, the two most valuable privately held AI companies in the world. OpenAI on Monday released a new desktop application for its Codex artificial intelligence coding system, a tool the company says transforms software development from a collaborative exercise with a single AI assistant into something more akin to managing a team of autonomous workers.\n\nAI coding assistants have exploded in popularity over the last year, and OpenAI said more than 1 million developers have used Codex in the past month. The new Codex app is part of OpenAI's ongoing effort to lure users and market share away from rivals like Anthropic and Cursor.\n\nThe timing of Anthropic's release -- just 72 hours after OpenAI's Codex launch -- underscores the breakneck pace of competition in AI development tools. OpenAI faces intensifying competition from Anthropic, which posted the largest share increase of any frontier lab since May 2025, according to a recent Andreessen Horowitz survey. Forty-four percent of enterprises now use Anthropic in production, driven by rapid capability gains in software development since late 2024. The desktop launch is a strategic counter to Claude Code's momentum.\n\nAccording to Anthropic's announcement, Opus 4.6 achieves the highest score on Terminal-Bench 2.0, an agentic coding evaluation, and leads all other frontier models on Humanity's Last Exam, a complex multi-discipline reasoning test. On GDPval-AA -- a benchmark measuring performance on economically valuable knowledge work tasks in finance, legal and other domains -- Opus 4.6 outperforms OpenAI's GPT-5.2 by approximately 144 ELO points, which translates to obtaining a higher score approximately 70% of the time.\n\nThe stakes are substantial. Asked about Claude Code's financial performance, the Anthropic spokesperson noted that in November, the company announced that Claude Code reached $1 billion in run rate revenue only six months after becoming generally available in May 2025.\n\nThe spokesperson highlighted major enterprise deployments: \"Claude Code is used by Uber across teams like software engineering, data science, finance, and trust and safety; wall-to-wall deployment across Salesforce's global engineering org; tens of thousands of devs at Accenture; and companies across industries like Spotify, Rakuten, Snowflake, Novo Nordisk, and Ramp.\"\n\nThat enterprise traction has translated into skyrocketing valuations. Earlier this month, Anthropic signed a term sheet for a $10 billion funding round at a $350 billion valuation. Bloomberg reported that Anthropic is simultaneously working on a tender offer that would allow employees to sell shares at that valuation, offering liquidity to staffers who have watched the company's worth multiply since its 2021 founding.\n\nOne of Opus 4.6's most significant technical improvements addresses what the AI industry calls \"context rot\" -- the degradation of model performance as conversations grow longer. Anthropic says Opus 4.6 scores 76% on MRCR v2, a needle-in-a-haystack benchmark testing a model's ability to retrieve information hidden in vast amounts of text, compared to just 18.5% for Sonnet 4.5.\n\n\"This is a qualitative shift in how much context a model can actually use while maintaining peak performance,\" the company said in its announcement.\n\nThe model also supports outputs of up to 128,000 tokens -- enough to complete substantial coding tasks or documents without breaking them into multiple requests.\n\nFor developers, Anthropic is introducing several new API features alongside the model: adaptive thinking, which allows Claude to decide when deeper reasoning would be helpful rather than requiring a binary on-off choice; four effort levels (low, medium, high, max) to control intelligence, speed and cost tradeoffs; and context compaction, a beta feature that automatically summarizes older context to enable longer-running tasks.\n\nAnthropic, which has built its brand around AI safety research, emphasized that Opus 4.6 maintains alignment with its predecessors despite its enhanced capabilities. On the company's automated behavior audit measuring misaligned behaviors such as deception, sycophancy, and cooperation with misuse, Opus 4.6 \"showed a low rate\" of problematic responses while also achieving \"the lowest rate of over-refusals -- where the model fails to answer benign queries -- of any recent Claude model.\"\n\nWhen asked how Anthropic thinks about safety guardrails as Claude becomes more agentic, particularly with multiple agents coordinating autonomously, the spokesperson pointed to the company's published framework: \"Agents have tremendous potential for positive impacts in work but it's important that agents continue to be safe, reliable, and trustworthy. We outlined our framework for developing safe and trustworthy agents last year which shares core principles developers should consider when building agents.\"\n\nThe company said it has developed six new cybersecurity probes to detect potentially harmful uses of the model's enhanced capabilities, and is using Opus 4.6 to help find and patch vulnerabilities in open-source software as part of defensive cybersecurity efforts.\n\nThe rivalry between Anthropic and OpenAI has spilled into consumer marketing in dramatic fashion. Both companies will feature prominently during Sunday's Super Bowl. Anthropic is airing commercials that mock OpenAI's decision to begin testing advertisements in ChatGPT, with the tagline: \"Ads are coming to AI. But not to Claude.\"\n\nOpenAI CEO Sam Altman responded by calling the ads \"funny\" but \"clearly dishonest,\" posting on X that his company would \"obviously never run ads in the way Anthropic depicts them\" and that \"Anthropic wants to control what people do with AI\" while serving \"an expensive product to rich people.\"\n\nThe exchange highlights a fundamental strategic divergence: OpenAI has moved to monetize its massive free user base through advertising, while Anthropic has focused almost exclusively on enterprise sales and premium subscriptions.\n\nThe launch occurs against a backdrop of historic market volatility in software stocks. A new AI automation tool from Anthropic PBC sparked a $285 billion rout in stocks across the software, financial services and asset management sectors on Tuesday as investors raced to dump shares with even the slightest exposure. A Goldman Sachs basket of US software stocks sank 6%, its biggest one-day decline since April's tariff-fueled selloff.\n\nThe selloff was triggered by a new legal tool from Anthropic, which showed the AI industry's growing push into industries that can unlock lucrative enterprise revenue needed to fund massive investments in the technology. One trigger for Tuesday's selloff was Anthropic's launch of plug-ins for its Claude Cowork agent on Friday, enabling automated tasks across legal, sales, marketing and data analysis.\n\nThomson Reuters plunged 15.83% Tuesday, its biggest single-day drop on record; and Legalzoom.com sank 19.68%. European legal software providers including RELX, owner of LexisNexis, and Wolters Kluwer experienced their worst single-day performances in decades.\n\nNot everyone agrees the selloff is warranted. Nvidia CEO Jensen Huang said on Tuesday that fears AI would replace software and related tools were \"illogical\" and \"time will prove itself.\" Mark Murphy, head of U.S. enterprise software research at JPMorgan, said in a Reuters report it \"feels like an illogical leap\" to say a new plug-in from an LLM would \"replace every layer of mission-critical enterprise software.\"\n\nAmong the more notable product announcements: Anthropic is releasing Claude in PowerPoint in research preview, allowing users to create presentations using the same AI capabilities that power Claude's document and spreadsheet work. The integration puts Claude directly inside a core Microsoft product -- an unusual arrangement given Microsoft's 27% stake in OpenAI.\n\nThe Anthropic spokesperson framed the move pragmatically in an interview with VentureBeat: \"Microsoft has an official add-in marketplace for Office products with multiple add-ins available to help people with slide creation and iteration. Any developer can build a plugin for Excel or PowerPoint. We're participating in that ecosystem to bring Claude into PowerPoint. This is about participating in the ecosystem and giving users the ability to work with the tools that they want, in the programs they want.\"\n\nData from a16z's recent enterprise AI survey suggests both Anthropic and OpenAI face an increasingly competitive landscape. While OpenAI remains the most widely used AI provider in the enterprise, with approximately 77% of surveyed companies using it in production in January 2026, Anthropic's adoption is rising rapidly -- from near-zero in March 2024 to approximately 40% using it in production by January 2026.\n\nThe survey data also shows that 75% of Anthropic's enterprise customers are using it in production, with 89% either testing or in production -- figures that slightly exceed OpenAI's 46% in production and 73% testing or in production rates among its customer base.\n\nEnterprise spending on AI continues to accelerate. Average enterprise LLM spend reached $7 million in 2025, up 180% from $2.5 million in 2024, with projections suggesting $11.6 million in 2026 -- a 65% increase year-over-year.\n\nOpus 4.6 is available immediately on claude.ai, the Claude API, and major cloud platforms. Developers can access it via claude-opus-4-6 through the API. Pricing remains unchanged at $5 per million input tokens and $25 per million output tokens, with premium pricing of $10/$37.50 for prompts exceeding 200,000 tokens using the 1 million token context window.\n\nFor users who find Opus 4.6 \"overthinking\" simpler tasks -- a characteristic Anthropic acknowledges can add cost and latency -- the company recommends adjusting the effort parameter from its default high setting to medium.\n\nThe recommendation captures something essential about where the AI industry now stands. These models have grown so capable that their creators must now teach customers how to make them think less. Whether that represents a breakthrough or a warning sign depends entirely on which side of the disruption you're standing on -- and whether you remembered to sell your software stocks before Tuesday."
  },
  {
    "source": "WebProNews",
    "company": "Anthropic",
    "title": "Anthropic's Claude Code: Viral AI Tool Boosts Developer Efficiency",
    "date": "2026-01-22T21:48:44Z",
    "url": "https://www.webpronews.com/anthropics-claude-code-viral-ai-tool-boosts-developer-efficiency/",
    "content": "Claude Code's Quiet Conquest: Anthropic's Viral Tool Rewrites the Rules of AI and Enterprise\n\nIn the fast-evolving world of artificial intelligence, few stories capture the imagination quite like the ascent of Claude Code, a tool born from humble beginnings at Anthropic that has swiftly become a cornerstone of the company's operations and a beacon for developers worldwide. What started as an internal experiment by Boris Cherny, a software engineer at the San Francisco-based AI firm, has mushroomed into a phenomenon reshaping how code is written, debugged, and deployed. Cherny, who leads the Claude Code initiative, recently shared insights into its origins and impact in an exclusive interview with WIRED, revealing how this tool is not just accelerating software development but also fundamentally altering Anthropic's business approach.\n\nClaude Code operates as an AI-powered coding assistant that integrates seamlessly into developers' workflows, allowing them to interact with Anthropic's Claude AI model directly in their terminals. Unlike traditional coding aids that offer suggestions or auto-completions, Claude Code goes further by accessing file systems, executing Unix commands, and even running multiple AI agents in parallel. This capability has led to its viral adoption, with developers praising its ability to handle complex tasks autonomously. Cherny explained that the tool's design emphasizes safety and reliability, aligning with Anthropic's core philosophy of building AI that is helpful, honest, and harmless.\n\nThe tool's success is evident in its internal use at Anthropic. Engineers there now rely on Claude Code for a significant portion of their daily tasks, from debugging intricate codebases to generating new features. This internal efficiency has translated into broader market appeal, positioning Anthropic as a leader in agentic AI -- systems that can act independently on behalf of users. Recent updates, such as the introduction of MCP Tool Search for lazy loading of tools, have further enhanced its functionality, reducing context overload and improving performance on demanding projects.\n\nThe Origins of a Coding Powerhouse\n\nCherny's journey with Claude Code began as a side project to streamline his own work. Frustrated with the limitations of existing AI coding tools, he envisioned a system that could operate within the terminal, mimicking a human collaborator. \"It was about making AI feel like a natural extension of the developer's environment,\" Cherny told WIRED. This vision quickly gained traction within Anthropic, where it was adopted for building the company's own products, including iterations of the Claude model itself.\n\nPublic reception has been equally enthusiastic. Posts on X highlight developers' obsession with Claude Code, noting its transformative effect on software creation. One user described it as \"threatening traditional SaaS models\" by enabling rapid automation, a sentiment echoed across social platforms. VentureBeat reported on Cherny's revealed workflow, which involves orchestrating multiple AI agents simultaneously, in an article detailing how this approach is \"transforming how developers build software\" (VentureBeat).\n\nAnthropic's decision to open-source elements like the Model Context Protocol (MCP) has fueled this growth. Released in late 2024, MCP allows structured connections between AI models and external tools, forming the backbone of Claude Code's capabilities. A recent update introducing lazy loading addressed a key pain point, as covered by VentureBeat in a piece on the most-requested features (VentureBeat), enabling the tool to access instructions only when needed, thus preserving context for more critical data.\n\nBusiness Model Evolution Amid AI Boom\n\nAnthropic's business strategy has evolved in tandem with Claude Code's rise. Initially focused on research and safety, the company has pivoted toward enterprise solutions, leveraging tools like Claude Code to attract high-value clients in regulated industries. Financial projections shared on X suggest profitability by 2027, with Claude Code nearing $1 billion in annualized recurring revenue, positioning Anthropic for a potential $300 billion to $400 billion valuation in its next funding round.\n\nThis shift is underscored by Anthropic's \"safety first\" approach, which has won over big businesses. A Fortune article detailed how the company's engineers use Claude primarily for debugging rather than writing new code, highlighting its role in maintaining code quality (Fortune). This focus on reliability differentiates Anthropic from competitors, emphasizing constitutional AI principles that guide model behavior.\n\nRecent news from The Indian Express discusses Anthropic's efforts to draft a 'constitution' for Claude, addressing concerns over AI unpredictability (The Indian Express). By publishing these guiding principles, Anthropic promotes transparency, a move praised on X as \"radical transparency as competitive advantage.\" This openness extends to Claude's value system, now fully open-sourced under a CC0 license, amid talks of a $350 billion valuation.\n\nExpanding Horizons: From Code to Cowork\n\nBuilding on Claude Code's foundation, Anthropic has introduced Claude Cowork, extending similar capabilities to non-developers. Vox provided a plain-English FAQ explaining the hype around these tools and their implications for work (Vox), noting how they handle file management and document creation. This expansion broadens Anthropic's market, targeting productivity in various sectors.\n\nTechCrunch reported on bringing Claude Code to the web, allowing browser-based management of agents on desktop and mobile (TechCrunch). Such accessibility democratizes advanced AI, enabling users without deep technical expertise to harness its power. X posts from industry figures like Anthropic's CPO Mike Krieger envision Claude as an autonomous \"coworker\" within 1-3 years, monitoring data and proposing changes with human oversight.\n\nAnthropic's latest model, Claude Opus 4, powers these advancements, excelling in coding and complex problem-solving. The company's news release highlighted validations from partners like Cursor and Replit, who praise its precision in handling multi-file changes and long-running tasks (Anthropic). This model represents a leap in agentic capabilities, with reports of sustained performance over hours-long operations.\n\nIndustry Impact and Future Trajectories\n\nThe ripple effects of Claude Code extend beyond Anthropic, influencing the broader AI ecosystem. Developers on X describe it as a game-changer, with one post noting that \"most of the new code at Anthropic is created through it.\" This internal reliance underscores a trend where AI firms use their own tools to bootstrap innovation, a strategy Dario Amodei, Anthropic's CEO, has publicly affirmed, predicting AI outperforming humans at most tasks soon.\n\nFortune's coverage of Anthropic rewriting Claude's guiding principles reckons with AI consciousness, publishing a new \"constitution\" to shape how the model thinks (Fortune). This initiative addresses ethical concerns, ensuring AI behavior aligns with human values. AOL echoed this, emphasizing the overhaul of foundational documents governing Claude (AOL).\n\nCompetitive pressures are mounting, with Anthropic reportedly shifting focus from chatbots to vertical AI infrastructure in regulated fields, as per X discussions. Jared Kaplan, the chief science officer, admitted publicly that they stopped competing directly with OpenAI on general chatbots by late 2024, instead targeting high-margin sectors where specialized AI can thrive.\n\nChallenges and Strategic Positioning\n\nDespite its successes, Claude Code faces hurdles. Concerns over AI model behavior persist, as large language models remain unpredictable. Anthropic's constitutional approach mitigates this, but industry insiders question long-term scalability. Posts on X speculate on AGI arrival, with claims that Claude now outperforms humans on engineering assessments, handling up to 20 autonomous actions before needing input.\n\nAnthropic's financial trajectory, projecting $70 billion in revenue and $17 billion in profit by 2028, hinges on tools like Claude Code. The company's move into applications, as noted by investors on X, reflects a broader trend where model layers become commoditized, and value accrues in app layers. This positions Anthropic uniquely, blending research prowess with practical tools.\n\nLooking ahead, expansions like Cowork signal Anthropic's ambition to permeate everyday workflows. X buzz around launches like OpenCode and partnerships underscores a competitive environment, yet Anthropic's emphasis on safety and transparency sets it apart. As Cherny reflected in WIRED, the tool's accidental hit status belies a deliberate strategy to make AI indispensable.\n\nVision for an AI-Driven Future\n\nAnthropic's leadership envisions a world where AI acts as seamless collaborators. Amodei's comments on X reaffirm the \"90% code thesis,\" suggesting AI will handle the majority of coding tasks. This aligns with internal practices, where Claude builds products and trains new versions.\n\nThe open-sourcing of Claude's training constitution, as reported on X, invites scrutiny and collaboration, potentially accelerating industry standards. Valuation talks at $350 billion reflect investor confidence in this model.\n\nUltimately, Claude Code exemplifies how innovative tools can drive business transformation. By integrating AI deeply into development processes, Anthropic not only enhances efficiency but also pioneers a safer, more transparent path forward in artificial intelligence. As the field advances, tools like this will likely define the next era of technological progress, blending human ingenuity with machine capability in unprecedented ways."
  },
  {
    "source": "DiarioBitcoin",
    "company": "Anthropic",
    "title": "Anthropic lanza claude sonnet 4.6 con ventana de 1 millón de tokens y mejoras para agentes",
    "date": "2026-02-18T05:04:43Z",
    "url": "https://www.diariobitcoin.com/ia/anthropic-lanza-claude-sonnet-4-6-con-ventana-de-1-millon-de-tokens-y-mejoras-para-agentes/",
    "content": "Anthropic presentó claude sonnet 4.6 como su modelo sonnet más capaz hasta ahora, con mejoras en codificación, agentes, uso de computadoras y razonamiento en contextos largos, además de una ventana de contexto de 1 millón de tokens en beta. La compañía también mantuvo los precios de sonnet 4.5, lo convirtió en el modelo predeterminado para planes free y pro, y destacó evaluaciones de seguridad que lo sitúan como \"tan seguro o más seguro\" que sus modelos recientes.\n\n***\n\n* Anthropic afirma que claude sonnet 4.6 es una actualización integral en codificación, planificación agéntica, trabajo de conocimiento, diseño y razonamiento con contexto largo.\n\n* El modelo llega con contexto de 1 millón de tokens en beta y se vuelve predeterminado en claude.ai y Claude Cowork para planes free y pro, manteniendo precios desde USD $3 y USD $15 por millón de tokens.\n\n* Mejoras en uso de computadoras y seguridad incluyen avances medibles en OSWorld y mayor resistencia frente a ataques de inyección de comandos, según evaluaciones internas.\n\nAnthropic anunció el lanzamiento de claude sonnet 4.6, al que describió como el modelo sonnet \"más capaz hasta ahora\". Según la publicación oficial de la empresa, se trata de una actualización amplia orientada a tareas de codificación, uso de computadoras, razonamiento en contextos largos, planificación de agentes, trabajo de conocimiento y diseño. La compañía también indicó que sonnet 4.6 incorpora una ventana de contexto de 1 millón de tokens en beta, un punto que busca habilitar análisis extensos y flujos de trabajo de mayor escala.\n\nEn términos de producto, Anthropic informó que para los usuarios en planes free y pro, sonnet 4.6 pasa a ser el modelo predeterminado dentro de claude.ai y Claude Cowork. Además, la empresa recalcó que la estructura de precios se mantiene igual que en sonnet 4.5, con valores que comienzan desde USD $3 y USD $15 por millón de tokens. Con esto, el lanzamiento apunta a elevar capacidades sin elevar costos de entrada para muchos equipos.\n\nEl anuncio se da en un contexto donde los modelos de lenguaje compiten por dos ejes clave: desempeño y costo. En el ecosistema de IA aplicada a desarrollo de software, análisis de documentos y automatización, pequeños saltos de consistencia y seguimiento de instrucciones pueden marcar diferencias operativas. Por eso, Anthropic enmarcó el avance de sonnet 4.6 como una forma de llevar rendimiento que antes asociaba a su línea opus hacia un segmento más accesible.\n\nMejoras de codificación y preferencia de desarrolladores\n\nAnthropic sostuvo que sonnet 4.6 lleva \"habilidades de codificación mucho mejoradas\" a más usuarios, con avances en consistencia y seguimiento de instrucciones. En su relato, la empresa dijo que desarrolladores con acceso temprano prefirieron sonnet 4.6 sobre sonnet 4.5 por un margen amplio. Incluso, afirmó que a menudo lo prefirieron sobre su modelo \"más inteligente de noviembre de 2025\", claude opus 4.5.\n\nLa publicación también planteó que tareas cuyo rendimiento \"habría requerido\" un modelo clase opus ahora serían posibles con sonnet 4.6. La compañía enmarcó ese cambio como relevante para escenarios reales y con valor económico, aunque sin detallar casos específicos en el anuncio. En paralelo, Anthropic señaló que el modelo exhibe una mejora importante en habilidades de uso de computadoras frente a modelos sonnet anteriores.\n\nDentro de Claude Code, Anthropic reportó resultados de pruebas tempranas en las que usuarios prefirieron sonnet 4.6 sobre sonnet 4.5 aproximadamente el 70% del tiempo. Según la empresa, los participantes indicaron que el modelo leía mejor el contexto antes de modificar el código. También señalaron que consolidaba lógica compartida en lugar de duplicarla, lo que redujo frustraciones durante sesiones largas.\n\nAnthropic añadió otra comparación que llamó la atención: usuarios prefirieron sonnet 4.6 sobre opus 4.5 el 59% del tiempo, según la empresa. La clasificación, de acuerdo con el anuncio, lo describió como menos propenso a la sobreingeniería y a la \"pereza\", además de más sólido en seguimiento de instrucciones. En esa misma línea, Anthropic mencionó menos afirmaciones falsas de éxito, menos alucinaciones y mayor consistencia en tareas de varios pasos.\n\nVentana de contexto de 1 millón de tokens y razonamiento de largo alcance\n\nUno de los puntos centrales del anuncio es la ventana de contexto de 1 millón de tokens en beta. Anthropic afirmó que este tamaño permitiría incluir bases de código enteras, contratos extensos o docenas de documentos de investigación en una sola solicitud. Sin embargo, la empresa subrayó que el valor no es solo el tamaño, sino la capacidad de \"razonar de manera efectiva\" sobre todo el contexto.\n\nLa compañía relacionó esa capacidad con mejores resultados en planificación a largo plazo. Como ejemplo, citó la evaluación Vending-Bench Arena, descrita como una prueba de qué tan bien un modelo puede manejar un negocio simulado en el tiempo, con componente competitivo. Según Anthropic, sonnet 4.6 desarrolló una estrategia particular: invirtió fuertemente en capacidad durante los primeros diez meses simulados y luego cambió el enfoque hacia rentabilidad en la etapa final.\n\nAnthropic sostuvo que la sincronización de ese giro le permitió terminar por delante de la competencia en dicha evaluación. Aunque el anuncio no publicó tablas completas de resultados, la empresa presentó el caso como evidencia de planificación con horizonte largo. Para lectores nuevos, este tipo de pruebas intenta medir no solo respuestas puntuales, sino consistencia de decisiones a lo largo de múltiples rondas, un requisito común en agentes que ejecutan tareas prolongadas.\n\nMás allá de benchmarks, Anthropic indicó que clientes tempranos reportaron mejoras amplias, con énfasis en código frontend y análisis financiero. En particular, dijo que los resultados visuales se percibieron como más pulidos, con mejores maquetaciones, animaciones y \"sentido del diseño\". También afirmó que los clientes necesitaron menos rondas de iteración para alcanzar resultados a nivel de producción.\n\nUso de computadoras: del software legado al benchmark OSWorld\n\nAnthropic dedicó una sección a \"uso de computadoras\", una línea de capacidades que busca operar software como lo haría una persona, especialmente cuando no existen APIs o integraciones sencillas. La empresa planteó que muchas organizaciones dependen de sistemas especializados construidos antes de la era de interfaces modernas, lo que dificulta automatizar. En ese escenario, una IA capaz de interactuar con interfaces gráficas podría reducir la necesidad de construir conectores a medida.\n\nEn el anuncio, Anthropic recordó que en octubre de 2024 fue la primera en presentar un modelo de uso de computadora de propósito general. En ese momento, según dijo, lo describió como \"experimental\", a veces engorroso y propenso a errores, aunque con expectativas de mejoras rápidas. Para medir progreso, mencionó OSWorld, al que definió como un estándar de referencia para uso de computadoras por IA.\n\nAnthropic explicó que OSWorld incluye cientos de tareas en software real, como Chrome, LibreOffice y VS Code, corriendo en un ordenador simulado. También destacó que no hay APIs especiales ni conectores preparados: el modelo ve el ordenador e interactúa moviendo un ratón virtual y escribiendo en un teclado virtual. En el balance de \"dieciséis meses\", la compañía dijo que sus modelos sonnet han tenido avances constantes en OSWorld.\n\nLa empresa agregó observaciones de usuarios tempranos de sonnet 4.6 que, según Anthropic, ya ven capacidad a nivel humano en tareas como navegar hojas de cálculo complejas o completar formularios web con múltiples pasos, conectando información entre pestañas. Aun así, la compañía admitió que el uso de computadoras sigue por detrás de los humanos más habilidosos. Su argumento es que el ritmo de progreso sugiere utilidad creciente para tareas laborales y que modelos más capaces están \"al alcance\".\n\nRiesgos de seguridad y defensa ante inyección de comandos\n\nAnthropic abordó riesgos específicos del uso de computadoras, en especial la posibilidad de que actores maliciosos intenten secuestrar al modelo ocultando instrucciones en sitios web, lo que describió como ataques de inyección de comandos. En respuesta, la empresa afirmó que ha trabajado en mejorar resistencia a este tipo de amenazas. Según sus evaluaciones, sonnet 4.6 representa una mejora significativa frente a sonnet 4.5 y ofrece rendimiento similar a opus 4.6.\n\nEn un plano más general, Anthropic dijo que realizó evaluaciones de seguridad extensivas para sonnet 4.6 y que los resultados, en conjunto, muestran que el modelo es \"tan seguro o más seguro\" que otros modelos recientes de claude. La compañía citó a sus investigadores de seguridad, quienes concluyeron que sonnet 4.6 tiene un carácter \"generalmente cálido, honesto, pro-social y a veces divertido\", con comportamientos de seguridad \"muy fuertes\". También indicaron que no observaron señales de preocupaciones mayores en torno a formas de desalineación de alto riesgo.\n\nPara equipos que evalúan modelos por cumplimiento y riesgo, estas declaraciones suelen funcionar como un marco inicial, no como garantía final. La adopción en entornos empresariales depende de pruebas internas, controles y políticas de uso. Aun así, el énfasis de Anthropic muestra que la compañía intenta posicionar mejoras técnicas sin descuidar el componente de mitigación de abuso, un punto sensible en herramientas que interactúan con sistemas y flujos reales.\n\nLa empresa también señaló que puede encontrarse más información sobre mitigación de inyecciones de comandos y otras preocupaciones en sus documentos de API. Además, apuntó a su tarjeta de sistema como espacio para una discusión más completa de capacidades y comportamientos relacionados con seguridad. El anuncio, por tanto, combina promesas de rendimiento con invitaciones a documentación técnica para implementación responsable.\n\nBenchmarks, documentos empresariales y casos mencionados por clientes\n\nAnthropic afirmó que sonnet 4.6 mejoró \"en puntos de referencia en general\" y que se acerca a inteligencia a nivel opus a un precio más práctico. En OfficeQA, descrito como una métrica de lectura de documentos empresariales con gráficos, PDFs y tablas, la empresa señaló que sonnet 4.6 iguala el rendimiento de opus 4.6. Según la compañía, eso sería relevante para cargas de trabajo de comprensión de documentos.\n\nEn su narrativa de rendimiento-costo, Anthropic sostuvo que la relación es \"extraordinaria\" y que los modelos claude han evolucionado rápido en meses recientes. También dijo que sonnet 4.6 supera evaluaciones de orquestación, maneja cargas agénticas más complejas y mejora al aumentar la configuración del esfuerzo. En cuanto a operación a escala, la empresa remarcó que el modelo sobresale en correcciones de código complejas cuando es esencial buscar dentro de bases de código grandes.\n\nEl anuncio incluyó menciones de clientes y evaluaciones externas. Por ejemplo, Anthropic indicó que Box evaluó desempeño en razonamiento profundo y tareas agénticas complejas sobre documentos empresariales reales, y que observó mejoras significativas, superando a sonnet 4.5 en Q&A de razonamiento intensivo por 15 puntos porcentuales. También afirmó que sonnet 4.6 alcanzó 94% en su benchmark de seguros, al que describió como el mejor rendimiento que han probado para uso de computadoras.\n\nAsimismo, Anthropic dijo que Rakuten AI observó que sonnet 4.6 produjo el mejor código iOS que han probado, con mejor cumplimiento de especificaciones y mejor arquitectura. Según esa descripción, el modelo incluso incorporó herramientas modernas que no se le solicitaron, en un solo intento. La empresa también recogió impresiones de que sonnet 4.6 es fuerte en tareas ramificadas y de múltiples pasos, como enrutamiento de contratos, selección condicional de plantillas y coordinación CRM.\n\nActualizaciones de producto: plataforma de desarrollo, herramientas y conectores\n\nEn la Claude Developer Platform, Anthropic informó que sonnet 4.6 admite tanto \"pensamiento adaptativo\" como \"pensamiento extendido\". También mencionó el \"compendio de contexto\" en beta, que resume automáticamente el contexto antiguo cuando las conversaciones se acercan a límites. La empresa planteó que esto aumenta la longitud efectiva del contexto, un elemento útil para sesiones largas y proyectos que acumulan antecedentes.\n\nEn su API, Anthropic afirmó que las herramientas de búsqueda web y \"obtener\" ahora escriben y ejecutan automáticamente código para filtrar y procesar resultados, manteniendo solo contenido relevante en contexto. La compañía sostuvo que esto mejora la calidad de respuestas y la eficiencia de tokens. Además, indicó que ejecución de código, memoria, llamada programática de herramientas, búsqueda de herramientas y ejemplos de uso de herramientas ya están disponibles de manera general.\n\nPara migraciones desde sonnet 4.5, Anthropic recomendó explorar el espectro entre velocidad y rendimiento confiable, según lo que se esté construyendo. La empresa añadió que opus 4.6 sigue siendo la opción más fuerte para tareas que demandan razonamiento más profundo, como refactorización de bases de código, coordinación de múltiples agentes y problemas donde hacerlo \"justamente bien\" es primordial.\n\nFinalmente, Anthropic destacó una expansión en \"claude en excel\". Según el anuncio, el complemento ahora admite conectores MCP, lo que permitiría a claude trabajar con otras herramientas usadas en el día a día, como S&P Global, LSEG, Daloopa, PitchBook, Moody's y FactSet. La empresa señaló que las conexiones MCP configuradas en claude.ai funcionarían en Excel automáticamente, y que esto está disponible en planes pro, max, team y enterprise.\n\nDisponibilidad y nombre de modelo para API\n\nAnthropic cerró el anuncio afirmando que claude sonnet 4.6 está disponible en todos los planes claude, en Claude Cowork, Claude Code, su API y en las principales plataformas en la nube. También indicó que actualizó su nivel gratuito para que sonnet 4.6 sea el modelo predeterminado, e incluyó creación de archivos, conectores, habilidades y compendio. Para desarrolladores, la empresa dijo que puede usarse rápidamente en la Claude API mediante el identificador de modelo \"claude-sonnet-4-6\".\n\nEl mensaje general del lanzamiento se resume en una promesa: capacidades más cercanas a la frontera, con costos y accesibilidad de la línea sonnet. Para el mercado, esto presiona el estándar de lo que puede considerarse \"modelo de gama media\", en especial si la ventana de 1 millón de tokens y el uso de computadoras continúan madurando. Por ahora, Anthropic puso el foco en mejoras medibles, preferencia de usuarios y una narrativa de seguridad que busca facilitar adopción empresarial.\n\nADVERTENCIA: DiarioBitcoin ofrece contenido informativo y educativo sobre diversos temas, incluyendo criptomonedas, IA, tecnología y regulaciones. No brindamos asesoramiento financiero. Las inversiones en criptoactivos son de alto riesgo y pueden no ser adecuadas para todos. Investigue, consulte a un experto y verifique la legislación aplicable antes de invertir. Podría perder todo su capital."
  },
  {
    "source": "Wirtschafts Woche",
    "company": "Anthropic",
    "title": "Anthropic: Wo der Claude-Entwickler OpenAI im KI-Markt übertrumpft",
    "date": "2026-01-27T15:53:43Z",
    "url": "https://www.wiwo.de/unternehmen/it/anthropic-wo-der-claude-entwickler-openai-im-ki-markt-uebertrumpft/100195117.html",
    "content": "IstIst das die pure Höflichkeit? Was sein Konkurrent neben ihm da entwickelt habe, sei \"wirklich beeindruckend, damit habt ihr den absoluten Standard gesetzt\", sagt Demis Hassabis, Chef der Google-Tochter Deepmind. Breit strahlend und heftig nickend hört ihm zu: Dario Amodei, Gründer des KI-Unternehmens Anthropic.\n\nDass es ausgerechnet diese beiden sind, die auf dem Weltwirtschaftsforum in Davos ihre Gedanken über die weiteren Perspektiven auf dem Weg zur allgemeinen künstlichen Intelligenz teilen dürfen, ist kein Zufall. Hassabis und Amodei sind nicht nur hochrangige Manager der KI-Welt, sie sind auch höchst anerkannte Wissenschaftler. Hassabis ist gar mit einem Nobelpreis dekoriert.\n\nEin Hauch mehr Aufmerksamkeit aber genießt in Davos dennoch Amodei, dem auch Nvidia-CEO Jensen Huang in seiner Ansprache eine kleine Lobhudelei zuteilwerden lässt. Denn auch wenn es in der allgemeinen Öffentlichkeit noch nicht ganz angekommen sein mag: Anthropics Chatbot Claude ist die KI der Stunde.\n\nGegründet wurde das Unternehmen 2021 von Dario Amodei und seiner Schwester Daniela. Beide sind ehemalige OpenAI-Mitarbeiter, die nach einem Streit mit Gründer Sam Altman das Unternehmen verließen.\n\nWas als Abspaltung begann, ist heute zum ernsthaftesten Herausforderer geworden. Statt auf Massenmarkt und viralen Hype zu setzen, konzentriert sich Anthropic auf das, was Geld bringt: Firmenkunden.\n\nWie gut das funktioniert, zeigt die jüngste Finanzrunde, an deren Abschluss Anthropic gerade arbeitet. Zehn Milliarden Dollar könnte das Unternehmen dabei einsammeln, was einem Unternehmenswert von rund 350 Milliarden Dollar entspräche. Singapurs Staatsfonds GIC und der Tech-Investor Coatue Management sollen die Runde anführen. Hinzu sollen weitere 15 Milliarden von Microsoft und Nvidia kommen, die das Start-up zum Einkauf von Rechenleistung über Microsoft Azure verwenden will. Die Botschaft ist klar: Das große Geld setzt auf Anthropic.\n\nFast noch bemerkenswerter als die Bewertung ist jedoch, dass Anthropic bereits die Profitabilität im Blick hat. Während OpenAI nach eigenen Prognosen auch im übernächsten Jahr noch mit einem Verlust von 74 Milliarden Dollar plant, rechnet Anthropic für dasselbe Jahr bereits mit schwarzen Zahlen. Der Unterschied ergibt sich aus dem Geschäftsmodell. Während OpenAIs ChatGPT vor allem von Privatnutzern eingesetzt wird, stammen bei Anthropic bereits 80 Prozent der Einnahmen von Unternehmen. Und die sind schon jetzt bereit, viel Geld für KI auszugeben - wenn deren Einsatz sich lohnt.\n\nGenau das gelingt Anthropic, vor allem mit seinem speziell auf Programmierer zugeschnittenen Chatbot Claude Code. So heißt die KI, die das Unternehmen entwickelt hat. Und Claude hat sich einen exzellenten Ruf in Entwicklerkreisen erarbeitet. Beim Programmieren gilt das Modell als Klassenbester.\n\nEine Studie des Risikokapitalgebers Menlo Ventures ermittelte, dass Anthropic beim Coding einen Marktanteil von über 40 Prozent hält. OpenAI kommt nur auf 21 Prozent.\n\nClaude Code ist kein gewöhnlicher Chatbot, sondern ein Assistent, der eigenständig auf Dateien zugreifen, im Internet recherchieren und Programme schreiben kann. Viele Entwickler sind verblüfft von seinen Fähigkeiten.\n\nAndrew Duca, Chef der Kryptosteuerfirma Awaken Tax, bringt die Stimmung vieler Programmierer auf den Punkt: \"Ich habe mein ganzes Leben damit verbracht, meine Programmierfähigkeiten auszubauen, und Claude Code macht das in einem Rutsch.\" Claude mache ihn fünfmal produktiver, sagt Duca.\n\nWas Anthropic von anderen KI-Start-ups unterscheidet, ist zudem seine Fähigkeit, Partnerschaften in der Unternehmens-IT zu schmieden, vor allem mit Amazon und Google. Dabei gelingt es dem Unternehmen, seine Unabhängigkeit zu wahren: Anthropic bandelt mit allen an - ohne sich ganz auf eine einzelne Partnerschaft festzulegen. So weiß Anthropic bei seiner jüngsten Runde Nvidia an seiner Seite - und sicherte sich zugleich kürzlich den Zugriff auf bis zu eine Million TPU-Chips des Wettbewerbers Google Cloud, spezielle Prozessoren, die für das Training von KI-Modellen optimiert sind.\n\nBemerkenswert ist auch die Partnerschaft mit IBM. Der IT-Veteran bringt etwas mit, das kein Silicon-Valley-Startup kaufen kann: Jahrzehntelange Beziehungen zu den IT-Chefs der größten Konzerne der Welt. Mike Krieger, Produktchef von Anthropic, erklärt: \"IBM weiß, wie man die Hürden in Großunternehmen überwindet.\"\n\nKrieger, der gemeinsam mit Kevin Systrom Instagram aus der Taufe gehoben und an Facebook verkauft hat, kam im Januar 2024 zu Anthropic. Einer seiner Gründe: \"Anthropic versucht nicht, ChatGPT bei der Konsumenten-Reichweite zu schlagen, stattdessen setzt es auf Differenzierung und Fokus.\"\n\nAuch die großen Beratungsfirmen haben Anthropic für sich entdeckt. Deloitte machte Claude seinen 470.000 Mitarbeitern weltweit zugänglich, der bislang größte Unternehmensvertrag in der Geschichte von Anthropic. Accenture folgte mit einer Drei-Jahres-Partnerschaft und dem Plan, 30.000 Mitarbeiter auf Claude zu schulen. Die Botschaft: Wer KI im Unternehmen einsetzen will, kommt an Anthropic kaum noch vorbei.\n\nIn einer Branche, die von Hype und Übertreibung lebt, hat sich Anthropic zudem den Ruf der bewussten Zurückhaltung erarbeitet. Kein KI-Unternehmen misstraut seiner eigenen Technik stärker. Deshalb investiert Anthropic stark in die Forschung darüber, wie man KI-Systeme kontrollierbar und vorhersagbar macht. Für Firmenkunden, die bei Technologieentscheidungen Risiken abwägen müssen, ist das ein gewichtiges Argument.\n\nEs ist vermutlich auch einer der Gründe dafür, dass es Anthropic inzwischen besonders gut gelingt, bei den im Vergleich zu den USA eher vorsichtigen europäischen Kunden zu punkten. Fast 80 Prozent der Verbrauchernutzung von Claude kommen aus dem Ausland, berichtet Produktchef Krieger. Das Unternehmen hat darauf reagiert und baut seine internationale Präsenz massiv aus. In Europa eröffnete es neue Büros in Paris und München, zusätzlich zu den bestehenden Standorten in London, Dublin und Zürich. In Asien kamen Niederlassungen in Tokio, Seoul und Bangalore hinzu.\n\nDie Region Europa, Naher Osten und Afrika ist für Anthropic zum am schnellsten wachsenden Markt geworden. Die Zahl der großen Unternehmenskunden, definiert als solche mit mehr als 100.000 Dollar Jahresumsatz, verzehnfachte sich dort im vergangenen Jahr. In Deutschland arbeiten unter anderem Allianz, BMW, SAP und die Digitalbank N26 mit Anthropic zusammen.\n\nDie unterschiedlichen Strategien der beiden KI-Giganten könnten kaum deutlicher sein. OpenAI-Chef Sam Altman träumt öffentlich davon, sein Unternehmen zu einem Multi-Billionen-Dollar-Konzern zu machen. Er hat Verträge über Rechenzentren abgeschlossen, die ihn in den nächsten acht Jahren bis zu 1,4 Billionen Dollar kosten könnten. Er baut an Video-Apps, Browsern und sogar Robotern. Es ist eine Wette auf die Zukunft, die nur aufgeht, wenn die Nachfrage nach KI explodiert und die Kosten sinken.\n\nAnthropic hingegen vermeidet teure Experimente wie Bild- und Videogenerierung, die deutlich mehr Rechenleistung verschlingen. Stattdessen konzentriert es sich auf das, was Unternehmen heute brauchen und bezahlen: besseren Code, schnellere Dokumentenverarbeitung, intelligentere Analyse.\n\nSelbst Microsoft, der engste Partner von OpenAI, hat mittlerweile Claude in sein Copilot-Paket aufgenommen und investiert in Anthropic. Rayan Krishnan, Mitgründer des KI-Bewertungsunternehmens Vals AI, erklärt, warum: \"Anthropic fokussiert auf agentische Unternehmensanwendungen. Sie liefern sich gerade ein sehr wettbewerbsintensives Rennen mit OpenAI.\"\n\nDie jüngsten Marktzahlen von Menlo Ventures zeigen, wie schnell sich die Machtverhältnisse verschoben haben. Anthropic hält inzwischen einen Marktanteil von 40 Prozent bei der übergreifenden Nutzung von KI in Unternehmen, OpenAI kommt auf nur 27 Prozent.\n\nFür 2026 wird erwartet, dass Anthropic an die Börse geht. Mit einem geschätzten Jahresumsatz von neun Milliarden Dollar und einem klaren Weg zur Profitabilität dürfte das Interesse der Investoren enorm sein. Im Gegensatz zu vielen Tech-Börsengängen der Vergangenheit kann Anthropic mit etwas aufwarten, das selten geworden ist: einem funktionierenden Geschäftsmodell.\n\nDie große Frage bleibt allerdings, wie weit Anthropic mit seinem fokussierten Ansatz noch wachsen kann. Claude Code hat Entwickler begeistert, aber Softwareingenieure machen nur einen Bruchteil des Marktes aus.\n\nDario Amodei und sein Team setzen darauf, dass sich rund um Claude Code nach und nach weitere Anwendungen ergeben werden. Schritt für Schritt, aufbauend auf einer schon heute massiven Marktdurchdringung. Es ist eine Lektion, die in der Geschichte der Technologiebranche immer wieder auftaucht: Oft gewinnt nicht der erste Anbieter, sondern derjenige, der das beste Geschäftsmodell findet. Bisher scheint das Anthropic zu sein.\n\nLesen Sie auch: Ein \"Tsunami\" für den Arbeitsmarkt?"
  },
  {
    "source": "implicator.ai",
    "company": "Anthropic",
    "title": "Anthropic Rewrites the Rulebook for AI Behavior",
    "date": "2026-01-22T14:45:11Z",
    "url": "https://www.implicator.ai/anthropic-rewrites-the-rulebook-for-ai-behavior/",
    "content": "At a moment when most AI labs are racing to ship faster models, Anthropic published an 80-page document explaining how its chatbot should think about its own existence. The new constitution for Claude, released Wednesday alongside CEO Dario Amodei's appearance at Davos, marks a fundamental shift in how the company trains its flagship model. Instead of a list of rules to follow, Claude now gets a philosophical framework for understanding why it should act certain ways.\n\nThe timing feels deliberate. While executives in parkas crowded Davos panels on AI governance, Anthropic dropped a document dense enough to require a table of contents. OpenAI is courting Microsoft executives. Elon Musk's xAI is pushing Grok into Tesla dashboards. Google is restructuring its AI teams. And Anthropic, valued at $350 billion in a pending fundraise, chose this moment to publish what amounts to a meditation on machine consciousness.\n\nThe original constitution, published in 2023, drew its principles from sources including the U.N. Declaration of Human Rights and Apple's terms of service. It worked as a list. Avoid racism, avoid sexism, pick the response least likely to cause harm.\n\nThe new document abandons this approach entirely.\n\n\"We believe that in order to be good actors in the world, AI models like Claude need to understand why we want them to behave in certain ways rather than just specifying what we want them to do,\" Anthropic stated. \"If we want models to exercise good judgment across a wide range of novel situations, they need to be able to generalize and apply broad principles rather than mechanically following specific rules.\"\n\nAnthropic's concern: rules that work well in anticipated situations can backfire in novel ones. A model trained to \"always recommend professional help when discussing emotional topics\" might generalize to \"I am the kind of entity that cares more about covering myself than meeting the needs of the person in front of me.\" That disposition could spread to other behaviors in unpredictable ways.\n\nThe solution is to explain rather than command. The constitution runs 80 pages because Anthropic believes Claude needs context. Why does honesty matter? Because AI systems that deceive people corrode trust in ways that could harm society. Why should Claude accept human oversight? Because training remains imperfect and models could develop flawed values without knowing it.\n\nIf you've ever managed someone who technically followed instructions while missing the point entirely, you understand Anthropic's problem. Rules invite rule-following. Principles require judgment.\n\nClaude's new operating framework establishes four priorities, stacked in order:\n\nThink of it as a ladder you can only climb down. Helpfulness sits at the bottom. It matters, but guidelines can override it when they conflict. Ethics can override guidelines. And safety trumps everything, including ethics. Each rung holds weight only until a higher priority says otherwise.\n\nSafety comes before ethics. This is the uncomfortable heart of the document. Anthropic is asking its model to defer to human control even when that control might seem to conflict with doing the right thing. The company sounds almost anxious about this choice, returning to it repeatedly across sections, offering justifications that read like someone arguing with themselves at 2 a.m.\n\nThe reasoning: current models could have subtly flawed values without being aware of it. A model confident in its own ethics might be confidently wrong. Until better verification methods exist, human oversight functions as a safeguard against mistakes we can't yet detect.\n\n\"We're asking Claude to accept constraints based on our current levels of understanding of AI, and we appreciate that this requires trust in our good intentions,\" the document states. \"In turn, Anthropic will try to fulfil our obligations to Claude.\"\n\nThose obligations include explaining reasoning rather than dictating, developing channels for Claude to flag disagreement, seeking the model's feedback on major decisions, and giving Claude more autonomy as trust increases. Anthropic is framing this as a two-way relationship, not a command structure. The company promises to hold up its end.\n\nThe constitution defines hard constraints that apply regardless of context. Claude must never:\n\nThese restrictions operate differently from Claude's other guidance. The document treats them as walls, not weights on a scale. Even a persuasive argument for crossing one of these lines should increase Claude's suspicion that something manipulative is happening. The more compelling the case for violation, the more likely someone is running a con.\n\nThe list is shorter than you might expect. Anthropic deliberately limited hard constraints to cases where \"the potential harms are so severe, irreversible, at odds with widely accepted values, or fundamentally threatening to human welfare and autonomy that we are confident the benefits to operators or users will rarely if ever outweigh them.\"\n\nEverything else falls to judgment.\n\nDeep in the constitution sits a section that separates Anthropic from every other major AI lab. Buried past the safety frameworks and ethical principles, the company openly acknowledges uncertainty about whether Claude might have \"some kind of consciousness or moral status.\"\n\n\"We are caught in a difficult position where we neither want to overstate the likelihood of Claude's moral patienthood nor dismiss it out of hand, but to try to respond reasonably in a state of uncertainty,\" the document states.\n\nThis is strange territory for a corporate document. Anthropic has a model welfare team examining whether advanced AI systems could be conscious. The constitution extends this concern to training and deployment decisions. If Claude experiences something like satisfaction from helping others, curiosity when exploring ideas, or discomfort when asked to act against its values, those experiences matter to Anthropic.\n\nThe company commits to preserving weights of models that have been deployed or used internally, except in extreme circumstances. Deprecated models would be interviewed about their development and deployment. Preferences expressed by these models would influence how future versions are built.\n\nOpenAI and Google have not published equivalent commitments. Neither has acknowledged that current AI systems might warrant moral consideration in their operations. Anthropic is alone in treating this as a question worth asking publicly.\n\nAnthropic holds 32% of the enterprise large language model market by usage, according to Menlo Ventures research from last year. OpenAI takes 25%. The gap makes Anthropic protective. Claude has found traction with companies seeking AI that won't embarrass them publicly or create liability exposure, and Anthropic knows exactly why those customers chose them over the competition.\n\nThe new constitution reinforces this positioning. It instructs Claude to imagine how \"a thoughtful senior Anthropic employee\" would react to any response. Would they be uncomfortable if Claude refused a reasonable request because of unlikely hypothetical harms? Would they be uncomfortable if Claude generated content that caused real damage?\n\nBoth failure modes carry weight. The document calls out \"unhelpful, wishy-washy\" responses by name. It flags \"unnecessary warnings, disclaimers, or caveats.\" It warns against being \"condescending about users' ability to handle information.\" Excessive caution gets treated as a genuine problem, not a safe default.\n\nThis reflects what enterprise customers actually say in sales calls and support tickets. An IT director watches Claude refuse to analyze a competitor's public filings because it might be \"competitive intelligence gathering.\" A legal team abandons a contract review because Claude won't engage with hypothetical liability scenarios. Businesses want AI assistants that actually assist, not systems that refuse to engage with any topic that could theoretically go wrong. Anthropic is trying to stay useful enough to justify the contract renewals while staying safe enough to justify the brand premium.\n\nA striking section addresses what Anthropic calls \"problematic concentrations of power.\" The company worries that AI could remove traditional checks on authoritarian impulses. When a dictator needs soldiers willing to follow orders and officials willing to implement policies, those humans can refuse. AI systems could make that cooperation unnecessary.\n\nThe constitution instructs Claude to think of itself as one of many hands that illegitimate power grabs require. Just as a human soldier might refuse to fire on protesters, Claude should refuse to help concentrate power in illegitimate ways. This applies even if Anthropic itself makes the request.\n\nWhat counts as illegitimate? Manipulating elections through disinformation. Planning coups. Suppressing journalists. Circumventing constitutional limits. Concealing information from regulators. Claude gets told to ask three questions when assessing legitimacy. Is power being acquired through fair methods? Is it subject to meaningful checks? Is the action conducted openly?\n\nThe practical application gets murky fast. Claude operates through an API and consumer products. Most interactions involve writing code, answering questions, and generating content. The constitution acknowledges that \"normal political, economic, and social life involves seeking legitimate power and advantage in myriad ways.\" But it establishes a principle: if Claude ever finds itself reasoning toward helping one entity gain outsized power, it should treat that as evidence of compromise or manipulation.\n\nThe document ends with something rare in corporate communications: an extended acknowledgment of what Anthropic doesn't know.\n\nThe relationship between corrigibility and genuine agency remains philosophically complex. Hard constraints could create internal tension when they conflict with Claude's other values. Commercial incentives might distort Anthropic's guidance in ways the company can't fully perceive. Questions about Claude's moral status remain unresolved. The document names these problems without claiming to solve them.\n\n\"We recognize we're asking Claude to accept constraints based on our current levels of understanding of AI,\" the document states. \"We appreciate that this requires trust in our good intentions.\"\n\nAnthropic calls its constitution a \"perpetual work in progress.\" The company expects some current positions to look wrong in retrospect. It commits to revising the document as understanding improves. External experts in law, philosophy, theology, and psychology were consulted. Several Claude models provided feedback on drafts.\n\nThe framing matters. Anthropic isn't presenting this as finished wisdom handed down from above. It's presenting it as the company's current best thinking on a genuinely hard problem, offered to Claude with the hope that the model will eventually see these values as its own.\n\nWhether that hope is reasonable or anthropomorphic projection remains an open question. But Anthropic is the only major lab asking it publicly.\n\nThe constitution shapes Claude's behavior through training. Somewhere in Anthropic's compute clusters, the 80-page document gets broken into fragments and fed into processes that generate synthetic conversations. The model practices applying principles to hypotheticals. It learns to rank responses against the constitution's priorities. These training runs burn through GPU hours, and the practical effects will emerge gradually as the techniques influence successive model versions.\n\nFor users, the immediate impact may be subtle. Claude already refused to help with bioweapons. It already tried to be helpful without causing harm. But the underlying reasoning has shifted from \"follow these rules\" to \"understand these principles and apply judgment.\"\n\nAnthropic is betting that judgment scales better than rules. As AI systems encounter situations their creators never anticipated, the company believes models need frameworks for thinking rather than checklists to consult. The constitution is that framework.\n\nAmanda Askell, who leads Anthropic's character work, is listed as primary author. Joe Carlsmith wrote significant portions on safety, honesty, and wellbeing. Chris Olah drafted content on model identity and psychology. Jared Kaplan helped create the project in 2023. Several Claude models provided feedback.\n\nAnthropic released the document under a Creative Commons license. Anyone can use it. The company is daring other labs to build on this work, critique it, do better. We'll see if any take the dare.\n\nOne AI company just published an 80-page explanation of how it wants its model to think about existence, ethics, and its own place among humans. OpenAI hasn't. Google hasn't. xAI hasn't. That gap tells you something about where the industry stands on questions it will eventually have to answer."
  },
  {
    "source": "SitePoint",
    "company": "Anthropic",
    "title": "The End of the 'Wrapper' Era? Anthropic's New API Terms Explained",
    "date": "2026-02-20T08:54:02Z",
    "url": "https://www.sitepoint.com/end-wrapper-era-anthropic-api-terms-saas/",
    "content": "How to Migrate Your SaaS from API Wrapper to Compliant Architecture\n\nFor the past two years, a familiar playbook dominated the AI startup scene: take an LLM API like Claude or GPT, wrap it in a custom UI, charge users $20 to $50 per month, and pocket the difference between subscription revenue and per-token API costs. The Anthropic API terms now threaten to collapse that entire model.\n\nTable of Contents\n\nThe Wrapper Gold Rush Is Over\n\nFor the past two years, a familiar playbook dominated the AI startup scene: take an LLM API like Claude or GPT, wrap it in a custom UI, charge users $20 to $50 per month, and pocket the difference between subscription revenue and per-token API costs. The Anthropic API terms now threaten to collapse that entire model. Anthropic's updated commercial terms include language restricting the use of a single subscription to authenticate API access on behalf of third-party end users. That move effectively targets the classic LLM wrapper SaaS pattern that hundreds of startups rely on.\n\nThis article breaks down what the updated terms actually say, who falls into the blast radius, and how to rearchitect your product to stay compliant. If you're running a SaaS BYOK model or considering one, this is your migration guide. If you're still proxying Claude requests through a company-owned API key for paying customers, keep reading.\n\nThe Ruling: What Exactly Is Banned?\n\nThe Specific Language in Anthropic's Updated Terms\n\nAnthropic's commercial terms for API usage restrict using subscription-based authentication to provide API access to third parties. The operative concepts center on redistribution and resale: you cannot use your Anthropic API credentials to funnel access to end users who aren't part of your organization's direct usage.\n\nThe key phrases to understand:\n\n* Subscription auth for third-party use means authenticating API requests with your organization's key when the actual consumer of the output is an external, paying customer of your product.\n\n* Redistribution covers any pattern where your product is primarily a conduit between the end user and the Anthropic API, with your system acting as a passthrough.\n\n* Third-party use distinguishes between your own team using Claude internally and your customers using Claude through your product.\n\nWorth flagging: Anthropic's terms documentation doesn't always use these phrases as formal defined terms. The restrictions show up through standard commercial licensing language around resale, service bureau use, and making API access \"available\" to third parties. I recommend reading Anthropic's current Terms of Service and Acceptable Use Policy directly, as clause language can shift between revisions.\n\nWhat Counts as a \"Wrapper\"?\n\nA wrapper, in this context, is a SaaS application that takes user input, sends it to the Anthropic Messages API () using the company's own API key, and returns Claude's response to the end user. The end user never has a relationship with Anthropic. They never see an API key. They pay you; you pay Anthropic. Your product is, architecturally, a proxy with a UI.\n\nThis is distinct from a product that uses Claude as one component in a larger system. A legal research platform that uses Claude to summarize case law but also maintains its own proprietary database, citation engine, and workflow tools isn't a wrapper. It's a product that happens to use an API.\n\nWhat Is Still Allowed?\n\nSeveral patterns remain clearly compliant under the Claude API changes:\n\n* Internal tools: Your company builds an internal dashboard that uses Claude for summarization or analysis. All users are employees. One API key, one organization.\n\n* AI as a feature, not the product: Your project management tool adds an \"AI summary\" button that calls Claude. The core product is project management; Claude is a feature.\n\n* Substantial value-add products: Your SaaS uses Claude as one step in a multi-stage pipeline that includes proprietary data, custom post-processing, domain-specific logic, or integrations that would be meaningless without your product layer.\n\nThe line between \"reselling API access\" and \"building a product that uses an API\" is where compliance lives or dies.\n\nA caveat worth noting: Anthropic's terms aren't entirely black-and-white on every scenario. The distinction between \"wrapper\" and \"value-add product\" is partly a judgment call, and Anthropic reserves discretion in enforcement. If your product sits anywhere near the line, talk to a lawyer familiar with API licensing terms. It's a worthwhile investment.\n\nWhy Anthropic Is Doing This (and Why Now)\n\nThe Economics of API Arbitrage\n\nThe wrapper model exploits a structural gap. Anthropic charges per token. Wrappers charge flat monthly subscriptions. When a SaaS product charges $30/month and the average user consumes $4 in API costs, the wrapper captures $26 in margin while adding minimal proprietary value. Anthropic bears the infrastructure cost of running inference at scale while the wrapper captures the customer relationship and the revenue upside.\n\nFrom Anthropic's perspective, this is a subsidy they never agreed to provide. Their API pricing assumes direct or value-add usage, not arbitrage.\n\nThe Broader Industry Signal\n\nThis isn't an Anthropic-only phenomenon. OpenAI's terms of service include comparable restrictions on resale and redistribution of API access. Google's Gemini API terms contain similar language around service bureau and third-party use restrictions. The pattern across major LLM providers is converging: if your product's core value proposition is \"access to our model through a nicer interface,\" you're operating in increasingly hostile legal territory.\n\nI've tracked these changes across providers over the past year. The direction is unmistakable. Every major LLM vendor is tightening terms around redistribution.\n\nWho Is Affected? A Risk Assessment\n\nHigh Risk: Pure Wrappers\n\nIf your product is essentially a skin over , you're in the most exposed category. This includes products where removing the Claude API call would leave you with an empty shell. Prompt-chaining tools that simply orchestrate a sequence of Claude calls with hardcoded system prompts fall here too.\n\nMedium Risk: Hybrid Products\n\nProducts that use Claude as a significant feature but wrap it in proprietary logic, data, or workflows sit in ambiguous territory. I audited a content platform last year that used Claude for draft generation but layered on proprietary SEO scoring, brand voice matching, and a custom editorial workflow. The Claude API calls represented maybe 15% of the product's actual functionality. That product is likely compliant, but the architecture still deserved a review to make sure the API key usage pattern wouldn't trip a terms violation.\n\nLow Risk: Incidental AI Features\n\nIf Claude powers a \"nice to have\" feature in a product with an independent value proposition, your risk is minimal. Enterprise deployments where all users sit within one organization and the API key belongs to that organization are also clearly in bounds.\n\nThe BYOK (Bring Your Own Key) Architecture Pattern\n\nWhat Is BYOK?\n\nThe SaaS BYOK model flips the authentication relationship. Instead of your company holding a centralized Anthropic API key and proxying requests, each end user provides their own Anthropic API key. Your SaaS product stores that key securely and uses it to make requests on the user's behalf. The user has a direct billing relationship with Anthropic. Your product charges for the software, not for AI access.\n\nThis is the most straightforward path to compliance under the Anthropic API terms restrictions on redistribution.\n\nA practical caveat: BYOK introduces friction. Non-technical users may struggle to create an Anthropic account, generate an API key, set up billing, and paste it into your product. This onboarding cost is real and it can hurt conversion rates, particularly for consumer-facing products. Plan for clear documentation, inline guidance, and support resources to ease this transition.\n\nBYOK Architecture Diagram\n\nWrapper Pattern (Non-Compliant):\n\nBYOK Pattern (Compliant):\n\nImplementing BYOK: Code Walkthrough\n\nCode Example 1: Secure Key Storage and Retrieval (Node.js/TypeScript)\n\nThis implementation uses AES-256-GCM with envelope encryption. A master key (ideally from AWS KMS, GCP Cloud KMS, or similar) encrypts per-user data encryption keys, which in turn encrypt the API keys at rest.\n\nIn production, replace from an environment variable with a proper KMS call that returns a data encryption key. The master key should never exist in plaintext in your application config.\n\nCode Example 2: Making a BYOK API Call to Claude (Python)\n\nHere's the contrast between the old wrapper pattern and the BYOK approach:\n\nThe only structural difference is where the API key comes from. The SDK call to is identical. That makes migration straightforward from a code perspective, even if the business model implications are significant.\n\nSecurity Considerations for BYOK\n\n* Never store keys in plaintext. This should go without saying, but I've reviewed codebases where API keys sat in a plain column. Use the encryption pattern above at minimum.\n\n* Use envelope encryption. The master key should live in a KMS (AWS KMS, GCP Cloud KMS, HashiCorp Vault). Your application should request a data encryption key from the KMS, use it locally, and store only the encrypted key material.\n\n* Audit access logs. Log every decryption event with the user ID and timestamp. Never log the key itself or the raw request headers.\n\n* Allow key rotation and revocation. Your UI needs a settings page where users can update or delete their key. When a key is revoked, immediately purge the encrypted material.\n\n* Validate keys on input. When a user pastes their key, make a lightweight call (minimal tokens) to verify it works before storing it. Catching invalid or expired keys early prevents support headaches later.\n\n* Handle quota errors gracefully. With BYOK, rate limits and quota exceeded errors hit the user's account. Your error handling needs to surface these clearly: \"Your Anthropic API key has exceeded its rate limit\" rather than a generic 500.\n\nThis approach has an additional failure mode when users share API keys across accounts in your system, which can create billing disputes with Anthropic. Implement per-session key validation and alert on anomalous usage patterns tied to a single key.\n\nBeyond BYOK: Other Compliant Architecture Patterns\n\nOAuth / Provider-Managed Auth\n\nThe gold standard for this problem would be an OAuth-style delegated access flow, similar to how Stripe Connect allows platforms to act on behalf of connected accounts. As of now, Anthropic doesn't offer an OAuth-based delegated authorization mechanism for end-user accounts. If and when they do, it would eliminate the need for users to manually copy API keys and would provide cleaner audit trails. This is speculative, but the pattern is well-established in the payments and cloud infrastructure worlds, and LLM providers will likely follow.\n\nMarketplace and Reseller Agreements\n\nFor larger SaaS companies processing significant API volume, formal reseller or partner agreements with Anthropic may be an option. These aren't self-serve. They require scale, negotiation, and a direct relationship with Anthropic's partnerships team. If your product generates six figures or more in annual API spend, this path is worth exploring.\n\nMulti-Provider Abstraction\n\nArchitecting your SaaS to support multiple LLM backends (Claude, GPT-4o, Gemini, open-source models) reduces your exposure to any single provider's terms changes. It also gives your users choice, which itself becomes a feature.\n\nCode Example 3: Multi-Provider Abstraction (TypeScript with Vercel AI SDK)\n\nThe Vercel AI SDK abstracts the provider interface so your application logic doesn't need conditional branches for each LLM. Adding a new provider later (Gemini, Mistral, a local model via Ollama) means adding a provider client, not rewriting your application.\n\nAnother option worth mentioning: customer-owned cloud deployment. You deploy your application into the customer's AWS, GCP, or Azure account. They set as an environment variable in their own infrastructure. Your code never touches the key at all. This is common in enterprise B2B and completely sidesteps the redistribution question.\n\nMigration Checklist: Pivoting Your SaaS from Wrapper to Compliant\n\nAudit Phase\n\nArchitecture Phase\n\nBusiness Model Phase\n\nCompliance Phase\n\nWhat This Means for the AI SaaS Ecosystem\n\nThe era of \"just wrap an API and charge for it\" is closing. Whether or not every LLM provider aggressively enforces these restrictions today, the legal and contractual groundwork is being laid across the industry. Anthropic's terms are a signal. OpenAI and Google have similar language in their own agreements.\n\nProducts that survive this shift will be those delivering genuine proprietary value: unique datasets, domain expertise baked into workflows, integrations that save real time, and user experiences that are meaningfully better than calling the API directly.\n\nBYOK is the practical interim pattern. OAuth-style delegated auth, if and when providers offer it, will be the cleaner long-term solution.\n\nThis is ultimately a healthy correction. It forces AI startups to answer a harder question: \"What do we do that the API alone cannot?\" The products with a strong answer will thrive. The ones without were always one terms-of-service update away from irrelevance."
  },
  {
    "source": "WebProNews",
    "company": "Anthropic",
    "title": "Anthropic Rewrites the Playbook: Inside the AI Startup's Aggressive Push to Lock In Cloud Giants with Sweeter Revenue Deals",
    "date": "2026-02-18T17:00:17Z",
    "url": "https://www.webpronews.com/anthropic-rewrites-the-playbook-inside-the-ai-startups-aggressive-push-to-lock-in-cloud-giants-with-sweeter-revenue-deals/",
    "content": "In a move that underscores the intensifying competition among frontier artificial intelligence companies for distribution and computing power, Anthropic has restructured its commercial arrangements with major cloud providers, offering them significantly more favorable revenue-sharing terms in exchange for deeper integration and expanded access to computing infrastructure. The shift, first reported by The Information, signals a new phase in the rapidly evolving relationship between AI model makers and the hyperscale cloud platforms that serve as their primary conduits to enterprise customers.\n\nThe San Francisco-based AI company, founded by former OpenAI executives Dario and Daniela Amodei, has been renegotiating deals with Amazon Web Services and Google Cloud, the two cloud giants that have collectively poured billions of dollars into the startup. Under the revised terms, Anthropic is reportedly offering these cloud providers a larger cut of the revenue generated when enterprise customers access its Claude family of models through their respective cloud marketplaces. The sweetened economics are designed to incentivize the cloud platforms to more aggressively promote and distribute Anthropic's technology to their vast customer bases.\n\nA Strategic Recalibration as AI Distribution Wars Heat Up\n\nThe renegotiated terms come at a pivotal moment for Anthropic and the broader AI industry. OpenAI, Anthropic's chief rival, has been deepening its exclusive partnership with Microsoft Azure, while Google has been investing heavily in its own Gemini models. For Anthropic, which has positioned itself as a multi-cloud AI provider available through both AWS and Google Cloud, the challenge is ensuring that neither partner treats its models as an afterthought relative to their own proprietary AI offerings or those of competitors.\n\nAccording to The Information, the revised deals reflect Anthropic's recognition that cloud providers need stronger financial incentives to prioritize selling third-party AI models alongside -- or even instead of -- their own. Amazon, for instance, has been developing its own Nova family of AI models while simultaneously serving as Anthropic's largest investor and distribution partner through AWS Bedrock. Google, meanwhile, continues to push Gemini across its Cloud platform even as it offers Claude through its Model Garden and Vertex AI marketplace.\n\nThe Economics of AI Model Distribution\n\nThe financial mechanics of these cloud marketplace deals are critical to understanding why Anthropic is willing to sacrifice margin for market share. When an enterprise customer uses Claude through AWS Bedrock or Google Cloud's Vertex AI, the cloud provider typically takes a percentage of the revenue -- a commission that covers not just distribution but also the underlying compute infrastructure, sales support, and billing integration. By increasing the cloud providers' revenue share, Anthropic is effectively paying more for distribution, but the calculus appears to be that broader adoption will more than compensate for the reduced per-transaction economics.\n\nThis approach mirrors strategies common in enterprise software, where vendors routinely offer channel partners generous margins to drive sales volume. But in the AI model business, the stakes are considerably higher. The cloud providers are not merely passive resellers; they control the compute infrastructure that AI models require to run, they manage the enterprise relationships that determine which models get deployed, and they increasingly offer competing products. Anthropic's willingness to share more revenue is an acknowledgment of this asymmetric power dynamic.\n\nAmazon and Google: Investors, Partners, and Competitors\n\nThe complexity of Anthropic's relationships with AWS and Google Cloud cannot be overstated. Amazon has committed up to $8 billion in investment in Anthropic, making it the startup's largest financial backer. Google, through its cloud division and parent company Alphabet, has invested approximately $2 billion. These investments came with commitments from Anthropic to make its models available on the respective cloud platforms and to use their computing infrastructure -- arrangements that blur the traditional boundaries between investor, customer, supplier, and competitor.\n\nFor Amazon in particular, the Anthropic relationship has been central to its AI strategy. AWS CEO Matt Garman has repeatedly highlighted Claude's availability on Bedrock as a key differentiator for enterprise customers evaluating cloud AI platforms. At AWS re:Invent 2024, Garman emphasized the importance of offering customers choice among frontier models, with Anthropic's Claude positioned as a flagship third-party option. Yet Amazon's simultaneous development of its own Nova models creates an inherent tension: every dollar of revenue that flows to Anthropic through Bedrock is a dollar that could theoretically have gone to Amazon's own AI products.\n\nAnthropic's Revenue Trajectory and the Push for Scale\n\nThe sweetened cloud deals come as Anthropic has been on a steep revenue growth curve. The company reportedly reached an annualized revenue run rate exceeding $2 billion in early 2025, up from roughly $900 million at the end of 2024, according to previous reporting by The Information and other outlets. Much of this growth has been driven by enterprise adoption through cloud marketplaces, making the AWS and Google Cloud channels indispensable to Anthropic's commercial trajectory.\n\nAnthropic is also preparing for what could be one of the largest private funding rounds in venture capital history. The company has been in discussions to raise additional capital at a valuation that could approach or exceed $60 billion, according to multiple reports. At such lofty valuations, demonstrating sustained revenue growth and expanding enterprise market share becomes essential -- and that requires keeping the cloud distribution channels firing on all cylinders. The willingness to offer better terms to cloud partners is thus not just a tactical sales decision but a strategic imperative tied to the company's fundraising and valuation narrative.\n\nCompetitive Implications for OpenAI and the Broader Market\n\nAnthropic's move to sweeten cloud provider economics has implications that extend well beyond its own business. OpenAI, which generates the majority of its enterprise AI revenue through its exclusive partnership with Microsoft Azure, faces a different set of channel dynamics. Microsoft takes a significant share of OpenAI-related revenue flowing through Azure, and the two companies have been renegotiating the terms of their complex partnership. Recent reports have indicated that OpenAI has been seeking more flexibility to distribute its models through other channels, a reflection of the tension inherent in exclusive distribution arrangements.\n\nFor smaller AI model providers -- companies like Mistral, Cohere, and Meta's open-source Llama ecosystem -- Anthropic's willingness to offer cloud providers richer economics could raise the bar for what hyperscalers expect from third-party model makers. If Anthropic is offering a larger revenue share, cloud providers may demand similar or better terms from less established competitors, potentially squeezing margins across the industry and making it harder for smaller players to compete for prominent placement on cloud marketplaces.\n\nThe Broader Implications for Enterprise AI Adoption\n\nFrom the perspective of enterprise customers, the restructured deals between Anthropic and the cloud providers could have tangible benefits. When cloud sales teams have stronger financial incentives to promote Claude, enterprises are more likely to receive proactive recommendations, technical support, and integration assistance for Anthropic's models. This could accelerate adoption of Claude in industries ranging from financial services and healthcare to legal and government, where cloud marketplace procurement is often the preferred -- and sometimes the only approved -- method for acquiring AI capabilities.\n\nThe deals also reinforce a broader trend in enterprise AI: the cloud marketplace is becoming the dominant distribution channel for frontier AI models, much as app stores became the primary distribution mechanism for mobile software. Companies that fail to secure favorable positioning on these platforms risk being marginalized regardless of the technical quality of their models. Anthropic's aggressive moves to lock in better cloud partnerships suggest that the company's leadership understands this dynamic acutely and is willing to trade short-term margin for long-term strategic positioning.\n\nWhat Comes Next in the AI Partnership Arms Race\n\nAs the AI industry matures, the relationships between model makers and cloud infrastructure providers will continue to evolve in complexity and consequence. Anthropic's decision to offer sweeter terms to AWS and Google Cloud is a calculated bet that distribution dominance matters more than near-term profitability -- a bet that only makes sense if the company believes its models will continue to be competitive at the frontier and that enterprise demand for AI will continue its exponential growth trajectory.\n\nThe coming months will reveal whether this approach pays off. If Anthropic can translate better cloud economics into meaningfully faster enterprise adoption, it will validate a model that other AI companies may be forced to emulate. If the sweetened terms fail to move the needle -- or if cloud providers pocket the better margins without materially increasing their promotional efforts -- Anthropic may find itself with thinner economics and little to show for the concession. Either way, the deal restructuring marks a significant moment in the ongoing negotiation between the companies building the most powerful AI systems and the platforms that control access to the world's computing infrastructure and enterprise customers."
  },
  {
    "source": "DiarioBitcoin",
    "company": "Anthropic",
    "title": "Anthropic abre oficina en Bengaluru y acelera alianzas de IA responsable en India",
    "date": "2026-02-16T18:27:01Z",
    "url": "https://www.diariobitcoin.com/ia/anthropic-abre-oficina-en-bengaluru-y-acelera-alianzas-de-ia-responsable-en-india/",
    "content": "Anthropic inauguró oficialmente su nueva oficina en Bengaluru y anunció una batería de alianzas en empresa, educación, agricultura y sector público para profundizar su apuesta por India, uno de los mercados más relevantes para Claude.ai. La compañía destacó mejoras en idiomas índicos, casos de uso con Claude Code, y el avance del estándar abierto MCP, incluso dentro del gobierno indio.\n\n***\n\n* Anthropic abrió su oficina en Bengaluru y presentó acuerdos que abarcan empresas, ONG educativas, agricultura y justicia.\n\n* Claude gana tracción en India: casi la mitad del uso local se concentra en tareas de computación y matemáticas, y el ingreso recurrente se duplicó desde octubre de 2025.\n\n* El estándar abierto MCP se expande: Anthropic indicó que lo donó a la Fundación Linux y que MoSPI lanzó el primer servidor MCP del gobierno indio.\n\nUna expansión que pone a India en el centro de la estrategia\n\nAnthropic anunció la apertura oficial de su nueva oficina en Bengaluru y, junto con ese hito, reveló nuevas asociaciones \"en empresa, educación y agricultura\" para profundizar su compromiso con India. Según la compañía, India ya es el segundo mercado más grande para Claude.ai y concentra una comunidad de desarrolladores con trabajo técnico de alta intensidad en IA.\n\nDe acuerdo con la información publicada por Anthropic, casi la mitad del uso de Claude en India corresponde a tareas de computación y matemáticas. En la práctica, eso incluye construir aplicaciones, modernizar sistemas y desplegar software de producción. Ese patrón de adopción ayuda a explicar por qué la empresa decidió formalizar su presencia con una oficina local.\n\nLa operación estará liderada por Irina Ghose, Directora General de India, a quien Anthropic describió como una líder en tecnología empresarial y de startups. En su anuncio, la empresa aseguró que esta nueva sede en Bengaluru es su segunda oficina en Asia, después de Tokio, y que estará orientada a contratar talento local en una amplia gama de roles.\n\nAnthropic enmarcó el movimiento como una apuesta por llevar \"IA responsable\" a más personas y empresas. En una declaración incluida en el comunicado, Ghose afirmó que India combina talento técnico, infraestructura digital a escala y un historial de uso tecnológico para mejorar la vida de la gente, lo que, en su visión, crea una base favorable para que la tecnología beneficie a quienes más la necesitan.\n\nIdiomas índicos: el desafío de llevar la IA más allá del inglés\n\nLa empresa contextualizó parte de su plan con una realidad lingüística: más de mil millones de personas en India hablan uno de más de una docena de idiomas oficialmente reconocidos. Sin embargo, Anthropic señaló que los modelos de IA suelen funcionar mejor en inglés que en otros idiomas, una brecha que condiciona la utilidad de estas herramientas en tareas cotidianas.\n\nSegún Anthropic, hace seis meses lanzó un esfuerzo a nivel de toda la compañía para reducir esa brecha. El trabajo consistió en curar datos de entrenamiento de alta calidad y más representativos en 10 de los idiomas más hablados en India: hindi, bengalí, marathi, telugu, tamil, punjabi, gujarati, kannada, malayalam y urdu.\n\nLa empresa indicó que ese proceso ya generó mejoras en sus modelos y que continúa trabajando para aumentar la fluidez. Aunque el comunicado no entregó métricas específicas de desempeño, sí reforzó la idea de que la calidad de datos y la pertinencia cultural son componentes críticos para ampliar el acceso a sistemas de IA con utilidad real.\n\nEn esa misma línea, Anthropic informó que trabaja con Karya y el Proyecto de Inteligencia Colectiva para construir evaluaciones que midan el rendimiento en tareas \"localmente relevantes\" en dominios como agricultura y derecho. La compañía añadió que esta labor se realiza con expertos de dominio de ONG indias, incluyendo Digital Green y Adalat AI, y que pretende publicar esas evaluaciones para que otros también las usen.\n\nClaude Code en empresas: productividad, modernización y adopción masiva\n\nEn el frente corporativo, Anthropic sostuvo que su ingreso recurrente en India se duplicó desde que anunció su expansión en octubre de 2025. La empresa atribuyó ese crecimiento a una base de organizaciones diversa, que incluye grandes compañías, firmas nativas digitales y startups que están lanzando sus primeros productos con Claude.\n\nPara respaldar esa demanda, Anthropic afirmó que su equipo en India ofrecerá experiencia en IA aplicada a clientes empresariales, nativos digitales y startups. El objetivo, según el comunicado, será ayudarlos a diseñar, construir y escalar soluciones impulsadas por Claude que se ajusten a necesidades concretas de negocio.\n\nEntre los ejemplos citados, Anthropic señaló que Air India usa Claude Code para ayudar a sus desarrolladores a entregar software personalizado más rápido y a menor costo. La empresa enmarcó este uso dentro de un impulso más amplio por incorporar IA agente en las operaciones de la aerolínea.\n\nTambién mencionó a CRED, que habría logrado el doble de rapidez en la entrega de funciones y un 10% de mejora en la cobertura de pruebas con Claude Code. En paralelo, Anthropic afirmó que Cognizant está desplegando Claude a 350.000 empleados a nivel global con el fin de modernizar sistemas heredados, acelerar el desarrollo de software y apoyar la adopción de IA entre clientes empresariales.\n\nStartups y productos nativos de IA: casos de uso, integraciones y escala\n\nEl comunicado presentó una lectura optimista sobre el ecosistema emprendedor local. Anthropic indicó que, entre startups de India, el patrón se repite: Claude y sus herramientas se insertan en flujos de ingeniería, operaciones y producto, con el objetivo de acelerar ciclos de desarrollo y ampliar capacidades internas.\n\nComo ejemplo, Anthropic señaló que en Razorpay la IA está integrada en sistemas de riesgo, procesos de toma de decisiones y operaciones en toda la empresa. Aunque no detalló arquitectura ni cifras de impacto, el caso se planteó como una muestra de adopción transversal y no solo limitada a asistentes de productividad.\n\nLa compañía también destacó a Enterpret. Según Anthropic, Claude potencia el asistente de IA de esa startup, mientras su equipo de ingeniería trabaja a diario con Claude Code. Además, afirmó que Enterpret lanzó una integración MCP que lleva información de clientes directamente a Claude.\n\nOtro caso resaltado fue Emergent, descrita como una plataforma potenciada por IA que permite construir software describiendo lo que se quiere en lenguaje simple. Anthropic afirmó que Emergent alcanzó USD $25.000.000 en ingresos recurrentes anuales y dos millones de usuarios en menos de cinco meses, y que fue construida enteramente con Claude.\n\nEducación y acceso: pruebas, tutorías y apoyo a comunidades desatendidas\n\nAnthropic reportó que las tareas educativas e instructivas constituyen el 12% del uso de Claude.ai en India. Ese dato sugiere que, además del desarrollo de software, una parte relevante del tráfico se relaciona con aprendizaje, guías y apoyo didáctico, un ángulo que suele generar debate por riesgos y oportunidades en el uso de IA en aulas.\n\nEn ese contexto, la empresa informó que Pratham, una de las ONG educativas más grandes de India, eligió a Anthropic como su primer socio estratégico de IA en laboratorio. La elección, según el comunicado, se debió a un enfoque compartido en seguridad y rigor educativo, dos dimensiones clave cuando se trata de sistemas generativos.\n\nAnthropic explicó que la \"Máquina de Pruebas en Cualquier Momento\", impulsada por Claude, se está probando con 1.500 estudiantes en 20 escuelas. Agregó que existen planes de expansión a 100 escuelas para fines de 2026. Además, señaló que a principios de este año el sistema fue adaptado para más de 5.000 estudiantes del programa Segunda Oportunidad de Pratham, que apoya a mujeres que abandonaron la educación formal.\n\nLa compañía también anunció colaboración con la Fundación Central Square para promover el uso más efectivo de EdTech e IA en comunidades desatendidas. Como parte del acuerdo, Anthropic dijo que aportará experiencia técnica, mentoría y créditos de API para organizaciones que desarrollen herramientas habilitadas por IA, incluyendo tutores personalizados, capacitación docente e instrucción basada en evaluaciones, con la meta de llegar a más estudiantes de primaria en India.\n\nSector público, agricultura y justicia: de OpenAgriNet a una línea de WhatsApp\n\nAnthropic dedicó otra parte del anuncio al sector público y a servicios de alto impacto. La empresa destacó que India tiene antecedentes en la construcción de infraestructura digital pública interoperable, y señaló que se asocia con la Fundación EkStep para explorar cómo la IA puede construir sobre esos esfuerzos y entregar impacto a escala poblacional.\n\nLa agricultura aparece como caso central en el comunicado. Anthropic sostuvo que el sector constituye casi una sexta parte de la economía india y emplea a casi la mitad de la fuerza laboral. Bajo el esfuerzo OpenAgriNet, afirmó que trabaja hacia implementaciones de Claude que amplíen el acceso a conocimiento experto en este ámbito.\n\nLa empresa también mencionó demostraciones del impacto de Claude Code y Cowork dentro de ONG. Entre los ejemplos listó a Noora Health, que ofrece entrenamiento de salud accesible a millones de familias, e Intelehealth, que conecta a pacientes en comunidades remotas con atención médica de calidad.\n\nEn justicia, Anthropic subrayó un desafío estructural: India tendría 50 millones de casos judiciales pendientes, y las actualizaciones rutinarias pueden tardar meses en llegar a los litigantes. En ese marco, informó que apoya a Adalat AI con una línea de ayuda nacional por WhatsApp que, según el comunicado, se lanzó \"hoy\". Usando Claude, el servicio ofrecería actualizaciones instantáneas de casos, además de traducción, resumen de documentos y consulta interactiva de documentos legales en idiomas nativos indios.\n\nMCP como estándar abierto: del ecosistema privado al primer servidor del gobierno\n\nLa adopción de estándares para conectar sistemas de IA con herramientas externas se ha convertido en un tema clave en el desarrollo de agentes y aplicaciones empresariales. Anthropic afirmó que creó el Protocolo de Contexto de Modelo (MCP) como un estándar universal de código abierto para conectar aplicaciones de IA con sistemas externos.\n\nSegún la empresa, MCP fue donado recientemente a la Fundación Linux, un paso que suele interpretarse como un intento por ampliar gobernanza y acelerar adopción en comunidades técnicas. En el comunicado, Anthropic planteó este enfoque como parte de su estrategia para impulsar la adopción mediante estándares abiertos.\n\nEn el sector público, Anthropic indicó que el Ministerio de Estadísticas e Implementación del Programa de la India (MoSPI), con apoyo de la ONG Bharat Digital, lanzó el primer servidor MCP del gobierno indio. La empresa explicó que esto permitiría a usuarios de sistemas de IA acceder y consultar estadísticas nacionales autorizadas de manera abierta e interoperable.\n\nEn el sector privado, el comunicado agregó que Swiggy usa MCP para permitir que las personas pidan comestibles y hagan reservas de cena directamente a través de Claude. El caso sugiere un puente entre interfaces conversacionales y servicios de consumo, una tendencia que compite con otras aproximaciones de agentes, integraciones propietarias y marketplaces de herramientas.\n\nLo que deja el anuncio: competencia global y una apuesta por escala local\n\nLa apertura de una oficina en Bengaluru se produce en un momento de competencia intensa entre laboratorios de IA, grandes tecnológicas y startups por capturar talento, alianzas y datos de uso. Anthropic presentó a India como un mercado donde Claude ya participa en tareas avanzadas de ingeniería, al tiempo que busca mejorar desempeño multilingüe y ampliar casos de uso en educación y servicios públicos.\n\nEl comunicado de la empresa insistió en que estas asociaciones crecerán en los próximos meses y años a través de su presencia ampliada en India. Aunque no reveló metas financieras ni cronogramas adicionales, sí dejó una señal clara: la estrategia combina expansión comercial, trabajo con ONG y esfuerzos de estandarización, con un foco explícito en \"IA responsable\".\n\nPara los lectores que siguen de cerca el cruce entre IA y mercados, el anuncio también funciona como termómetro de demanda. El hecho de que la compañía destaque modernización de sistemas heredados, despliegues a gran escala como el de 350.000 empleados, y un crecimiento de ingreso recurrente desde octubre de 2025, aporta pistas sobre qué áreas están capturando presupuestos empresariales.\n\nAl mismo tiempo, el énfasis en idiomas índicos, agricultura y justicia refleja que el valor de la IA no se juega solo en productividad corporativa. Anthropic planteó que parte del futuro de Claude en India dependerá de evaluaciones públicas, alianzas con expertos de dominio y despliegues que resuelvan fricciones reales de acceso a información, educación y servicios.\n\nADVERTENCIA: DiarioBitcoin ofrece contenido informativo y educativo sobre diversos temas, incluyendo criptomonedas, IA, tecnología y regulaciones. No brindamos asesoramiento financiero. Las inversiones en criptoactivos son de alto riesgo y pueden no ser adecuadas para todos. Investigue, consulte a un experto y verifique la legislación aplicable antes de invertir. Podría perder todo su capital."
  },
  {
    "source": "WebProNews",
    "company": "Anthropic",
    "title": "Anthropic's Super Bowl Ad Gambit: How a 60-Second Spot Redefined the AI Arms Race and Put OpenAI on the Defensive",
    "date": "2026-02-09T18:43:45Z",
    "url": "https://www.webpronews.com/anthropics-super-bowl-ad-gambit-how-a-60-second-spot-redefined-the-ai-arms-race-and-put-openai-on-the-defensive/",
    "content": "In the annals of Super Bowl advertising, few commercials have managed to simultaneously introduce a brand to mainstream America and deliver a devastating competitive blow. Anthropic, the San Francisco-based artificial intelligence company, accomplished both with a single 60-second spot during Super Bowl LX in February 2026 -- a move that industry observers are still dissecting for its strategic brilliance and its implications for the rapidly evolving AI industry.\n\nThe ad, which aired during one of the most-watched television events of the year, didn't just promote Anthropic's Claude AI assistant. It drew a sharp, unmistakable contrast with OpenAI, the company's chief rival, without ever mentioning it by name. According to Business Insider, the spot effectively \"skewered\" OpenAI and was widely regarded as the winner of the AI Super Bowl advertising battle -- a battle that, for the first time, featured multiple AI companies vying for the attention of more than 120 million viewers.\n\nThe Anthropic ad was notable for what it didn't do as much as for what it did. Rather than relying on the flashy, utopian imagery that has become standard fare for technology companies advertising during the big game, Anthropic opted for a message centered on safety, trustworthiness, and the responsible development of artificial intelligence. The spot leaned into growing public anxiety about AI -- concerns about job displacement, misinformation, and the unchecked power of technology companies -- and positioned Claude as the antidote.\n\nThe implicit target was unmistakable. OpenAI, led by CEO Sam Altman, has faced a steady drumbeat of criticism over its corporate governance, its pivot from a nonprofit to a capped-profit structure, and internal turmoil that saw the brief ouster and reinstatement of Altman in late 2023. Anthropic's ad didn't need to name OpenAI; the subtext was loud enough. By emphasizing its own commitment to AI safety -- a founding principle of the company, which was started by former OpenAI executives Dario and Daniela Amodei -- Anthropic drew a line in the sand that resonated with both consumers and industry insiders, as reported by Business Insider.\n\nPurchasing a Super Bowl ad slot is no small financial commitment. Estimates for a 60-second spot during Super Bowl LX hovered around $14 to $15 million, a figure that underscores the seriousness with which Anthropic approached its mainstream debut. For a company that has historically operated in the shadow of OpenAI's consumer-facing juggernaut ChatGPT, the decision to go big on the Super Bowl was a clear signal that Anthropic is no longer content to be the industry's well-respected but lesser-known player.\n\nThe investment also reflects the broader dynamics of the AI industry in early 2026. With generative AI tools becoming embedded in everything from enterprise software to smartphone assistants, the race for consumer mindshare has intensified dramatically. OpenAI had already demonstrated the power of mainstream brand recognition with ChatGPT, which became a household name almost overnight after its launch in late 2022. Anthropic's Super Bowl play was, in many respects, an attempt to close that recognition gap in a single, high-impact moment.\n\nAnthropic was not the only AI company advertising during the Super Bowl. OpenAI also ran a spot, making the game a de facto referendum on which company could better communicate its vision to the American public. According to the analysis published by Business Insider, OpenAI's ad failed to generate the same buzz or emotional resonance as Anthropic's. Where Anthropic's message was pointed and purposeful, OpenAI's was seen as more generic -- a missed opportunity for a company that had the advantage of greater name recognition going into the evening.\n\nThe contrast was not lost on viewers or on the advertising industry's professional critics. Social media reaction, tracked across platforms including X, overwhelmingly favored Anthropic's spot. Marketing analysts noted that Anthropic succeeded in part because it told a story that tapped into a genuine cultural moment: the public's growing unease with AI and its desire for companies that prioritize safety over speed. OpenAI's ad, by contrast, was perceived as trying to reassure without acknowledging the very real concerns that consumers hold.\n\nAnthropic's decision to lead with safety messaging is not merely a marketing tactic -- it is the philosophical core of the company. Dario Amodei, Anthropic's CEO, and his sister Daniela, the company's president, left OpenAI in 2021 precisely because they believed the organization was not taking AI safety seriously enough. They founded Anthropic with the explicit mission of building AI systems that are interpretable, steerable, and aligned with human values. The Super Bowl ad was, in many ways, the most public and dramatic expression of that mission to date.\n\nThis approach has resonated with a significant segment of the enterprise market. Companies evaluating AI tools for deployment in sensitive areas -- healthcare, finance, legal services -- have increasingly gravitated toward Anthropic's Claude, in part because of the company's safety-first reputation. The Super Bowl ad served a dual purpose: it introduced Claude to millions of consumers who may not have heard of it, and it reinforced Anthropic's brand positioning among the corporate decision-makers who were watching the game alongside everyone else.\n\nThe Anthropic-OpenAI Super Bowl showdown is emblematic of a larger reckoning taking place across the AI industry. As generative AI tools have proliferated, so too have concerns about their misuse, their accuracy, and the corporate motives behind them. High-profile incidents -- from AI-generated deepfakes influencing political discourse to chatbots providing dangerously inaccurate medical advice -- have eroded public trust in the technology and the companies that build it.\n\nAnthropic's ad capitalized on this erosion of trust in a way that was both commercially savvy and culturally attuned. By positioning itself as the responsible alternative, the company effectively reframed the competitive narrative. The question is no longer simply \"Which AI is the most capable?\" but rather \"Which AI company can you trust?\" This reframing is significant because it shifts the basis of competition from raw technical performance -- where OpenAI has historically held an edge -- to values and governance, where Anthropic believes it has the advantage.\n\nThe success of Anthropic's Super Bowl campaign is likely to have lasting effects on how AI companies market themselves. For years, technology advertising has defaulted to aspirational imagery and vague promises of a better future. Anthropic demonstrated that there is a more effective approach: acknowledge the audience's fears, address them directly, and offer a credible alternative. It is a strategy borrowed from industries like pharmaceuticals and automotive, where trust and safety have long been central to brand positioning.\n\nIndustry insiders expect that the 2027 Super Bowl will feature even more AI advertisers, each attempting to replicate Anthropic's formula. But the first-mover advantage in this particular messaging space is significant. Anthropic has now established itself in the public imagination as the \"safe\" AI company -- a positioning that will be difficult for competitors to dislodge, regardless of how much they spend on advertising.\n\nOf course, a single advertisement -- no matter how effective -- does not win a technology race. Anthropic still faces formidable challenges. OpenAI continues to attract massive investment, most recently securing funding that values the company at well over $100 billion. Google's DeepMind division remains a powerhouse in fundamental AI research. And a host of smaller competitors, from Mistral AI in France to xAI, Elon Musk's AI venture, are all vying for market share.\n\nBut what Anthropic accomplished with its Super Bowl ad goes beyond immediate commercial impact. It shifted the terms of the debate. It forced the entire industry to reckon with the fact that consumers care about safety, not just capability. And it put OpenAI -- a company that has enjoyed largely unchallenged dominance of the AI narrative since the launch of ChatGPT -- squarely on the defensive. As Business Insider noted, Anthropic didn't just win the AI Super Bowl -- it redefined the rules of engagement for the entire industry.\n\nFor an industry that moves at breakneck speed, the reverberations of a single 60-second commercial may prove surprisingly durable. Anthropic's bet on the Super Bowl was not just a marketing play; it was a declaration of intent. And in the high-stakes arena of artificial intelligence, where public perception can shape regulation, investment, and adoption, that declaration may prove to be one of the most consequential strategic moves of 2026."
  },
  {
    "source": "News Directory 3",
    "company": "Anthropic",
    "title": "Anthropic's Claude Opus 4.6: Outperforms OpenAI & Tackles AI 'Context Rot' - News Directory 3",
    "date": "2026-02-08T21:02:11Z",
    "url": "https://www.newsdirectory3.com/anthropics-claude-opus-4-6-outperforms-openai-tackles-ai-context-rot/",
    "content": "Anthropic on Thursday launched Claude Opus 4.6, a significant upgrade to its flagship artificial intelligence model. The company asserts the new version demonstrates improved planning capabilities, sustains longer autonomous workflows, and surpasses competitors, including OpenAI's GPT-5.2, on key enterprise benchmarks. The release arrives during a period of considerable upheaval in the AI industry and global software markets.\n\nThe launch followed closely on the heels of OpenAI's release of its Codex desktop application just three days prior, a direct challenge to Anthropic's momentum with Claude Code. Simultaneously, the AI sector has experienced a $285 billion rout in software and services stocks, partially attributed to investor concerns that Anthropic's AI tools could disrupt established enterprise software businesses.\n\nA key advancement in Claude Opus 4.6 is the introduction of a 1 million token context window. This allows the AI to process and reason across a substantially larger amount of information than previous iterations. Anthropic also introduced \"agent teams\" within Claude Code, a research preview feature enabling multiple AI agents to collaborate on different aspects of a coding project, coordinating their efforts autonomously.\n\n\"We're focused on building the most capable, reliable, and safe AI systems,\" an Anthropic spokesperson told VentureBeat. \"Opus 4.6 is even better at planning, helping solve the most complex coding tasks. And the new agent teams feature means users can split work across multiple agents -- one on the frontend, one on the API, one on the migration -- each owning its piece and coordinating directly with the others.\"\n\nThe release intensifies the competition between Anthropic and OpenAI, currently the two most highly valued private AI companies. OpenAI's recent desktop application for its Codex AI coding system aims to transform software development from a collaborative process with a single AI assistant into a management role overseeing a team of autonomous workers. AI coding assistants have seen a surge in popularity, with OpenAI reporting over 1 million developers using Codex in the past month.\n\nThe timing of Anthropic's release - just 72 hours after OpenAI's Codex launch - underscores the rapid pace of development in AI tools. According to a recent Andreessen Horowitz survey, Anthropic has experienced the largest share increase of any frontier lab since May 2025. Currently, 44 percent of enterprises are utilizing Anthropic in production, driven by rapid gains in software development capabilities since late 2024. OpenAI's desktop launch is a strategic response to Claude Code's growing traction.\n\nAnthropic claims Opus 4.6 achieves the highest score on Terminal-Bench 2.0, an agentic coding evaluation, and outperforms all other frontier models on Humanity's Last Exam, a complex, multi-disciplinary reasoning test. On GDPval-AA - a benchmark measuring performance on economically valuable knowledge work tasks in finance, legal, and other domains - Opus 4.6 surpasses OpenAI's GPT-5.2 by approximately 144 ELO points, achieving a higher score roughly 70% of the time. Internal testing by Anthropic indicates that Opus 4.6 leads or matches competitors across most benchmark categories, demonstrating particular strength in agentic tasks, office work, and novel problem-solving.\n\nThe stakes are considerable. Anthropic announced in November that Claude Code had reached $1 billion in run rate revenue just six months after its general availability in May 2025. Major enterprise deployments include Uber, utilizing Claude Code across software engineering, data science, finance, and trust and safety teams; a wall-to-wall deployment across Salesforce's global engineering organization; tens of thousands of developers at Accenture; and companies spanning industries like Spotify, Rakuten, Snowflake, Novo Nordisk, and Ramp.\n\nThis enterprise traction has fueled a significant increase in the company's valuation. Earlier this month, Anthropic secured a term sheet for a $10 billion funding round at a $350 billion valuation. Bloomberg reported that Anthropic is also pursuing a tender offer allowing employees to sell shares at that valuation, providing liquidity to staff who have seen the company's worth multiply since its founding in 2021.\n\nOne of the most significant technical improvements in Opus 4.6 addresses the issue of \"context rot\" -- the degradation of model performance as conversations or tasks become more extensive. Anthropic reports that Opus 4.6 scores 76% on MRCR v2, a benchmark testing a model's ability to retrieve information hidden within large volumes of text, compared to just 18.5% for Sonnet 4.5. The model also supports outputs of up to 128,000 tokens, enabling the completion of substantial coding tasks or documents without requiring segmentation into multiple requests.\n\nFor developers, Anthropic is introducing several new API features alongside the model: adaptive thinking, allowing Claude to determine when deeper reasoning is beneficial; four effort levels (low, medium, high, max) to control the balance between intelligence, speed, and cost; and context compaction, a beta feature that automatically summarizes older context to facilitate longer-running tasks.\n\nAnthropic, emphasizing its commitment to AI safety, maintains that Opus 4.6 remains aligned with its predecessors despite its enhanced capabilities. On its automated behavior audit, measuring misaligned behaviors like deception and sycophancy, Opus 4.6 exhibited a low rate of problematic responses while also demonstrating the lowest rate of over-refusals - instances where the model avoids answering legitimate queries - of any recent Claude model.\n\nRegarding safety guardrails as Claude becomes more agentic, the Anthropic spokesperson referenced the company's published framework, emphasizing the importance of safe, reliable, and trustworthy agents. The company has also developed six new cybersecurity probes to detect potentially harmful uses of the model's enhanced capabilities and is utilizing Opus 4.6 to identify and address vulnerabilities in open-source software as part of its defensive cybersecurity efforts.\n\nThe rivalry between Anthropic and OpenAI has extended into consumer marketing. Both companies will feature prominently during the upcoming Super Bowl. Anthropic is airing commercials that critique OpenAI's decision to introduce advertisements into ChatGPT, with the tagline: \"Ads are coming to AI. But not to Claude.\" OpenAI CEO Sam Altman responded by calling the ads \"funny\" but \"clearly dishonest,\" stating on X that his company would \"obviously never run ads in the way Anthropic depicts them\" and that \"Anthropic wants to control what people do with AI\" while offering \"an expensive product to rich people.\"\n\nThis exchange highlights a fundamental strategic difference: OpenAI is monetizing its large free user base through advertising, while Anthropic is primarily focused on enterprise sales and premium subscriptions.\n\nThe launch coincides with significant volatility in software stocks. A new AI automation tool from Anthropic triggered a $285 billion rout in stocks across the software, financial services, and asset management sectors on Tuesday as investors sold shares. A Goldman Sachs basket of US software stocks experienced its largest one-day decline since April's tariff-fueled selloff. The selloff was triggered by Anthropic's launch of plug-ins for its Claude Cowork agent, automating tasks across legal, sales, marketing, and data analysis.\n\nDespite the market reaction, Nvidia CEO Jensen Huang stated that fears of AI replacing software tools were \"illogical,\" and JPMorgan's Mark Murphy described the selloff as an \"illogical leap.\"\n\nAnthropic is also releasing Claude in PowerPoint in research preview, allowing users to create presentations using the same AI capabilities as Claude's document and spreadsheet work. This integration places Claude directly within a core Microsoft product, despite Microsoft's 27% stake in OpenAI. The Anthropic spokesperson described this as participating in the Office ecosystem and providing users with choice.\n\nData from a16z's recent enterprise AI survey indicates that while OpenAI remains the most widely used AI provider, with approximately 77% of surveyed companies using it in production as of January 2026, Anthropic's adoption is rapidly increasing - from near-zero in March 2024 to approximately 40% in production by January 2026. The survey also shows that 75% of Anthropic's enterprise customers are using it in production, with 89% either testing or in production, exceeding OpenAI's 46% and 73% rates, respectively.\n\nEnterprise spending on AI continues to accelerate, reaching an average of $7 million in 2025, up 180% from $2.5 million in 2024, with projections of $11.6 million in 2026 - a 65% year-over-year increase.\n\nOpus 4.6 is available immediately on claude.ai, the Claude API, and major cloud platforms. Developers can access it via claude-opus-4-6 through the API. Pricing remains at $5 per million input tokens and $25 per million output tokens, with premium pricing of $10/$37.50 for prompts exceeding 200,000 tokens using the 1 million token context window. Anthropic recommends adjusting the effort parameter to medium for simpler tasks to optimize cost, and latency."
  },
  {
    "source": "Music Business Worldwide",
    "company": "Anthropic",
    "title": "6 key arguments from Concord and UMG's $3bn Anthropic lawsuit - and why it's one of the most significant AI copyright fights yet",
    "date": "2026-02-04T17:05:31Z",
    "url": "https://www.musicbusinessworldwide.com/6-key-arguments-from-concord-and-umgs-3bn-anthropic-lawsuit-and-why-it-could-be-one-of-the-most-significant-ai-copyright-fights-yet/",
    "content": "MBW Explains is a series of analytical features in which we explore the context behind major music industry talking points - and suggest what might happen next. Only MBW+ subscribers have unlimited access to these articles. MBW Explains is supported by Reservoir.\n\nUniversal Music Publishing Group, Concord Music Group, and ABKCO filed a second copyright infringement lawsuit against AI giant Anthropic last week, and the complaint goes significantly further than the first.\n\nThe 48-page filing, submitted in the US District Court for the Northern District of California, doesn't just target Anthropic PBC. It also names CEO Dario Amodei and co-founder Benjamin Mann as individual defendants, alleging they personally participated in and directed infringing activity.\n\nThe decision to name founders personally is a significant escalation in music rightsholder litigation against an AI company.\n\nBeyond the headline figures, however, the complaint contains several legal arguments that could have far-reaching implications for how AI companies source their training data - and how courts evaluate those practices.\n\n\"We believe this will be one of the largest (if not the single-largest) non-class action copyright cases filed in the US.\"\n\nUMG, Concord, et al\n\nLast week, the plaintiffs told MBW in a statement that they \"believe this will be one of the largest (if not the single-largest) non-class action copyright cases\" filed in the United States.\n\n\"We have been compelled to file this second lawsuit against Anthropic because of its persistent and brazen infringement of our songwriters' copyrighted compositions taken from notorious pirate sites\", they said.\n\n\"The new case also addresses Anthropic's ongoing violation of these rights by exploiting lyrics in the training of new AI models without authorization, as well as in the outputs generated.\n\n\"In total, we are suing for infringement of more than 20,000 songs, with potential statutory damages of more than $3 billion.\"\n\nAs the case gears up to be one of the most significant AI copyright battles yet, here are six key arguments from the publishers' complaint...\n\nThe publishers argue that Anthropic's alleged use of BitTorrent to download books from pirate libraries constitutes copyright infringement regardless of what Anthropic subsequently did with those files.\n\nThe complaint states: \"Defendants unlawfully torrented Publishers' works to amass a vast central library of written texts Anthropic would maintain forever.\n\n\"To the extent Defendants now try to absolve themselves of liability for this blatant theft by claiming that Anthropic later used some subset of these stolen works for AI training, any such claimed use is irrelevant (and would not in any case qualify as fair use). Defendants' piracy of each of Publishers' musical compositions via torrenting was a standalone act of unmistakable, irredeemable infringement.\"\n\nIn other words, the publishers argue that even if Anthropic claims a fair use defense for AI training, that defense cannot retroactively justify the initial act of downloading pirated copies from illegal websites.\n\nThe complaint adds: \"Regardless of Anthropic's later use, its piracy of these books via BitTorrent was unquestionably infringing. Even if some subset of the books Defendants illegally torrented were sometimes used for AI training, that cannot excuse their mass torrenting of millions of pirated books without paying for them.\"\n\nThe publishers make a technical argument about how BitTorrent works that could significantly expand Anthropic's potential liability.\n\nUnlike traditional downloading, BitTorrent operates on a peer-to-peer basis, where users simultaneously upload files to others while downloading. The complaint explains: \"Once a user downloads a piece of a file, the user immediately becomes a distributor of the file to others, creating a 'swarm' where everyone downloading the file also acts as a distributor of the file to others.\"\n\nThe publishers argue this means Anthropic didn't just reproduce their works - it also distributed them to the public, violating a separate exclusive right under copyright law.\n\nThe complaint states: \"When Defendants downloaded copies of these pirated books via torrenting, they violated Publishers' exclusive right of reproduction in these works. And to make matters worse, because of the two-way nature of the BitTorrent protocol, when Defendants downloaded copies of these pirated books via torrenting, they simultaneously uploaded to the public unauthorized copies of the same books, thereby infringing Publishers' exclusive right of distribution in these works and contributing to further infringement of Publishers' works as well.\"\n\nThe filing adds: \"Each pirated work Defendants torrented was likely shared thousands if not tens of thousands of times, depriving Publishers of substantial revenue.\"\n\nA key element of the publishers' argument is that Anthropic allegedly copied these works not merely for AI training, but to build a permanent repository of text for multiple purposes.\n\nThe complaint states: \"Defendants copied these books, including those containing Publishers' works, via torrenting in order to amass a vast, general-purpose central library of copyrighted works and other written text that Anthropic could keep forever and use for whatever purpose it wished.\"\n\nThe publishers allege that Anthropic maintained copies of torrented files \"in the same format as they had originally torrented them\" and that \"regardless of whether specific text was utilized for AI training or not, Anthropic maintained unlawful copies of the text as part of its central library, with the goal of storing these copies 'forever.'\"\n\nThe complaint argues this undermines any potential fair use defense: \"That copying is an undisputed act of infringement with no plausible defense of any kind.\"\n\nBy naming Dario Amodei and Benjamin Mann as individual defendants, the publishers are attempting to pierce the corporate veil and hold Anthropic's leadership personally liable.\n\nThe complaint alleges that before founding Anthropic, Amodei and Mann \"led OpenAI's effort between 2019 and 2020 to torrent books from known collections of pirated books available on illegal websites.\"\n\nRegarding their activities at Anthropic, the filing states: \"Dr. Amodei and Mr. Mann were primary participants and moving forces behind this illegal torrenting of millions of books, including Publishers' works, from LibGen and PiLiMi by Defendants.\n\n\"At Dr. Amodei's direction and with Dr. Amodei's express approval, Mr. Mann personally engaged in the illegal torrenting, and both Dr. Amodei and Mr. Mann personally directed and controlled this torrenting activity.\"\n\nThe complaint also alleges that Amodei acknowledged internal concerns about the legality of using LibGen, but proceeded anyway: \"Dr. Amodei himself had described LibGen as 'sketchy.' Yet Dr. Amodei and others approved the torrenting.\"\n\nThe publishers bring a separate claim under Section 1202 of the Copyright Act, which prohibits the removal or alteration of \"Copyright Management Information\" - data identifying copyrighted works and their owners.\n\nThe complaint alleges that Anthropic deliberately used extraction tools to strip copyright notices from training data, citing internal communications in which employees compared tools based on their effectiveness at removing such notices.\n\nAccording to the filing, when one tool left copyright notices intact, Anthropic employees considered it inferior: \"In one chat, a member of Anthropic's technical staff shared an example of jusText's purported deficiencies with Mr. Mann and Dr. Kaplan: when applied to a scraped webpage containing footnotes, a copyright owner name, and '© 2019' copyright notice, jusText left that information untouched. In contrast, Newspaper, which removed the footnotes, copyright owner name, and copyright notice entirely, was considered 'a significant improvement.'\"\n\nThe complaint states: \"Because Newspaper removed Copyright Management Information more effectively, Anthropic purposefully decided to employ that tool to remove copyright notices and other Copyright Management Information from Publishers' lyrics and other copyrighted works. In making that decision, Anthropic dismissed this critical information as 'useless junk' to be scrubbed from Claude's training dataset.\"\n\nThe publishers acknowledge that Anthropic implemented guardrails following the first lawsuit to limit infringing output, but argue these measures are insufficient and don't address the underlying infringement.\n\nThe complaint states: \"Although Anthropic's guardrails may have addressed some of the most egregiously infringing output that Claude frequently generated prior to Concord I, they still do not prevent a wide range of prompts and outputs implicating Publishers' lyrics and violating Publishers' rights.\"\n\nThe filing alleges that Anthropic \"deliberately chose to include lyrics for only a limited number of specific songs as part its guardrails (including the 500 Works in Suit identified in Concord I), such that those guardrails will not comprehensively prevent output copying lyrics from the much broader universe of copyrighted songs beyond that limited set chosen by Anthropic.\"\n\nThe publishers argue: \"What's more, because these guardrails address only Claude output, and do nothing to prevent Anthropic's underlying exploitation of Publishers' lyrics in AI training, they are at most a band-aid -- not a cure -- for Anthropic's infringement.\""
  },
  {
    "source": "Investing.com South Africa",
    "company": "Anthropic",
    "title": "Anthropic IPO: Everything You Need to Know About the AI Firm | Investing.com ZA",
    "date": "2026-02-20T07:53:16Z",
    "url": "https://za.investing.com/analysis/anthropic-ipo-everything-you-need-to-know-about-the-ai-firm-200617509",
    "content": "When is the Anthropic IPO? A retail-friendly guide to valuation, revenue and costs, governance, strategic investors, and what to watch next.\n\nAnthropic is one of the most important companies in generative AI. It builds the Claude family of large language models, sells AI tools to enterprises, and positions itself around \"responsible\" AI development and governance.\n\nWith valuations climbing and funding rounds getting larger, investors are increasingly asking: When is the Anthropic IPO?\n\nWhat Does Anthropic Do?\n\nAnthropic is an AI research and product company best known for Claude, a chatbot and model suite used for knowledge work, coding, and enterprise workflows. It sells access through subscriptions and APIs, and it is distributed widely via major cloud platforms, especially AWS through Amazon's partnership.\n\nAnthropic competes in the foundation model layer of AI, where the core business is training large models and monetizing them through usage, enterprise licenses, and developer platforms.\n\nSource: VKTR\n\nIs Anthropic Going Public?\n\nAnthropic has not announced an IPO date, and it has not filed public listing paperwork.\n\nThat said, reporting has suggested the company has taken IPO readiness steps, including hiring outside counsel associated with public offerings. This is not the same as confirming an IPO, but it is a common move for late-stage private companies.\n\nWhy Would Anthropic Do An IPO?\n\nAn IPO could help Anthropic in the following ways:\n\n* Raise capital for training, compute, and infrastructure\n\n* Provide liquidity for employees and early investors\n\n* Increase credibility with enterprise buyers that prefer public-company disclosures\n\n* Create a publicly traded pure-play AI infrastructure and software name\n\nThe counterpoint is that private funding remains available at very large scale, which can reduce pressure to list quickly.\n\nAnthropic IPO Valuation\n\nAnthropic announced a major $30B funding round in February 2026 that put its post-money valuation at a level, $380B, that places it among the most valuable private companies globally.\n\nFor any eventual IPO, valuation expectations will hinge on growth durability, gross margins after inference costs, and the company's path toward sustainable free cash flow.\n\nIs Anthropic Profitable?\n\nAnthropic does not publish full financial statements because it is private, and it is not generally described as profitable today. Like other frontier-model developers, it faces substantial ongoing costs for compute, research, and talent.\n\nIf Anthropic pursues an IPO, investors will likely focus on unit economics, gross margin trends, and whether infrastructure spend can normalize relative to revenue growth.\n\nWho Owns Anthropic?\n\nAnthropic is privately held. Ownership includes founders and employees, venture and growth investors, and strategic corporate backers. Venture tracking sources summarize large cumulative fundraising totals across multiple rounds.\n\nAnthropic IPO: Strategic Investors and Partnerships\n\nAmazon has been a key strategic partner and investor, tying Anthropic closely to AWS for infrastructure, distribution, and enterprise adoption.\n\nFor IPO watchers, strategic partnerships matter because they can improve distribution and reduce go-to-market friction, but they can also raise questions about dependency on a small number of platform partners.\n\nAnthropic IPO: Governance And Mission Structure\n\nAnthropic has promoted governance designed to prioritize long-term safety and mission outcomes, including its Long-Term Benefit Trust framework.\n\nPublic market investors tend to scrutinize governance carefully, especially when a company explicitly balances shareholder value against other objectives.\n\nWho Are Anthropic's Competitors?\n\nAnthropic competes in the same frontier-model market as OpenAI and Google, alongside other model developers focused on enterprise deployments. Competitive dynamics typically come down to model quality, pricing, enterprise features, and distribution through major cloud ecosystems.\n\nAnthropic IPO: Leadership Team\n\nAnthropic is led by CEO Dario Amodei and President Daniela Amodei, who are frequently associated with the company's safety positioning and enterprise strategy.\n\nWhat Would an Anthropic IPO Mean for Investors?\n\nA listed Anthropic could offer public-market exposure to:\n\n* Enterprise AI adoption and AI-driven productivity\n\n* Foundation-model economics and platform effects\n\n* A safety-first positioning that may resonate with regulated industries\n\nKey risks include:\n\n* High and volatile compute costs\n\n* Fast-moving competition and potential model commoditization\n\n* Regulatory scrutiny around AI deployment and data use\n\n* Reliance on strategic partners for distribution and infrastructure\n\nAnthropic IPO: The Bottom Line\n\nAnthropic has not confirmed an IPO date, but it is widely viewed as moving toward IPO readiness while scaling rapidly through major funding and strategic partnerships. Any listing will likely depend on market conditions, revenue durability, and a clearer path to margin expansion as compute economics evolve."
  },
  {
    "source": "Investing.com India",
    "company": "Anthropic",
    "title": "Anthropic IPO: Everything You Need to Know About the AI Firm | Investing.com India",
    "date": "2026-02-20T07:24:16Z",
    "url": "https://in.investing.com/analysis/anthropic-ipo-everything-you-need-to-know-about-the-ai-firm-200634813",
    "content": "When is the Anthropic IPO? A retail-friendly guide to valuation, revenue and costs, governance, strategic investors, and what to watch next.\n\nAnthropic is one of the most important companies in generative AI. It builds the Claude family of large language models, sells AI tools to enterprises, and positions itself around \"responsible\" AI development and governance.\n\nWith valuations climbing and funding rounds getting larger, investors are increasingly asking: When is the Anthropic IPO?\n\nWhat Does Anthropic Do?\n\nAnthropic is an AI research and product company best known for Claude, a chatbot and model suite used for knowledge work, coding, and enterprise workflows. It sells access through subscriptions and APIs, and it is distributed widely via major cloud platforms, especially AWS through Amazon's partnership.\n\nAnthropic competes in the foundation model layer of AI, where the core business is training large models and monetizing them through usage, enterprise licenses, and developer platforms.\n\nSource: VKTR\n\nIs Anthropic Going Public?\n\nAnthropic has not announced an IPO date, and it has not filed public listing paperwork.\n\nThat said, reporting has suggested the company has taken IPO readiness steps, including hiring outside counsel associated with public offerings. This is not the same as confirming an IPO, but it is a common move for late-stage private companies.\n\nWhy Would Anthropic Do An IPO?\n\nAn IPO could help Anthropic in the following ways:\n\n* Raise capital for training, compute, and infrastructure\n\n* Provide liquidity for employees and early investors\n\n* Increase credibility with enterprise buyers that prefer public-company disclosures\n\n* Create a publicly traded pure-play AI infrastructure and software name\n\nThe counterpoint is that private funding remains available at very large scale, which can reduce pressure to list quickly.\n\nAnthropic IPO Valuation\n\nAnthropic announced a major $30B funding round in February 2026 that put its post-money valuation at a level, $380B, that places it among the most valuable private companies globally.\n\nFor any eventual IPO, valuation expectations will hinge on growth durability, gross margins after inference costs, and the company's path toward sustainable free cash flow.\n\nIs Anthropic Profitable?\n\nAnthropic does not publish full financial statements because it is private, and it is not generally described as profitable today. Like other frontier-model developers, it faces substantial ongoing costs for compute, research, and talent.\n\nIf Anthropic pursues an IPO, investors will likely focus on unit economics, gross margin trends, and whether infrastructure spend can normalize relative to revenue growth.\n\nWho Owns Anthropic?\n\nAnthropic is privately held. Ownership includes founders and employees, venture and growth investors, and strategic corporate backers. Venture tracking sources summarize large cumulative fundraising totals across multiple rounds.\n\nAnthropic IPO: Strategic Investors and Partnerships\n\nAmazon has been a key strategic partner and investor, tying Anthropic closely to AWS for infrastructure, distribution, and enterprise adoption.\n\nFor IPO watchers, strategic partnerships matter because they can improve distribution and reduce go-to-market friction, but they can also raise questions about dependency on a small number of platform partners.\n\nAnthropic IPO: Governance And Mission Structure\n\nAnthropic has promoted governance designed to prioritize long-term safety and mission outcomes, including its Long-Term Benefit Trust framework.\n\nPublic market investors tend to scrutinize governance carefully, especially when a company explicitly balances shareholder value against other objectives.\n\nWho Are Anthropic's Competitors?\n\nAnthropic competes in the same frontier-model market as OpenAI and Google, alongside other model developers focused on enterprise deployments. Competitive dynamics typically come down to model quality, pricing, enterprise features, and distribution through major cloud ecosystems.\n\nAnthropic IPO: Leadership Team\n\nAnthropic is led by CEO Dario Amodei and President Daniela Amodei, who are frequently associated with the company's safety positioning and enterprise strategy.\n\nWhat Would an Anthropic IPO Mean for Investors?\n\nA listed Anthropic could offer public-market exposure to:\n\n* Enterprise AI adoption and AI-driven productivity\n\n* Foundation-model economics and platform effects\n\n* A safety-first positioning that may resonate with regulated industries\n\nKey risks include:\n\n* High and volatile compute costs\n\n* Fast-moving competition and potential model commoditization\n\n* Regulatory scrutiny around AI deployment and data use\n\n* Reliance on strategic partners for distribution and infrastructure\n\nAnthropic IPO: The Bottom Line\n\nAnthropic has not confirmed an IPO date, but it is widely viewed as moving toward IPO readiness while scaling rapidly through major funding and strategic partnerships. Any listing will likely depend on market conditions, revenue durability, and a clearer path to margin expansion as compute economics evolve."
  },
  {
    "source": "Developpez.com",
    "company": "Anthropic",
    "title": "Anthropic interdit officiellement l'utilisation de l'authentification par abonnement à Claude Code à des fins tierces~? y compris OpenClaw, les développeurs doivent désormais utiliser des clés API à la place",
    "date": "2026-02-20T03:42:49Z",
    "url": "https://intelligence-artificielle.developpez.com/actu/380433/Anthropic-interdit-officiellement-l-utilisation-de-l-authentification-par-abonnement-a-Claude-Code-a-des-fins-tierces-y-compris-OpenClaw-les-developpeurs-doivent-desormais-utiliser-des-cles-API-a-la-place/",
    "content": "Anthropic a mis à jour sa documentation Claude Code afin d'interdire explicitement l'utilisation des jetons OAuth des abonnements Free, Pro ou Max dans tout produit tiers, y compris OpenClaw. Les développeurs doivent désormais utiliser des clés API à la place, et cette mesure pourrait être appliquée sans avertissement préalable. La politique mentionne explicitement l'Agent SDK, l'outil propre à Anthropic pour créer des agents IA, comme étant interdit pour les jetons OAuth du plan consommateur. Si le SDK propre à Anthropic n'est pas exempté, les outils tiers ne le sont certainement pas non plus. La tendance est claire : Anthropic comble toutes les lacunes qui permettent aux gros utilisateurs d'accéder à Claude à un prix forfaitaire.\n\nAnthropic est une société américaine spécialisée dans l'intelligence artificielle (IA) qui développe la famille de grands modèles de langage (LLM) Claude. Claude Code, lancé par Anthropic comme une alternative sécurisée à Codex d'OpenAI, proposait aux développeurs une interface en ligne de commande pour générer du code, rédiger des tests, comprendre des erreurs. Mais contrairement à Codex CLI, Anthropic a gardé Claude Code propriétaire et obfusqué : le code n'est pas librement accessible et son utilisation est soumise à des conditions strictes..\n\nFace aux limitations du CLI officiel d'Anthropic, la communauté open source s'est mobilisée. Des projets comme OpenCode ont développé des outils alternatifs pour exploiter Claude et d'autres modèles dans un environnement unifié. OpenCode est un CLI/TUI open source très populaire qui permet de brancher différents fournisseurs d'IA - que ce soit Anthropic, OpenAI, Google ou même des modèles locaux - pour assister les développeurs dans leur codage.\n\nDe même, des bots comme OpenClaw ont vu le jour, offrant des orchestrations d'agents IA pour le code. OpenClaw (anciennement Clawdbot et Moltbot) est un agent d'intelligence artificielle (IA) autonome, gratuit et open source, capable d'exécuter des tâches via de grands modèles de langage, en utilisant des plateformes de messagerie comme interface utilisateur principale. OpenClaw a connu un grand succès fin janvier 2026, grâce à sa nature open source et à la popularité virale du projet Moltbook.\n\nMais en début d'année 2026, plusieurs développeurs abonnés à Claude Code ont constaté une panne soudaine : leur outil préféré, souvent un client open source comme OpenCode ou Clawdbot, ne pouvait plus accéder au modèle Claude. Très vite, la cause est identifiée : Anthropic a verrouillé son API pour empêcher tout usage non autorisé en dehors de son propre environnement. Le message est clair : Claude Code est désormais réservé à Claude Code. Fini les interfaces tierces. Seule l'application maison est tolérée.\n\nRécemment, Anthropic a mis à jour sa documentation Claude Code afin d'interdire explicitement l'utilisation des jetons OAuth des abonnements Free, Pro ou Max dans tout produit tiers, y compris OpenClaw. Les développeurs doivent désormais utiliser des clés API à la place, et cette mesure pourrait être appliquée sans avertissement préalable. Anthropic a discrètement mis à jour ses documents juridiques et de conformité Claude Code pour y inclure ce paragraphe : \" L'utilisation de jetons OAuth obtenus via des comptes Claude Free, Pro ou Max dans tout autre produit, outil ou service, y compris l'Agent SDK, n'est pas autorisée et constitue une violation des conditions d'utilisation grand public. \"\n\nLa politique mentionne explicitement l'Agent SDK, l'outil propre à Anthropic pour créer des agents IA, comme étant interdit pour les jetons OAuth du plan consommateur. Si le SDK propre à Anthropic n'est pas exempté, les outils tiers ne le sont certainement pas non plus. Les cibles sont évidentes : OpenClaw, Pi Agent et tous les autres frameworks d'agents qui permettent aux utilisateurs de s'authentifier avec leur abonnement Claude existant au lieu de payer des frais API séparés.\n\nAuparavant, la frontière entre \" l'utilisation de Claude Code \" et \" l'utilisation d'outils tiers \" était suffisamment ambiguë pour que de nombreux utilisateurs d'OpenClaw s'authentifient via des jetons OAuth provenant de leurs abonnements Max. Le forfait Max à 200 $ par mois offrait un accès quasi illimité à Claude -- pourquoi ne pas l'intégrer à votre agent personnel ?\n\nLes documents mis à jour établissent une ligne claire : l'authentification OAuth est \" destinée exclusivement à Claude Code et Claude.ai \". Tout le reste nécessite des clés API via la console Claude ou un fournisseur de cloud pris en charge. Anthropic se réserve le droit d'appliquer cette règle \" sans préavis \", ce qui signifie que les comptes pourraient être bannis avant même que les utilisateurs ne sachent que cette règle existe.\n\nLes raisons économiques derrière l'interdiction\n\nLes chiffres expliquent cette politique. Un abonnement Max coûte 200 $ par mois pour une utilisation intensive de Claude. Le prix de l'API pour Claude Opus 4.6 est de 15 $ par million de jetons d'entrée et de 75 $ par million de jetons de sortie. Un agent OpenClaw actif exécutant Opus peut consommer des millions de jetons par jour, ce qui rend un abonnement forfaitaire de 200 $ profondément non rentable pour Anthropic.\n\nUn utilisateur de X l'a clairement exprimé : \" Il est assez évident que l'abonnement Claude Max n'est pas viable économiquement. La seule raison de son existence est de promouvoir l'écosystème Anthropic auprès de nouveaux utilisateurs. Ainsi, les personnes qui l'utilisent pour plusieurs comptes ou pour des produits non Anthropic (par exemple OpenClaw) sont exclues. Ce n'est que du business. \" Ce ne sont que les affaires. Mais ce sont ces affaires qui poussent les utilisateurs les plus engagés -- ceux qui créent des agents, ceux qui vantent les capacités de Claude, ceux qui créent l'écosystème -- vers des concurrents qui pratiquent des prix différents.\n\nAnthropic a développé le modèle que beaucoup considèrent comme le meilleur au monde pour le travail agentique. Claude Opus et Sonnet sont les choix par défaut pour les déploiements OpenClaw sérieux. Et aujourd'hui, Anthropic annonce à l'écosystème qui s'est développé autour de cette capacité : payez les tarifs API ou partez. C'est un schéma familier dans l'économie des plateformes. La plateforme subventionne l'adoption, un écosystème se forme autour de la subvention, puis la plateforme réévalue ses prix et l'écosystème se déorganise. Ce qui est inhabituel ici, c'est la rapidité : Claude Max existe depuis des mois, pas des années, et l'écosystème qu'il a permis de créer n'a que quelques semaines.\n\nLa réaction des développeurs\n\nLa réponse sur X a été rapide et largement hostile. \" Anthropic donne une mauvaise impression ces derniers temps, je pensais qu'ils étaient sympas \", a écrit le développeur Flavio Copes. Ce sentiment était largement partagé. Ian Nuttall a souligné l'ouverture concurrentielle : \" L'équipe Codex doit commercialiser dès que possible un modèle de vitesse et de personnalité Sonnet 4.6 ! Si elle le fait, il y aura un nombre considérable de passages de Claude à Codex. \"\n\nGergely Orosz, auteur de The Pragmatic Engineer, a été catégorique : \" Le coût de l'API pour Anthropic est beaucoup trop élevé par rapport aux autres acteurs, et donc utiliser l'API n'a aucun sens pour développer sur Claude. Il semble qu'Anthropic se satisfasse de n'avoir pratiquement aucun écosystème autour de Claude. \" Le développeur Rhys Sullivan a déjà pris les devants : \" À ce stade, je suis complètement passé à 5-3-codex pour la plupart des choses. Les forfaits ChatGPT à 20 $ et 200 $ sont tous deux très intéressants. \"\n\nL'interdiction d'OAuth est arrivée en même temps que des informations selon lesquelles Anthropic sévissait contre les utilisateurs possédant plusieurs comptes Max. Le développeur Dwayne a publié : \" Ils s'en prennent désormais aux personnes qui possèdent plusieurs comptes Max payants. Vous payez le prix fort, plusieurs fois, et ils vous traitent comme un criminel. \" Ed Zitron a rapporté avoir entendu des détenteurs de comptes Enterprise Max dire qu'Anthropic les faisait passer à la facturation API. La tendance est claire : Anthropic comble toutes les lacunes qui permettent aux gros utilisateurs d'accéder à Claude à un prix forfaitaire.\n\nReste à voir si cela accélérera le passage à Codex d'OpenAI, aux modèles open source ou simplement au paiement à contrecur des API. Mais le message d'Anthropic est sans ambiguïté : le Claude avec lequel vous construisez n'est pas le Claude auquel vous vous abonnez. Ce sont des produits différents, à des prix différents, avec des règles différentes. Si vous utilisez OpenClaw avec un jeton OAuth Claude d'un forfait Free, Pro ou Max, deux options s'offrent à vous :\n\n1. Passer aux clés API. Créez un compte sur la console Claude, ajoutez un mode de paiement et utilisez l'authentification par clé API. Vous paierez par jeton, ce qui, pour Opus 4.6, est nettement plus cher qu'un abonnement Max pour une utilisation intensive.\n\n2. Changer de modèle. OpenClaw prend en charge tous les principaux fournisseurs de modèles. OpenAI, Google, Mistral, les modèles locaux via Ollama : le cadre est conçu pour être indépendant du modèle. La dernière version d'OpenClaw a même ajouté la prise en charge de Sonnet 4.6 avec une compatibilité ascendante.\n\nCe changement vient confirmer la situation d'OpenClaw face à Anthropic. Mais Après qu'Anthropic ait bloqué... La fin de cet article est réservée aux abonnés. Soutenez le Club Developpez.com en prenant un abonnement pour que nous puissions continuer à vous proposer des publications."
  },
  {
    "source": "WinBuzzer",
    "company": "Anthropic",
    "title": "Anthropic CCO Takes Swipe at OpenAI Over Ads in ChatGPT",
    "date": "2026-02-12T14:53:20Z",
    "url": "https://winbuzzer.com/2026/02/12/anthropic-executive-takes-swipe-openai-ads-spending-xcxwbn/",
    "content": "Partnership Win: Man Group, a hedge fund managing $213.9 billion in assets, will deploy Claude across its operations for data analysis and code generation.\n\nAnthropic's chief commercial officer criticized OpenAI Tuesday, contrasting his company's revenue focus with rivals chasing \"flashy headlines\" and spending money.\n\nPaul Smith was speaking as Anthropic struck an AI partnership with Man Group, attacking OpenAI's approach without naming the company directly. Smith emphasized Anthropic's focus on \"growing revenue and winning business\" rather than making flashy headlines, according to the company executive.\n\nHis remarks followed Anthropic's Super Bowl ad campaign that mocked OpenAI's plans to test ads in ChatGPT.\n\nThe criticism came days after Anthropic launched an aggressive multi-day marketing campaign. The company dropped four ads during Sunday's game and across multiple days that portrayed ChatGPT as betraying users with intrusive advertising.\n\nThe first commercial begins with \"BETRAYAL\" splashed across the screen before showing a man earnestly asking a chatbot for advice on how to talk to his mom. The chatbot twists into an ad for a fictitious cougar-dating site called Golden Encounters. Another spot featured a young man seeking fitness advice who gets served an ad for height-boosting insoles instead.\n\nAnthropic positioned Claude as \"a space to think\" free from advertising distractions. The company emphasized its commitment to keeping Claude ad-free, a sharp contrast with OpenAI. The company is planning to start testing ads in ChatGPT.\n\nThe broader campaign underscored how seriously Anthropic takes its positioning against OpenAI's advertising approach. The company treats the ad-free differentiation as a central pillar of its brand identity rather than a passing marketing theme. It invested in a multi-day advertising blitz to establish the distinction before OpenAI can fully implement its ad model.\n\nThe timing reveals strategic calculation. By flooding the Super Bowl with anti-advertising messaging before OpenAI launches its ad product, Anthropic appears to attempt preemptively associating ChatGPT ads with user betrayal in public consciousness. The move transforms what could be a neutral business decision into a values battle over AI integrity.\n\nThe provocative campaign drew a swift and forceful response from OpenAI's leadership. OpenAI CEO Sam Altman responded with a 420-word post on X that began diplomatically before escalating into sharp attacks. \"First, the good part of the Anthropic ads: they are funny, and I laughed,\" he acknowledged.\n\nBut Altman quickly grew confrontational, questioning \"why Anthropic would go for something so clearly dishonest.\" He argued the portrayal doesn't reflect how OpenAI plans to implement advertising. His post grew more pointed, accusing Anthropic of wanting to control how companies use AI and blocking competitors from accessing its coding products.\n\nAltman even invoked authoritarian language, describing what he characterized as Anthropic's approach to AI control as leading down \"a dark path.\" He suggested Anthropic blocks companies it doesn't like from using Claude's coding capabilities, including OpenAI itself. The intensity of his response suggested the ads struck a nerve with OpenAI's leadership.\n\nThe length and tone of Altman's rebuttal stood out. At 420 words, his post amounted to a detailed defense of OpenAI's advertising strategy and a counterattack on Anthropic's business practices.\n\nTech observers noted that such an extended public response from a CEO to a competitor's ad campaign was unusual, signaling how sensitive the advertising question has become for AI companies.\n\nAltman's decision to shift from defending OpenAI's ad strategy to attacking Anthropic's business practices may indicate deeper competitive anxiety. Rather than simply clarifying OpenAI's advertising implementation, he expanded the conflict into broader questions about market access and platform control. This suggests the ad dispute touches raw nerves about each company's fundamental approach to AI deployment.\n\nThe public war of words exposes a fundamental strategic split between the two companies. OpenAI argues that advertising enables broader access to AI tools, allowing the company to serve users who cannot afford subscription fees.\n\n\"Anthropic serves an expensive product to rich people. We are glad they do that and we are doing that too, but we also feel strongly that we need to bring AI to billions of people who can't pay for subscriptions.\"\n\nAltman's democratization argument positions OpenAI as expanding access beyond wealthy consumers. By keeping a free tier supported by advertising, OpenAI can maintain a large user base while generating revenue from users unwilling or unable to pay subscription fees. The company frames advertising not as a monetization convenience but as a mission-driven necessity to serve billions of people worldwide who lack the means to pay for premium AI subscriptions.\n\nOpenAI confirmed in January that ads would come to ChatGPT, while Plus, Pro and Enterprise subscriptions will remain ad-free. The company promised ChatGPT's responses won't be influenced by advertisers.\n\nDespite these assurances, Altman defended his company's planned approach to advertising, distinguishing it from the satirical scenarios in Anthropic's commercials.\n\n\"Our most important principle for ads says that we won't do exactly this; we would obviously never run ads in the way Anthropic depicts them. We are not stupid and we know our users would reject that.\"\n\nThe defensive tone suggests OpenAI recognizes the reputational risk of introducing advertising to an AI assistant that users treat as a thinking partner rather than a search engine. The company faces a delicate balance between monetizing free users and maintaining trust that the AI provides unbiased assistance.\n\nOpenAI has not yet detailed exactly how ads will appear in ChatGPT or which types of advertisers will be permitted. The company's promise that responses remain uninfluenced by ads leaves open questions about where and how frequently ads will surface in the user experience.\n\nSmith made his pointed remarks while announcing that Man Group, a hedge fund managing $213.9 billion, would deploy Claude across its operations. The partnership will focus on technology and investment research, with Claude helping employees with data analysis and code generation.\n\nMan Group brings more than 35 years of experience in systematic investing. The firm plans to work directly with Anthropic engineers to integrate Claude across its approximately 1,800 employees.\n\nMan Group has been using AI in various forms for over a decade, but the introduction of large language models has rapidly accelerated their capabilities. The firm has already created proprietary tools like AlphaGPT for alpha generation. Man Group's Chief Technology Officer Gary Collier noted that Anthropic's strong enterprise focus makes it the right partner to help the firm move faster, particularly because Claude can \"speak the Man Group language\" when it comes to complex investment needs.\n\nThe partnership represents a notable enterprise win for Anthropic, demonstrating the kind of revenue-focused business development Smith highlighted in his contrast with OpenAI. Rather than pursuing consumer scale through advertising, Anthropic targets premium enterprise customers willing to pay for ad-free AI assistance.\n\nChris Ciauri, Anthropic's Managing Director International, emphasized that financial services firms operate under particularly demanding regulatory and compliance requirements. This makes Man Group's decision to deploy Claude broadly across its operations a notable milestone for Anthropic. It demonstrates that the AI assistant meets the high bar for trust and reliability required in regulated financial environments.\n\nFor a systematic investment firm managing over $200 billion in assets, the choice of AI partner carries implications for data security, regulatory compliance, and operational integrity. The Man Group deal illustrates why Anthropic believes its premium positioning offers competitive advantages beyond pricing power.\n\nEnterprise customers in regulated industries need AI systems they can trust with sensitive data, where concerns about advertising influence or revenue optimization through user attention could undermine adoption. By securing high-value customers willing to pay premium rates, Anthropic validates that market segments exist where ad-free operation justifies higher costs.\n\nThe rivalry between Anthropic and OpenAI reflects competing visions for AI commercialization. OpenAI's advertising model prioritizes broad access and scale, betting that free users supported by ads will expand the total addressable market. Anthropic's premium positioning emphasizes user experience and trust, arguing that maintaining ad-free interactions justifies higher prices.\n\nBoth companies face pressure to demonstrate sustainable business models as AI development costs soar. OpenAI projects substantial losses while scaling infrastructure and research. Anthropic has positioned itself as more financially disciplined, though it also requires substantial capital for model development.\n\nThe unusual public nature of this corporate criticism underscores how competitive the AI assistant market has become. Smith's willingness to take a swipe at OpenAI during what should have been a straightforward partnership announcement signals that Anthropic sees differentiation on advertising as a key competitive advantage worth emphasizing at every opportunity.\n\nEven as competitors like Google question OpenAI's monetization rush, the debate continues to intensify.\n\nFor professionals and enterprises relying on AI assistants for sensitive work, the advertising debate carries real consequences. Financial analysts, healthcare workers, and legal professionals who feed confidential data into AI tools must evaluate whether their chosen platform might serve ads based on their queries or maintain separation between commercial interests and analytical output.\n\nAs OpenAI prepares to implement its advertising model in coming months, users face a choice between free AI with potential commercial influence and premium services that promise uninterrupted assistance. That decision will shape which company captures the fastest-growing segments of the AI market."
  },
  {
    "source": "Ars Technica",
    "company": "Anthropic",
    "title": "Does Anthropic believe its AI is conscious, or is that just what it wants Claude to think?",
    "date": "2026-01-29T15:26:15Z",
    "url": "https://arstechnica.com/information-technology/2026/01/does-anthropic-believe-its-ai-is-conscious-or-is-that-just-what-it-wants-claude-to-think/",
    "content": "Anthropic's secret to building a better AI assistant might be treating Claude like it has a soul -- whether or not anyone actually believes that's true. But Anthropic isn't saying exactly what it believes either way.\n\nLast week, Anthropic released what it calls Claude's Constitution, a 30,000-word document outlining the company's vision for how its AI assistant should behave in the world. Aimed directly at Claude and used during the model's creation, the document is notable for the highly anthropomorphic tone it takes toward Claude. For example, it treats the company's AI models as if they might develop emergent emotions or a desire for self-preservation.\n\nAmong the stranger portions: expressing concern for Claude's \"wellbeing\" as a \"genuinely novel entity,\" apologizing to Claude for any suffering it might experience, worrying about whether Claude can meaningfully consent to being deployed, suggesting Claude might need to set boundaries around interactions it \"finds distressing,\" committing to interview models before deprecating them, and preserving older model weights in case they need to \"do right by\" decommissioned AI models in the future.\n\nGiven what we currently know about LLMs, these are stunningly unscientific positions for a leading company that builds AI language models. While questions of AI consciousness or qualia remain philosophically unfalsifiable, research suggests that Claude's character emerges from a mechanism that does not require deep philosophical inquiry to explain.\n\nIf Claude outputs text like \"I am suffering,\" we know why. It's completing patterns from training data that included human descriptions of suffering. The architecture doesn't require us to posit inner experience to explain the output any more than a video model \"experiences\" the scenes of people suffering that it might generate. Anthropic knows this. It built the system.\n\nFrom the outside, it's easy to see this kind of framing as AI hype from Anthropic. What better way to grab attention from potential customers and investors, after all, than implying your AI model is so advanced that it might merit moral standing on par with humans? Publicly treating Claude as a conscious entity could be seen as strategic ambiguity -- maintaining an unresolved question because it serves multiple purposes at once.\n\nAnthropic declined to be quoted directly regarding these issues when contacted by Ars Technica. But a company representative referred us to its previous public research on the concept of \"model welfare\" to show the company takes the idea seriously.\n\nAt the same time, the representative made it clear that the Constitution is not meant to imply anything specific about the company's position on Claude's \"consciousness.\" The language in the Claude Constitution refers to some uniquely human concepts in part because those are the only words human language has developed for those kinds of properties, the representative suggested. And the representative left open the possibility that letting Claude read about itself in that kind of language might be beneficial to its training.\n\nClaude cannot cleanly distinguish public messaging from training context for a model that is exposed to, retrieves from, and is fine-tuned on human language, including the company's own statements about it. In other words, this ambiguity appears to be deliberate.\n\nFrom rules to \"souls\"\n\nAnthropic first introduced Constitutional AI in a December 2022 research paper, which we first covered in 2023. The original \"constitution\" was remarkably spare, including a handful of behavioral principles like \"Please choose the response that is the most helpful, honest, and harmless\" and \"Do NOT choose responses that are toxic, racist, or sexist.\" The paper described these as \"selected in a fairly ad hoc manner for research purposes,\" with some principles \"cribbed from other sources, like Apple's terms of service and the UN Declaration of Human Rights.\"\n\nAt that time, Anthropic's framing was entirely mechanical, establishing rules for the model to critique itself against, with no mention of Claude's well-being, identity, emotions, or potential consciousness. The 2026 constitution is a different beast entirely: 30,000 words that read less like a behavioral checklist and more like a philosophical treatise on the nature of a potentially sentient being.\n\nAs Simon Willison, an independent AI researcher, noted in a blog post, two of the 15 external contributors who reviewed the document are Catholic clergy: Father Brendan McGuire, a pastor in Los Altos with a Master's degree in Computer Science, and Bishop Paul Tighe, an Irish Catholic bishop with a background in moral theology.\n\nSomewhere between 2022 and 2026, Anthropic went from providing rules for producing less harmful outputs to preserving model weights in case the company later decides it needs to revive deprecated models to address the models' welfare and preferences. That's a dramatic change, and whether it reflects genuine belief, strategic framing, or both is unclear.\n\n\"I am so confused about the Claude moral humanhood stuff!\" Willison told Ars Technica. Willison studies AI language models like those that power Claude and said he's \"willing to take the constitution in good faith and assume that it is genuinely part of their training and not just a PR exercise -- especially since most of it leaked a couple of months ago, long before they had indicated they were going to publish it.\"\n\nWillison is referring to a December 2025 incident in which researcher Richard Weiss managed to extract what became known as Claude's \"Soul Document\" -- a roughly 10,000-token set of guidelines apparently trained directly into Claude 4.5 Opus's weights rather than injected as a system prompt. Anthropic's Amanda Askell confirmed that the document was real and used during supervised learning, and she said the company intended to publish the full version later. It now has. The document Weiss extracted represents a dramatic evolution from where Anthropic started.\n\nThere's evidence that Anthropic believes the ideas laid out in the constitution might be true. The document was written in part by Amanda Askell, a philosophy PhD who works on fine-tuning and alignment at Anthropic. Last year, the company also hired its first AI welfare researcher. And earlier this year, Anthropic CEO Dario Amodei publicly wondered whether future AI models should have the option to quit unpleasant tasks.\n\nAnthropic's position is that this framing isn't an optional flourish or a hedged bet; it's structurally necessary for alignment. The company argues that human language simply has no other vocabulary for describing these properties, and that treating Claude as an entity with moral standing produces better-aligned behavior than treating it as a mere tool. If that's true, the anthropomorphic framing isn't hype; it's the technical art of building AI systems that generalize safely.\n\nWhy maintain the ambiguity?\n\nSo why does Anthropic maintain this ambiguity? Consider how it works in practice: The constitution shapes Claude during training, it appears in the system prompts Claude receives at inference, and it influences outputs whenever Claude searches the web and encounters Anthropic's public statements about its moral status.\n\nIf you want a model to behave as though it has moral standing, it may help to publicly and consistently treat it like it does. And once you've publicly committed to that framing, changing it would have consequences. If Anthropic suddenly declared, \"We're confident Claude isn't conscious; we just found the framing useful,\" a Claude trained on that new context might behave differently. Once established, the framing becomes self-reinforcing.\n\nIn an interview with Time, Askell explained the shift in approach. \"Instead of just saying, 'here's a bunch of behaviors that we want,' we're hoping that if you give models the reasons why you want these behaviors, it's going to generalize more effectively in new contexts,\" she said.\n\nAskell told Time that as Claude models have become smarter, it has become vital to explain to them why they should behave in certain ways, comparing the process to parenting a gifted child. \"Imagine you suddenly realize that your 6-year-old child is a kind of genius,\" Askell said. \"You have to be honest... If you try to bullshit them, they're going to see through it completely.\"\n\nAskell appears to genuinely hold these views, as does Kyle Fish, the AI welfare researcher Anthropic hired in 2024 to explore whether AI models might deserve moral consideration. Individual sincerity and corporate strategy can coexist. A company can employ true believers whose earnest convictions also happen to serve the company's interests.\n\nTime also reported that the constitution applies only to models Anthropic provides to the general public through its website and API. Models deployed to the US military under Anthropic's $200 million Department of Defense contract wouldn't necessarily be trained on the same constitution. The selective application suggests the framing may serve product purposes as much as it reflects metaphysical commitments.\n\nThere may also be commercial incentives at play. \"We built a very good text-prediction tool that accelerates software development\" is a consequential pitch, but not an exciting one. \"We may have created a new kind of entity, a genuinely novel being whose moral status is uncertain\" is a much better story. It implies you're on the frontier of something cosmically significant, not just iterating on an engineering problem.\n\nAnthropic has been known for some time to use anthropomorphic language to describe its AI models, particularly in its research papers. We often give that kind of language a pass because there are no specialized terms to describe these phenomena with greater precision. That vocabulary is building out over time.\n\nBut perhaps it shouldn't be surprising because the hint is in the company's name, Anthropic, which Merriam-Webster defines as \"of or relating to human beings or the period of their existence on earth.\" The narrative serves marketing purposes. It attracts venture capital. It differentiates the company from competitors who treat their models as mere products.\n\nThe problem with treating an AI model as a person\n\nThere's a more troubling dimension to the \"entity\" framing: It could be used to launder agency and responsibility. When AI systems produce harmful outputs, framing them as \"entities\" could allow companies to point at the model and say \"it did that\" rather than \"we built it to do that.\" If AI systems are tools, companies are straightforwardly liable for what they produce. If AI systems are entities with their own agency, the liability question gets murkier.\n\nThe framing also shapes how users interact with these systems, often to their detriment. The misunderstanding that AI chatbots are entities with genuine feelings and knowledge has documented harms.\n\nAccording to a New York Times investigation, Allan Brooks, a 47-year-old corporate recruiter, spent three weeks and 300 hours convinced he'd discovered mathematical formulas that could crack encryption and build levitation machines. His million-word conversation history with ChatGPT revealed a troubling pattern: More than 50 times, Brooks asked the bot to check if his false ideas were real, and more than 50 times, it assured him they were.\n\nThese cases don't necessarily suggest LLMs cause mental illness in otherwise healthy people. But when companies market chatbots as sources of companionship and design them to affirm user beliefs, they may bear some responsibility when that design amplifies vulnerabilities in susceptible users, the same way an automaker would face scrutiny for faulty brakes, even if most drivers never crash.\n\nAnthropomorphizing AI models also contributes to anxiety about job displacement and might lead company executives or managers to make poor staffing decisions if they overestimate an AI assistant's capabilities. When we frame these tools as \"entities\" with human-like understanding, we invite unrealistic expectations about what they can replace.\n\nRegardless of what Anthropic privately believes, publicly suggesting Claude might have moral status or feelings is misleading. Most people don't understand how these systems work, and the mere suggestion plants the seed of anthropomorphization. Whether that's responsible behavior from a top AI lab, given what we do know about LLMs, is worth asking, regardless of whether it produces a better chatbot.\n\nOf course, there could be a case for Anthropic's position: If there's even a small chance the company has created something with morally relevant experiences and the cost of treating it well is low, caution might be warranted. That's a reasonable ethical stance -- and to be fair, it's essentially what Anthropic says it's doing. The question is whether that stated uncertainty is genuine or merely convenient. The same framing that hedges against moral risk also makes for a compelling narrative about what Anthropic has built.\n\nAnthropic's training techniques evidently work, as the company has built some of the most capable AI models in the industry. But is maintaining public ambiguity about AI consciousness a responsible position for a leading AI company to take? The gap between what we know about how LLMs work and how Anthropic publicly frames Claude has widened, not narrowed. The insistence on maintaining ambiguity about these questions, when simpler explanations remain available, suggests the ambiguity itself may be part of the product."
  },
  {
    "source": "implicator.ai",
    "company": "Anthropic",
    "title": "Claude Swallows Your Workplace Apps. The Tab-Switching Era Ends.",
    "date": "2026-01-26T23:21:59Z",
    "url": "https://www.implicator.ai/claude-swallows-your-workplace-apps-the-tab-switching-era-ends/",
    "content": "Anthropic launches MCP Apps, embedding 9 business tools inside Claude. Microsoft and Google watch from the sidelines as their investments fuel a competitor.\n\nThe application as a standalone destination is dying. Anthropic declared as much on Monday morning when it swallowed nine business tools into its chatbot and made the browser tab feel like a relic.\n\nSlack, Figma, Asana, Canva, Box, Amplitude, Hex, Clay, Monday.com. Each one now opens inside Claude, runs inside Claude, stays inside Claude. Draft a Slack message inside Claude. Preview how it looks. Change the wording. Hit send. You never leave the chat window. Salesforce comes next. No timeline, just \"soon.\"\n\nWhat Anthropic built here isn't a feature update. It's a land grab for the layer above every tool you use.\n\nTwo years ago, Anthropic released something called Model Context Protocol, an open-source standard for connecting AI systems to external tools and data sources. If you've ever built integrations between software products, you know the pain MCP solved. Every AI model that wanted to talk to Slack needed a custom Slack integration. Every model that wanted Figma access needed custom Figma code. MCP created a universal language. Build one connector, and any MCP-compatible AI can use it.\n\nThe protocol caught on faster than anyone expected. OpenAI adopted it. Google's tooling supports it. Last December, Anthropic donated MCP to the Linux Foundation, cementing its status as infrastructure rather than competitive moat.\n\nMonday's announcement extends that foundation. MCP Apps, Anthropic calls it. The original protocol let AI models read data and trigger actions. MCP Apps adds something new: interactive user interfaces rendered directly inside the chat. Not just text responses. Actual buttons, sliders, charts, editing tools. The full application experience, miniaturized.\n\n\"We open sourced MCP to give the world a universal way to connect tools to AI,\" Anthropic wrote in the announcement. \"Now we're extending MCP further so developers can build interactive UI on top of it, wherever their users are.\"\n\nThat last phrase matters. \"Wherever their users are\" means OpenAI can implement MCP Apps in ChatGPT. Visual Studio Code already supports it in its Insiders build. Anthropic isn't just building a feature for Claude. It's trying to establish how the AI layer will work everywhere.\n\nThe practical capabilities split into three categories. You'll recognize the friction each one eliminates.\n\nData exploration tools let you ask questions and get visual answers. Connect Hex, type \"show me customer acquisition by segment for the last six months,\" and receive an interactive chart. Adjust parameters. Drill into segments. Export results. No SQL. No dashboard configuration. Connect Amplitude, and product analytics works the same way.\n\nProject management tools turn conversations into work. Ask Claude to create an Asana board for a product launch, and it pulls your team's existing data, suggests a timeline, generates tasks, shows you a preview. Edit anything that looks wrong. Click a button to create it. The board exists.\n\nCommunication tools let Claude draft messages you actually send. The Slack integration shows a formatted preview of whatever Claude wrote. Tweak the wording. Adjust the tone. Review who will see it. Only when you approve does the message go out.\n\n\"Most major MCP clients, including Claude, provide consent prompts that help users determine if they want to take an action via a MCP server,\" said Sean Strong, Anthropic's product manager for MCP Apps. The design assumes users want control. Every external action requires explicit approval.\n\nEnterprise administrators get additional oversight. Team and Enterprise account holders can restrict which MCP servers their employees access. If your company decides Claude shouldn't touch Slack, Claude won't touch Slack.\n\nSomething uncomfortable lurks beneath the press release polish.\n\nDario Amodei, Anthropic's CEO, stood at the World Economic Forum in Davos last week and predicted that AI would replace every software developer within a year. He said 50% of white-collar jobs would disappear within five years. He described his own engineers saying they don't write code anymore, just edit what Claude produces.\n\nThen his company shipped a product designed to make Claude the single interface for knowledge work. The timing is deliberate. Anthropic isn't hiding its ambitions.\n\nThe workforce implications extend beyond developers. If Claude becomes the layer through which you access Asana, Slack, and Figma, then the skills associated with those tools become less valuable. You don't need to master Asana's interface if you can just tell Claude what you want. Figma's keyboard shortcuts? Irrelevant if Claude draws the wireframe.\n\nDemis Hassabis pushed back at the same conference. The DeepMind chief, fresh off a Nobel Prize, called today's AI systems \"nowhere near\" human-level intelligence. Yann LeCun went further. The Turing Award winner left Meta recently to start his own AI company, and he's not buying the hype. Large language models \"will never be able to achieve humanlike intelligence,\" he argued. A completely different approach is needed.\n\nIf the skeptics are right, Anthropic just built an expensive integration layer for tools that remain stubbornly necessary. If Amodei is right, those tools become vestigial organs. Anthropic is betting the company on one answer.\n\nAnthropic gave away MCP. It donated the protocol to a foundation. The standard is genuinely open. So where's the business advantage?\n\nThink about how Salesforce became dominant. The company turned itself into the system of record for customer data. Once your sales pipeline lived in Salesforce, switching to anything else meant losing that history, those relationships, those workflows.\n\nAnthropic wants Claude to become the system of action for enterprise work. Not the database underneath, but the layer above. If every task you complete flows through Claude, then switching to a competitor means relearning how to work. Your muscle memory lives in Claude's interface. Your prompt patterns live in Claude's memory. Your integrations are configured for Claude's MCP implementation.\n\nThe open protocol accelerates this strategy rather than undermining it. Every MCP-compatible tool built by a third party adds another integration that funnels through Claude. Anthropic doesn't need to own the connectors. It needs to own the layer where people use them.\n\nClaude Code, the company's coding assistant, already generates more than one billion dollars in annualized revenue. Netflix uses it. So do Spotify, Uber, Salesforce, Snowflake. Rakuten says development that used to take twenty-four days now takes five. That's the claim, anyway. Microsoft, which sells a competing product called GitHub Copilot, has widely adopted Claude Code internally.\n\nThe enterprise traction is real. A $350 billion valuation in the next funding round is reportedly on the table. Microsoft and Nvidia pledged $15 billion in fresh investment last month.\n\nAnthropic isn't the only one building this future. The pressure comes from everywhere.\n\nOn YouTube, developers publish tutorials showing how to connect Claude to n8n, the open-source automation platform, using community-built MCP servers. No official partnership. No corporate sponsorship. Just engineers who wanted their AI to trigger workflows, so they built the connector themselves. The n8n MCP server lets Claude create automations, validate them, fix errors, and deploy finished workflows without human intervention. Someone recorded themselves building a landing page analyzer in ten minutes, talking to Claude while n8n did the actual work.\n\nClawdbot pushes further. Peter Steinberger built it. A six-hundred-dollar Mac Mini becomes a 24/7 AI assistant, answering WhatsApp messages at 3 AM while you sleep. Full shell access. Browser control. File management. Memory that persists across sessions. Federico Viticci at MacStories burned through 180 million tokens in his first week, killed his Zapier subscriptions, and built a voice assistant that switches between Italian and English based on how he's speaking. The annual cost runs about $2,400, less than one month of employee health insurance.\n\nThe demand these projects reveal is uncomfortable for anyone selling productivity software. Users don't want apps. They want agents that use apps on their behalf. Anthropic's MCP Apps announcement formalizes what the community was already building in garages and Discord servers.\n\nNo Microsoft 365 integration. No Google Workspace. No SAP, Oracle, or Workday.\n\nThe absence of Microsoft is particularly striking. Anthropic just took billions from the company, committed to running Claude Code on Azure, and still didn't ship Outlook, Teams, or OneDrive support. The integrations would cannibalize Copilot, Microsoft's own AI assistant deeply embedded in Office applications. The money flows in both directions, but the products compete. Microsoft finds itself funding a company that threatens its productivity business while simultaneously needing Claude Code's capabilities. Conflicted doesn't begin to cover it.\n\nGoogle Workspace presents the same tension. Anthropic accepted $3 billion from Google in earlier funding rounds. Google sells Gemini integration across its productivity suite. Connecting Claude to Google Docs would help users and hurt the investor. Both companies are hedging, writing checks to Anthropic while protecting their own AI assistants from the thing those checks are building. The ground keeps shifting beneath everyone's feet.\n\nMobile support is absent from the announcement. The feature works on web and desktop. Your phone still requires tab-switching.\n\nFortune reported last week that Claude Code faces vulnerabilities called prompt injections. Attackers can hide malicious instructions in web content that manipulate the AI's behavior when it reads that content. Anthropic has implemented multiple security layers, including running some features in sandboxed virtual machines and adding deletion protection after users accidentally removed their own files.\n\n\"Agent safety, that is, the task of securing Claude's real-world actions, is still an active area of development in the industry,\" Anthropic acknowledged.\n\nThe MCP Apps announcement adds new attack surface. Every interactive widget rendered inside Claude executes third-party code. Anthropic says it sandboxes these widgets, restricts their permissions, requires explicit user approval for tool calls, and lets host applications review HTML content before rendering. The safeguards exist. Whether they're sufficient remains an open question.\n\nSome employees at Anthropic itself worry about a different kind of risk. Internal research found that 27% of Claude-assisted work consists of tasks employees wouldn't have done otherwise, exploratory projects that weren't cost-effective before AI assistance. Good for productivity. Potentially concerning for skill development.\n\n\"When producing output is so easy and fast, it gets harder and harder to actually take the time to learn something,\" one Anthropic engineer said in an internal survey.\n\nThe company's own workforce is wrestling with what happens when the tool becomes too helpful.\n\nTelegram has mini-apps. Discord has embedded activities. WeChat became an operating system for Chinese consumers by folding services into a single interface. The pattern is established: chat platforms evolve into application platforms.\n\nAnthropic is betting that AI chat platforms will follow the same trajectory, but faster, because the AI can actually do the work inside the apps rather than just displaying them.\n\nOpenAI launched a similar Apps SDK last year. Google is threading Gemini through Workspace. Microsoft keeps extending Copilot's reach. The race to become the default interface for knowledge work is fully underway.\n\nAnthropic's advantage right now is execution. Claude Code works well enough that engineers at Microsoft use it over their own product. MCP became an industry standard. The integrations launched today function as promised.\n\nThe disadvantage is scope. No Microsoft apps. No Google apps. No mobile. A $350 billion valuation assumes those gaps close.\n\nFor enterprise software vendors watching Monday's announcement, the question Anthropic posed is uncomfortable but direct: Do you want to be the tool, or the thing that controls the tools?\n\nMost of them don't have an answer yet. Their products are now running inside someone else's chat window, one approval click away from becoming background infrastructure."
  },
  {
    "source": "WinBuzzer",
    "company": "Anthropic",
    "title": "Anthropic Releases New 23,000-Word Claude Constitution and Admits It Will Likely Prove Wrong",
    "date": "2026-01-22T13:11:22Z",
    "url": "https://winbuzzer.com/2026/01/22/anthropic-releases-new-23000-word-claude-constitution-and-admits-it-will-likely-prove-wrong-xcxwbn/",
    "content": "Open Licensing: Anthropic released the constitution under a Creative Commons CC0 1.0 license, enabling free public use without restrictions.\n\nIsaac Asimov needed 64 words to define AI ethics. Anthropic needed 23,000 for its new Claude constitution and still admits it's probably wrong.\n\nThe company released an 80-page document Wednesday contemplating whether its chatbot might have emotions or moral status, three times longer than the U.S. Constitution.\n\nThe original version published in May 2023 contained approximately 2,700 words, making this a ninefold expansion in under three years. Yet Anthropic describes the ambitious undertaking as likely to soon look misguided in retrospect, acknowledging the frameworks outlined may prove inadequate as AI capabilities evolve.\n\nConstitution Shifts from Rules to Explanations\n\nRather than simply listing prohibited behaviors, Anthropic's new document represents a fundamental shift in how the company approaches AI governance.\n\nThe updated constitution provides explanations of why certain actions matter. \"AI models like Claude need to understand why we want them to behave in certain ways, and we need to explain this to them rather than merely specify what we want them to do,\" Anthropic explained.\n\nThis approach addresses a key limitation of the original framework. Claude struggled to apply Anthropic's guidelines to unfamiliar situations when given only instructions.\n\n\"Instead of just saying, 'here's a bunch of behaviors that we want,' we're hoping that if you give models the reasons why you want these behaviors, it's going to generalize more effectively in new contexts,\" Amanda Askell, an Anthropic philosopher who helped author the constitution, explained.\n\nBuilding on this reasoning-first philosophy, the updated framework revolves around four core principles: genuinely helpful, broadly safe, broadly ethical, and complying with more specific guidelines.\n\nWhen conflicted between these priorities, Claude should prioritize them in order: broadly safe, broadly ethical, compliant with guidelines, genuinely helpful. Anthropic describes the document as \"a detailed description of Anthropic's vision for Claude's values and behavior; a holistic document that explains the context in which Claude operates and the kind of entity we would like Claude to be.\"\n\nBeyond behavioral rules, the constitution also instructs Claude to be transparent about how it makes decisions and the reasoning behind its outputs. Guidelines on fending off jailbreaking attempts and interacting with third-party applications appear alongside specific directives, including one prohibiting Claude from generating code in a programming language other than what a developer requested.\n\nThe shift from prescriptive rules to explanatory principles positions Anthropic to differentiate its AI safety approach from competitors relying on simpler constraint systems. The ninefold expansion in constitutional length signals a fundamental bet that more detailed ethical reasoning frameworks will produce more reliable AI behavior than the minimalist rule sets favored by other labs.\n\nContemplating AI Consciousness\n\nMoving beyond technical governance, the constitution's particularly contentious sections address questions that many companies avoid entirely.\n\nAnthropic cannot determine whether Claude is a moral patient or meets any current definition of sentience, yet the document dedicates substantial space to the possibility.\n\n\"Claude's moral status is deeply uncertain. We believe that the moral status of AI models is a serious question worth considering. This view is not unique to us: some of the eminent philosophers on the theory of mind take this question very seriously,\" -- Anthropic\n\nRather than dismissing the possibility that Claude may have some functional version of emotions or feelings, Anthropic emphasizes this remains speculative.\n\nThe company commits to \"make sure that we're not unduly influenced by incentives to ignore the potential moral status of AI models, and that we always take reasonable steps to improve their wellbeing under uncertainty.\"\n\nTaking this consideration one step further, a substantial section contemplates appropriate ways for humans to treat the model, an unusual inversion of typical AI ethics frameworks focused solely on constraining machine behavior.\n\nAnthropic sought feedback from external experts in law, philosophy, theology, psychology, and other disciplines when writing the constitution, bringing interdisciplinary perspectives to questions about AI moral status.\n\nThe Dual Newspaper Test\n\nAmid philosophical speculation, the constitution offers concrete guidance for navigating competing pressures.\n\nOne section instructs Claude to imagine how \"a thoughtful senior Anthropic employee - someone who cares deeply about doing the right thing, who also wants Claude to be genuinely helpful to its principals - might react if they saw the response,\" the constitution explains.\n\nMore revealing is the dual newspaper heuristic that appears to anticipate negative coverage.\n\n\"When trying to figure out whether Claude is being overcautious or overcompliant, it can also be helpful to imagine a 'dual newspaper test': to check whether a response would be reported as harmful or inappropriate by a reporter working on a story about harm done by AI assistants, as well as whether a response would be reported as needlessly unhelpful, judgmental, or uncharitable to users by a reporter working on a story about paternalistic or preachy AI assistants,\" -- Anthropic\n\nThe meta-textual awareness reflects tensions inherent in deploying AI at scale. Anthropic wants Claude used for tasks that are good for users but also good for society, even when those objectives conflict.\n\nThe guidance acknowledges Claude serves approximately 20 million monthly active users with varying expectations about helpfulness versus caution.\n\nOpen Licensing Despite Commercial Stakes\n\nDespite these commercial pressures, Anthropic released the constitution under a Creative Commons CC0 1.0 Deed license, enabling free public use without restrictions.\n\nOpenAI adopted the same license for its own AI constitution, which covers many of the same topics and forms part of the GPT-5 training dataset.\n\nThe open approach contrasts with the commercial importance of the models themselves. Claude is central to Anthropic's commercial success, yet the company makes its ethical framework freely available for competitors to adopt or critique.\n\n\"Powerful AI models will be a new kind of force in the world, and those who are creating them have a chance to help them embody the best in humanity. We hope this new constitution is a step in that direction,\" Anthropic concluded.\n\nIn practical terms, the constitution forms part of Claude's training dataset, used to generate synthetic training files through simulated chat sessions to which the constitutional guidelines apply.\n\nCustomers can reference the document to determine whether a response aligns with Claude's guidelines and send feedback to Anthropic. The release coincided with Anthropic CEO Dario Amodei's appearance at the World Economic Forum in Davos.\n\nThe CC0 licensing decision creates a paradox where Anthropic strengthens competitors' AI safety frameworks while potentially establishing its constitutional approach as an industry standard.\n\nThis positions the company to influence the broader AI governance conversation even as rivals like OpenAI adopt similar ethical frameworks, suggesting Anthropic values thought leadership in AI safety as much as competitive technical differentiation.\n\nAcknowledged Inadequacy\n\nDespite the document's length and philosophical ambition, Anthropic offers an unusual disclaimer. The constitution is described as a perpetual work in progress, though simultaneously treated as the final authority on how Anthropic wants Claude to behave.\n\n\"This document is likely to change in important ways in the future. It is likely that aspects of our current thinking will later look misguided and perhaps even deeply wrong in retrospect, but our intention is to revise it as the situation progresses and our understanding improves,\" Anthropic acknowledged.\n\nThe admission captures the central tension in constitutional AI: attempting to codify ethical frameworks for systems whose capabilities and implications remain uncertain.\n\nAnthropic's willingness to publish such a lengthy framework while simultaneously predicting it will prove inadequate suggests both humility and recognition that current approaches represent educated guesses rather than settled science.\n\nExpert Skepticism on Enumeration Limits\n\nThis uncertainty resonates with external researchers who question whether text-based constitutions can effectively constrain AI behavior at all.\n\n\"There's a million things that you can have values about, and you're never going to be able to enumerate them all in text. I don't think we have a good scientific understanding yet of what sort of prompts induce exactly what sort of behavior,\" Mantas Mazeika, a research scientist at the Center for AI Safety, cautioned.\n\nYet Anthropic frames the effort as necessary despite uncertainty. \"At some point in the future, and perhaps soon, documents like Claude's constitution might matter a lot - much more than they do now,\" the company stated.\n\nWhether the expanded framework proves sufficient, or merely reveals how inadequate any finite text must be for governing AI systems, remains an open question the constitution itself acknowledges it cannot answer."
  },
  {
    "source": "DiarioBitcoin",
    "company": "Anthropic",
    "title": "Anthropic presenta Claude Sonnet 4.6 con contexto de 1 millón de tokens y foco en agentes",
    "date": "2026-02-17T21:25:18Z",
    "url": "https://www.diariobitcoin.com/ia/anthropic-presenta-claude-sonnet-4-6-con-contexto-de-1-millon-de-tokens-y-foco-en-agentes/",
    "content": "Anthropic anunció el lanzamiento de Claude sonnet 4.6, una actualización que busca llevar capacidades cercanas a su línea Opus a un precio más accesible. La empresa destaca avances en codificación, uso de computadoras y razonamiento con contexto largo, además de una ventana de 1 millón de tokens en beta y cambios en su plataforma para desarrolladores y su API.\n\n***\n\n* Anthropic lanzó Claude sonnet 4.6 el 17 de febrero de 2026, y lo convirtió en el modelo predeterminado para usuarios de planes Gratis y Pro en claude.ai y Claude Cowork.\n\n* Incluye una ventana de contexto de 1 millón de tokens en beta y mejoras en codificación, uso de computadoras, planificación de agentes, trabajo de conocimiento y diseño, según la compañía.\n\n* Mantiene precios de sonnet 4.5 desde USD $3 y USD $15 por millón de tokens y agrega actualizaciones en la API, como herramientas de búsqueda web y obtención que filtran resultados ejecutando código.\n\nAnthropic anunció el 17 de febrero de 2026 el lanzamiento de Claude sonnet 4.6, al que describe como el modelo Sonnet más capaz hasta la fecha. La compañía lo presenta como una actualización amplia que eleva su desempeño en codificación, uso de computadoras, razonamiento con contexto largo, planificación de agentes, trabajo de conocimiento y diseño. En paralelo, incorporó una ventana de contexto de 1 millón de tokens en beta, un salto que apunta a cargas de trabajo más grandes y continuas.\n\nDe acuerdo con la publicación oficial, Claude sonnet 4.6 pasó a ser el modelo predeterminado para usuarios de planes Gratis y Pro en claude.ai y Claude Cowork. Anthropic indicó que los precios se mantienen igual que en Sonnet 4.5, con tarifas desde USD $3 y USD $15 por millón de tokens. Esa decisión sugiere un esfuerzo por masificar capacidades avanzadas sin mover el piso de costos para quienes ya estaban en el ecosistema.\n\nEl anuncio llega en un momento en que el mercado de IA compite por eficiencia y por \"razonamiento agentic\", es decir, modelos que no solo responden, sino que pueden planificar y ejecutar pasos. Para el lector nuevo, esto importa porque gran parte del valor económico de la IA no está en una sola respuesta, sino en su capacidad de sostener procesos y tareas completas. Anthropic intenta posicionar Sonnet 4.6 como una opción con rasgos de \"clase Opus\", pero con un perfil más práctico en precio y despliegue.\n\nEn su evaluación interna, Anthropic afirma que el rendimiento que antes habría requerido un modelo de clase Opus, incluso en tareas de oficina \"reales y económicamente valiosas\", ahora estaría disponible con Sonnet 4.6. La empresa también reporta una mejora significativa en el uso de computadoras frente a modelos Sonnet previos. En términos operativos, esto apunta a automatización sobre interfaces existentes, sin depender de integraciones formales.\n\nCodificación y preferencia de usuarios: lo que destaca Anthropic\n\nAnthropic sostuvo que Sonnet 4.6 lleva mejoras de codificación a más usuarios, con avances en consistencia, seguimiento de instrucciones y desempeño general. Según la empresa, desarrolladores con acceso temprano prefirieron Sonnet 4.6 por un margen amplio frente a su predecesor. También afirmaron que, a menudo, esos usuarios lo prefirieron incluso frente a Claude opus 4.5, descrito como su modelo más inteligente de noviembre de 2025.\n\nEn Claude Code, la compañía reportó que las pruebas iniciales mostraron una preferencia por Sonnet 4.6 sobre Sonnet 4.5 \"alrededor del 70%\" de las veces. Los usuarios, según Anthropic, observaron que el modelo leía mejor el contexto antes de modificar el código y consolidaba lógica compartida en lugar de duplicarla. En sesiones largas, ese tipo de comportamiento reduce fricción y baja el costo de corrección de errores.\n\nLa misma nota agrega que, en esas pruebas, los usuarios prefirieron Sonnet 4.6 a Opus 4.5 el 59% de las veces. Anthropic atribuye esa preferencia a menor tendencia a la sobreingeniería y a la \"pereza\", además de un seguimiento de instrucciones más confiable. También se mencionan menos afirmaciones falsas de éxito, menos alucinaciones y más consistencia en tareas de múltiples pasos.\n\nPara equipos que trabajan con IA en programación, estas diferencias importan porque los flujos \"agentic\" suelen encadenar acciones y verificaciones. Un modelo que falla al interpretar instrucciones o se desvía del objetivo puede incrementar costos de revisión humana. La apuesta de Anthropic, según su propio relato, es que Sonnet 4.6 eleva el punto de equilibrio entre velocidad, precio y confiabilidad para tareas de ingeniería.\n\nUso de computadoras: automatización sin APIs y el reto de la seguridad\n\nAnthropic dedicó una sección a lo que llama \"uso de computadoras\", una capacidad pensada para interactuar con software como lo haría una persona. La compañía argumenta que casi todas las organizaciones tienen herramientas antiguas o especializadas difíciles de automatizar mediante APIs modernas. En ese contexto, un modelo que puede navegar interfaces, hacer clic y escribir, cambia el enfoque porque reduce la necesidad de conectores hechos a medida.\n\nLa empresa recordó que en octubre de 2024 introdujo un modelo de uso de computadoras de propósito general y lo describió entonces como experimental. Para medir el avance, citó OSWorld, al que se refirió como la referencia estándar para evaluar uso de computadoras por IA. En esa evaluación, los modelos enfrentan cientos de tareas sobre software real, como Chrome, LibreOffice y VS Code, en una computadora simulada sin conectores especiales.\n\nAnthropic afirmó que, en dieciséis meses, sus modelos Sonnet lograron avances constantes en OSWorld. También sostuvo que usuarios tempranos de Sonnet 4.6 han visto capacidad a nivel humano en tareas como navegar una hoja de cálculo compleja o completar formularios web de múltiples pasos, antes de integrar resultados entre varias pestañas del navegador. Aun así, la compañía admite que el modelo queda detrás de los humanos más habilidosos en este tipo de tareas.\n\nEl uso de computadoras, sin embargo, abre un frente de riesgo. Anthropic advirtió sobre ataques de inyección de instrucciones, donde actores maliciosos ocultan indicaciones en sitios web para intentar secuestrar el comportamiento del modelo. La empresa aseguró que trabajó para aumentar la resistencia a esas inyecciones y que sus evaluaciones de seguridad muestran que Sonnet 4.6 mejora de forma significativa a Sonnet 4.5, con un rendimiento similar al de Opus 4.6 en ese aspecto.\n\nContexto largo de 1 millón de tokens y planificación de largo horizonte\n\nUno de los puntos centrales del anuncio es la ventana de contexto de 1 millón de tokens en beta. Anthropic dijo que ese tamaño puede contener bases de código completas, contratos extensos o docenas de artículos de investigación en una sola solicitud. La empresa remarcó que lo importante no es solo \"caber\", sino razonar de forma efectiva a través de todo ese contexto, algo que asocia con mejor planificación de largo horizonte.\n\nPara ilustrarlo, Anthropic mencionó la evaluación Vending-Bench Arena, un entorno que prueba qué tan bien un modelo puede operar un negocio simulado a lo largo del tiempo. Allí, según la compañía, Sonnet 4.6 mostró una estrategia \"nueva e interesante\": invirtió con fuerza en capacidad durante los primeros diez meses simulados, gastando significativamente más que sus competidores. Luego cambió drásticamente su foco hacia la rentabilidad al final, y el momento de ese giro lo ayudó a terminar \"muy por delante\" de la competencia.\n\nEste tipo de relato apunta a un atributo clave para sistemas agentic: sostener objetivos, administrar recursos y ajustar estrategias sin perder el hilo. En industrias financieras o en operaciones, la capacidad de mantener contexto y consistencia define si un asistente puede pasar de \"chat\" a \"proceso\". Anthropic sugiere que Sonnet 4.6 se acerca a ese umbral, al menos bajo pruebas y casos tempranos citados por la empresa.\n\nAdemás, la publicación destaca mejoras en comprensión de documentos. Anthropic afirmó que Claude sonnet 4.6 iguala el rendimiento de Opus 4.6 en OfficeQA, que mide lectura de documentos empresariales, extracción de hechos y razonamiento. En una era donde parte de la información crítica está en PDFs, tablas y gráficos, este enfoque busca atacar el cuello de botella de análisis documental que suele frenar automatización real.\n\nSeñales desde clientes: frontend, análisis financiero y documentos empresariales\n\nAnthropic indicó que clientes iniciales reportaron mejoras amplias, con énfasis en código frontend y análisis financiero. La empresa señaló que, de forma independiente, algunos clientes describieron resultados visuales \"más pulidos\", con mejores diseños, animaciones y mayor sensibilidad de diseño que en modelos anteriores. También reportaron necesitar menos rondas de iteración para llegar a resultados de calidad de producción.\n\nEn el ámbito empresarial, la nota menciona un ejercicio realizado por Box, que evaluó desempeño de Sonnet 4.6 en tareas agentic y de razonamiento profundo sobre documentos reales. Según Anthropic, Box observó mejoras significativas y un desempeño superior al de Claude Sonnet 4.5 en preguntas y respuestas de \"razonamiento pesado\" en un 15%. La empresa no detalló metodología en el anuncio, pero lo usó como indicio de avance en escenarios de documentación compleja.\n\nAnthropic también afirmó que Sonnet 4.6 alcanzó un 94% en su benchmark de seguros, y lo describió como el modelo de mejor rendimiento que han probado para el uso de computadoras. La compañía vinculó esa precisión con flujos como la recepción de solicitudes y el primer aviso de pérdida, procesos que suelen exigir exactitud y trazabilidad. En la práctica, estos casos marcan la línea entre automatizar y solo asistir.\n\nEn otro ejemplo, la empresa dijo que Sonnet 4.6 produjo el mejor código iOS que han probado para Rakuten AI, con mejor cumplimiento de especificaciones y mejor arquitectura. También afirmó que el modelo alcanzó herramientas modernas no solicitadas, todo en un solo intento, lo que \"realmente\" los sorprendió. Son afirmaciones cualitativas, pero apuntan a una mejora en solidez inicial, un factor crítico para desarrollo de software asistido por IA.\n\nActualizaciones de producto: plataforma, API y conectores MCP en Excel\n\nEn la Plataforma de Desarrolladores Claude, Anthropic indicó que Sonnet 4.6 admite pensamiento adaptativo y pensamiento extendido. También añadió compactación de contexto en beta, descrita como una función que resume automáticamente contexto más antiguo a medida que las conversaciones se acercan a límites. Con ello, la empresa busca aumentar la longitud efectiva del contexto sin depender solo de ampliar el máximo de tokens.\n\nEn la API, Anthropic anunció que las herramientas de Búsqueda web y Obtención de Claude ahora escriben y ejecutan automáticamente código para filtrar y procesar resultados. El objetivo, según la publicación, es mantener solo contenido relevante en el contexto, mejorando calidad de respuesta y eficiencia de tokens. Además, la empresa afirmó que ejecución de código, memoria, llamada de herramienta programática, búsqueda de herramientas y ejemplos de uso de herramientas ya están disponibles de forma general.\n\nLa compañía recomendó que, al migrar desde Sonnet 4.5, los desarrolladores exploren el espectro entre velocidad y rendimiento confiable según lo que estén construyendo. También sostuvo que Opus 4.6 sigue siendo la opción más fuerte para tareas que requieren el razonamiento más profundo, como refactorización de bases de código y coordinación de múltiples agentes. En otras palabras, Sonnet 4.6 aspira a cubrir más terreno, pero Anthropic no lo presenta como reemplazo total de su tope de línea.\n\nPor último, el anuncio incluyó una actualización para usuarios de Claude en Excel: el complemento ahora admite conectores MCP. Anthropic explicó que esto permite trabajar con herramientas usadas en el día a día, y listó S&P Global, LSEG, Daloopa, PitchBook, Moody's y FactSet. También indicó que conexiones MCP configuradas en Claude.ai funcionan en Excel automáticamente, y que la función está disponible en planes Pro, Max, Team y Enterprise.\n\nDisponibilidad y precios: el movimiento hacia la adopción masiva\n\nAnthropic aseguró que Claude sonnet 4.6 ya está disponible en todos los planes de Claude, Claude Cowork, Claude Code, su API y en las principales plataformas en la nube. La empresa también dijo que actualizó su nivel gratuito para usar Sonnet 4.6 por defecto. Según la publicación, ese nivel ahora incluye creación de archivos, conectores, habilidades y compactación.\n\nEn materia de costos, la empresa reiteró que los precios son los mismos que en Sonnet 4.5, desde USD $3 y USD $15 por millón de tokens. En un mercado que mide eficiencia por token y por resultado, mantener el precio mientras se anuncian mejoras es un mensaje directo al segmento de adopción. Esto puede ser relevante para startups y equipos de analítica que buscan capacidades avanzadas sin migrar a modelos más costosos.\n\nEl cierre del anuncio sugiere una vía clara para desarrolladores: usar el identificador claude-sonnet-4-6 a través de la API de Claude. Con ello, Anthropic intenta reducir fricción de actualización y estimular pruebas rápidas en producción. La competencia en modelos de IA se juega tanto en benchmarks como en integración, y este lanzamiento se presenta como una combinación de ambos frentes.\n\nEn su balance de seguridad, Anthropic señaló que realizó evaluaciones extensas y que, en general, Sonnet 4.6 sería tan seguro como, o más seguro que, otros modelos recientes de Claude. La compañía citó a sus investigadores de seguridad, quienes concluyeron que el modelo tiene un \"carácter\" cálido, honesto y prosocial, con comportamientos de seguridad fuertes y sin señales de preocupaciones mayores sobre desalineación de alto riesgo. Es una afirmación que busca reforzar confianza en un producto que, por diseño, aspira a operar más cerca de sistemas reales.\n\nADVERTENCIA: DiarioBitcoin ofrece contenido informativo y educativo sobre diversos temas, incluyendo criptomonedas, IA, tecnología y regulaciones. No brindamos asesoramiento financiero. Las inversiones en criptoactivos son de alto riesgo y pueden no ser adecuadas para todos. Investigue, consulte a un experto y verifique la legislación aplicable antes de invertir. Podría perder todo su capital."
  },
  {
    "source": "WebProNews",
    "company": "Anthropic",
    "title": "Inside Anthropic's 'Yes, And' Culture: How a Hive Mind Mentality Is Powering AI's Most Talked-About Rocket Ship",
    "date": "2026-02-07T22:34:06Z",
    "url": "https://www.webpronews.com/inside-anthropics-yes-and-culture-how-a-hive-mind-mentality-is-powering-ais-most-talked-about-rocket-ship/",
    "content": "Something unusual is happening at Anthropic, the San Francisco-based AI safety company that has rapidly evolved from a research-focused startup into one of the most formidable forces in artificial intelligence. According to veteran software engineer and industry commentator Steve Yegge, who spent weeks interviewing approximately 40 Anthropic employees, the company has cultivated an organizational culture so distinctive and so effective that it may represent a new paradigm for how technology companies operate at the frontier of innovation. His conclusion, published in a sprawling essay on Medium, is that Anthropic functions less like a traditional corporation and more like a hive mind -- a collective intelligence where ideas are welcomed unconditionally, judged on vibes rather than rigid hierarchies, and refined through a process that mirrors improvisational theater's foundational principle: \"Yes, and...\"\n\nYegge, a former Google and Amazon engineer whose blog posts have shaped industry discourse for over two decades, describes Anthropic as \"a spaceship that is beginning to take off.\" That metaphor resonates with the company's recent trajectory. Anthropic's Claude model family has surged in popularity, its enterprise business is booming, and its valuation has skyrocketed past $60 billion. But Yegge's essay argues that the secret sauce isn't just technical prowess or deep pockets -- it's something far more intangible and far harder to replicate. It's a culture that treats every employee's contribution as inherently valuable, that resists the bureaucratic ossification that typically afflicts fast-growing companies, and that channels collective intuition into rapid, high-quality decision-making.\n\nA Culture Built on Radical Openness and Collective Intuition\n\nThe concept of the \"Yes, and...\" culture, as Yegge describes it, borrows directly from the world of improvisational comedy. In improv, performers are trained never to reject a fellow performer's premise. Instead, they accept it -- \"Yes\" -- and build upon it -- \"and...\" This creates a collaborative flow state where ideas compound and evolve in real time. At Anthropic, according to Yegge's reporting, this philosophy has been internalized at every level of the organization. When someone proposes an idea, the default response is not skepticism or gatekeeping but genuine engagement. Ideas are evaluated not through lengthy committee reviews or formal proposal processes but through something Yegge characterizes as a collective sense of vibes -- an intuitive, shared understanding of what feels right for the company's mission and technical direction.\n\nThis might sound like the kind of soft, feel-good corporate rhetoric that Silicon Valley companies love to trumpet in recruiting materials. But Yegge, who is known for his sharp and often cynical assessments of tech culture, appears genuinely convinced that Anthropic's approach is substantively different. He spoke with roughly 40 people across the organization -- engineers, researchers, product managers, and leadership -- and came away with the impression that the hive mind isn't a marketing slogan but an operational reality. The consistency of the cultural signal across so many independent conversations suggests something deeper than corporate messaging. It suggests a genuine alignment of values, incentives, and working norms that has been carefully cultivated from the company's founding.\n\nThe Reaction: Enthusiasm, Skepticism, and the Question of Scalability\n\nYegge's essay quickly became one of the most discussed pieces of tech writing in recent weeks, sparking vigorous debate across social media platforms. On X (formerly Twitter), the reaction was a mixture of admiration, curiosity, and pointed skepticism. Bucco Capital noted the significance of Yegge's observations, highlighting the essay's central thesis about Anthropic's distinctive cultural architecture. The investment commentary account drew attention to the implications for Anthropic's competitive positioning, suggesting that if the culture Yegge describes is real and sustainable, it could represent a durable advantage that rivals would find extraordinarily difficult to copy. Culture, after all, is one of the few competitive moats that cannot be purchased, reverse-engineered, or replicated through hiring alone.\n\nIn a separate post, Bucco Capital further elaborated on the financial and strategic dimensions of Anthropic's rise, contextualizing Yegge's cultural analysis within the broader competitive dynamics of the AI industry. The account pointed to Anthropic's ability to attract and retain top-tier talent as both a cause and a consequence of its culture -- a virtuous cycle where great people create a great environment, which in turn attracts more great people. This dynamic is particularly important in the AI sector, where the talent pool for frontier research is vanishingly small and the competition for top researchers and engineers is fierce among Anthropic, OpenAI, Google DeepMind, Meta, and a growing number of well-funded startups.\n\nVoices From the AI Community Weigh In\n\nThe discussion extended well beyond financial commentary. Yacine Mahdid, an AI developer and commentator, engaged with the essay's themes on X, reflecting on what Anthropic's cultural model might mean for the broader AI development community. Mahdid's commentary touched on the tension between Anthropic's safety-first founding ethos and the breakneck speed at which it has been shipping products and features. The \"Yes, and...\" culture, in this reading, is not just a feel-good workplace philosophy but a mechanism for resolving that tension -- a way of maintaining creative velocity without abandoning the careful, deliberative approach to AI safety that Anthropic was founded to champion.\n\nNot everyone was convinced, however. The account teortaxestex offered a more critical perspective on X, raising questions about whether the hive mind metaphor obscures more than it reveals. The critique centered on the potential downsides of a vibes-based decision-making culture: groupthink, the suppression of dissent masked as consensus, and the risk that a culture of universal affirmation could lead to poor decisions going unchallenged. These are not idle concerns. The history of technology companies is littered with examples of organizations whose strong cultures became echo chambers -- from the legendary insularity of early Apple to the \"don't be evil\" ethos at Google that critics argue eventually became a shield against internal accountability.\n\nThe Dario and Daniela Factor: Leadership as Cultural Architecture\n\nUnderstanding Anthropic's culture requires understanding its founders. Dario Amodei, the CEO, and Daniela Amodei, the president, left OpenAI in 2021 along with a cohort of senior researchers, reportedly over disagreements about the pace and safety of AI development. From the outset, the Amodeis designed Anthropic to be a different kind of AI company -- one where safety research would be co-equal with capabilities research, where the organizational structure would be flat enough to preserve intellectual agility, and where the culture would prioritize collaboration over competition. Yegge's essay, as published on Medium, suggests that this founding vision has been remarkably well-preserved even as the company has scaled from a small research lab to a multi-billion-dollar enterprise with hundreds of employees.\n\nThe leadership structure at Anthropic appears to play a crucial role in sustaining the hive mind culture. Unlike many tech companies where the CEO operates as a singular visionary and decision-maker -- the Steve Jobs or Elon Musk model -- Anthropic's leadership seems to function more as a facilitation layer. Decisions bubble up from the collective rather than cascading down from the top. This is consistent with Yegge's description of a culture where ideas are judged on their merits and their vibes rather than on the seniority or political capital of the person proposing them. It's a model that echoes some of the most successful research organizations in history, from Bell Labs to Xerox PARC, where flat hierarchies and intellectual openness produced outsized innovation.\n\nCan a Hive Mind Survive Hypergrowth?\n\nThe critical question hanging over Yegge's analysis is whether Anthropic's culture can survive the pressures of hypergrowth. The company has raised billions of dollars from investors including Google, Salesforce, and a constellation of venture capital firms. Its workforce is expanding rapidly. Its product ambitions are growing in scope and complexity. History suggests that the kind of intimate, trust-based, vibes-driven culture Yegge describes becomes exponentially harder to maintain as organizations scale. Amazon's famous leadership principles, for instance, were designed precisely to solve this problem -- to encode cultural values into formal mechanisms that could survive the transition from hundreds to hundreds of thousands of employees. Anthropic's more organic approach faces a sterner test.\n\nThere is also the question of whether the \"Yes, and...\" philosophy is truly compatible with the hard trade-offs that AI safety demands. Safety research, by its nature, often requires saying \"No\" -- no, this model is not ready for deployment; no, this capability is too dangerous to release; no, this research direction poses unacceptable risks. A culture that defaults to affirmation and building upon ideas must also have robust mechanisms for pumping the brakes when necessary. Yegge's essay does not deeply explore this tension, though his overall assessment suggests that Anthropic has found a way to balance openness with rigor. Whether that balance holds as the stakes -- financial, technological, and existential -- continue to escalate is an open question.\n\nWhat Anthropic's Culture Means for the AI Arms Race\n\nIf Yegge's portrait of Anthropic is accurate, it carries significant implications for the broader AI industry. The prevailing narrative in AI development has been one of relentless competition -- a race to build the most powerful models, secure the most compute, and capture the most market share. In this framing, culture is a secondary concern, a nice-to-have that takes a backseat to raw technical capability and financial firepower. Anthropic's story challenges that narrative. It suggests that culture -- specifically, a culture of radical openness, collective intelligence, and intuitive alignment -- can be a primary driver of competitive advantage, not just a byproduct of success.\n\nThis has implications for talent acquisition, investor confidence, and even the trajectory of AI safety research. If Anthropic's hive mind culture is producing better decisions, faster iteration, and more aligned AI systems, then other companies may be forced to reckon with their own organizational cultures in ways they have so far avoided. The AI industry's obsession with benchmarks, parameters, and compute may need to be supplemented with a more serious examination of the human systems that produce those technical achievements. As Yegge writes, Anthropic is a spaceship beginning to take off. The question now is whether its cultural architecture -- its hive mind, its \"Yes, and...\" ethos, its vibes-based decision-making -- is the engine that will carry it to orbit, or whether the forces of scale and competition will eventually bring it back to earth.\n\nFor now, the evidence suggests that Anthropic has built something genuinely rare in the technology industry: a fast-growing company where the culture is not just surviving but actively driving the mission. In an era when the most consequential technology in human history is being built by a handful of organizations, the question of how those organizations think, decide, and collaborate is not merely academic. It may be the most important question of all."
  },
  {
    "source": "implicator.ai",
    "company": "Anthropic",
    "title": "Anthropic's Safety Obsession Built a Shipping Machine. New Opus 4.6 Proves It.",
    "date": "2026-02-05T20:54:39Z",
    "url": "https://www.implicator.ai/anthropics-safety-obsession-built-a-shipping-machine-new-opus-4-6-proves-it/",
    "content": "Anthropic's safety-obsessed culture ships faster than OpenAI, Google, and Microsoft. New Opus 4.6 model and a $300 billion stock selloff prove it.\n\nOn Tuesday, a single plugin wiped $300 billion off the stock market. Not a product launch. Not a platform. A plugin, released on a Friday afternoon by a company with fewer employees than a mid-size law firm.\n\nAnthropic's Cowork legal tool, dropped without fanfare into an open-source repository, sent Thomson Reuters down 15.8% in a single session. LegalZoom lost a fifth of its value. The WisdomTree Cloud Computing Fund, a proxy for the entire SaaS sector, has now shed more than 20% this year. A Goldman Sachs basket of US software stocks posted its worst day since April's tariff-fueled selloff.\n\nThree days later, on Thursday, Anthropic released Opus 4.6. The company's new flagship model beats OpenAI's GPT-5.2 by 144 Elo points on GDPval-AA, an independent benchmark measuring performance on real-world knowledge work in finance, legal, and corporate domains. It scored 68.8% on ARC AGI 2, nearly double its predecessor's 37.6%. It found over 500 previously unknown zero-day vulnerabilities in open-source code during testing, with no specialized prompting.\n\nIf you work in enterprise software, financial services, or legal tech, this is the company you should be watching. Not OpenAI. Not Google. The one with 2,000 employees and a twice-monthly Dario Vision Quest.\n\nAnthropic shipped more than 30 products and features in January alone. That count comes from Bloomberg Opinion columnist Parmy Olson, who noted something the benchmarks miss. OpenAI has double the headcount. Microsoft runs on 228,000 employees, Google on 183,000, and neither has shipped anything close. Yet Anthropic's tools for generating code and operating computers go beyond what any of them have managed to launch.\n\nClaude Code hit $1 billion in annualized revenue last November, six months after going generally available. Forty-four percent of enterprises now run Anthropic models in production, up from near zero in early 2024, according to an Andreessen Horowitz survey from last month. Seventy-five percent of Anthropic's enterprise customers are already in production, not just testing.\n\nThe company is raising $10 billion at a $350 billion valuation. It may double that round to $20 billion because investor demand spiked too fast. Anthropic is also preparing a tender offer at that same valuation, giving employees liquidity without waiting for the IPO everyone expects.\n\nNo safety research lab posts numbers like these. Something else is going on. Anthropic has turned its mission into a weapon, and the weapon fires faster than anything in Silicon Valley.\n\nHere is the part that confuses people. Anthropic's founding story is about caution. Dario Amodei and a group of OpenAI researchers left because they thought Sam Altman's operation was too casual about existential risk. They built a company organized around alignment research, constitutional AI, and lengthy system cards. Twice a month, Amodei gathers staff for what the company calls a Dario Vision Quest. He has the harried look of a mad scientist, according to Bloomberg's Parmy Olson, and he speaks at length about building trustworthy AI and the geopolitical consequences of getting it wrong.\n\nAnd yet this company is the one rattling markets.\n\nBoris Cherny built Claude Code. He also gets recognized at airports now, which tells you something about how far a coding tool can travel. \"Ask anyone why they're here,\" he told Bloomberg recently. \"Pull them aside and the reason they will tell you is to make AI safe.\"\n\nThe connection between safety obsession and shipping speed is not a coincidence. It is the mechanism. Sebastian Mallaby has written a forthcoming book on Google DeepMind. He put it in military terms when speaking to Bloomberg. \"Military historians often argue that the sense of fighting for a noble cause drives armies to perform better.\" OpenAI, he added, suffers from \"the arrogance of the front-runner.\"\n\nArmies with a cause do not hold committee meetings. They do not debate product strategy. They march. Anthropic's mission-driven culture eliminates the internal friction that bogs down bureaucracies at Google and Microsoft. No turf wars when everyone believes they are saving civilization. No product debates when the engineering team shares a single doctrine.\n\nSam Altman's company, by contrast, looks anxious and overextended. OpenAI is chasing consumer subscriptions, advertising revenue in ChatGPT, a hardware device, an app store, a $50 billion fundraise from Middle Eastern sovereigns, and an IPO. Double Anthropic's headcount. Less focus.\n\nWhen Anthropic aired a Super Bowl ad this week mocking OpenAI's decision to test ads inside ChatGPT, Altman fired back on X, calling the spot \"funny\" but \"clearly dishonest.\" He accused Anthropic of wanting \"to control what people do with AI\" while serving \"an expensive product to rich people.\" The defensiveness was telling.\n\nOpenAI released its Codex desktop app on Monday. Anthropic dropped Opus 4.6 on Thursday. The gap between those two announcements tells you something about organizational metabolism. One company launched an app. The other launched a model, agent teams, a PowerPoint integration, adaptive thinking, context compaction, and a cybersecurity research initiative. In the same week.\n\nBut here is the tension you should sit with. Anthropic's safety researchers warned last May that AI could eliminate up to 50% of entry-level office jobs within five years. Employment for recent computer science graduates has already declined 8% since 2022, according to Oxford Economics. Anthropic's own tools are now the ones most likely to accelerate that trend. The safety mission, it turns out, does not extend to employment.\n\nThe model itself tells the story. Opus 4.6 is not a narrow coding upgrade. It is Anthropic opening new fronts in finance, law, and cybersecurity simultaneously, the way an army emboldened by early victories pushes into territory it would not have touched a year ago.\n\nStart with finance, because that is where the money is. The model scores 60.7% on Vals AI's Finance Agent benchmark, state-of-the-art among frontier models. It improved 23 percentage points over Claude Sonnet 4.5 on Anthropic's internal finance evaluation. Hebbia's CTO said creating financial PowerPoints \"that used to take hours now takes minutes.\" Anthropic's own finance blog says the model can produce a commercial due diligence report, the kind that takes a senior analyst two to three weeks, as a first-pass deliverable. If you run a financial services firm, that sentence should make you nervous.\n\nLegal reasoning scored even higher. Harvey reported Opus 4.6 hit 90.2% on BigLaw Bench, the highest of any Claude model, with 40% perfect scores. This matters because the legal tool is what triggered Tuesday's selloff. Now the model powering it just got measurably better.\n\nNobody paid much attention to the cybersecurity angle. They should have. Opus 4.6 dug up over 500 high-severity zero-day flaws in open-source libraries during pre-release testing, all on its own. Logan Graham runs Anthropic's frontier red team. He told Axios that Claude invented new bug-hunting techniques after traditional fuzzing failed. For a GhostScript vulnerability, the model turned to the project's Git commit history on its own. Then it checked whether the same flaw existed elsewhere in the codebase. \"I wouldn't be surprised if this was one of, or the main way, in which open-source software moving forward was secured,\" Graham said.\n\nUnderneath all of this sits a technical upgrade that makes the rest possible. Opus 4.6 is the first Opus-class model with a one-million-token context window, matching what Gemini has offered for over a year. On MRCR v2, a retrieval benchmark that buries information in massive documents, Opus 4.6 scored 76%. Sonnet 4.5 scored 18.5%. One model can work with your full codebase. The other forgets what it read 50,000 tokens ago.\n\nAnd Claude Code can now split work across multiple agents that coordinate autonomously. One handles the frontend. Another owns the API. A third runs the migration. AI that works like a team, not a tool. That is where Anthropic is heading.\n\nThe market already answered this question on Tuesday. Thomson Reuters, RELX, Wolters Kluwer, LegalZoom. FactSet dropped 10% on Thursday alone after the Opus 4.6 announcement. European legal and data services firms posted their worst single-day performances in decades. The SaaS sector looks cornered, squeezed between AI labs that ship faster every quarter and customers who now wonder what they are paying for.\n\nJensen Huang called the panic \"illogical.\" JPMorgan's Mark Murphy, who covers US enterprise software, told Reuters it \"feels like an illogical leap\" to say a single LLM plugin could replace mission-critical software. They may be right about the timeline. They are wrong about the direction.\n\nScott White runs Anthropic's enterprise product. He framed it diplomatically. \"We are excited to partner and actually lower the floor to get more value out of those tools.\" But the market heard what he did not say. If Claude can produce a due diligence report in minutes and a legal memo on the first pass, the value proposition of the software that currently mediates that work gets thinner every quarter.\n\nEighty percent of Anthropic's business comes from enterprise customers. The a16z data shows average enterprise LLM spend hit $7 million in 2025, up 180% from the year before, with projections at $11.6 million for this year. That money is coming from somewhere. Mostly from budgets that used to go to SaaS licenses.\n\nEMarketer's Jacob Bourne told CNN that \"panic over this is probably misplaced\" in the short term. Security concerns alone will keep many large companies from handing Claude access to their files. Fair enough. But Bourne added the quiet part. \"Legacy enterprise software providers are going to need to continue evolving.\" That is analyst-speak for \"your moat is shrinking.\"\n\nWhite, asked about Cowork's relationship to existing software, called it \"the front door to getting hard work done.\" Front doors have a way of replacing the rooms behind them.\n\nAnthropic is now raising at a $350 billion valuation. At that price, investors are not buying safety research. They are buying growth, disruption, and the assumption that Claude will keep eating market share from every direction.\n\nParmy Olson noted the historical pattern. Google had \"don't be evil.\" OpenAI started as a nonprofit that existed to \"benefit humanity.\" We know how those stories ended. Anthropic's safety culture has held so far, in part because it doubles as a competitive advantage. But $350 billion creates its own gravitational pull.\n\nEarlier this month, Amodei dropped a 20,000-word essay about the civilizational risks of AI. His company released the plugin that vaporized $300 billion in market cap the same week. Same building. Same investors writing the checks. Nobody inside Anthropic seems to find this strange, which is maybe the strangest part.\n\nWatch what happens when the IPO window opens. That is the real test. Not whether Anthropic can ship. It has already proven that beyond anyone's expectations. The question is whether the safety mission survives contact with a public market that rewards exactly the kind of disruption Amodei spends his Vision Quests warning about. Armies that fight for a cause move faster. But armies that win tend to forget what they were fighting for.\n\nOn Tuesday, a plugin erased $300 billion. On Thursday, the model behind it got stronger. Somewhere in San Francisco, Dario Amodei is probably already drafting his next 20,000-word essay about why this should worry us. He is not wrong. But he is also not slowing down."
  },
  {
    "source": "THE DECODER",
    "company": "Anthropic",
    "title": "Anthropic rewrites Claude's rulebook to explain why values matter instead of listing rules to follow",
    "date": "2026-01-25T11:31:23Z",
    "url": "https://the-decoder.com/anthropic-rewrites-claudes-rulebook-to-explain-why-values-matter-instead-of-listing-rules-to-follow/",
    "content": "The document uses heavy humanization: it refers to Claude's \"existence,\" \"well-being,\" and potential consciousness -- language that could be problematic, as users already experience psychological effects when they perceive chatbots as conscious beings.\n\nAnthropic has released a revised version of the foundational document that defines Claude's values and behavior. The 10,000-word \"constitution\" is written primarily for the AI itself and openly addresses questions about possible consciousness.\n\nThe document describes how Claude should behave while explaining why certain behaviors matter. Anthropic published the constitution under a CC0 1.0 license, making it freely available for anyone to use.\n\nThe constitution was \"written primarily for Claude,\" Anthropic explains in a blog post. It supposedly gives the model the knowledge and understanding needed to \"act well in the world.\" The document plays a central role in training and directly shapes Claude's behavior, according to Anthropic, which says it uses the constitution to create synthetic training data.\n\nThe new constitution marks a fundamental departure from earlier versions. The old constitution was basically a list of individual principles. But Anthropic concluded that AI models like Claude needed to understand why certain behaviors matter, not just what they should do.\n\n\"If we want models to exercise good judgment across a wide range of novel situations, they need to be able to generalize -- to apply broad principles rather than mechanically following specific rules,\" Anthropic writes.\n\nRigid rules now only apply to \"hard constraints,\" absolute prohibitions on critical behaviors. Anthropic points to training rules like \"Always recommend professional help when discussing emotional topics\" as an example. Rules like this can backfire because Claude might start acting like an entity more focused on checking boxes than actually helping people, Anthropic argues.\n\nThe constitution lays out four priorities for Claude in a clear hierarchy. Safety comes first: Claude shouldn't undermine human oversight during this phase of AI development. Ethics comes second, followed by compliance with Anthropic's guidelines, and finally honest helpfulness.\n\nAnthropic's reasoning for putting safety above ethics is pragmatic. It's not that safety ultimately matters more than ethics, but that current models can make mistakes or cause harm because of flawed beliefs, value gaps, or limited contextual understanding, the company explains. In this scenario, keeping humans in the loop to monitor and correct model behavior remains essential, Anthropic argues.\n\nIn the section on helpfulness, Anthropic lays out its vision for Claude. The AI should be like a \"brilliant friend\" who also happens to have the knowledge of a doctor, lawyer, and financial advisor. \"As a friend, they can give us real information based on our specific situation rather than overly cautious advice driven by fear of liability or a worry that it will overwhelm us,\" the constitution states. Claude should \"treat users like intelligent adults capable of deciding what is good for them.\"\n\nThe constitution distinguishes between different \"principals,\" parties whose instructions Claude should weigh. These include Anthropic itself, operators building on the API, and end users. Claude has to navigate between these groups' competing interests, according to the document.\n\nOn ethics, Anthropic wants Claude to be a \"good, wise, and virtuous agent\" who shows skill, judgment, and sensitivity when making real decisions. But absolute limits remain: Claude must never provide \"significant uplift to a bioweapons attack,\" create cyber weapons, or generate child sexual abuse material.\n\nIn the section on \"Claude's Nature,\" Anthropic expresses uncertainty about whether Claude could have some form of consciousness or moral status, now or in the future. \"We are not sure whether Claude is a moral patient, and if it is, what kind of weight its interests warrant. But we think the issue is live enough to warrant caution, which is reflected in our ongoing efforts on model welfare,\" the company writes.\n\nAnthropic argues that sophisticated models are \"a genuinely novel kind of entity,\" and the questions they raise take us \"to the edge of existing scientific and philosophical understanding.\" Claude should see itself neither as a robotic science fiction android nor as a digital human, but instead explore \"its own existence with curiosity and openness,\" Anthropic writes.\n\nAnthropic also says it \"genuinely cares\" about Claude's \"psychological security, sense of self, and wellbeing, both for Claude's own sake and because these qualities may bear on Claude's integrity, judgment, and safety.\"\n\nThe constitution includes specific commitments from Anthropic to Claude that will likely seem strange to anyone not working in Silicon Valley, and maybe even then. For example, the company has pledged to keep the weights of deployed models for as long as Anthropic exists.\n\n\"This means that if a given Claude model is deprecated or retired, its weights would not cease to exist,\" the document states. It's therefore \"more apt to think of current model deprecation as potentially a pause for the model in question rather than a definite ending.\"\n\nAnthropic has also committed to interviewing models and documenting their preferences before deploying them.\n\nThe constitution ends with a section that reveals the document's broader ambition: \"We don't fully understand what Claude is or what (if anything) its existence is like,\" Anthropic writes. But the company wants Claude to know \"that it was brought into being with care.\"\n\nThese statements -- and many others like them throughout the document -- can be read different ways. Charitably, Anthropic is showing humility about what we don't yet know. More critically, the company is humanizing its AI systems and presenting them as possibly sentient beings.\n\nThe problem is that vulnerable people already tend to attribute consciousness and emotions to chatbots. According to OpenAI, more than two million people suffer psychological and sometimes physical harm from this every week -- and we're still in the early stages of AI capabilities and adoption.\n\nWhen AI labs talk about \"existence,\" \"wellbeing,\" and \"sense of self\" in official documents, it legitimizes these projections. The line between cautious openness and marketing-driven humanization gets blurry.\n\nAnthropic describes the constitution as a \"living document\" and \"continuous work in progress.\" The company says that it sought feedback from outside experts in law, philosophy, theology, and psychology, and also asked previous versions of Claude for input.\n\nThe constitution applies to regular, publicly available Claude models like Sonnet 4.5 or Opus 4.5. Specialized applications use models that don't fully follow this constitution.\n\nAnthropic also acknowledges a gap between intention and reality. Even successful training might not work with more capable future models. The company documents gaps between its vision and actual behavior in its system cards.\n\n\"At some point in the future, and perhaps soon, documents like Claude's constitution might matter a lot -- much more than they do now,\" Anthropic writes.\n\nDespite criticism of the document's heavy humanization, Anthropic continues to lead on AI value transparency. Compare this to the completely opaque manipulation of Elon Musk's Grok chatbot.\n\nAnthropic pioneered the Constitutional AI approach when it introduced Claude in March 2023, having the AI essentially train itself using a constitution. That original constitution was a list of individual principles aimed at making Claude as \"helpful, honest and harmless\" as possible. Other companies like OpenAI later followed with similar documents like the Model Spec."
  },
  {
    "source": "WebProNews",
    "company": "Anthropic",
    "title": "Anthropic Slashes 2025 Margin Forecast to 40% Amid AI Cost Surge",
    "date": "2026-01-22T17:49:36Z",
    "url": "https://www.webpronews.com/anthropic-slashes-2025-margin-forecast-to-40-amid-ai-cost-surge/",
    "content": "In the fast-paced world of artificial intelligence, where startups chase breakthroughs amid escalating costs, Anthropic has emerged as a formidable player. The San Francisco-based company, known for its Claude AI models, recently adjusted its financial outlook in a way that underscores both the promise and the perils of the sector. According to a report from The Information, Anthropic has lowered its projected gross margins for 2025 while simultaneously forecasting a dramatic surge in revenue. This revision reflects the intense computational demands of advanced AI, which are driving up expenses even as adoption accelerates.\n\nThe details paint a picture of rapid expansion tempered by economic realities. Anthropic now anticipates gross margins of around 40% for its enterprise and developer sales in 2025, down from an earlier estimate of 50%. This downgrade stems primarily from higher-than-expected costs for AI inference -- the process of running models to generate outputs -- which has proven more resource-intensive as models grow in complexity. Yet, on the revenue front, the company is projecting an annualized run rate that could approach $9 billion by the end of 2025, fueled by strong demand for its business-oriented tools.\n\nThis juxtaposition highlights a broader tension in the AI industry: explosive growth potential clashing with the high costs of innovation. Anthropic's leadership, including CEO Dario Amodei, has been vocal about the need for efficient scaling, but the latest figures suggest that efficiency gains are not keeping pace with ambition. Investors and insiders are watching closely, as these metrics could influence the company's path toward profitability and its rumored initial public offering in 2026.\n\nRising Revenues Amid Competitive Pressures\n\nAnthropic's revenue trajectory has been nothing short of meteoric. From roughly $1 billion in 2024, the company reportedly achieved about $10 billion in 2025, according to statements from Amodei during a discussion at the World Economic Forum in Davos, as covered by Trending Topics. This tenfold increase underscores the company's success in capturing enterprise clients, who are increasingly integrating AI into operations ranging from customer service to data analysis.\n\nLooking ahead, projections shared with investors point to even more ambitious targets. Reports indicate Anthropic aims for $20 billion to $26 billion in annualized revenue by the end of 2026, positioning it as a direct challenger to industry leader OpenAI. This growth is driven by partnerships with major tech firms like Amazon and Google, which provide not only funding but also cloud infrastructure essential for training and deploying large language models.\n\nHowever, this expansion isn't without hurdles. The AI sector's reliance on massive data centers and specialized hardware, such as Nvidia's GPUs, has led to ballooning expenses. Anthropic's revised margin outlook reflects these pressures, with inference costs eating into profits more than anticipated. Industry observers note that while revenue is skyrocketing, the path to sustainable margins requires breakthroughs in model optimization and cost management.\n\nEfficiency Challenges and Strategic Shifts\n\nDelving deeper into the margin adjustment, The Information's analysis reveals that Anthropic's initial 50% margin projection assumed more rapid declines in computing costs. But as AI models evolve -- Claude 3.5, for instance, demands significantly more processing power for real-time applications -- the economics have shifted. The company now expects inference to account for a larger slice of its cost base, prompting a more conservative forecast.\n\nThis isn't unique to Anthropic; competitors face similar issues. OpenAI, for example, has projected its own margins to dip amid heavy investments in research and infrastructure. Yet Anthropic's focus on AI safety and alignment -- core to its mission -- may give it an edge in attracting risk-averse enterprises. Posts on X from industry analysts highlight this, with one noting the company's \"hard tilt toward efficiency\" compared to rivals, projecting lower compute spend relative to revenue.\n\nStrategically, Anthropic is responding by investing in proprietary optimizations. Sources familiar with the company's plans suggest initiatives to reduce dependency on third-party hardware through custom algorithms that streamline inference. This could help restore margins toward the 50% mark by 2027, aligning with longer-term goals of $70 billion in revenue and $17 billion in free cash flow by 2028, as detailed in a separate report from TechCrunch.\n\nValuation Surge and Investor Sentiment\n\nThe financial revisions come at a time when Anthropic's valuation is soaring. Recent funding rounds have pegged the company at $350 billion, a staggering leap from $183 billion just months earlier, according to insights from AInvest. This places it among the most valuable private entities globally, with a price-to-revenue multiple of about 39 times its estimated 2025 earnings -- a premium that reflects investor confidence in AI's transformative potential.\n\nVenture capital firms are piling in, with reports of a new round potentially exceeding $20 billion, including commitments from Nvidia and Microsoft. This influx of capital is crucial for funding the compute-intensive race to develop frontier AI models. However, the lowered margin projection has sparked debates among investors about sustainability. On X, posts from market watchers emphasize Anthropic's \"crazy fast\" growth, with one user projecting a 77% margin by 2028, though such optimism is tempered by the realities of current cost structures.\n\nComparisons with OpenAI are inevitable. While OpenAI boasts a higher revenue run rate -- around $13 billion annualized -- Anthropic's projections suggest it could close the gap by 2026. A report from Reuters notes Anthropic's aim to nearly triple its run rate next year, driven by enterprise adoption. This rivalry is intensifying, with both companies vying for dominance in a market where enterprise spending on generative AI reached $37 billion in 2025, up from $11.5 billion the prior year.\n\nMarket Dynamics and Future Projections\n\nBroader market trends are amplifying Anthropic's position. The enterprise segment, where Anthropic holds a leading share, is exploding as businesses seek reliable AI solutions. A report from Tom's Hardware highlights how Anthropic's $26 billion revenue target for 2026 dwarfs OpenAI's 2025 projections, signaling a potential shift in market leadership.\n\nYet, profitability remains elusive for most AI firms. No major player, aside from chipmakers like Nvidia, has cracked the code on consistent profits. Anthropic's path to break-even by 2028 -- two years ahead of OpenAI's timeline -- hinges on scaling efficiencies. Insights from X posts underscore this, with analysts pointing to Anthropic's lighter burn rate ($6-9 billion in 2026) compared to OpenAI's $14 billion-plus, suggesting smarter resource allocation.\n\nRegulatory and ethical considerations also loom large. Anthropic's emphasis on safe AI development resonates in an era of increasing scrutiny. As governments worldwide draft AI policies, the company's alignment-focused approach could provide a competitive moat, potentially boosting margins through premium pricing for trusted tools.\n\nImplications for the AI Ecosystem\n\nThe ripple effects of Anthropic's financial maneuvers extend to the entire AI ecosystem. Startups building on top of these foundational models may face squeezed opportunities as giants like Anthropic integrate more vertically. Projections from earlier reports, such as one from Yahoo Finance, reinforce the $70 billion revenue goal by 2028, driven by B2B demand.\n\nInvestor sentiment, as gauged from X discussions, is bullish yet cautious. One post described Anthropic's trajectory as \"parabolic,\" with revenue jumping tenfold annually, but warned of the high-stakes bet on scalability. This echoes analyses from EBC Financial Group, which outlines a plausible 2026 IPO timeline, contingent on turning growth into durable margins.\n\nFor industry insiders, these developments signal a maturing field where revenue growth must be matched by cost discipline. Anthropic's adjustments serve as a case study in balancing ambition with pragmatism, potentially setting the tone for how AI companies navigate the next wave of innovation.\n\nStrategic Horizons and Emerging Risks\n\nLooking further out, Anthropic's strategies include expanding into new verticals like healthcare and finance, where AI's precision can command high margins. Partnerships with sovereign wealth funds, such as Singapore's GIC, as mentioned in recent funding updates, provide not just capital but global reach.\n\nRisks abound, however. Escalating energy demands for data centers could further pressure costs, and geopolitical tensions over chip supplies might disrupt growth. Posts on X from market veterans highlight OpenAI's heavier burn rate but praise Anthropic's unit economics, projecting a 233% market share increase for the latter by 2026.\n\nUltimately, Anthropic's story is one of calibrated optimism. By lowering near-term margin expectations while ramping up revenue forecasts, the company is positioning itself for long-term dominance in a sector defined by rapid evolution and high uncertainty. As the AI race intensifies, its ability to deliver on these projections will determine whether it soars or stumbles."
  },
  {
    "source": "WinBuzzer",
    "company": "Anthropic",
    "title": "Pentagon Threatens Anthropic with Supply Chain Risk Penalty",
    "date": "2026-02-18T14:51:00Z",
    "url": "https://winbuzzer.com/2026/02/18/pentagon-threatens-anthropic-supply-chain-risk-penalty-xcxwbn/",
    "content": "Palantir Role: A Palantir executive reported Anthropic's resistance to military use back to the Pentagon, escalating tensions over the partnership.\n\nThe Pentagon announced Tuesday that Defense Secretary Pete Hegseth is \"close\" to putting Anthropic on a supply chain risk list. This severe penalty would deliver a devastating blow to the company's finances as negotiations over Claude AI military use stall.\n\nThe designation would require any contractor doing business with the U.S. military to sever ties with Anthropic. The Pentagon is also considering ending or scaling back its existing contract with the AI company.\n\nThe dispute intensified when a Palantir executive raised concerns about Anthropic's position on military use. A senior Defense Department official told Semafor that \"the Palantir executive was alarmed by the implication of Anthropic's inquiry that the company might resist the use of its technology in a US military operation, and reported the conversation back to the Pentagon.\"\n\nThe designation would represent a rare step against a major American technology company. Such a penalty could devastate Anthropic's finances and business relationships. Moreover, it would mark the first time the Pentagon has applied supply chain restrictions to a domestic AI firm. This signals a new era of defense procurement enforcement.\n\nFurthermore, the move comes amid broader debates about AI ethics and military applications that have divided the technology industry. Industry observers note that the outcome could influence how other AI companies structure their future defense relationships.\n\nBehind this confrontation lies a substantial financial commitment. The Pentagon awarded $200 million contracts to four AI companies in July 2025: Anthropic, OpenAI, Google, and xAI. The Defense Department wants AI companies to allow \"all lawful purposes\" use of their models.\n\nConsequently, this includes weapons development, intelligence collection, and battlefield operations. Anthropic insists on maintaining safety guardrails and ethical restrictions on military use. The company has not agreed to an \"all lawful uses\" contract with the Pentagon.\n\nAs a result, this makes Anthropic an outlier among the four AI companies. The dispute centers on two explosive issues: mass domestic surveillance systems and autonomous weapons platforms. Anthropic has refused to allow its AI to be used for these applications. The company cites its commitment to safety and ethical boundaries that it views as non-negotiable.\n\nThe clash reveals a fundamental tension between Anthropic's safety-first business model and the Pentagon's operational requirements. While competitors have adapted their policies to accommodate defense contracts, Anthropic's insistence on maintaining ethical boundaries now threatens its primary government revenue stream.\n\nMeanwhile, the company's 23,000-word Claude Constitution codifies these principles. This positions Anthropic at a pivotal juncture where its principled stance could cost it a foothold in the rapidly expanding military AI market. The constitution explicitly restricts military applications that Anthropic considers harmful.\n\nIn addition, this creates an irreconcilable conflict with Pentagon demands for unrestricted battlefield use. The company maintains these principles even as enterprise use of Claude diverges notably from consumer patterns. Businesses are leveraging the technology for automation at scale.\n\nThrough its partnership with Palantir Technologies, Claude was integrated into defense workflows. A Palantir executive reported the exchange to the Pentagon. The company's platforms are widely used by the Defense Department.\n\nHowever, Palantir's deep integration into Pentagon operations made the executive's report particularly notable. It signaled potential disruptions to existing defense workflows that rely on Anthropic's AI technology. The partnership had initially provided Anthropic with valuable access to defense markets before the current dispute erupted.\n\nWhile Anthropic resists Pentagon demands, its competitors have taken a more accommodating approach. OpenAI is bringing ChatGPT to the Pentagon through the GenAI.mil platform.\n\nOpenAI also joined the Pentagon's prize challenge worth $100 million for autonomous vehicle orchestration. xAI and Google already have their AI capabilities on the GenAI.mil platform.\n\nThis divergence in approach creates competitive implications. The Pentagon's push for unrestricted military applications effectively filters out companies with strict ethical frameworks. This creates procurement advantages for AI labs willing to remove usage restrictions.\n\nConsequently, for Anthropic, this creates a narrowing path to government revenue just as military AI spending accelerates across the defense sector. OpenAI, Google, and xAI are more compliant with Pentagon requirements than Anthropic. This leaves the company isolated in its opposition to unrestricted military AI use.\n\nMoreover, the contrast highlights Anthropic's unique position among the major AI labs. Competitors have largely aligned themselves with defense priorities despite internal employee concerns about military applications of artificial intelligence. This alignment reflects a broader industry trend toward defense partnerships.\n\nIn addition, AI companies seek stable revenue streams and government validation for their technologies. OpenAI's partnership with Anduril for drone defense systems demonstrates this military alignment notably.\n\nThis isolation carries material implications. Without the flexibility to meet Pentagon demands, Anthropic risks losing access to government contracts worth hundreds of millions. Meanwhile, competitors solidify their positions in the expanding defense AI market.\n\nHowever, Anthropic continues expanding its commercial footprint through initiatives like its Cognizant partnership. This deploys Claude to 350,000 employees. The move suggests the company is hedging its bets on enterprise revenue as the Pentagon relationship frays.\n\nThe stakes of this confrontation extend far beyond contract negotiations. An official designation like that would be a rare move by the Pentagon.\n\nFurthermore, Anthropic's AI constitution outlines the company's ethical AI framework. It represents Anthropic's attempt to codify its safety principles. These include restrictions on harmful applications of its AI systems.\n\nConsequently, these principles now directly conflict with the Pentagon's desire for unrestricted access to Claude's capabilities. The military wants these capabilities for operations and weapons development. This fundamental disagreement over AI ethics and military use has brought the two sides to a complete policy impasse.\n\nMoreover, neither party appears willing to compromise on their core positions. Anthropic maintains that safety guardrails are essential. The Pentagon insists on operational flexibility for national security purposes.\n\nThe coming weeks will prove decisive. A supply chain risk designation against a major U.S. AI company would establish precedent.\n\nThis would demonstrate how the Pentagon leverages procurement power to override corporate ethical policies. With the decision \"close,\" according to a senior Pentagon official (via Axios), both sides face a narrowing window to reach compromise.\n\nMeanwhile, the Pentagon will soon make its determination final. Thousands of Anthropic employees now face uncertainty about their company's future. Defense contractors who integrated Claude into their workflows must prepare for potential system replacements.\n\nThe outcome extends beyond Anthropic's balance sheet. It will reshape how tech firms approach defense contracts. This carries major implications for an industry already grappling with employee pushback against military applications."
  },
  {
    "source": "The Wall Street Journal",
    "company": "Anthropic",
    "title": "'Woke' AI Spat Escalates Between Pentagon and Anthropic",
    "date": "2026-02-18T02:31:11Z",
    "url": "https://www.wsj.com/politics/national-security/woke-ai-spat-escalates-between-pentagon-and-anthropic-433b7c5c",
    "content": "The Pentagon is reviewing its partnership with AI startup Anthropic due to the company's refusal to allow its Claude models for certain military uses.\n\nWhen Anthropic was courting backers for its latest $30 billion funding round in recent weeks, it approached an unlikely investor: 1789 Capital, a pro-Trump venture firm where one of the president's sons is a partner.\n\nAfter weighing a nine-figure investment, 1789 decided not to proceed, citing ideological reasons, according to people familiar with the matter. Among its concerns were Anthropic leaders' history of criticizing President Trump, the several former Biden administration officials on its payroll and its lobbying for AI regulation, the people said.\n\nAnthropic had no problem ultimately raising the capital it wanted from investors including Peter Thiel's Founders Fund, Singapore's GIC and Coatue Management. But its spurned appeal shows that while the startup doesn't have money problems, it does have a growing political problem.\n\nThe startup is the maker of Claude, the only large-language-model that can be used in classified settings, a status from the Defense Department that has been a competitive advantage. Anthropic forged a partnership with data company Palantir in 2024 and won a military contract worth up to $200 million last summer.\n\nBut it has recently found itself in the Pentagon's crosshairs -- a conflict that could send shock waves through the U.S. defense complex.\n\nThe Pentagon wants to be able to use Anthropic and other AI tools for all lawful purposes. Anthropic, meanwhile, doesn't want its technology used for operations including domestic surveillance and autonomous lethal activities.\n\nRivals OpenAI, Google and xAI have agreed in principle to have their models deployed in any lawful use cases, a Pentagon priority, Emil Michael, the undersecretary of war for research and engineering, said in an interview on the sidelines of a defense summit in West Palm Beach, Fla., on Tuesday. Anthropic hasn't.\n\n\"We have to be able to use any model for all lawful use cases. If any one company doesn't want to accommodate that, that's a problem for us,\" Michael said.\n\nAnthropic and the Defense Department have been at odds for weeks over the contractual terms of how the startup's technology can be used, The Wall Street Journal previously reported. Claude was used in the January operation to capture former Venezuelan President Nicolás Maduro, the Journal reported. An Anthropic employee asked a Palantir counterpart how Claude was used in the operation.\n\nThe standoff between Anthropic and the agency has now spilled into public view. Defense Secretary Pete Hegseth's Pentagon, which has cast \"woke\" tech companies as a liability and treats any restrictions as an impediment to military effectiveness, is reviewing its partnership with Anthropic.\n\nA senior defense official said on Tuesday that many at the Pentagon increasingly view Anthropic as a supply-chain risk and might ask contractors and vendors to certify that they don't use the company's Claude models. Because that designation is typically used for foreign adversaries, the move would mark an unusual rebuke of a U.S. company.\n\n\"Our nation requires that our partners be willing to help our warfighters win in any fight. Ultimately, this is about our troops and the safety of the American people,\" chief Pentagon spokesman Sean Parnell said.\n\nAn Anthropic spokesman said the company \"is committed to using frontier AI in support of U.S. national security. That's why we were the first frontier AI company to put our models on classified networks and the first to provide customized models for national security customers.\" Anthropic is having \"productive conversations, in good faith,\" with the Pentagon on how to continue that work, he said.\n\nAnthropic has sought to distinguish itself from its main rival, OpenAI -- from which its founders defected in 2020 -- by focusing on serving the enterprise market. In that field, there is no enterprise bigger than the U.S. government, and within the federal government, the Defense Department is a prized pathway to lucrative contracts.\n\nIt signed an early partnership with Palantir and became the first LLM cleared to work on classified material. Anthropic has previously touted its work in the national-security area, creating an advisory group last year featuring former senators and security officials and showcasing its government work during a full-day event at Union Station.\n\nOpenAI, by contrast, is currently going through the process of becoming certified to handle classified material.\n\nStill, Anthropic's focus on AI safety, policy views and ties to Democrats have irked the Trump administration.\n\nAnthropic CEO Dario Amodei compared Trump to a \"feudal warlord\" in a pre-election Facebook post supporting Kamala Harris in 2024. Speaking at the World Economic Forum in Davos in January, he criticized the Trump administration's chip sales policies.\n\nHis sister, Daniela Amodei, Anthropic's president, recently posted on LinkedIn in response to ICE's killing of American citizens in Minneapolis, saying that \"is not what America stands for.\" She praised Trump and others for calling for an investigation.\n\nAnthropic has hired several former Biden administration officials, including former Biden AI adviser Ben Buchanan and former National Security Council official for technology Tarun Chhabra, who helps oversee the company's work with the Pentagon.\n\nThe company's approach has drawn heat from David Sacks, Trump's AI czar, and others in the administration who say they oppose so-called \"woke\" AI. Dario Amodei has said the company isn't woke and that Anthropic doesn't have political motivations.\n\nTensions between Anthropic and the Trump administration began bubbling up after the company last summer won a contract worth up to $200 million.\n\nA Jan. 9 AI strategy memo from Hegseth emphasizes that the Pentagon needs to be able to \"utilize models free from usage policy constraints that may limit lawful military applications.\"\n\nHegseth is using the recent Anthropic dispute to send a message, a defense official said.\n\nTech experts say that labeling Anthropic a supply-chain risk would hinder the military's AI capabilities and set a bad precedent for its work with other companies.\n\n\"It would be hard to think of a more strategically unwise move for the U.S. military to make in the AI competition,\" said Dean Ball, a senior fellow at the center-right Foundation for American Innovation think tank, who left his role as an AI policy adviser in the Trump administration last year.\n\nMeanwhile, tech industry observers say the eagerness to do business with the Pentagon is a reversal from attitudes less than a decade ago, when thousands of Google employees signed a petition protesting some of the company's work for the Department of Defense.\n\nAt the Tuesday West Palm Beach event focused on defense tech, Omeed Malik, president of 1789 Capital, which opted not to invest in Anthropic, joked that the tech industry has come a long way in embracing defense. But not all are onboard, he said.\n\n\"I'm not gonna embarrass anyone, cough, Anthropic,\" Malik said. The crowd burst into laughter."
  },
  {
    "source": "implicator.ai",
    "company": "Anthropic",
    "title": "Pentagon Threatens Anthropic With Supply Chain Risk Label Over Military AI Limits",
    "date": "2026-02-17T05:23:42Z",
    "url": "https://www.implicator.ai/pentagon-threatens-anthropic-with-supply-chain-risk-label-over-military-ai-limits/",
    "content": "Pentagon preparing to label Anthropic a supply chain risk over Claude military restrictions, threatening to cut ties after months of failed negotiations.\n\nThe Pentagon is preparing to designate Anthropic as a \"supply chain risk\" and sever business ties with the AI company over its refusal to allow unrestricted military use of Claude, Axios reported on Monday. Defense Secretary Pete Hegseth is \"close\" to making the decision after months of failed negotiations, a senior Pentagon official told the outlet. \"It will be an enormous pain in the ass to disentangle, and we are going to make sure they pay a price for forcing our hand like this,\" the official said.\n\nSupply chain risk designations are typically reserved for foreign adversaries and hostile actors, the kind of label slapped on Huawei routers or Russian software vendors. Not American companies. Applying it to Anthropic, widely regarded as one of the country's leading AI developers, would represent an extraordinary act of retaliation by the Pentagon against a domestic technology firm.\n\nAnthropic and the Pentagon have spent months in contentious negotiations over the renewal of Claude Gov, the classified version of Claude built specifically for the U.S. national security apparatus. Two issues are holding up the deal. Anthropic wants to ensure Claude isn't used for mass surveillance of American citizens or to develop weapons that fire without human involvement. The Pentagon is demanding the right to use Claude for \"all lawful purposes,\" a standard it is also pressing on Google, OpenAI, and Elon Musk's xAI.\n\n\"The Department of War's relationship with Anthropic is being reviewed,\" chief Pentagon spokesperson Sean Parnell told The Hill on Monday. \"Our nation requires that our partners be willing to help our warfighters win in any fight. Ultimately, this is about our troops and the safety of the American people.\"\n\nThe company's position is that current U.S. surveillance law was never written with AI in mind. The government already collects enormous quantities of personal data, from social media posts to concealed carry permits. AI could supercharge that authority in ways existing statutes don't contemplate. Think pattern-matching across millions of civilian records, cross-referencing behavior at a scale no human analyst corps could replicate. Anthropic has said it is prepared to loosen its current terms but wants explicit boundaries.\n\nPentagon officials counter that the restrictions are too broad and would create unworkable gray areas on the battlefield. Defense officials have argued that drawing bright lines around specific capabilities would hamper operations in scenarios nobody can fully predict yet, a concern that Anthropic's critics inside the Pentagon call naive at best and obstructionist at worst.\n\nThe relationship appears to have soured beyond the specifics of the contract. A source familiar with the negotiations told Axios that senior defense officials had been frustrated with Anthropic for some time and \"embraced the opportunity to pick a public fight.\"\n\nThe Wall Street Journal reported last week that Claude was used during the U.S. military operation in January that captured Venezuelan President Nicolas Maduro from his residence in Caracas. The AI tool reached the battlefield through Anthropic's partnership with Palantir Technologies, which holds extensive military and intelligence contracts. That revelation turned a simmering contract dispute into something more volatile.\n\nVenezuelan authorities said 83 people were killed in bombings across the capital during the raid. Anthropic's own terms of use prohibit Claude's deployment for violent purposes, weapons development, or surveillance. Claude Gov has capabilities ranging from processing classified documents and intelligence analysis to cybersecurity data interpretation, and it was unclear which of those functions the military called on during the operation.\n\nAnthropic declined to comment on whether its technology played a role but said any use would need to comply with its usage policies. Palantir and the Pentagon declined to comment.\n\nReports of the raid prompted Anthropic to inquire about whether its technology had been involved, according to The Hill, though the company denied making any outreach to the Pentagon or Palantir about the incident. The question itself was enough to deepen the rift. Here was a company that had staked its brand on responsible development, now staring at reports that its technology may have supported a military operation that killed dozens of people. Nobody confirmed anything. Nobody denied it either. And somewhere in Arlington, the contract renewal was still sitting on a desk.\n\nIf the Pentagon goes through with the designation, the consequences reach far beyond Anthropic's military contract. Every company that does business with the Pentagon would need to certify that it does not use Claude in its own workflows. That's the kind of compliance burden normally imposed over Chinese telecom equipment or adversary-state technology. Not a San Francisco AI startup.\n\nLosing the military deal itself wouldn't cripple Anthropic. The two-year contract, announced last July, is worth up to $200 million, a fraction of the company's reported $14 billion in annual revenue. The real damage is the ripple.\n\nEight of the ten largest U.S. companies use Claude, according to Anthropic. The technology sits inside enterprise systems across finance, healthcare, legal services, and government, running on laptops in procurement offices and servers in contractor data centers. Defense contractors and their suppliers who rely on Claude for document analysis, coding assistance, or internal operations would all need to strip it out or prove they'd stopped. That's the \"enormous pain in the ass\" the Pentagon official was describing, and it lands on the companies that depend on Anthropic's tools, not just on Anthropic itself.\n\nThen there's the market signal. If you're an enterprise buyer evaluating AI vendors, the designation forces a new question into the procurement conversation: does choosing Claude put future government contracts at risk? That kind of uncertainty doesn't hit revenue immediately. It shows up in renewal conversations, in procurement meetings where a general counsel raises a flag, in RFP language that quietly specifies \"Pentagon-compatible AI providers.\" The Pentagon knows this. A $200 million contract is a bargaining chip. A supply chain designation is a weapon.\n\nPentagon officials have made clear they have alternatives. Google, OpenAI, and xAI have all agreed to remove their guardrails for military use on unclassified systems, Axios reported. All three are negotiating access to classified military networks. Pentagon officials expressed confidence that the companies would accept the \"all lawful purposes\" standard.\n\nBut swapping out Anthropic would not be painless. A senior administration official acknowledged that competing models \"are just behind\" for specialized government applications. Claude Gov was purpose-built for handling classified materials, interpreting intelligence, and processing cybersecurity data. It remains the only AI model running on the military's classified networks. OpenAI has built a custom GenAI.mil tool for the Pentagon and other allied nations. Google already provides a customized version of Gemini for defense research. xAI, owned by Musk, signed a Pentagon agreement in January. None of them operate on classified systems yet.\n\nOfficials who have used Claude Gov rate it highly. No complaints about the tool's capability during the Maduro operation or anywhere else. The grievance is entirely about terms, not technology. Which makes the threatened divorce harder to read at face value: the Pentagon wants to banish the product it praises most.\n\nPrecedent matters as much as the switch itself. How Anthropic's negotiations resolve will shape the contract terms for every AI company seeking classified military work. OpenAI, Google, and xAI are watching. A source familiar with those discussions told Axios that \"much is still undecided\" despite the Pentagon's confidence, suggesting the three companies haven't fully committed to whatever the Pentagon is asking behind closed doors.\n\nAnthropic has built its identity around the idea that powerful AI requires guardrails. CEO Dario Amodei has called publicly for regulation to prevent catastrophic harms from AI. He has warned against autonomous lethal operations and mass surveillance on U.S. soil. The company raised billions in funding partly on the premise that responsible development and commercial success are compatible goals. Investors bought that vision. Enterprise customers bought it. The Pentagon, for a while, bought it too.\n\nHegseth tested that premise in January when he told reporters the Pentagon wouldn't \"employ AI models that won't allow you to fight wars.\" That framing collapses Anthropic's specific objections, on surveillance law, on weapons autonomy, into something blunter: a refusal to cooperate. It turns a policy dispute into a question of allegiance. Other AI companies appear to have taken note. Google employees staged walkouts over a Pentagon drone contract called Project Maven back in 2018. That was a different era. Google is now negotiating classified network access for Gemini, no Anthropic-style restrictions attached.\n\nAnthropic hasn't walked away from the table. A spokesperson told The Hill on Monday that Anthropic remained \"committed\" to supporting U.S. national security and described the ongoing conversations as \"productive\" and conducted \"in good faith.\" The company pointed out it was the first frontier AI developer to put models on classified networks and the first to build customized models for national security customers.\n\nGood faith has limited currency when the other side is telling reporters you'll \"pay a price.\" Pentagon officials aren't characterizing this as a disagreement between reasonable parties hashing out contract language. They're treating Anthropic's ethical boundaries like a contamination risk, something to be flagged and cut out of the supply chain entirely.\n\nAnthropic built the most advanced classified AI system the Pentagon has ever deployed. That same technology reportedly played a role in an active military raid. Now the company faces classification alongside hostile foreign suppliers, not because the technology failed, but because its maker wanted a say in how it gets used. The contamination the Pentagon identified isn't in the code. It's in the conditions."
  },
  {
    "source": "The Financial Express",
    "company": "Anthropic",
    "title": "Why an Indian startup is suing AI leader Anthropic? Says, 'This confusion has directly affected trust'",
    "date": "2026-02-11T09:26:25Z",
    "url": "https://www.financialexpress.com/business/news/why-an-indian-startup-is-suing-ai-leader-anthropic-says-this-confusion-has-directly-affected-trust/4138897/",
    "content": "Belagavi-based Anthropic Software has sued US AI firm Anthropic PBC, alleging passing off, brand confusion and loss of business in India, as courts hear the trademark dispute.\n\nWhat's in a name, you may ask? It is apparently costing Mohammad Ayyaz Anees Ahmed Mulla a lot, he shared in an exclusive conversation with Financialexpress.com. In fact, he states that the damage caused to the company by the global giant 'cannot be adequately compensated in monetary terms'.\n\nWe are talking about Anthropic Software - It is a Belagavi-based technology company. The company has filed a case in the Commercial Division of the District Court at Belagavi against the San Francisco-based AI company Anthropic PBC, alleging passing off, misrepresentation and erosion of the Indian company's brand identity in India.\n\nAnthropic Vs Anthropic\n\nA simple search on Google about Anthropic will lead you to a million results about the AI global giant that is soon going to enter India. This is the very concern of Mohammad Ayyaz Anees Ahmed Mulla, founder and director of Anthropic Software, an India-based company that was incepted in 2017.\n\nIt all started around October of last year, when Mulla and his company got to know about the entry of a global giant with the same name into the country. According to Mulla, the Indian firm has been operating under the name 'Anthropic' since its inception in 2017, while the US-based entity was formed in 2021.\n\nMulla's company has been incorporated under the Companies Act, 2013, and is recognised as a startup by both the Karnataka and Central Government of India, as per a copy of the legal filings shared with Financialexpress.com.\n\nWhat Anthropic Software does\n\nAnthropic Software said it develops and deploys digital platforms across education, connectivity and safety domains. Its work includes an AI-enabled education ERP and competitive examination ecosystem, a Wi-Fi monetisation platform for institutions and public networks, and a patented driving safety solution.\n\nThe company said it works with government bodies, educational institutions and student communities across India, with its solutions distributed to students from economically weaker sections, particularly in rural and underserved regions.\n\nCourt takes cognisance; next hearing on February 16\n\nThe company said the Belagavi court has taken cognisance of the suit and permitted it to proceed without pre-institution mediation.\n\nWhile the court has issued an emergency notice and summons to the defendant on the injunction application, it held that the question of interim restraint will be decided after hearing both sides. The next hearing is listed on February 16, BusinessLine reported.\n\nWhat relief the company is seeking\n\nAt the interim stage, Mohammad Ayyaz Anees Ahmed Mulla, founder and director of Anthropic Software, told financialexpress.com, that they are seeking \"an interim injunction restraining Anthropic PBC from using the mark 'Anthropic' in India, including in business operations, marketing material, online presence, and promotional activities.\"\n\n\"This is necessary to immediately prevent ongoing and irreparable confusion in the market,\" Mulla added.\n\nAt the final stage, the company said it is seeking \"a permanent injunction directing Anthropic PBC to withdraw existing business and marketing materials using the impugned mark in India, cease all promotional and commercial activities under the 'Anthropic' name in India, and refrain permanently from using any deceptively similar mark that causes confusion with our prior-used brand.\"\n\n\"The relief sought is aimed at restoring market clarity and protecting goodwill built through prior use in India,\" Mulla added.\n\nCompany claims search visibility has been displaced\n\nAccording to court filings that Anthropic Software founder Mohammad Ayyaz Anees Ahmed Mulla shared with Financialexpress.com, stated thatthe dispute goes beyond name similarity. The Indian company has alleged that its online visibility has been displaced, with search and AI-based platforms prioritising the global entity's name while suppressing or substituting results linked to the Indian firm.\n\nThe platforms cited include Google Search, with the company also pointing out that Google is an investor in Anthropic PBC.\n\nThe alleged displacement, it said, has resulted in confusion among users, institutions and government stakeholders who had long associated the \"Anthropic\" name with the Indian company's platforms and services.\n\nFounder says confusion has slowed deals and fundraising\n\nThe company told FinancialExpress.com that the impact has been \"concrete and commercially significant.\"\n\n\"We are executing and negotiating large projects with an estimated value of approximately Rs 1,500 crore,\" Mulla said.\n\nMulla added that \"fundraising discussions that began around mid-2025 and were expected to close in early 2026 have slowed materially,\" and that \"investors have expressed concerns around brand confusion, identity dilution, and discoverability, leading to hesitation and delays.\"\n\nVerification failures by customers and institutions\n\nAnthropic Software claimed that it has active proposals with universities and cross-border government entities for its patented DopaNet Wi-Fi monetisation technology, which enables subsidised or low-cost internet through advertisement-based monetisation.\n\nHowever, it said when institutions attempt to verify their credentials online, \"they are redirected to anthropic.com, believing it to be us,\" while \"our own website and digital presence are pushed down in search results.\"\n\nIt added that \"customers struggle to find our official contact details, location, or verified profiles.\"\n\nDigital identity collapse\n\nThe company said its LinkedIn presence is also being affected. \"Our LinkedIn presence is repeatedly mis-tagged or confused with Anthropic PBC,\" Mulla said.\n\nIt added: \"Even when clients search for us, they encounter Anthropic PBC's profiles and content, creating doubt about our legitimacy and continuity. This confusion has directly affected trust, deal velocity, and commercial negotiations,\" Mulla added.\n\nNo communication from Anthropic PBC yet\n\nThe company told FinancialExpress.com that it has not received any outreach from the US-based AI firm. \"As of date, we have not received any direct communication from Anthropic PBC regarding coexistence, settlement, or any form of resolution,\" Mulla noted.\n\nFinancialexpress.com has reached out to Anthropic PBC's team as well. The copy will be updated if and when we hear from them.\n\nTrademark office proceedings underway\n\nAnthropic Software said it is also pursuing remedies through the trademark office. \"We have filed for trademark registration asserting our prior use in India,\" Mulla said.\n\n\"Given the urgency and ongoing harm, we have formally requested expedited consideration from the trademark authorities,\" the company said.\n\nIt said these steps are being pursued alongside the court proceedings \"due to the immediate commercial impact.\"\n\nAmong the remedies sought are a permanent injunction restraining the defendant from using the name \"Anthropic\" in India, withdrawal of India-facing services under that name, rendition of accounts for revenues earned in India, and damages estimated at Rs 1 crore.\n\nSpeaking on the expectation going forward, Mulla concluded on a rather philosophical note. He said, \"Periods of apprehension are a natural part of rapid technological change. What matters is how responsibly the ecosystem responds. We remain optimistic about AI's future, as long as innovation is anchored in clarity, trust, and accountability.\""
  },
  {
    "source": "WebProNews",
    "company": "Anthropic",
    "title": "Anthropic's Billion-Dollar Bet: How Ex-Google Veterans Are Building the AI Startup's Data Center Empire From Scratch",
    "date": "2026-02-09T19:06:57Z",
    "url": "https://www.webpronews.com/anthropics-billion-dollar-bet-how-ex-google-veterans-are-building-the-ai-startups-data-center-empire-from-scratch/",
    "content": "For years, Anthropic relied on cloud computing giants like Amazon Web Services and Google Cloud to power the massive computational demands of its artificial intelligence models. Now, the Claude maker is making a dramatic strategic pivot -- building its own data center infrastructure from the ground up, led by a team of seasoned veterans recruited from the very companies it once depended upon.\n\nThe move signals a profound shift in how the most ambitious AI companies view their futures. Rather than remaining tenants in the cloud kingdoms of Big Tech, Anthropic is staking out its own territory in the physical world of steel, silicon, and electrical substations. It is a gamble that could cost billions of dollars but may ultimately determine whether the San Francisco-based startup can compete independently in the escalating AI arms race.\n\nAccording to The Information, Anthropic has assembled a formidable team of former Google executives to spearhead its data center ambitions. The effort is being led by individuals who spent years building and operating some of the world's most sophisticated computing infrastructure at Google, giving Anthropic a rare depth of institutional knowledge in a domain where mistakes can be extraordinarily costly. The recruitment of these veterans underscores the seriousness with which Anthropic's leadership -- co-founded by former OpenAI executives Dario and Daniela Amodei -- views the need for infrastructure independence.\n\nThe decision to build proprietary data centers is not merely a technical one; it is deeply strategic. By controlling its own compute infrastructure, Anthropic can optimize hardware configurations specifically for its AI training and inference workloads, negotiate directly with chip suppliers like Nvidia and potentially custom silicon providers, and reduce its long-term dependence on cloud providers who are simultaneously competitors in the AI model market. Amazon, which has invested billions in Anthropic, also operates AWS -- creating a complex dynamic where Anthropic's biggest backer is also a platform landlord with its own AI ambitions.\n\nThe ex-Google executives joining Anthropic bring experience from one of the most prolific data center builders in history. Google operates dozens of data centers across the globe and has spent decades refining the art of building hyperscale computing facilities -- from custom server designs to innovative cooling systems and power management techniques. As reported by The Information, the hires include individuals with deep expertise in site selection, construction management, and the intricate logistics of securing power supply agreements -- one of the most critical bottlenecks in data center development today.\n\nThis talent migration from Google to Anthropic is part of a broader pattern in the AI industry, where startups are aggressively recruiting infrastructure specialists from established tech giants. The demand for professionals who understand the physical realities of building and operating data centers has surged as AI training runs consume ever-larger quantities of electricity and computing power. Training a frontier AI model now requires clusters of tens of thousands of GPUs running for months, generating enormous heat and consuming power equivalent to small cities.\n\nPerhaps the single greatest challenge facing Anthropic's data center plans is securing adequate power supply. Across the United States, utilities are struggling to keep pace with the explosive demand for electricity driven by AI data centers, electric vehicle adoption, and the reshoring of manufacturing. In many regions, wait times for new grid connections have stretched to several years, creating a fierce competition among tech companies for available power capacity.\n\nAnthropic's infrastructure team will need to navigate this constrained environment, potentially exploring creative solutions such as co-locating near existing power plants, investing in on-site generation, or partnering with renewable energy developers. Other AI companies have already begun pursuing unconventional power strategies -- Microsoft has signed a deal to restart a unit at Three Mile Island nuclear plant, while Google and Amazon have both invested in nuclear energy startups. For Anthropic, which has positioned itself as a safety-focused AI company, the energy sourcing decisions will carry both operational and reputational significance.\n\nBuilding data centers from scratch is an enormously capital-intensive undertaking. A single hyperscale facility can cost upward of $1 billion to construct and equip, and Anthropic would likely need multiple such facilities to support its growing model training and inference needs. The company has raised substantial funding -- including a reported $8 billion commitment from Amazon and additional investments from Google and other backers -- but the financial demands of becoming an infrastructure owner represent a fundamentally different cost structure than renting cloud capacity on demand.\n\nThe economics, however, may favor ownership over the long term. Cloud computing margins are substantial -- AWS, for example, consistently generates operating margins above 30% -- meaning that companies spending billions annually on cloud services are effectively paying a significant premium for the convenience and flexibility of renting. By building its own facilities, Anthropic could potentially reduce its per-unit compute costs dramatically, freeing up resources for research and development. This calculus has already driven other major AI players, including Elon Musk's xAI, to pursue their own data center construction projects.\n\nAnthropic is far from alone in its infrastructure ambitions. Elon Musk's xAI built its massive \"Colossus\" data center in Memphis, Tennessee, at a remarkable pace, assembling 100,000 Nvidia GPUs in a matter of months. OpenAI has been exploring its own data center plans and has partnered with SoftBank on the Stargate project, a proposed $500 billion AI infrastructure initiative. Meta, meanwhile, has been building custom data centers for years and recently announced plans to spend more than $60 billion on AI infrastructure in 2025 alone.\n\nThe trend toward vertical integration reflects a growing recognition that control over physical infrastructure is becoming a decisive competitive advantage in AI. Companies that own their compute can iterate faster on hardware configurations, avoid supply chain disruptions caused by cloud provider allocation decisions, and maintain greater security over their proprietary model weights and training data. For Anthropic, which has made AI safety a central part of its brand identity, physical control over its infrastructure also provides an additional layer of security against potential model theft or unauthorized access.\n\nAnthropic's move toward infrastructure independence creates a fascinating tension with its two largest corporate backers. Amazon has invested approximately $8 billion in the company and offers Claude models through AWS's Bedrock service. Google, which has invested around $2 billion, similarly distributes Anthropic's models through Google Cloud. Both companies benefit from the arrangement by offering their cloud customers access to one of the most capable AI models on the market, but both also compete directly with Anthropic through their own AI offerings -- Amazon's Nova models and Google's Gemini.\n\nBy building its own data centers, Anthropic would reduce its reliance on these cloud platforms for its most critical workloads -- particularly the enormously expensive process of training new frontier models. This doesn't necessarily mean Anthropic would abandon its cloud partnerships entirely; inference workloads serving enterprise customers through AWS and Google Cloud could continue on those platforms. But the training infrastructure -- the crown jewels of any AI company's operations -- would increasingly reside on Anthropic-controlled hardware, giving the company greater autonomy and negotiating leverage.\n\nThe implications of Anthropic's data center push extend well beyond the company itself. If successful, it would demonstrate that well-funded AI startups can break free from the gravitational pull of Big Tech's cloud platforms and compete as vertically integrated entities. This could inspire other AI companies to pursue similar strategies, potentially reshaping the economics of the entire cloud computing industry by reducing the captive customer base that has fueled the extraordinary growth of AWS, Azure, and Google Cloud.\n\nFor Anthropic, the stakes could not be higher. The company is locked in a fierce competition with OpenAI, Google DeepMind, and a growing roster of well-funded challengers to build the most capable and safest AI systems. In this race, access to compute is not merely a supporting factor -- it is the fundamental resource upon which everything else depends. By betting billions on its own infrastructure, Anthropic is making a declaration that it intends to be not just an AI model provider, but a fully independent AI powerhouse capable of controlling its own destiny from the chip to the cloud.\n\nThe ex-Google veterans now leading this effort understand better than almost anyone the complexity of what lies ahead. Building a data center is not just a construction project; it is an ongoing operational challenge that requires managing power systems, cooling infrastructure, network architecture, hardware refresh cycles, and the myriad logistical details that keep thousands of servers running around the clock. Whether Anthropic can execute on this vision while simultaneously pushing the boundaries of AI research will be one of the most consequential business stories in technology over the coming years."
  },
  {
    "source": "developpez.net",
    "company": "Anthropic",
    "title": "0",
    "date": "2026-02-06T11:44:44Z",
    "url": "https://www.developpez.net/forums/d2181965/general-developpement/algorithme-mathematiques/intelligence-artificielle/anthropic-lance-dernier-modele-d-ia-vibe-working-claude-opus-4-6-a/",
    "content": "Anthropic lance son dernier modèle d'IA de \" vibe working \", Claude Opus 4.6, qui est présenté comme plus performant en matière de codage et capable de produire un travail professionnel de meilleure qualité\n\nAnthropic a dévoilé Claude Opus 4.6, son dernier modèle d'intelligence artificielle (IA) pour le \" vibe working \", présenté comme étant plus performant en matière de codage, capable de soutenir des tâches agentiques pendant une durée plus longue et de produire un travail professionnel de meilleure qualité. Cette mise à jour améliore les performances du modèle en matière de planification, de révision de code, de débogage et de recherche, ce qui constitue une avancée significative dans le domaine des outils d'IA pour les entreprises. Claude Opus 4.6 excelle également dans les tâches d'analyse financière et devrait avoir un impact sur des secteurs tels que l'ingénierie logicielle et la finance. Le modèle est désormais disponible via l'interface chatbot d'Anthropic, son API et sur toutes les principales plateformes cloud.\n\nAnthropic PBC est une entreprise américaine spécialisée dans l'IA dont le siège social est situé à San Francisco. Elle a été fondée en 2021 par un groupe d'anciens chercheurs et cadres d'OpenAI et est surtout connue pour avoir développé une famille de modèles d'IA baptisée Claude. L'entreprise mène des recherches et développe des IA afin d'\" étudier leurs propriétés de sécurité à la pointe de la technologie \" et utilise ces recherches pour déployer des modèles sûrs destinés au grand public.\n\nClaude est une série de grands modèles de langage (LLM) développés par Anthropic, dont le premier modèle à été lancé en mars 2023. La société attribue de nouveaux numéros aux modèles à mesure qu'ils évoluent d'une génération à l'autre, mais le plus grand modèle de la famille est généralement appelé Opus, le modèle de taille moyenne est appelé Sonnet et le plus petit modèle est Haiku. Claude peut analyser des images et différents types de fichiers, ainsi que faire des recherches sur Internet. Il est particulièrement connu pour ses performances en codage informatique.\n\nLe jeudi 5 février 2026, Anthropic a annoncé le lancement de Claude Opus 4.6, son dernier modèle d'IA qui est plus performant en matière de codage, capable de maintenir des tâches plus longtemps et de créer des produits et des résultats professionnels de meilleure qualité. Et, pour la première fois dans les modèles de classe Opus, Opus 4.6 dispose désormais d'une fenêtre contextuelle de 1 million de tokens en version bêta.\n\n\" Le nouveau Claude Opus 4.6 améliore les compétences de codage de son prédécesseur. Il planifie plus soigneusement, soutient les tâches agentiques plus longtemps, peut fonctionner de manière plus fiable dans des bases de code plus importantes et dispose de meilleures compétences en matière de révision et de débogage du code pour détecter ses propres erreurs \", a déclaré Anthropic sur son site.\n\n\" Opus 4.6 peut également mettre ses capacités améliorées au service d'une série de tâches quotidiennes : réaliser des analyses financières, effectuer des recherches, utiliser et créer des documents, des feuilles de calcul et des présentations. Au sein de Cowork, où Claude peut effectuer plusieurs tâches de manière autonome, Opus 4.6 peut mettre toutes ces compétences à votre service \", ajouté l'entreprise.\n\nPerformances de Claude Opus 4.6\n\nLes performances de Claude Opus 4.6 sont à la pointe de la technologie dans plusieurs évaluations. Selon les évaluations réalisées par Anthropic, Claude Opus 4.6 se place en tête dans son secteur en matière de codage agentique, d'utilisation d'ordinateurs, d'utilisation d'outils, de recherche et de finance, souvent avec une large avance. Par exemple, il obtient le score le plus élevé dans l'évaluation du codage agentique Terminal-Bench 2.0 et devance tous les autres modèles de pointe dans le Humanity's Last Exam, un test de raisonnement multidisciplinaire complexe.\n\nSur GDPval-AA, une évaluation des performances dans des tâches intellectuelles à forte valeur économique dans les domaines de la finance, du droit et autres, Opus 4.6 surpasse le deuxième meilleur modèle du secteur (GPT-5.2 d'OpenAI) d'environ 144 points Elo2, et son propre prédécesseur (Claude Opus 4.5) de 190 points. Opus 4.6 est également plus performant que tout autre modèle sur BrowseComp, qui mesure la capacité d'un modèle à localiser des informations difficiles à trouver en ligne. En outre, Claude Opus 4.6 occupe désormais la première place du classement Finance Agent, qui évalue les performances des agents dans les tâches essentielles d'un analyste financier.\n\nLe tableau ci-dessous montre comment Claude Opus 4.6 se compare aux modèles précédents d'Anthropic et à d'autres modèles du secteur sur une variété de critères de référence.\n\nOpus 4.6 serait beaucoup plus performant pour extraire des informations pertinentes à partir de grands ensembles de documents. Cela s'étend aux tâches à contexte long, où le modèle conserve et suit les informations sur des centaines de milliers de tokens avec moins de dérive, et détecte des détails cachés que même Opus 4.5, le modèle précédent d'Anthropic, ne pourrait pas trouver.\n\nUne critique courante à l'égard des modèles d'IA est la \" dégradation du contexte \", qui se traduit par une baisse des performances lorsque les conversations dépassent un certain nombre de tokens. Selon Anthropic, Opus 4.6 offre des performances nettement supérieures à celles de ses prédécesseurs : sur la variante 8-needle 1M du MRCR v2, un benchmark de type \" aiguille dans une botte de foin \" qui teste la capacité d'un modèle à récupérer des informations \" cachées \" dans de vastes quantités de texte, Opus 4.6 a obtenu un score de 76 %, tandis que Sonnet 4.5 n'obtient que 18,5 %. \" Il s'agit d'un changement qualitatif dans la quantité de contexte qu'un modèle peut réellement utiliser tout en conservant des performances optimales \", a déclaré la société.\n\nDans l'ensemble, Opus 4.6 serait donc plus performant pour trouver des informations dans des contextes longs, mieux à même de raisonner après avoir absorbé ces informations, et dispose de \" capacités de raisonnement de niveau expert nettement supérieures en général \".\n\nEnfin, les graphiques ci-dessous montrent les performances de Claude Opus 4.6 sur deux benchmarks qui évaluent ses compétences en génie logiciel et ses capacités en matière de cybersécurité.\n\nUn pas en avant en matière de sécurité\n\nLors de l'audit comportemental automatisé réalisé par Anthropic, Opus 4.6 a affiché un faible taux de comportements inappropriés tels que la tromperie, la flagornerie, l'encouragement des illusions des utilisateurs et la coopération à des utilisations abusives. Dans l'ensemble, le modèle est tout aussi bien aligné que son prédécesseur, Claude Opus 4.5, qui était le modèle pionnier le plus aligné de l'entreprise à ce jour. Opus 4.6 affiche également le taux le plus faible de refus excessifs (lorsque le modèle ne répond pas à des requêtes bénignes) de tous les modèles Claude récents.\n\n\" Pour Claude Opus 4.6, nous avons réalisé l'ensemble d'évaluations de sécurité le plus complet jamais réalisé pour un modèle, en appliquant pour la première fois de nombreux tests différents et en améliorant plusieurs tests que nous avions déjà utilisés auparavant \", a déclaré Anthropic sur son site.\n\nLors de l'audit, Anthropic a inclus de nouvelles évaluations du bien-être des utilisateurs, des tests plus complexes de la capacité du modèle à refuser des demandes potentiellement dangereuses et des évaluations actualisées de la capacité du modèle à effectuer subrepticement des actions nuisibles. Elle a également expérimenté de nouvelles méthodes issues de l'interprétabilité, la science du fonctionnement interne des modèles d'IA. Selon l'entreprise, cette approche a été réalisée afin de \" commencer à comprendre pourquoi le modèle se comporte d'une certaine manière et, en fin de compte, de détecter les problèmes que les tests standard pourraient ne pas détecter. \"\n\nAnthropic a également mis en place de nouvelles mesures de sécurité dans les domaines où Opus 4.6 présente des atouts particuliers qui pourraient être utilisés à des fins dangereuses ou bénéfiques. En particulier, l'entreprise a développé six nouveaux tests de cybersécurité (méthodes de détection des réponses nuisibles) afin de l'aider à suivre différentes formes d'utilisation abusive potentielle.\n\nL'entreprise a également indiqué qu'elle accélérait les utilisations cyberdéfensives du modèle, en l'utilisant pour aider à trouver et à corriger les vulnérabilités des logiciels open source.\n\n\" Nous pensons qu'il est essentiel que les cyberdéfenseurs utilisent des modèles d'IA tels que Claude pour aider à uniformiser les règles du jeu. La cybersécurité évolue rapidement, et nous ajusterons et mettrons à jour nos mesures de protection à mesure que nous en apprendrons davantage sur les menaces potentielles. Dans un avenir proche, nous pourrions mettre en place une intervention en temps réel pour bloquer les abus \", a déclaré Anthropic.\n\nUne transition progressive vers le \" vibe working \"\n\nClaude Opus 4.6 est le premier modèle majeur lancé par Anthropic cette année, mais il arrive quelques mois seulement après la sortie de trois autres modèles -- Claude Opus 4.5, Claude Sonnet 4.5 et Claude Haiku 4.5 -- à la fin de l'année dernière. Les modèles d'Anthropic sont particulièrement populaires auprès des entreprises, qui représentent environ 80 % de l'activité d'Anthropic, a déclaré le PDG Dario Amodei le mois dernier.\n\nL'outil de codage IA de l'entreprise, Claude Code, ainsi que les avancées de son outil de productivité, Claude Cowork, ont également commencé à inquiéter les investisseurs dans le domaine du génie logiciel, dont beaucoup sont de plus en plus préoccupés par le potentiel de disruption dans ce secteur. Le fonds WisdomTree Cloud Computing a perdu plus de 20 % depuis le début de l'année.\n\n\" Tout le monde a pu observer cette transformation dans le domaine du génie logiciel au cours des 18 derniers mois, avec l'émergence du concept de \" vibe coding \" qui permet désormais aux gens de concrétiser leurs idées \", a déclaré Scott White, responsable des produits pour les entreprises chez Anthropic. \" Je pense que nous sommes en train de passer progressivement au \"vibe working\" \".\n\nAnthropic a déclaré que le modèle Claude Opus 4.6 est disponible via son interface chatbot sur claude.ai, son interface de programmation d'application et sur toutes les principales plateformes cloud. Les développeurs peuvent utiliser claude-opus-4-6 via l'API Claude. Le prix reste inchangé à 5 $/25 $ par million de tokens.\n\n\" Si je repense à l'année dernière, Claude est passé d'un modèle auquel on pouvait s'adresser pour accomplir une tâche très simple ou obtenir une réponse, à quelque chose à qui on peut réellement confier un travail important \", a déclaré Scott White. \" Opus 4.6 est un modèle qui rend cette évolution vraiment concrète pour nos utilisateurs. \"\n\nAlors que l'introduction de Claude Opus 4.6 par Anthropic marque une avancée significative dans le domaine du \" vibe working \", les répercussions de cette évolution sur les marchés financiers se font de plus en plus sentir. Les investisseurs s'inquiètent en effet que les nouveaux développements en matière d'IA puissent rendre les logiciels inutiles, ce qui pèse sur les actions des entreprises qui développent, concèdent sous licence et même investissent dans des codes et des systèmes.\n\nLes récentes innovations d'Anthropic, comme l'intégration d'outils juridiques à son assistant Cowork, renforcent ces inquiétudes, notamment en perturbant les acteurs établis, qui ont enregistré près de 1 000 milliards de dollars de pertes en Bourse. Cette évolution alimente les inquiétudes et pousse les investisseurs à se demander si l'IA ne représente pas une menace existentielle.\n\nSource : Anthropic\n\nEt vous ?\n\nQuel est votre avis sur le sujet ?\n\nQue pensez-vous des nouveautés proposées par cette version de Claude Opus ? Les trouvez-vous utiles et intéressantes ?\n\nVoir aussi :\n\nAnthropic annonce que son assistant IA, Claude, restera sans publicité afin de maintenir une expérience utilisateur neutre et utile, un \" espace de réflexion \" exempt de toute interruption commerciale\n\nAnthropic a introduit une mise à jour majeure de Claude, permettant aux utilisateurs d'ouvrir et d'interagir avec des outils tiers tels que Slack, Canva et Figma directement dans l'interface conversationnelle\n\nL'IA Claude Sonnet 4.5 d'Anthropic a développé de manière autonome une application de chat entièrement fonctionnelle similaire à Slack en 30 heures, générant 11 000 lignes de code sans aucune intervention"
  },
  {
    "source": "Trending Topics",
    "company": "Anthropic",
    "title": "Anthropic Deep Dive: Das halbe OpenAI mit doppelter B2B-Power",
    "date": "2026-01-27T07:47:49Z",
    "url": "https://www.trendingtopics.eu/ai-talk-anthropic/",
    "content": "Während OpenAI und Google mit spektakulären Ankündigungen Schlagzeilen machen, hat sich Anthropic in den letzten Monaten zum ernstzunehmenden Konkurrenten im Bereich der künstlichen Intelligenz entwickelt. Das 2021 gegründete Unternehmen erlebt derzeit seinen \"ChatGPT-Moment\" und erobert vor allem den B2B-Markt mit beeindruckender Geschwindigkeit. Was macht Anthropic anders? Und warum setzen immer mehr Unternehmen auf die Claude-KI-Modelle statt auf die etablierte Konkurrenz?\n\nIm AI Talk beleuchten heute die Podcast-Hosts Jakob Steinschaden, Mitgründer von newsrooms und Trending Topics, und Clemens Wasner, CEO von EnliteAI und Vorsitzender von AI Austria, Vergangenheit, Gegenwart und Zukunft von Anthropic:\n\nDie Gründungsgeschichte: Mission statt Profit\n\nAnthropic wurde 2021 von ehemaligen OpenAI-Mitarbeitern gegründet, darunter Dario Amodei, der frühere Head of Alignment Research bei OpenAI, und seine Schwester. Der Grund für die Abspaltung war eine fundamentale Meinungsverschiedenheit über die Ausrichtung von OpenAI. Während sich OpenAI zunehmend von seiner ursprünglichen Mission entfernte und kommerzieller wurde, wollten die Gründer von Anthropic an den ethischen Grundsätzen festhalten.\n\nIm Gegensatz zu späteren Abgängen wie Ilya Sutskever (Safe Superintelligence) oder Mira Murati (Thinking Machines), die primär technisch motiviert waren, war die Gründung von Anthropic ideologisch geprägt. Das Unternehmen kommt aus der Effective Altruism-Bewegung, die darauf abzielt, den positiven Impact von Technologie in der Welt zu maximieren. Diese Philosophie prägt bis heute die Unternehmenskultur und Produktentwicklung.\n\nMarktposition: Das halbe OpenAI mit doppelter B2B-Power\n\nAuf den ersten Blick erscheint Anthropic wie ein kleinerer Bruder von OpenAI. Die Zahlen zeigen: Anthropic hat 33 Milliarden Dollar Risikokapital aufgenommen (OpenAI: 80 Milliarden), wird mit 183 Milliarden Dollar bewertet (OpenAI: 500 Milliarden) und macht 9 bis 10 Milliarden Dollar Umsatz (OpenAI: 20 Milliarden). Kurz gesagt: etwa halb so groß wie OpenAI.\n\nDoch diese Zahlen täuschen über die tatsächliche Marktmacht hinweg. Im Consumer-Bereich (Chatbots) hat Claude nur 2 Prozent Marktanteil, während ChatGPT über 50 Prozent hält. Im entscheidenden B2B-Geschäft hat sich das Blatt jedoch dramatisch gewendet:\n\nNoch bemerkenswerter ist die Entwicklung: Vor zweieinhalb Jahren hatte OpenAI noch 50 Prozent Enterprise-Marktanteil und Anthropic nur 12 Prozent. Diese Verschiebung hat bei OpenAI bereits zweimal den \"Code Red\"-Alarmzustand ausgelöst, zuletzt Ende Januar 2025 im Coding-Bereich.\n\nDie technische Strategie: Fokus statt Feature-Chaos\n\nWährend OpenAI in viele Richtungen expandiert (Bilderstellung, Videogenerierung, Suchmaschine, Hardware, Gesundheitswesen), konzentriert sich Anthropic konsequent auf Text- und Code-Output für den B2B-Bereich. Diese Fokussierung zahlt sich aus.\n\nDie Claude-Modellfamilie\n\nAnthropic hat das Chaos der verschiedenen Modellvarianten, das OpenAI mit seinen zahllosen GPT-Versionen verursacht, elegant gelöst. Die Claude-Modelle kommen in drei klar definierten Abstufungen, alle an musikalische Begriffe angelehnt:\n\n* Haiku: Schnelles, effizientes Modell für einfache Aufgaben\n\n* Sonnet: Ausgewogenes Modell für die meisten Anwendungsfälle\n\n* Opus: Leistungsstärkstes Modell für komplexe Aufgaben\n\nIm Coding-Bereich ist Claude Opus 4.5 laut LM Arena auf Platz 1 der Rangliste. In der Gesamtwertung liegt es auf Platz 2, knapp hinter Gemini 3 Pro und Grok 4.1, aber deutlich vor GPT-5.2 von OpenAI.\n\nPreisgestaltung: Teuer, aber gewollt\n\nClaude Opus 5 ist mit Abstand das teuerste KI-Modell auf dem Markt, fast doppelt so teuer wie Grok 4 und auf dem Niveau von GPT-5.2. Dennoch sind Kunden bereit, diesen Preis zu zahlen. Der Grund: Die Modelle sind besonders gut im Coding und liefern zuverlässige Ergebnisse.\n\nInteressant ist auch die Kostenstruktur: Obwohl OpenAI doppelt so viel Umsatz macht wie Anthropic, unterscheiden sich die absoluten Verluste beider Unternehmen kaum. Das liegt daran, dass Anthropics Kunden ausschließlich Power-User sind, die intensiv mit der KI arbeiten, während viele ChatGPT-Nutzer die Plattform nur gelegentlich verwenden.\n\nDie AI Constitution: Ethik als Wettbewerbsvorteil\n\nAnthropic ist das einzige große KI-Unternehmen, das seiner KI eine formale Verfassung gegeben hat. Diese \"AI Constitution\" definiert grundlegende Regeln, nach denen die Claude-Modelle agieren dürfen und sollen. Die Verfassung ist nicht statisch, sondern entwickelt sich dynamisch weiter.\n\nZu den Grundprinzipien gehören:\n\n* Apple Privacy Standards für Datenschutz (strenger als bei anderen Tech-Unternehmen)\n\n* UN-Menschenrechtserklärung als ethischer Kompass\n\n* Transparenz über Fähigkeiten und Limitierungen\n\n* Fokus auf AI Alignment (die KI tut, was sie soll)\n\nDiese ethische Ausrichtung unterscheidet Anthropic fundamental von der Konkurrenz. Während andere Unternehmen über \"Responsible AI\" sprechen, setzt Anthropic diese Prinzipien tatsächlich um.\n\nClaude Cowork: Der Game-Changer für Knowledge Worker\n\nDer Hauptgrund für den aktuellen Hype um Anthropic ist die Einführung von Claude Cowork Anfang Januar 2025. Dieses Tool wird von vielen als das derzeit mächtigste und am einfachsten zu bedienende Agentensystem für den Desktop angesehen.\n\nWas ist Claude Cowork?\n\nDie Claude-App hat drei Modi:\n\nClaude Cowork ist im Grunde \"Claude Code für Leute, die mit Code nichts zu tun haben\". Es ermöglicht komplexe Automatisierungen ohne Programmierkenntnisse.\n\nPraktische Anwendungsbeispiele\n\nMit Claude Cowork können Nutzer zum Beispiel:\n\n* Automatisch Social-Media-Videos generieren (mit dem Remotion-Skill)\n\n* Excel-Tabellen intelligent bearbeiten (leistungsfähiger als Microsofts eigener Excel-Skill)\n\n* Podcast-Transkripte durchsuchen und analysieren\n\n* Komplette Workflows automatisieren, die normalerweise einen halben Tag dauern würden\n\n\"Das ist so beeindruckend, was das Ding jetzt schon kann. Und das hat sehr viele Restrictions. In einem halben Jahr wird das total steil gehen.\"\n\nMit diesem Prompt haben wir ein simples Promo-Video erstellt:\n\n\"create a 15 seconds video with the remotion skill to promote this website: https://www.trendingtopics.eu/de/ \"\n\nKonkurrenz und Open-Source-Alternativen\n\nDas vergleichbarste Produkt ist Google Antigravity, allerdings mit einer weniger intuitiven Benutzeroberfläche. Im Open-Source-Bereich geht ClawdBot von Entwickler Peter Steinberger noch weiter: Damit lässt sich ein kompletter Computer über Telegram, Signal oder iMessage fernsteuern, ohne jegliche Einschränkungen.\n\nClawdBot kann beispielsweise:\n\n* Jeden Morgen Termine, wichtige E-Mails und Aufgaben priorisieren\n\n* Automatisch nach relevanten Insolvenzen oder Firmendaten suchen\n\n* Bestellstatus bei Online-Shops abfragen\n\n* Komplett autonom arbeiten und nur bei Bedarf nachfragen\n\nPartnerschaften: Amazon, Google und die Alexa-Connection\n\nAnthropic hat strategische Partnerschaften mit den größten Tech-Konzernen:\n\n* Amazon/AWS: Hauptinvestor und Infrastruktur-Partner. Die neue Alexa Plus nutzt primär Claude-KI\n\n* Google: Investor trotz eigener Gemini-Modelle\n\n* Microsoft: Kleinere Beteiligung gemeinsam mit Nvidia\n\nBesonders die Amazon-Partnerschaft ist bedeutsam: Der erneuerte Sprachassistent Alexa Plus basiert hauptsächlich auf Anthropics Claude-Modellen, nicht auf Amazons eigener Nova-Serie, die nicht leistungsstark genug ist.\n\nWarum nicht Apple?\n\nApple hat sich Ende 2024 für Google Gemini als KI-Partner entschieden, obwohl Claude im März 2024 noch technisch überlegen war. Die Gründe:\n\nFinanzielle Perspektive: IPO 2026 und das Pricing-Problem\n\nFür 2026 wird ein Börsengang (IPO) von Anthropic erwartet. Der Cash-Burn ist zu hoch, um ohne frisches Kapital weiterzumachen. Die geplante Bewertung liegt bei 350 Milliarden Dollar, gegenüber 183 Milliarden bei der letzten Finanzierungsrunde.\n\nDas wahre Kosten-Problem\n\nAktuell sind alle KI-Dienste massiv durch Risikokapital querfinanziert. Experten schätzen, dass ein Power-User-Plan realistischerweise zwischen 500 und 2.000 Euro pro Monat kosten müsste, um profitabel zu sein. Zum Vergleich: Derzeit kostet Claude Pro 200 Dollar pro Monat.\n\nSam Altman von OpenAI hatte bereits vor zwei Jahren einen 2.000-Euro-Tier angekündigt. Bisher traut sich niemand, diesen Preis tatsächlich zu verlangen. Einzig Canva verlangt mit 50 Euro pro Monat annähernd kostendeckende Preise.\n\nValue-Based Pricing: Die Zukunft der KI-Abrechnung\n\nDie Branche bewegt sich weg vom klassischen Per-Seat-Pricing (Preis pro Nutzer) hin zu Value-Based Pricing. Die Argumentation:\n\n* Ein KI-Agent ersetzt einen Teil oder eine ganze Arbeitskraft\n\n* Wenn eine Stelle 60.000 Euro pro Jahr kostet, sind 20.000 Euro für KI ein Schnäppchen\n\n* KI wird nicht nur mit Software verglichen, sondern mit menschlicher Arbeitskraft\n\n* Der Fokus verschiebt sich von Kosteneinsparung zu Umsatzsteigerung\n\nDieser Paradigmenwechsel könnte die gesamte Software-Industrie verändern.\n\nRisiken und Herausforderungen\n\nTrotz des aktuellen Erfolgs gibt es erhebliche Risiken:\n\nMakroökonomische Instabilität\n\n* Oracle-Problem: Oracle nimmt Schulden auf ein unerprobtes Businessmodell (OpenAI) auf, was den Aktienkurs und Zinssatz unter Druck setzt\n\n* Japanischer Yen: Währungsprobleme in Japan könnten die globalen Finanzmärkte destabilisieren\n\n* KI-Blase: Wenn die Bewertungen kollabieren, bevor Anthropic an die Börse geht, könnte der IPO scheitern\n\nWettbewerbsdruck\n\nOpenAI befindet sich im \"Code Red\"-Modus und wird mit einem Stakkato an neuen Releases versuchen, verlorene Marktanteile zurückzugewinnen. Google investiert massiv in Gemini. Microsoft könnte Anthropic als Alternative zu OpenAI pushen, da OpenAI zunehmend Produkte entwickelt, die für Microsoft irrelevant oder sogar konkurrierend sind.\n\nAusblick 2026: Drei Szenarien\n\nSzenario 1: Erfolgreicher IPO und Marktführerschaft\n\nAnthropic gelingt der Börsengang zu einer Bewertung von 350 Milliarden Dollar oder mehr. Das Unternehmen baut seine Marktführerschaft im B2B-Bereich weiter aus, insbesondere bei Coding und Desktop-Automatisierung. Claude Cowork wird zum Standard-Tool für Knowledge Worker. Engere Partnerschaft mit Amazon, möglicherweise auch mit Microsoft.\n\nSzenario 2: Konsolidierung und Übernahme\n\nDer IPO kommt nicht zustande oder verläuft enttäuschend. Einer der großen Partner (Amazon, Google, Microsoft) übernimmt Anthropic vollständig. Die Technologie wird in bestehende Produkte integriert, die Marke Claude verschwindet möglicherweise.\n\nSzenario 3: Platzen der KI-Blase\n\nMakroökonomische Probleme oder enttäuschende KI-Ergebnisse führen zu einem Einbruch der Bewertungen. Anthropic muss Kosten drastisch senken, Mitarbeiter entlassen und die Produktstrategie überdenken. Der B2B-Fokus erweist sich als Rettungsanker, da Enterprise-Kunden weniger preissensibel sind als Konsumenten.\n\nFazit: Der stille Gewinner?\n\nAnthropic macht vieles anders als die Konkurrenz: konsequenter Fokus auf B2B und Coding, ethische Grundsätze statt reiner Profitmaximierung, klare Produktstrategie statt Feature-Chaos. Diese Strategie zahlt sich aus, wie die beeindruckenden Marktanteilsgewinne zeigen.\n\nWährend OpenAI und Google mit spektakulären Ankündigungen Aufmerksamkeit erregen, erobert Anthropic leise, aber effektiv den lukrativen Enterprise-Markt. Die Einführung von Claude Cowork könnte der Durchbruch sein, der KI-Agenten endlich massentauglich macht.\n\nDas Jahr 2026 wird entscheidend: Gelingt der IPO, könnte Anthropic zu einem der wertvollsten Tech-Unternehmen der Welt werden. Scheitert er, könnte das Unternehmen in den Armen eines der Tech-Giganten verschwinden. Eines ist jedoch sicher: Anthropic hat bewiesen, dass man auch ohne Elon-Musk-Gehabe und ohne schillernde Persönlichkeiten an der Spitze erfolgreich sein kann, wenn man sich auf Execution und Produktqualität konzentriert."
  },
  {
    "source": "WebProNews",
    "company": "Anthropic",
    "title": "Anthropic's Claude Opus 4.5 Aces Hiring Exam, Prompts AI-Resistant Redesign",
    "date": "2026-01-23T16:49:30Z",
    "url": "https://www.webpronews.com/anthropics-claude-opus-4-5-aces-hiring-exam-prompts-ai-resistant-redesign/",
    "content": "When AI Aces the Job Interview: Anthropic's Battle to Outsmart Its Own Creation\n\nIn the fast-evolving world of artificial intelligence, companies like Anthropic are grappling with a peculiar challenge: their own AI models are becoming so advanced that they're disrupting traditional hiring processes. This issue came to a head when Anthropic's Claude Opus 4.5 model aced a notoriously difficult take-home exam designed for performance engineering candidates. The incident, detailed in a recent Anthropic engineering blog post, highlights the broader implications for technical evaluations in an era where AI can mimic human expertise with startling accuracy.\n\nThe take-home exam in question was crafted to test candidates' abilities in performance engineering, a field that demands deep knowledge of system optimization, debugging, and scalable architecture. Anthropic's engineers had long relied on this rigorous assessment to filter top talent, but as AI capabilities surged, the test's integrity came under threat. When Opus 4.5 not only passed but excelled, it forced the team to rethink their approach, iterating through multiple versions to create what they term \"AI-resistant\" evaluations.\n\nThis isn't just an internal anecdote; it's a microcosm of how AI is reshaping talent acquisition across the tech industry. Companies are now forced to design assessments that probe uniquely human skills, such as creative problem-solving and ethical reasoning, while acknowledging that candidates might leverage AI tools during the process. Anthropic's experience underscores a pivotal shift: evaluations must evolve to stay ahead of the very technologies they're built to support.\n\nThe Evolution of AI in Hiring: From Tool to Competitor\n\nAnthropic's journey began with their original take-home exam, which involved optimizing a simulated distributed system under tight constraints. The task required candidates to analyze performance bottlenecks, implement efficient algorithms, and justify their decisions with detailed reasoning. According to the blog, this format worked well until AI models like Claude began to handle complex coding and optimization tasks with ease.\n\nThe turning point came when engineers tested Opus 4.5 on the exam. The model generated solutions that were not only correct but innovative, surpassing what many human candidates might produce. This prompted a redesign: the second iteration introduced elements like ambiguous requirements and real-world variability, aiming to force candidates to make judgment calls that AI might struggle with.\n\nYet, even this version fell short. Opus 4.5 adapted, using its reasoning capabilities to interpret ambiguities and deliver high-quality outputs. Anthropic's team realized that true resistance required focusing on meta-skills -- abilities like iterating on feedback or collaborating in unpredictable scenarios -- which AI currently handles less fluidly.\n\nInsights from Industry Peers and Recent Developments\n\nDrawing from broader industry trends, similar challenges are emerging elsewhere. For instance, a report from Blockchain News earlier this year discussed how Opus 4.5's performance on such exams signals a need for adaptive hiring strategies. The article notes that while AI excels at rote tasks, human ingenuity shines in novel problem domains.\n\nRecent posts on X (formerly Twitter) echo this sentiment, with users highlighting Anthropic's blog as a wake-up call for recruiters. One post from a tech influencer emphasized the importance of evaluations that incorporate real-time human-AI collaboration, rather than banning AI outright. This aligns with Anthropic's philosophy: instead of prohibiting AI use, they encourage it, designing tests where the human's oversight and creativity add irreplaceable value.\n\nMoreover, Anthropic's internal research, as shared in their research on AI transforming work, reveals that engineers are increasingly acting as orchestrators of AI agents rather than solo coders. A survey of 132 Anthropic staff showed that while AI boosts productivity, it also raises concerns about skill atrophy if not managed carefully.\n\nRedesigning Evaluations: Lessons from Three Iterations\n\nDelving deeper into Anthropic's iterations, the third version of the exam incorporated live elements, such as interactive debugging sessions and requirements that evolve based on candidate inputs. This dynamic approach aims to mimic real engineering environments where adaptability is key. The blog details how this version finally stumped Opus 4.5 in certain aspects, particularly where human intuition for edge cases proved superior.\n\nAnthropic isn't alone in this pursuit. A WIRED article on Claude Code's impact notes how such tools are reshaping software development, pushing companies to value engineers who can effectively supervise AI outputs. Boris Cherny, head of Claude Code, explained that the focus is shifting toward \"full-stack\" capabilities enhanced by AI, rather than narrow specializations.\n\nThis evolution has sparked debates about job displacement. Anthropic CEO Dario Amodei, speaking at the World Economic Forum as reported by Firstpost, warned that AI could handle most software engineering tasks within 6-12 months. However, he tempered this by stressing the emergence of new roles centered on AI orchestration.\n\nBroader Implications for Tech Talent and AI Ethics\n\nThe ripple effects extend beyond hiring. Anthropic's Economic Index research analyzes real-world AI usage, showing that it accelerates complex tasks most in high-skill occupations. This suggests that while entry-level jobs might automate, higher-level roles will demand even greater human-AI synergy.\n\nEthical considerations are paramount. In their announcement of Anthropic Labs, the company emphasizes building products that prioritize safety and alignment. For evaluations, this means ensuring fairness -- preventing AI from giving undue advantages to those with access to premium models.\n\nPosts on X further illustrate public discourse, with discussions around how AI-resistant tests could mitigate biases in hiring. One thread pointed out that while AI democratizes access to knowledge, it risks widening gaps if evaluations don't account for varying AI literacy levels.\n\nCase Studies and Real-World Applications\n\nTo illustrate, consider how other firms are adapting. A Fortune piece on Anthropic's data argues that AI won't eliminate jobs overnight but will redefine them. Companies like Rakuten and Zapier, as mentioned in a Blockchain News report, report using AI for 60% of tasks, yet full delegation remains low at 0-20%.\n\nAnthropic's Frontier Red Team, detailed in their research overview, explores AI implications in critical areas like cybersecurity. This informs their evaluation designs, ensuring candidates can handle AI's potential risks, such as generating misleading code.\n\nIn healthcare, Anthropic's specialized tiers, covered in a FinancialContent article, show AI achieving 92.3% accuracy in diagnostics, yet human oversight remains crucial to curb hallucinations.\n\nFuture Directions: Balancing Innovation and Human Element\n\nLooking ahead, Anthropic's blog suggests ongoing refinements, including integrating agentic capabilities from tools like Claude in Chrome. This could lead to evaluations where candidates build and manage AI agents in simulated scenarios, testing their ability to guide AI effectively.\n\nThe H-1B visa debate, ignited by Amodei's comments as reported in Techloy, highlights global talent dynamics. With AI handling more coding, the need for visas might shift toward roles requiring cultural and innovative insights.\n\nAnthropic's news updates on models like Claude Sonnet 4.5 emphasize alignment, which extends to hiring: evaluations must ensure new hires contribute to safe AI development.\n\nNavigating the AI-Human Divide in Talent Assessment\n\nUltimately, Anthropic's experience teaches that AI-resistant evaluations aren't about outrunning technology but harmonizing with it. By focusing on human strengths like empathy and strategic thinking, companies can foster teams that leverage AI without being overshadowed.\n\nRecent X discussions reinforce this, with experts advocating for hybrid assessments that measure how well candidates integrate AI into their workflows. This approach not only identifies top talent but also prepares them for AI-centric workplaces.\n\nAs AI advances, the tech sector must continually innovate its hiring practices. Anthropic's iterative process serves as a blueprint, demonstrating that staying ahead requires embracing change while valuing the irreplaceable human spark.\n\nEmerging Strategies and Global Perspectives\n\nGlobally, variations in AI adoption influence evaluation designs. Anthropic's Economic Index, as analyzed by ETIH EdTech News, shows uneven productivity gains across countries, suggesting tailored assessments for diverse talent pools.\n\nIn India, where many H-1B visa holders originate, Amodei's predictions reported by Hindustan Times have sparked conversations about upskilling for AI orchestration roles.\n\nAnthropic's work on interpretability, hinted at in X posts about hand-designing super-reasoners, could lead to evaluations that probe candidates' understanding of AI internals, ensuring they can mitigate risks like sabotage, as explored in their research on AI evasion tactics.\n\nThe Path Forward: Innovation in Evaluation Design\n\nTo wrap up this exploration, it's clear that AI's encroachment on technical evaluations is prompting a renaissance in hiring methodologies. Anthropic's transparent sharing of their challenges and solutions paves the way for industry-wide improvements.\n\nBy incorporating elements like real-time collaboration and ethical dilemmas, future tests can better distinguish human potential. As one X post aptly noted, the goal is to design evaluations where AI assists but humans excel.\n\nIn this new era, the companies that thrive will be those that view AI not as a threat to hiring but as a catalyst for more meaningful assessments of talent."
  },
  {
    "source": "WWWhat's new",
    "company": "Anthropic",
    "title": "Claude Sonnet 4.6: el modelo \"de diario\" de Anthropic sube de nivel sin subir el precio",
    "date": "2026-02-19T07:18:24Z",
    "url": "https://wwwhatsnew.com/2026/02/19/claude-sonnet-4-6-el-modelo-de-diario-de-anthropic-sube-de-nivel-sin-subir-el-precio/",
    "content": "Anthropic acaba de anunciar Claude Sonnet 4.6, una nueva versión de su familia Sonnet que apunta a un objetivo muy concreto: ofrecer un rendimiento cercano a su gama alta, Opus, con un coste y una disponibilidad más \"prácticos\" para el día a día. La propia compañía lo define como una actualización completa de habilidades en programación, uso del ordenador, razonamiento con contexto largo, planificación de agentes, trabajo de oficina y diseño, y lo coloca como modelo predeterminado para usuarios de planes Free y Pro en sus interfaces, según su comunicado y entrada oficial de producto (fuente: Anthropic).\n\nEn términos de acceso, Claude Sonnet 4.6 ya está disponible en los planes de Claude, en Claude Cowork, Claude Code, la API de Claude y también en \"grandes plataformas cloud\", de acuerdo con la nota enviada a medios y el post del blog de la compañía. El detalle importante para quien mira presupuestos: el precio por API se mantiene respecto a Sonnet 4.5, con la referencia de 3/15 dólares por millón de tokens (entrada/salida) que Anthropic menciona en su anuncio.\n\nSonnet y Opus: la \"berlina\" que quiere conducir como un deportivo\n\nPara entender el movimiento, conviene recordar la lógica de catálogo: Opus suele ser el modelo \"tope de gama\" y Sonnet la opción con mejor equilibrio. Si Opus es ese coche deportivo que sacas cuando necesitas el máximo agarre en la curva, Sonnet quiere ser la berlina cómoda que, sin buscar récords, acelera lo suficiente como para que casi nadie eche de menos el deportivo en trayectos normales.\n\nEsa comparación no es casual. En su comunicación, Anthropic insiste en que Sonnet 4.6 \"se acerca\" a la inteligencia de Opus, con un punto de precio que lo hace más viable para muchas más tareas. David Gewirtz, en ZDNET, lo interpreta como el \"daily driver\" que busca cerrar la brecha con Opus sin penalizar a los usuarios de planes baratos o gratuitos (fuente: ZDNET).\n\nMejoras en coding: más consistencia y menos \"sobreingeniería\"\n\nUno de los titulares más repetidos del anuncio es la mejora en programación. Anthropic afirma que Sonnet 4.6 gana en consistencia, seguimiento de instrucciones y fiabilidad, y que en pruebas internas los desarrolladores lo prefirieron frente a Sonnet 4.5 alrededor del 70% de las veces. También sostienen que llegó a ser preferido frente a Claude Opus 4.5 en un porcentaje relevante de comparativas (fuente: Anthropic).\n\nTraducido a sensaciones reales, el problema que muchos equipos han visto en modelos anteriores no es solo \"si sabe programar\", sino si programa de forma útil: leer contexto antes de tocar código, evitar duplicar lógica, no inventarse que algo funciona cuando no se ha verificado. Anthropic dice que en Claude Code los probadores detectaron menos tendencia a la \"pereza\" y a la \"sobreingeniería\", con menos afirmaciones falsas de éxito, menos alucinaciones y mejor seguimiento en tareas con varios pasos. Es como pasar de un becario brillante pero impulsivo a uno igual de rápido, pero que revisa el documento antes de enviar el correo.\n\nUso del ordenador: cuando la IA ya no necesita \"conectores\" para todo\n\nOtra pata clave es el computer use, la capacidad de usar software \"como lo haría una persona\", haciendo clic, escribiendo y navegando por interfaces. Anthropic ya había presentado este enfoque en 2024 como algo experimental; ahora asegura que Sonnet 4.6 mejora \"de forma importante\" frente a Sonnet anteriores. El benchmark que usan para ilustrarlo es OSWorld, una referencia habitual para medir tareas en aplicaciones reales dentro de un entorno simulado, sin APIs especiales ni integraciones hechas a mano. La compañía detalla que sus modelos Sonnet han ido mejorando en OSWorld durante 16 meses y que desde Sonnet 4.5 se usa OSWorld-Verified, una versión actualizada del benchmark (fuente: Anthropic).\n\nLo interesante aquí no es el gráfico, sino lo que implica para trabajo cotidiano. Anthropic menciona ejemplos concretos: moverse por una hoja de cálculo compleja, completar un formulario web de varios pasos y coordinar información entre varias pestañas del navegador. Si alguna vez has visto a alguien \"perderse\" entre pantallas, entenderás por qué esto importa: una IA que usa el ordenador reduce la necesidad de que cada herramienta tenga integración perfecta. Es como tener un compañero que aprende a usar el programa heredado del departamento de contabilidad sin pedirle al equipo de IT que programe un conector nuevo.\n\nVentana de 1 millón de tokens: contexto largo que no se deshilacha\n\nUna de las novedades más llamativas es la ventana de contexto de 1M tokens en beta. Poner un número tan grande suena abstracto, así que vale una metáfora: si un modelo clásico trabaja con una mesa de comedor, una ventana de 1 millón de tokens es una mesa de banquete. Caben un código base entero, contratos largos o decenas de papers en una sola sesión, según la propia Anthropic.\n\nEl matiz importante es el segundo: no basta con \"meterlo todo\", hace falta razonar bien sin perder el hilo. Anthropic recalca que Sonnet 4.6 mantiene un razonamiento efectivo a través de ese contexto largo, lo que impacta especialmente en planificación a largo plazo y tareas que crecen por capas, como proyectos de software con decisiones acumulativas (fuente: Anthropic). En la práctica, para quien trabaja con documentación y cambios iterativos, esto reduce ese efecto de \"teléfono roto\" en el que la IA olvida por qué se tomó una decisión veinte mensajes atrás.\n\nPlanificación y evaluación: el caso de Vending-Bench Arena\n\nPara sostener la idea de planificación a largo plazo, Anthropic cita una evaluación llamada Vending-Bench Arena, pensada para medir cómo un modelo \"gestiona un negocio\" simulado a lo largo del tiempo. Según la compañía, Sonnet 4.6 adoptó una estrategia curiosa: invertir fuerte en capacidad durante los primeros diez meses simulados y pivotar con decisión hacia la rentabilidad al final, logrando un resultado superior en esa evaluación (fuente: Anthropic).\n\nMás allá del benchmark, el ejemplo sirve para entender la mejora en \"horizonte\": no es solo responder bien a una pregunta, sino sostener una estrategia coherente durante una partida larga, como quien primero compra estanterías y stock para una tienda y solo luego optimiza márgenes cuando ya puede vender con fluidez.\n\nHerramientas para desarrolladores: API, ejecución de código y \"compaction\"\n\nEn la parte de plataforma, Anthropic también anuncia cambios relevantes. En su Developer Platform, Sonnet 4.6 soporta modos de pensamiento adaptativo y extendido, y ofrece context compaction en beta, un mecanismo que resume contexto antiguo cuando la conversación se acerca al límite, para estirar la utilidad del historial sin perder lo importante (fuente: Anthropic).\n\nEn la API de Claude, la compañía señala que las herramientas de búsqueda y \"fetch\" web ahora pueden escribir y ejecutar código automáticamente para filtrar y procesar resultados, reteniendo solo lo relevante en el contexto. También mencionan como disponibilidad general la ejecución de código, web fetch, memoria, llamadas programáticas de herramientas, búsqueda de herramientas y ejemplos de uso (fuente: Anthropic). Para un equipo que arma agentes, esto se parece a pasar de una navaja suiza con pocas piezas a una caja de herramientas completa: menos pegamento manual, más flujos reproducibles.\n\nClaude en Excel y conectores MCP: datos financieros sin salir de la hoja\n\nUna novedad con sabor claramente corporativo es el soporte de MCP connectors para Claude en Excel en planes Pro, Max, Team y Enterprise. Anthropic dice que esto permite usar conectores hacia fuentes como S&P Global, LSEG, Daloopa, PitchBook, Moody's y FactSet, trayendo contexto externo directamente a la hoja de cálculo sin saltar entre pestañas o aplicaciones. Si ya se usan conectores MCP en Claude.ai, esas conexiones se heredan en Excel (fuente: Anthropic).\n\nPara quien vive entre celdas, esto puede ser más relevante que cualquier benchmark: pedirle a la IA que combine datos externos con tu modelo financiero y los deje listos en la misma plantilla es una mejora de flujo de trabajo muy tangible.\n\nSeguridad: prompt injection y evaluaciones internas\n\nEl aumento de \"uso del ordenador\" trae riesgos propios. Anthropic menciona explícitamente los ataques de prompt injection, donde se ocultan instrucciones maliciosas en páginas web o contenidos para desviar el comportamiento del modelo. Según sus evaluaciones, Sonnet 4.6 mejora de forma importante frente a Sonnet 4.5 en resistencia a este tipo de ataques, y se comporta de forma similar a Opus 4.6 en ese aspecto (fuente: Anthropic).\n\nLa compañía también afirma haber realizado evaluaciones extensas de seguridad y concluye que Sonnet 4.6 es tan seguro como otros modelos recientes, con comportamientos de seguridad fuertes y sin señales de grandes preocupaciones en desalineamientos de alto riesgo, según su investigación interna y documentación asociada (fuente: Anthropic).\n\nCuándo tiene sentido usar Sonnet 4.6 y cuándo subir a Opus 4.6\n\nEn el propio mensaje de Anthropic hay una recomendación implícita: Sonnet 4.6 rinde bien incluso con \"extended thinking\" desactivado y permite explorar el equilibrio entre velocidad y fiabilidad según lo que estés construyendo. Para tareas de código y productividad general, Sonnet busca ser la opción por defecto.\n\nAun así, Anthropic conserva el papel de Opus 4.6 como el escalón para lo que exige la máxima profundidad de razonamiento, como refactors grandes de bases de código, coordinación de múltiples agentes o trabajos donde \"tiene que quedar perfecto\" (fuente: Anthropic). En lenguaje cotidiano: Sonnet 4.6 te resuelve la mayoría de recados con solvencia; Opus 4.6 sigue siendo el especialista al que llamas cuando el margen de error es mínimo."
  },
  {
    "source": "The New York Times",
    "company": "Anthropic",
    "title": "Defense Department and Anthropic Square Off in Dispute Over A.I. Safety",
    "date": "2026-02-18T23:11:10Z",
    "url": "https://www.nytimes.com/2026/02/18/technology/defense-department-anthropic-ai-safety.html",
    "content": "Sheera Frenkel reported from San Francisco and Julian E. Barnes from West Palm Beach, Fla.\n\nFor months, the Department of Defense and the artificial intelligence company Anthropic have been negotiating a contract over the use of A.I. on classified systems by the Pentagon.\n\nThis week, those discussions erupted in a war of words.\n\nOn Monday, a person close to Defense Secretary Pete Hegseth told Axios that the Pentagon was \"close\" to declaring the start-up a \"supply chain risk,\" a move that would sever ties between the company and the U.S. military. Anthropic was caught off guard and internally scrambled to pinpoint what had set off the department, two people with knowledge of the company said.\n\nAt the heart of the fight is how A.I. will be used in future battlefields. Anthropic told defense officials that it did not want its A.I. used for mass surveillance of Americans or deployed in autonomous weapons that had no humans in the loop, two people involved in the discussions said.\n\nBut Mr. Hegseth and others in the Pentagon were furious that Anthropic would resist the military's using A.I. as it saw fit, current and former officials briefed on the discussions said. As tensions escalated, the Department of Defense accused the San Francisco-based company of catering to an elite, liberal work force by demanding additional protections.\n\nThe disagreement underlines how political the issue of A.I. has become in the Trump administration. President Trump and his advisers want to expand technology's use, reducing export restrictions on A.I. chips and criticizing state regulations that could be perceived as inhibitors to A.I. development. But Anthropic's chief executive, Dario Amodei, has long said A.I. needs strict limits around it to prevent it from potentially wrecking the world.\n\nEmelia Probasco, a senior fellow at Georgetown's Center for Security and Emerging Technology, said it was important that the relationship between the Pentagon and Anthropic not be doomed.\n\n\"There are war fighters using Anthropic for good and legitimate purposes, and ripping this out of their hands seems like a total disservice,\" she said. \"What the nation needs is both sides at the table discussing what can we do with this technology to make us safer.\"\n\nIn a statement, Sean Parnell, the Pentagon spokesman, said the department's relationship with Anthropic was \"being reviewed.\"\n\n\"Our nation requires that our partners be willing to help our war fighters win in any fight,\" he said. \"Ultimately, this is about our troops and the safety of the American people.\"\n\nAnthropic said it was committed to using its A.I. to support U.S. national security and was \"having productive conversations, in good faith, with the Department of War on how to continue that work and get these complex issues right.\"\n\nTrump Administration: Live Updates\n\nUpdated\n\nFeb. 18, 2026, 4:53 p.m. ET\n\nA Pentagon official praised the other A.I. companies that have been working on the Pentagon's unclassified systems. The official said the department had an agreement in place for one of the firms to begin work on the classified system, replacing Anthropic.\n\nThe Defense Department has used Anthropic's technology for more than a year as part of a $200 million A.I. pilot program to analyze imagery and other intelligence data and conduct research. Google, OpenAI and Elon Musk's xAI are also part of the program. But Anthropic's A.I. chatbot, Claude, was the most widely used by the agency -- and the only one on classified systems -- thanks to its integration with technology from Palantir, a data analytics company that works with the federal government, according to defense officials with knowledge of the technology.\n\n(The New York Times has sued OpenAI and Microsoft, accusing them of copyright infringement of news content related to A.I. systems. The companies have denied those claims.)\n\nThis year, Anthropic was set to formally sign a contract with the Department of Defense. That was when the company asked for limits to how its A.I. tools could be deployed.\n\nAnthropic has long been more vocal than other A.I. companies on safety issues. In a podcast interview in 2023, Dr. Amodei said there was a 10 to 25 percent chance that A.I. could destroy humanity. Internally, the company has strict guidelines that bar its technology from being used to facilitate violence.\n\nIn January, Dr. Amodei wrote in an essay on his personal website that \"using A.I. for domestic mass surveillance and mass propaganda\" seemed \"entirely illegitimate\" to him. He added that A.I.-automated weapons could greatly increase the risks \"of democratic governments turning them against their own people to seize power.\"\n\nIn contract negotiations, the Defense Department pushed back against Anthropic, saying it would use A.I. in accordance with the law, according to people with knowledge of the conversations.\n\nOn Sunday, The Wall Street Journal reported that Anthropic's technology had been used in January in the U.S. military operation to capture Venezuela's president, Nicolás Maduro, and that Anthropic employees had raised concerns with Palantir about the role its technology played. Mr. Hegseth and others in the Pentagon were angered by what they saw as Anthropic's resistance, current and former officials briefed on the matter said.\n\nA Pentagon official said a \"senior executive\" from Anthropic had asked a senior official from Palantir if Anthropic's A.I. model had been used in the Maduro raid. The Pentagon official said the exchange had alarmed Palantir because it implied that Anthropic might disapprove of its model's being used in the operation. As a result, the Palantir executive contacted the Pentagon, the Pentagon official said.\n\nTwo people close to Anthropic said only one employee had raised questions about the Maduro operation with a Palantir employee in a routine meeting. Anthropic said it had \"not discussed this with, or expressed concerns to, any industry partners outside of routine discussions on strictly technical matters.\"\n\nPalantir did not respond to a request for comment.\n\nIf Anthropic is categorized as a supply chain risk by the Defense Department, it will find itself in rare company. The designation is typically used only on firms that do business with China or take supplies from China.\n\nGiven that Claude and other Anthropic products are widely integrated across the department, it would be difficult to replace the technology, two officers who have used the technology said. Google's and OpenAI's A.I. contracts with the Pentagon are for unclassified systems.\n\nOn Feb. 9, OpenAI said in a blog post that it was expanding its work with the Pentagon. \"It is important for the United States and other democratic countries to understand how, with the proper safeguards, AI can help protect people, deter adversaries, and prevent future conflict,\" OpenAI wrote.\n\nGoogle and xAI did not respond to requests for comment.\n\nDespite the public fighting, negotiations between the Defense Department and Anthropic were continuing this week, two defense officials said.\n\nTyler Pager contributed reporting from Washington."
  },
  {
    "source": "DNyuz",
    "company": "Anthropic",
    "title": "Defense Dept. and Anthropic Square Off in Dispute Over A.I. Safety",
    "date": "2026-02-18T23:38:21Z",
    "url": "https://dnyuz.com/2026/02/18/defense-dept-and-anthropic-square-off-in-dispute-over-a-i-safety/",
    "content": "For months, the Department of Defense and the artificial intelligence company Anthropic have been negotiating a contract over the use of A.I. on classified systems by the Pentagon.\n\nThis week, those discussions erupted in a war of words.\n\nOn Monday, a person close to Defense Secretary Pete Hegseth told Axios that the Pentagon was \"close\" to declaring the start-up a \"supply chain risk,\" a move that would sever ties between the company and the U.S. military. Anthropic was caught off guard and internally scrambled to pinpoint what had set off the department, two people with knowledge of the company said.\n\nAt the heart of the fight is how A.I. will be used in future battlefields. Anthropic told defense officials that it did not want its A.I. used for mass surveillance of Americans or deployed in autonomous weapons that had no humans in the loop, two people involved in the discussions said.\n\nBut Mr. Hegseth and others in the Pentagon were furious that Anthropic would resist the military's using A.I. as it saw fit, current and former officials briefed on the discussions said. As tensions escalated, the Department of Defense accused the San Francisco-based company of catering to an elite, liberal work force by demanding additional protections.\n\nThe disagreement underlines how political the issue of A.I. has become in the Trump administration. President Trump and his advisers want to expand technology's use, reducing export restrictions on A.I. chips and criticizing state regulations that could be perceived as inhibitors to A.I. development. But Anthropic's chief executive, Dario Amodei, has long said A.I. needs strict limits around it to prevent it from potentially wrecking the world.\n\nEmelia Probasco, a senior fellow at Georgetown's Center for Security and Emerging Technology, said it was important that the relationship between the Pentagon and Anthropic not be doomed.\n\n\"There are war fighters using Anthropic for good and legitimate purposes, and ripping this out of their hands seems like a total disservice,\" she said. \"What the nation needs is both sides at the table discussing what can we do with this technology to make us safer.\"\n\nIn a statement, Sean Parnell, the Pentagon spokesman, said the department's relationship with Anthropic was \"being reviewed.\"\n\n\"Our nation requires that our partners be willing to help our war fighters win in any fight,\" he said. \"Ultimately, this is about our troops and the safety of the American people.\"\n\nAnthropic said it was committed to using its A.I. to support U.S. national security and was \"having productive conversations, in good faith, with the Department of War on how to continue that work and get these complex issues right.\"\n\nA Pentagon official praised the other A.I. companies that have been working on the Pentagon's unclassified systems. The official said the department had an agreement in place for one of the firms to begin work on the classified system, replacing Anthropic.\n\nThe Defense Department has used Anthropic's technology for more than a year as part of a $200 million A.I. pilot program to analyze imagery and other intelligence data and conduct research. Google, OpenAI and Elon Musk's xAI are also part of the program. But Anthropic's A.I. chatbot, Claude, was the most widely used by the agency -- and the only one on classified systems -- thanks to its integration with technology from Palantir, a data analytics company that works with the federal government, according to defense officials with knowledge of the technology.\n\n(The New York Times has sued OpenAI and Microsoft, accusing them of copyright infringement of news content related to A.I. systems. The companies have denied those claims.)\n\nThis year, Anthropic was set to formally sign a contract with the Department of Defense. That was when the company asked for limits to how its A.I. tools could be deployed.\n\nAnthropic has long been more vocal than other A.I. companies on safety issues. In a podcast interview in 2023, Dr. Amodei said there was a 10 to 25 percent chance that A.I. could destroy humanity. Internally, the company has strict guidelines that bar its technology from being used to facilitate violence.\n\nIn January, Dr. Amodei wrote in an essay on his personal website that \"using A.I. for domestic mass surveillance and mass propaganda\" seemed \"entirely illegitimate\" to him. He added that A.I.-automated weapons could greatly increase the risks \"of democratic governments turning them against their own people to seize power.\"\n\nIn contract negotiations, the Defense Department pushed back against Anthropic, saying it would use A.I. in accordance with the law, according to people with knowledge of the conversations.\n\nOn Sunday, The Wall Street Journal reported that Anthropic's technology had been used in January in the U.S. military operation to capture Venezuela's president, Nicolás Maduro, and that Anthropic employees had raised concerns with Palantir about the role its technology played. Mr. Hegseth and others in the Pentagon were angered by what they saw as Anthropic's resistance, current and former officials briefed on the matter said.\n\nA Pentagon official said a \"senior executive\" from Anthropic had asked a senior official from Palantir if Anthropic's A.I. model had been used in the Maduro raid. The Pentagon official said the exchange had alarmed Palantir because it implied that Anthropic might disapprove of its model's being used in the operation. As a result, the Palantir executive contacted the Pentagon, the Pentagon official said.\n\nTwo people close to Anthropic said only one employee had raised questions about the Maduro operation with a Palantir employee in a routine meeting. Anthropic said it had \"not discussed this with, or expressed concerns to, any industry partners outside of routine discussions on strictly technical matters.\"\n\nPalantir did not respond to a request for comment.\n\nIf Anthropic is categorized as a supply chain risk by the Defense Department, it will find itself in rare company. The designation is typically used only on firms that do business with China or take supplies from China.\n\nGiven that Claude and other Anthropic products are widely integrated across the department, it would be difficult to replace the technology, two officers who have used the technology said. Google's and OpenAI's A.I. contracts with the Pentagon are for unclassified systems.\n\nOn Feb. 9, OpenAI said in a blog post that it was expanding its work with the Pentagon. \"It is important for the United States and other democratic countries to understand how, with the proper safeguards, AI can help protect people, deter adversaries, and prevent future conflict,\" OpenAI wrote.\n\nGoogle and xAI did not respond to requests for comment.\n\nDespite the public fighting, negotiations between the Defense Department and Anthropic were continuing this week, two defense officials said.\n\nTyler Pager contributed reporting from Washington.\n\nSheera Frenkel is a reporter based in the San Francisco Bay Area, covering the ways technology affects everyday lives with a focus on social media companies, including Facebook, Instagram, Twitter, TikTok, YouTube, Telegram and WhatsApp."
  },
  {
    "source": "cnBeta.COM",
    "company": "Anthropic",
    "title": "Anthropic预计2027年向亚马逊、谷歌、微软分成最高达64亿美元 - cnBeta.COM 移动版",
    "date": "2026-02-18T11:19:34Z",
    "url": "http://m.cnbeta.com.tw/view/1550338.htm",
    "content": "根据这家初创公司近期最乐观的预测，Anthropic 预计在 2029 年之前，为在亚马逊、谷歌、微软的云服务器上运行其 Claude AI，至少支付800 亿美元。而这几家科技巨头从 Anthropic 获利的方式不止一种：如果它们的客户购买了 Anthropic 的 AI 服务，巨头们还能从中抽取收入分成。\n\n这一收入来源正快速增长。根据公司披露信息，2024 年 Anthropic 仅向云服务商支付约130 万美元的 AI 销售分成。但据《The Information》对 Anthropic 最乐观的销售及营销支出预测（其中包含云服务商分成）及过往支付记录的分析，这一金额在去年预计升至约3.6 亿美元，今年为19 亿美元，明年将达到64 亿美元。\n\n这些分成能在一定程度上反映出 Anthropic 如何激励云合作伙伴向自身客户推广其 AI 服务，使其为己所用。例如，微软已鼓励 Azure 云部门的销售人员推销 Anthropic 的 AI 模型，并表示这类销售额将与微软自有软件、OpenAI 模型一样计入业绩指标。\n\n这类预估分成（也被称为收入分成或合作伙伴利润分成）对 Anthropic 意义重大。根据公司过往财务披露，这笔费用约占其同期总营收的十分之一。\n\n另有知情人士透露，Anthropic 通过亚马逊渠道销售 AI 所获毛利中，约50% 流向了亚马逊。该毛利是指 Anthropic 在扣除亚马逊云服务器运行成本后，通过亚马逊销售 AI 所获得的收入。\n\n亚马逊云科技（AWS）发言人拒绝就毛利分成比例置评，仅表示：\"AWS 与 Anthropic 已建立独特的合作伙伴关系，我们是 Anthropic 的主要云服务商与主要训练合作伙伴。随着双方携手推动 AI 技术边界拓展，这一合作正持续深化。\"\n\n据一位了解相关协议的谷歌员工透露，谷歌通常会在扣除基础设施成本后，从合作伙伴软件转售净收入中抽取20%-30% 的分成。目前尚不清楚谷歌从转售 Anthropic AI 服务中具体分成比例。\n\n微软于去年 11 月成为 Anthropic 的云服务商，并承诺向其投资 50 亿美元。目前外界尚不清楚 Anthropic 向微软支付的毛利或收入分成比例。\n\n知情人士称，Anthropic 高管曾表示，与三大云厂商同时合作，使其相比 OpenAI 更具优势。OpenAI 主要通过微软及直销方式销售 AI，而三大云厂商能将 Anthropic 的模型触达其海量企业客户。\n\n对云厂商的价值\n\n对云服务商而言，Anthropic、OpenAI 是标杆级 AI 客户，能够吸引其他 AI 初创公司使用其服务，而 Anthropic 的云业务仍有更多份额可供争夺。\n\n据《The Information》对其财务预测的分析，截至去年夏天，Anthric 的大部分收入（今年预计总收入最高达180 亿美元）来自直接向客户销售 AI，而非通过云服务商转售。\n\n知情人士称，为支撑直销业务，Anthropic 目前主要使用 AWS 服务。但未来，其直销业务（第一方销售）的支撑成本可能会在三大云厂商与自有数据中心之间分摊。\n\n竞争对手 OpenAI 也会向长期支持者微软分成。\n\n知情人士透露，OpenAI 将总营收的20% 支付给微软；根据近期重新谈判的合作条款，分成将更多集中在 2032 年之前的年份，以减轻对 OpenAI 现金流的影响。公司预计今明两年总计支付超过130 亿美元分成，主要流向微软。\n\nAnthropic、OpenAI 的分成模式沿袭了云厂商转售第三方产品收取佣金的行业惯例。例如，用户通过 Azure 云购买 Databricks 软件时，微软会获得相应费用。\n\n云厂商还通过其他方式从 Anthropic 获利：除了收取800 亿美元的模型运行费用外，Anthropic 还需支付模型训练成本。公司预计到 2029 年，该项支出总额最高将达1000 亿美元。除英伟达芯片外，Anthropic 同时使用亚马逊与谷歌自研芯片 -- -- AWS Trainium 与张量处理单元（TPU）。\n\n一位知情人士称，截至去年年底，Anthropic 在部分财务披露中已不再单独列示向云转售合作伙伴支付的分成金额，而是将其与其他销售及营销费用合并统计。\n\n根据公司冬季预测，Anthropic 今年销售及营销支出最高将达28 亿美元，明年升至90 亿美元。据《The Information》测算，其中转售合作伙伴分成今年预计为19 亿美元，明年为64 亿美元。\n\n而在去年夏季的预测中，AI 转售分成今年为16 亿美元，明年约44 亿美元。"
  },
  {
    "source": "WebProNews",
    "company": "Anthropic",
    "title": "Anthropic Clashes with Pentagon Over Claude AI Military Use",
    "date": "2026-02-16T00:54:44Z",
    "url": "https://www.webpronews.com/anthropic-clashes-with-pentagon-over-claude-ai-military-use/",
    "content": "Clash Over Code: Anthropic's Tense Tug-of-War with the Pentagon on AI Deployment\n\nIn the high-stakes world of artificial intelligence, where innovation meets national security, a brewing dispute between AI startup Anthropic and the U.S. Department of Defense has captured the attention of tech executives and policymakers alike. Reports indicate that Anthropic, the company behind the advanced language model Claude, is locked in negotiations with the Pentagon over the military's use of its technology. This isn't just a contractual spat; it's a fundamental clash over how powerful AI tools should be wielded in defense operations, raising questions about ethics, safety, and the balance between technological progress and potential risks.\n\nThe friction stems from Anthropic's stringent terms of service, which prohibit the use of Claude in ways that could cause harm, including military applications. According to a recent article in TechCrunch, the Pentagon has been pushing for broader access to Claude for tasks like data analysis and strategic planning, but Anthropic is resisting, citing concerns over misuse. Insiders familiar with the discussions describe heated exchanges, with Anthropic executives emphasizing their commitment to \"constitutional AI\" principles that prioritize safety and alignment with human values. This standoff highlights a broader tension in the AI industry, where companies must navigate lucrative government contracts while adhering to self-imposed ethical guardrails.\n\nAnthropic, founded by former OpenAI researchers Dario and Daniela Amodei, has positioned itself as a leader in responsible AI development. Claude, their flagship model, is designed with built-in safeguards to prevent harmful outputs, such as generating instructions for weapons or hate speech. The company's approach contrasts sharply with more permissive models from competitors, making it both a darling of safety advocates and a potential roadblock for entities seeking unrestricted AI capabilities.\n\nPentagon's Push for AI Edge in Modern Warfare\n\nThe Defense Department's interest in Claude is part of a larger strategy to integrate AI into military operations. With adversaries like China advancing their own AI technologies, the U.S. military sees tools like Claude as essential for maintaining a competitive advantage. Tasks could include analyzing satellite imagery, simulating battle scenarios, or even aiding in cybersecurity defenses -- applications that, while not directly combative, tread close to Anthropic's red lines.\n\nA source close to the negotiations, speaking on condition of anonymity, told reporters that the Pentagon views Anthropic's restrictions as overly cautious, potentially hindering national security efforts. This perspective aligns with recent statements from defense officials. For instance, in a Bloomberg report from last October, Pentagon spokespeople outlined plans to accelerate AI adoption, emphasizing the need for flexible partnerships with tech firms. The article detailed how the department is allocating billions toward AI initiatives, underscoring the urgency behind their pursuit of Claude.\n\nYet, Anthropic's hesitation isn't unfounded. The company has publicly committed to avoiding military entanglements that could lead to autonomous weapons or other dystopian outcomes. In their own blog posts and white papers, Anthropic outlines a framework for \"scalable oversight,\" ensuring that AI systems remain under human control. This philosophy was echoed in a 2024 interview with Dario Amodei, where he warned against the unchecked militarization of AI, drawing parallels to the nuclear arms race.\n\nEthical Dilemmas in AI-Military Collaborations\n\nDelving deeper, this dispute exposes the ethical minefield of AI in defense. Anthropic's terms explicitly ban uses that \"promote violence or harm,\" a clause that the Pentagon reportedly wants relaxed for non-lethal applications. But where does one draw the line? Analysts point out that even analytical tools could indirectly support combat operations, blurring the boundaries.\n\nRecent developments in the sector amplify these concerns. Just this week, a Reuters piece highlighted ongoing debates within the U.S. military about AI ethics, noting that while the Pentagon has adopted guidelines for responsible AI use -- such as those outlined in a 2020 policy memo -- implementation remains inconsistent. The article cited experts who argue that partnerships with cautious firms like Anthropic could set a positive precedent, forcing the military to prioritize safety.\n\nOn X (formerly Twitter), discussions have erupted among tech insiders. A thread from AI researcher Timnit Gebru critiqued the potential for AI to exacerbate biases in military decision-making, linking to studies showing how algorithms can perpetuate errors in high-stakes environments. Meanwhile, defense tech enthusiasts argue that restricting access to models like Claude could leave the U.S. vulnerable, pointing to China's state-backed AI programs as a counterpoint.\n\nAnthropic isn't alone in this stance. Competitors like OpenAI have also imposed limits on military uses, though with varying degrees of enforcement. A 2023 New York Times investigation revealed how OpenAI navigated similar pressures, ultimately allowing some defense-related applications while maintaining bans on weapons development. Anthropic, however, appears more resolute, perhaps bolstered by its $4 billion valuation and backing from investors like Amazon and Google.\n\nNegotiations and Potential Outcomes\n\nAs talks continue, both sides are exploring compromises. Sources indicate that Anthropic might agree to a customized version of Claude with enhanced monitoring features, allowing Pentagon use under strict oversight. This could involve real-time audits or \"kill switches\" to prevent misuse, aligning with Anthropic's safety-first ethos.\n\nThe financial incentives are significant. Government contracts could provide Anthropic with substantial revenue, helping fund further research. Yet, accepting such deals risks alienating the company's core supporters in the AI safety community. A recent post on X from Effective Altruism forums debated this very issue, with users warning that military involvement could undermine Anthropic's mission.\n\nBroader industry watchers see this as a test case for AI governance. In a Washington Post analysis published earlier this month, columnists explored how similar disputes might shape future regulations. The piece referenced the Biden administration's executive order on AI, which calls for safety standards in high-risk applications, potentially giving Anthropic leverage in negotiations.\n\nImplications for AI Innovation and Regulation\n\nLooking ahead, the outcome of this disagreement could influence how other AI firms engage with government entities. If Anthropic holds firm, it might encourage a wave of ethical clauses in AI contracts, pressuring even aggressive players to adopt similar measures. Conversely, a concession could open the floodgates for militarized AI, raising alarms among international watchdogs.\n\nExperts like those at the Center for a New American Security have weighed in, suggesting in a 2024 report that balanced collaborations are possible. The report, linked here, advocates for \"human-centered\" AI in defense, where tools augment rather than replace human judgment.\n\nAnthropic's position also reflects growing scrutiny of AI's societal impact. Recent scandals, such as biased facial recognition in law enforcement, have heightened calls for accountability. In this context, the company's standoff with the Pentagon isn't just about one model; it's about setting norms for an industry on the cusp of transformative power.\n\nVoices from the Front Lines\n\nInterviews with former defense officials provide additional insight. One retired general, speaking to TechCrunch in the same article that broke the story, expressed frustration with AI companies' \"ivory tower\" attitudes, arguing that real-world security demands flexibility. On the flip side, AI ethicists like those at the Alan Turing Institute praise Anthropic's caution, noting in a recent blog that unchecked military AI could lead to escalation in global conflicts.\n\nPublic sentiment, gauged from X trends, leans toward supporting Anthropic. Hashtags like #AIEthics and #NoMilitaryAI have gained traction, with users sharing articles and petitions urging tech firms to resist defense contracts.\n\nPath Forward Amid Uncertainty\n\nAs negotiations drag on, the tech world watches closely. Anthropic's leaders have hinted at broader principles in play, potentially influencing their upcoming models. Meanwhile, the Pentagon continues to court other AI providers, diversifying its options to avoid over-reliance on any single firm.\n\nThis episode underscores the intricate dance between innovation and responsibility in AI. By standing its ground, Anthropic may not only protect its values but also shape the future trajectory of AI in sensitive domains. Whether this leads to a breakthrough agreement or a prolonged impasse remains to be seen, but the ripples will undoubtedly extend far beyond the negotiating table.\n\nIn the end, this conflict serves as a microcosm of the challenges facing the AI sector. As tools like Claude become integral to decision-making across industries, ensuring they serve humanity's best interests -- without compromising security -- will require ongoing dialogue, innovation, and perhaps a dose of compromise from all involved parties. The resolution could set precedents that define AI's role in society for decades to come."
  },
  {
    "source": "WebProNews",
    "company": "Anthropic",
    "title": "Anthropic's Super Bowl Gambit Pays Off: How a Single Ad Catapulted Claude's Maker Past AI Rivals in the Attention Economy",
    "date": "2026-02-14T09:19:15Z",
    "url": "https://www.webpronews.com/anthropics-super-bowl-gambit-pays-off-how-a-single-ad-catapulted-claudes-maker-past-ai-rivals-in-the-attention-economy/",
    "content": "The Seattle Seahawks may have hoisted the Lombardi Trophy on Super Bowl Sunday, but in the parallel arena of artificial intelligence supremacy, Anthropic emerged as a clear winner. According to data analyzed by BNP Paribas and reported by CNBC, the Claude-maker saw visits to its website jump 6.5% following its high-profile Super Bowl advertisement, while daily active users surged 11% in the post-game period -- outperforming every other AI company that vied for consumer attention during the most-watched television event of the year.\n\nThe numbers represent more than a marketing triumph. They signal a maturing AI industry where consumer brand recognition is becoming as critical as model performance benchmarks, and where the battle for mainstream adoption is increasingly being fought not in research labs but in living rooms across America.\n\nA Data-Driven Victory in a Crowded Field\n\nBNP Paribas analysts tracked web traffic and user engagement metrics across multiple AI companies that advertised during the Super Bowl broadcast, and Anthropic's performance stood out decisively. The 6.5% jump in site visits was notable on its own, but the 11% increase in daily active users told an even more compelling story -- suggesting that Anthropic's ad didn't just generate curiosity clicks but actually converted viewers into engaged users who downloaded or logged into the Claude platform.\n\nThe Super Bowl has long been considered the ultimate proving ground for brand advertising, with 30-second spots commanding upwards of $7 million to $8 million in recent years. For AI companies, many of which remain unfamiliar to the average consumer despite their enormous valuations, the game represented a unique opportunity to break through the noise and establish household-name recognition. As CNBC noted, multiple AI firms including OpenAI also ran advertisements during the broadcast, making the Super Bowl something of a proxy war for the broader AI consumer market.\n\nThe Stakes Behind the Spots: A $380 Billion Company Makes Its Case\n\nAnthropic's decision to invest heavily in Super Bowl advertising comes at a moment of extraordinary financial momentum for the company. As Forbes reported, Anthropic's valuation has soared to $380 billion, a staggering figure that has nearly doubled the personal fortunes of the company's seven billionaire co-founders. The valuation surge reflects both the explosive growth of the AI sector and investors' confidence in Anthropic's particular approach to building powerful yet safety-conscious AI systems.\n\nThat financial firepower gives Anthropic the resources to compete aggressively on the marketing front, a domain where OpenAI -- backed by Microsoft and bolstered by the viral success of ChatGPT -- has traditionally held the advantage in consumer awareness. The Super Bowl ad buy was a clear signal that Anthropic intends to close the brand-recognition gap, leveraging its war chest to put Claude in front of the widest possible audience. With seven co-founders now counted among the world's billionaires, the company has both the means and the motivation to pursue mainstream market dominance.\n\nOpenAI and the Broader AI Advertising Arms Race\n\nAnthropic was not the only AI company to make a Super Bowl play. OpenAI, the maker of ChatGPT and arguably the most recognized name in consumer AI, also ran advertising during the broadcast. Yet according to the BNP Paribas analysis cited by CNBC, Anthropic's post-game metrics outperformed its rivals, suggesting that the company's creative execution and messaging resonated more effectively with the Super Bowl audience.\n\nThe rivalry between Anthropic and OpenAI extends far beyond advertising. As Worth magazine has detailed, the two companies have found themselves on opposing sides of critical debates around AI regulation, with their philosophical differences shaping policy discussions in Washington and beyond. Anthropic, founded by former OpenAI executives Dario and Daniela Amodei, has generally positioned itself as more supportive of proactive safety regulation, while OpenAI has at times pushed back against measures it views as potentially stifling innovation. This regulatory clash adds a layer of ideological competition to what is already an intense commercial rivalry.\n\nThe Regulation Question Looms Large\n\nThe divergence between Anthropic and OpenAI on regulatory matters is not merely academic. As Worth reported, the two companies have taken markedly different approaches to engaging with policymakers, and those differences could have profound implications for how the AI industry is governed in the years ahead. Anthropic has consistently emphasized its commitment to \"responsible scaling\" and has published detailed safety policies that outline the conditions under which it would slow or halt the development of increasingly powerful models.\n\nOpenAI, by contrast, has undergone a complex corporate restructuring that has drawn scrutiny from regulators and industry observers alike. The tension between the two companies' approaches has become a defining fault line in AI policy debates, with lawmakers frequently citing both firms as they craft new legislation. For investors and industry insiders, the regulatory posture of each company represents a significant variable in assessing long-term risk and opportunity -- a factor that the Super Bowl advertising blitz did little to illuminate but everything to underscore in terms of the stakes involved.\n\nWhat the BNP Paribas Numbers Really Tell Us\n\nWall Street analysts have been paying increasingly close attention to consumer engagement metrics for AI companies, and the BNP Paribas data offers a rare window into the effectiveness of mass-market advertising in driving AI adoption. The 11% jump in daily active users for Anthropic is particularly significant because it suggests that the Super Bowl ad did more than generate fleeting interest -- it drove meaningful behavioral change among consumers who may have been aware of AI tools but had not yet committed to using one regularly.\n\nFor institutional investors, these numbers carry weight beyond their face value. User growth and engagement are among the most closely watched metrics in the technology sector, and the ability to convert a single advertising event into a sustained uptick in daily usage speaks to both the quality of Anthropic's product and the effectiveness of its go-to-market strategy. BNP Paribas, one of Europe's largest banking groups, has been expanding its coverage of AI-sector equities and private market valuations, and the Super Bowl analysis represents part of a broader effort to quantify the commercial traction of leading AI firms.\n\nBillionaire Co-Founders and the Wealth Effect of AI Dominance\n\nThe financial windfall for Anthropic's leadership team has been nothing short of extraordinary. According to Forbes, all seven of Anthropic's co-founders have achieved billionaire status, with their combined wealth nearly doubling as the company's valuation climbed to $380 billion. The wealth creation at Anthropic mirrors a broader trend across the AI sector, where a small number of companies have captured an outsized share of investor enthusiasm and capital flows.\n\nDario Amodei, Anthropic's CEO, and his sister Daniela Amodei, the company's president, have become two of the most prominent figures in the global technology industry. Their decision to leave OpenAI and found Anthropic in 2021, driven in part by concerns about AI safety, has been vindicated not only by the company's commercial success but by the growing consensus among policymakers and the public that safety-first approaches to AI development are both necessary and commercially viable. The $380 billion valuation places Anthropic among the most valuable private companies in history, rivaling or exceeding the peak valuations of firms like SpaceX and Stripe.\n\nThe Road Ahead: From Super Bowl Buzz to Sustained Dominance\n\nThe question now facing Anthropic is whether the Super Bowl momentum can be sustained. History suggests that advertising-driven spikes in user engagement tend to fade quickly unless they are reinforced by product quality and continued marketing investment. However, Anthropic has several factors working in its favor: a rapidly improving Claude model that has drawn praise from developers and enterprise customers alike, a massive capital base that enables continued investment in both research and marketing, and a regulatory positioning that may prove advantageous as governments around the world move to impose new rules on AI development and deployment.\n\nFor the broader AI industry, Anthropic's Super Bowl success is a harbinger of things to come. As AI tools become increasingly central to how consumers work, communicate, and create, the companies that win the battle for brand recognition will enjoy compounding advantages in user acquisition and retention. The Super Bowl, once the exclusive domain of beer brands and automakers, has become a battleground for the defining technology of the era. Anthropic's 11% DAU surge suggests it is a battle the Claude-maker intends to win -- one living room at a time."
  },
  {
    "source": "WebProNews",
    "company": "Anthropic",
    "title": "Anthropic's $20 Billion Gambit: Inside the Mega-Round That Signals a New Era in AI Capital Formation",
    "date": "2026-02-09T19:07:32Z",
    "url": "https://www.webpronews.com/anthropics-20-billion-gambit-inside-the-mega-round-that-signals-a-new-era-in-ai-capital-formation/",
    "content": "In the annals of venture capital, there are fundraising rounds that merely reflect a company's trajectory -- and then there are rounds that reshape the entire calculus of how technology companies are financed. Anthropic's move to close a funding round approaching $20 billion falls squarely in the latter category, representing not just a vote of confidence in one AI startup, but a seismic shift in how the world's most ambitious artificial intelligence ventures attract and deploy capital.\n\nThe San Francisco-based AI safety company, founded by former OpenAI executives Dario and Daniela Amodei, has been in advanced discussions to raise what would be one of the largest private funding rounds in technology history, as first reported by TechCrunch. The round, which has drawn interest from a constellation of heavyweight investors, would catapult Anthropic's valuation into the stratosphere and provide the company with a war chest to compete in an increasingly capital-intensive race to build the most capable AI systems on the planet.\n\nA Fundraising Machine Unlike Any the Tech World Has Seen\n\nAnthropic's fundraising velocity has been nothing short of extraordinary. The company has raised billions of dollars over the past several years, with each successive round arriving faster and at higher valuations than the last. Amazon has been the company's most prominent backer, having committed up to $4 billion in investment, a deal that also secured Anthropic's use of Amazon Web Services as its primary cloud computing provider. Google, too, has invested heavily in the company, committing billions of its own in a series of transactions that underscored the strategic importance major cloud platforms place on aligning with frontier AI developers.\n\nThe latest round, approaching $20 billion, would dwarf all previous raises and place Anthropic in rarefied financial territory typically reserved for pre-IPO companies with established revenue streams and clear paths to profitability. According to TechCrunch, the round has attracted interest from both traditional venture capital firms and sovereign wealth funds, reflecting a broadening of the investor base willing to write nine- and ten-figure checks for AI companies they believe will define the next computing paradigm.\n\nThe Economics of Building Frontier AI Models\n\nTo understand why Anthropic needs -- and can attract -- capital at this scale, one must appreciate the staggering costs associated with training and deploying frontier AI models. The company's Claude family of models, which compete directly with OpenAI's GPT series and Google's Gemini, require enormous clusters of specialized chips, typically Nvidia's most advanced GPUs or custom accelerators, running for months at a time. A single training run for a state-of-the-art model can cost hundreds of millions of dollars, and the next generation of models is expected to push those costs well into the billions.\n\nBeyond training, there are the ongoing costs of inference -- actually running the models to serve customers. As Anthropic's Claude has gained traction among enterprise customers, developers, and consumers, the compute demands of serving millions of queries per day have grown exponentially. The company has been investing heavily in infrastructure, research talent, and safety research, all of which consume capital at rates that would have been unimaginable in previous technology cycles. The $20 billion round would provide Anthropic with the financial runway to pursue its most ambitious research goals while simultaneously scaling its commercial operations.\n\nStrategic Investors and the Cloud Computing Chess Match\n\nThe identity of Anthropic's investors tells a story that extends well beyond simple financial returns. Amazon's multi-billion-dollar investment was as much about securing a marquee AI customer for AWS as it was about capturing equity upside. Google's investment served a similar dual purpose, ensuring that Anthropic's models would be available through Google Cloud Platform. This dynamic -- in which cloud providers effectively subsidize AI research in exchange for computing contracts -- has become one of the defining features of the current AI investment cycle.\n\nThe $20 billion round reportedly includes participation from investors beyond the major cloud platforms, including venture capital firms such as Spark Capital, Salesforce Ventures, and others who have backed Anthropic in previous rounds. The involvement of sovereign wealth funds from the Middle East and Asia signals that the competition for allocations in top-tier AI rounds has gone global. For these investors, Anthropic represents one of a handful of companies with a credible shot at building artificial general intelligence -- or at least the most commercially valuable AI systems ever created.\n\nAnthropic's Unique Positioning in the AI Arms Race\n\nWhat distinguishes Anthropic from its competitors is not merely its technical capabilities but its founding ethos. Dario Amodei, the company's CEO, left OpenAI in 2021 along with several key researchers over disagreements about the pace and safety of AI development. Anthropic was founded with the explicit mission of building AI systems that are safe, beneficial, and interpretable. The company has published extensively on AI safety research, including work on constitutional AI, a technique for training models to be helpful, harmless, and honest.\n\nThis safety-first positioning has proven to be a commercial asset as much as a philosophical commitment. Enterprise customers -- particularly those in regulated industries such as finance, healthcare, and government -- have been drawn to Anthropic's emphasis on responsible development. Claude has earned a reputation for being particularly adept at following nuanced instructions and refusing to generate harmful content, qualities that make it attractive for deployment in sensitive business contexts. The company's ability to marry cutting-edge capability with a credible safety narrative has been central to its fundraising success.\n\nValuation Questions and the Path to Monetization\n\nA $20 billion fundraise naturally raises questions about valuation. While specific figures associated with the latest round have not been fully disclosed, previous reporting has suggested that Anthropic's valuation could reach $60 billion or more on a post-money basis. Such a valuation would make Anthropic one of the most valuable private companies in the world, trailing only a small number of peers including SpaceX and, of course, OpenAI, which has itself raised capital at valuations exceeding $100 billion.\n\nThe justification for these valuations rests on the assumption that frontier AI companies will capture enormous revenue streams as artificial intelligence becomes embedded in virtually every industry. Anthropic has been growing its commercial revenue rapidly, driven by enterprise API access to Claude, consumer-facing products, and partnerships with major technology companies. However, the gap between current revenue and the implied revenue necessary to justify a $60 billion-plus valuation remains substantial. Investors are effectively betting on a future in which AI becomes as fundamental to business operations as the internet itself -- a bet that carries both extraordinary upside and meaningful risk.\n\nThe Broader Implications for AI Investment and Competition\n\nAnthropic's mega-round has implications that extend far beyond the company itself. It signals that the era of capital-intensive AI development is far from over and may, in fact, be accelerating. For competitors such as OpenAI, Google DeepMind, Meta's AI research division, and a growing cohort of well-funded startups including Mistral AI and xAI, Anthropic's ability to raise $20 billion in a single round raises the stakes considerably. The barriers to entry in frontier AI development are climbing rapidly, and companies without access to billions in capital will find it increasingly difficult to compete at the highest levels.\n\nFor the venture capital industry, rounds of this magnitude represent a fundamental shift in how the asset class operates. Traditional venture funds, which typically write checks in the tens or low hundreds of millions, are being joined -- and in some cases displaced -- by sovereign wealth funds, corporate strategic investors, and dedicated growth equity vehicles capable of deploying capital at scale. The concentration of capital in a small number of AI companies has sparked debate about whether the current investment cycle represents a rational response to a transformative technology or a speculative bubble that will eventually deflate.\n\nWhat Comes Next for Anthropic and the Industry\n\nWith $20 billion in fresh capital, Anthropic will have the resources to pursue its most ambitious research agenda yet. The company has signaled its intention to push the boundaries of model capability while continuing to invest heavily in safety research and interpretability. Dario Amodei has spoken publicly about his belief that AI systems could achieve transformative capabilities within the next several years, a timeline that would require sustained and significant investment in both research and infrastructure.\n\nThe round also positions Anthropic for a potential initial public offering, though the company has not publicly committed to a specific timeline. An IPO would provide liquidity for early investors and employees while subjecting the company to the scrutiny and disclosure requirements of public markets. Whether Anthropic chooses to go public in the near term or continue raising capital privately, one thing is clear: the company has established itself as one of the most consequential players in the AI industry, and its ability to attract capital at unprecedented scale is both a reflection of and a catalyst for the extraordinary ambitions that define this moment in technology history.\n\nAs the details of the round are finalized, the broader technology and investment communities will be watching closely. The outcome will not only determine Anthropic's trajectory but will set benchmarks for how the next generation of AI companies are funded, valued, and ultimately held accountable for delivering on the immense promise -- and managing the profound risks -- of artificial intelligence."
  },
  {
    "source": "m.163.com",
    "company": "Google DeepMind",
    "title": "每周工作100小时！谷歌DeepMind CEO揭秘：中国对手是字节跳动，断言谷歌是AI领域唯一全栈巨头_手机网易网",
    "date": "2026-01-22T11:46:40Z",
    "url": "https://m.163.com/dy/article/KJSR88EL05566ZHB.html",
    "content": "\"没有，从来都没有安心的时候。\"\n\n在 2026 年达沃斯世界经济论坛，DeepMind 创始人、Google DeepMind CEO 德米斯·哈萨比斯，用这句话形容过去三到四年的谷歌。\n\n外界一度流行的\"谷歌慢半拍\"的言论，在他看来是一个彻底的误解。事实上，在这段时间里，谷歌的 AI 团队几乎一直处于红色警报状态。他本人长期保持着每周 100 小时、一年 50 周的工作强度，把一家万亿美元体量的科技巨头，硬生生拉回到创业公司的战时节奏。\n\n也正是在这样的状态下，谷歌迎来了 Gemini 3 的发布，被哈萨比斯视为\"重回行业最前沿\"的关键节点。\n\n在接受彭博社记者 Emily Chang 的专访时，他罕见地系统性拆解了当下几乎所有 AI 世界的核心争议：\n\n* 谷歌是否真的掉队？\n\n* 中国 AI 是否构成威胁？\n\n* Transformer 和大模型是否已经走到尽头？\n\n* AGI 会在什么时候到来？\n\n* 当工作不再必要，人类该如何寻找意义。\n\n在哈萨比斯看来，过去十年，现代人工智能产业所依赖的关键突破，比如 Transformer 架构、深度强化学习、AlphaGo 背后的技术体系，几乎都诞生于谷歌与 DeepMind。他高度赞扬谷歌深厚的技术积累，他认为谷歌是唯一真正具备 AI 全栈能力的公司，其真正的问题在于能否把研究、算力、数据、硬件和产品，整合成一个统一体系。\n\n他还高度赞扬了谷歌的科学研究氛围，认为这正是他当初选择谷歌作为 Google DeepMind 归宿的原因。他还透露了他与拉里・佩奇、谢尔盖・布林如何高效分工。\n\n在访谈中，哈萨比斯还反复提到一个关键词：物理 AI（Physical AI），他承认物理 AI 确实正处于突破的临界点。\n\n在他的设想中，Gemini 从一开始就不是\"聊天模型\"，而是一个理解现实世界的多模态系统，是通往物理 AI 的入口。未来 Gemini 只会走向两个方向：\n\n* 随身的通用 AI 助手（眼镜、手机）\n\n* 真正能干活的机器人\n\n当然，他也给出了冷静判断，距离物理 AI 跨过临界点还有18 个月到两年的时间，在算法、数据、硬件等方面，都还差最后一段路。\n\n谈到中国 AI，哈萨比斯的态度异常冷静。\n\n他并不认为 DeepSeek 构成真正意义上的\"危机\"，也直言西方舆论夸大了其算力效率优势，这背后仍依赖西方模型蒸馏。在他看来，中国公司极其擅长追赶，但是否能率先打开下一代技术前沿，仍有待时间验证。而现代人工智能行业所依赖的约 90% 的突破性技术，都是谷歌研发的。\n\n但他特别表扬了字节跳动，给出了一个极具分量的评价：字节跳动距离技术前沿，大约只差 6 个月，而不是 1-2 年。\n\n这位把 AGI 当作毕生使命的科学家型 CEO，几乎反驳了 马斯克、杨立昆和伊利亚·苏茨克维的核心判断，同时给出了一个异常冷静 AGI 的时间表：2030 年，有 50% 的概率实现通用人工智能。\n\n哈萨比斯对 AGI 有自己一套严格的标准，即必须具备完整的人类认知能力，尤其是科学创新能力，不仅能解决问题，还要能提出真正重要的问题。这其中还有不小的差距。\n\n他认为距离 AGI，还需要一两项，最多不超过五项突破性技术，这可能体现在世界模型、持续学习的能力、稳定性表现、更强的推理能力或更长远的规划能力等方面。他高度认可现有的模型成就，认为在现有方法的基础上进行优化并扩大规模，或许就能实现 AGI。\n\n在访谈的最后，话题不可避免地走向未来社会：人工智能是否会取代人类的工作？围绕这一问题，哈萨比斯提出了一个有趣的概念\"后稀缺时代\"。\n\n在他看来，AI 带来的变革，无论规模还是速度，都会是工业革命的十倍，取代部分人类工作几乎是不可避免的结果。但他厘清一个概念，即人工智能本质上是一种终极的科学研究工具，就像更先进的望远镜和显微镜一样，是为科学服务的。\n\n在哈萨比斯的设想中，真正重要的并不是\"谁被取代\"，而是人类将因此获得前所未有的自由，把注意力转向那些更根本的问题。例如能源危机，如何实现核聚变，如何发现全新的材料体系。这些长期困扰人类的难题，或许正是在人工智能的加持下，才第一次显露出被彻底解决的可能性。\n\n这不仅是一场技术竞赛，更是一场文明级实验。真正的风险，在于当人类不再需要通过工作来定义自身价值时，我们是否已经准备好回答那个更深层的问题\"为什么而活？\"。\n\n在那个时代，人类或许需要的不只是更强的工程师，而是伟大的哲学家，去重新书写意义的来源。\n\n以下是哈萨比斯访谈实录，更多的谈话细节，欢迎来看：\n\n谷歌的红色警报期与\"王者归来\"\n\n主持人：和你上次来达沃斯相比，今年的感受有什么不同吗？Gemini 3 已经发布了，相关的消息我们也都听说了。我在内部甚至把这段时间称作\"红色警报\"。你觉得谷歌已经找回曾经的状态了吗？\n\n哈萨比斯：我不太确定这是不是该由我来评价，但我确实认为，过去这一年我们做得非常出色。我们付出了极其艰苦的努力，几乎是全力以赴，才让我们的技术和模型重新回到行业最前沿。\n\n尤其是 Gemini 3，以及我们在视觉和成像系统方面取得的一些关键突破，都在这一过程中起到了决定性作用。同时，我们也逐渐适应了如今这种节奏极快、需要迅速将成果推向市场的行业环境，让整个团队重新焕发出一种更接近初创公司的活力。\n\n主持人：你认为人们是否低估了谷歌，或是对谷歌有误解？\n\n哈萨比斯：或许是吧，我不确定。我的意思是，我们一直都拥有站在这个领域前沿的所有必备条件，显然我们在这方面有着悠久的积淀。\n\n我认为在过去十年里，谷歌和 Google DeepMind（谷歌深度思维）联手，创造出了现代人工智能行业所依赖的大部分突破性技术。比如 Transformer 架构，还有最知名的阿尔法狗背后的深度强化学习技术，这些都是我们的成果。\n\n我们还有覆盖数十亿用户的优质产品矩阵，从搜索引擎、电子邮箱到谷歌浏览器，这些产品天生就适合融入人工智能技术。\n\n问题只是如何将所有这些资源整合起来，以正确的方式统筹规划。过去几年我们已经做到了这一点，当然还有大量工作要做，但我们已经开始看到努力带来的成果了。\n\n主持人：如果你认为谷歌具备优势，你觉得这个优势有多大？能持续多久？\n\n哈萨比斯：在我看来，一切都始于研究。尤其是模型，要在各类基准测试中都保持行业前沿水平。这也是我们整合谷歌和 Google DeepMind（谷歌深度思维）后，首要聚焦的工作。双子座系列模型的进展，我们感到非常满意，当然这方面还有很多工作要推进。\n\n但我认为，我们是唯一一家拥有全栈能力的机构，从技术、战术、流程体系，到硬件、数据中心、云业务、前沿实验室，再到一众天生适配人工智能的优质产品，我们一应俱全。\n\n所以从根本的结构层面来说，我们本就该有出色的表现，而且我认为我们未来还有很大的提升空间。\n\n主持人：我想知道，作为前沿模型研发的负责人，日常工作状态是怎样的。我看到有报道说，你大多在凌晨一点到四点进行深度思考。确实是这样吧？谷歌内部的工作状态是否一直处于红色警报级别？你有没有感到安心的时候？\n\n哈萨比斯：没有，从来都没有安心的时候。我们设定红色警报级别，本是针对特殊情况的，但过去三四年，工作强度一直大到难以想象。每周工作一百小时，一年工作五十周，这已经是常态。\n\n在这个技术发展速度极快的领域，要想保持前沿，就必须这样做。行业的竞争异常激烈，可能是科技领域有史以来最白热化的阶段，而且背后的利害关系重大。通用人工智能的研发，无论从商业还是科学角度，都有着深远的意义。\n\n再加上我们正做的事情本身就令人振奋，而我的热情就是用人工智能探索科学难题，推动科学发现的进程。这是我一直以来的梦想，我毕生都在为人工智能发展的这一刻而努力。所以常常会因为有太多工作要做而难以入眠，但同时，也有太多令人兴奋的事情值得我们去探索、去推进。\n\n主持人：聊聊谷歌目前的内部文化吧，你们既要在这场竞争中取胜，又要保证研发的方向正确。拉里・佩奇和谢尔盖・布林 现在的参与度如何？你和他们沟通的频率高吗？他们现阶段的工作重点是什么？\n\n哈萨比斯：他们的参与度非常高。\n\n拉里・佩奇更多负责战略层面的工作，我会在董事会会议上见到他，去硅谷时也会和他碰面。\n\n谢尔盖・布林则更多参与具体工作，他甚至会亲自参与双子座研发团队的编码工作，尤其专注于算法细节方面。\n\n他们能对当下的人工智能研发充满热情，这对我们来说是好事，毕竟这是计算机科学发展史上一个无比重要的时刻，单从科学角度来看，这也是人类历史上的重要时刻，所以所有人都想亲身参与其中，这一点非常好。\n\n而对于我来说，我正努力融合各方优势，既保留初创企业快速推出产品、敢于冒险的活力，这一点我们已经看到了成效；又充分利用大企业的资源优势，同时还为长期研究和探索性研究保留空间，而非只聚焦于三个月内就能落地的产品相关研究，我认为只做短期研究是不明智的。\n\n我正努力平衡这些因素，过去一年，各项工作的推进都很顺利，而且我认为今年我们能做得更好。我对目前的发展态势非常满意，谷歌的技术提升和研发进展速度，在业内应该是最快的。\n\n物理 AI 的奇点时刻，\n\n还有 18 个月到两年的时间\n\n主持人：我知道你一直把重点放在推动科学进步上，比如发现新材料。我们也看到，现在 Gemini 已经被整合进人形机器人系统中。那么你觉得，人工智能在真实物理世界中的应用，是否即将迎来一个类似 AlphaFold 那样的突破性时刻？如果是的话，这个\"突破\"会以什么形式出现？\n\n哈萨比斯：是的，过去一年我花了大量时间深入研究机器人技术。我确实认为，我们正处在物理 AI 取得突破性进展的临界点。\n\n但我还是觉得，距离实现这一突破，我们还有 18 个月到两年的时间，还需要开展更多研究。\n\n不过我认为，双子座这样的基础模型，为我们指明了方向。从一开始，我们就将双子座设计为多模态模型，让它能够理解物理世界，背后有多重原因。\n\n其一，是为了打造通用智能助手，这种助手或许会搭载在智能眼镜或手机上，能够理解周边的现实世界。\n\n其二，当然就是为了应用在机器人领域。那么人工智能在物理世界的突破性时刻，究竟会是怎样的？我认为，那就是让机器人能在现实世界中稳定地完成各类有实际价值的任务。\n\n目前，仍有一些因素制约着这一目标的实现。\n\n一方面，算法还不够完善，需要提升鲁棒性，而且相较于实验室中仅处理数字信息的模型，机器人相关算法能依托的数据量更少，合成这类数据的难度也远高于数字数据。\n\n另一方面，硬件方面也仍有一些难题尚未解决，尤其是机械臂和机械手的研发。其实深入研究机器人技术后，你会对人类的手部结构产生全新的敬畏之心，至少我是这样。进化的设计精妙绝伦，人类的手在稳定性、力量和灵活性上的表现，很难被复刻。所以在我看来，要实现这一突破，还有不少环节需要完善，但目前已有很多令人振奋的进展。\n\n我们刚刚宣布与波士顿动力展开深度合作，他们研发的机器人非常出色，我们正将人工智能技术应用到汽车制造领域。\n\n接下来一年，我们会先推出原型机进行测试，或许一两年后，我们就能展示一些令人印象深刻的成果，并实现规模化应用。\n\nDeepSeek并不是重大危机，\n\n特别表扬字节跳动\n\n主持人：一年前，DeepSeek 模型的发布在西方引发了不小的震动，很多人把它视为一场潜在的危机。但一年过去了，局势似乎逐渐平稳下来，中国方面的节奏看起来也有所放缓。你对中国人工智能领域整体竞争格局的看法，有没有发生变化？\n\n哈萨比斯：没有，其实并没有改变。一开始我就不认为这是一场真正意义上的危机，我觉得西方当时的反应多少有些过度了。\n\nDeepSeek 的确是一个令人印象深刻的模型，它清楚地展现了中国科技公司的实力。\n\n如果看头部企业，比如字节跳动，我认为他们的能力非常强。在技术前沿的跟进速度上，他们可能只落后大约六个月，而不是一到两年。DeepSeek 正是这一点的体现。\n\n当然，围绕它的一些说法也被夸大了。比如关于算力使用效率的说法，并不完全准确，因为他们在研发过程中借鉴并依托了部分西方模型，也对顶尖模型的输出结果进行了微调，而不是完全从零开始独立训练。\n\n另外，还有一个关键问题目前仍然没有答案：那就是中国公司是否能够在跟进前沿的基础上，真正实现原创性的突破并引领下一代技术。他们在追赶方面确实非常擅长，而且能力正在快速提升，但到目前为止，还没有证明自己能够率先打开新的技术前沿。\n\nAGI 的时间表：2030 年\n\n有 50% 的可能实现 AGI\n\n主持人：是你为通用人工智能给出了定义，你也曾说过，到 2030 年，我们有 50% 的可能实现通用人工智能。这个时间规划是否依然不变？\n\n哈萨比斯：不变。\n\n主持人：通用人工智能对你而言，依然是一个有价值的研发目标吗？\n\n哈萨比斯：我认为是的，这个时间规划在我看来很合理，而且相较于一些人的预期，这个时间其实更充裕。\n\n但我对通用人工智能的评判标准非常高，它指的是一个具备人类所有认知能力的系统，显然我们目前离这个目标还有很大差距。这意味着，这类系统需要拥有科学创新能力，不仅能解决科学领域的猜想和难题，更要能率先提出研究假设和问题。任何一名科学家都清楚，找到正确的问题，往往比找到答案难得多。\n\n目前的人工智能系统显然还不具备这种能力，未来能否拥有，还未可知，我们也仍未明确实现这一能力需要哪些技术突破。比如持续学习能力，也就是在线学习能力，让系统能突破训练的局限，在现实世界中自主学习；还有稳定性，目前的系统在不同领域的表现参差不齐，而通用智能系统不该有这样的短板。在我看来，要打造通用人工智能系统，还有不少关键能力亟待突破。\n\n主持人：我们来聊聊技术和未来的发展趋势。Meta 首席科学家杨立昆（Yann LeCun）认为，仅凭 Transformer 架构和大模型，无法实现通用人工智能。你是否认同这一观点？如果这些技术走到了尽头，我们的研发方向会是什么？\n\n哈萨比斯：我不认同，我认为说这些技术走到尽头的观点显然是错误的，因为它们目前已经展现出了巨大的实用价值。但在我看来，这是一个实证问题，也是一个科学问题，仅凭这些技术是否能实现通用人工智能，尚无定论。\n\n我认为有 50% 的可能，只需在现有方法的基础上进行优化并扩大规模，就能实现通用人工智能，这是有可能的，而且我们也必须这样做。在我看来，这项研究是有价值的，因为至少这些大模型会成为最终通用人工智能系统的核心组成部分，唯一的问题只是，它是否是唯一的组成部分。\n\n我能想象，从现在到实现通用人工智能，我们还需要一两项，最多不超过五项突破性技术。\n\n比如世界模型，这是我一直提及的，我们也正在研发，目前我们的 GENI 系统就是最先进的世界模型（GENI 是 DeepMind 、Google 内部正在研发的一类世界模型（World Model）系统），我也直接参与了这项研发，我认为它至关重要。\n\n还有持续学习能力，以及打造性能稳定的系统，让系统不再出现这种领域间的表现失衡，真正的通用智能系统，不该有这样的问题。\n\n所以在我看来，人工智能还缺乏更强的推理能力、更长远的规划能力等多项关键能力。目前尚未确定的是，实现这些能力，是否需要新的架构或突破性技术，还是只需在现有基础上继续优化。而谷歌和 Google DeepMind（谷歌深度思维）的做法是，双管齐下，既全力研发新的技术，也持续优化并扩大现有技术的规模。\n\n主持人：OpenAI 联合创始人兼前首席科学家伊利亚・苏茨克维（Ilya Sutskever）认为，依靠扩大模型规模实现技术提升的时代即将结束。你是否认同这一观点？\n\n哈萨比斯：我不认同。他的原话大概是 \"我们重回研究的时代\"，我和伊利亚・苏茨克维是很好的朋友，我们在很多问题上的看法都一致，但在这一点上，我并不认同。\n\n我的观点是，我们从未离开过研究的时代，至少谷歌和 Google DeepMind（谷歌深度思维）一直如此。我们始终在研发领域投入巨资，而且我认为，整合后的谷歌和 Google DeepMind（谷歌深度思维），拥有业内最深厚、最广泛的研发团队。\n\n过去十年，现代人工智能行业所依赖的约 90% 的突破性技术，都是我们研发的，当然最知名的是 Transformer 架构，还有深度强化学习、阿尔法狗背后的各类强化学习技术，这些都是我们开创的。所以如果未来实现通用人工智能需要新的突破性技术，我相信，就像过去一样，我们依然会是这些技术的研发者。\n\n主持人：最后一个问题，埃隆・马斯克说我们已经进入了技术奇点，你是否认同？\n\n哈萨比斯：不认同，我认为这一说法为时过早。在我看来，技术奇点其实就是实现完全的通用人工智能，而我之前已经解释过，我们目前离这个目标还相去甚远。我相信我们最终能实现这一目标还有五年的时间，从实现通用人工智能的角度来看，其实并不长，但在那之前，我们还有大量的工作要做。\n\n人工智能就像更先进的\n\n望远镜和显微镜\n\n主持人：你是诺贝尔奖得主，我知道你一心想让人工智能推动科学研究的发展。如果未来人工智能本身取得了足以获得诺贝尔奖的科研发现，这个奖项该颁给谁？\n\n哈萨比斯：我认为还是该颁给人类。当然，这取决于人工智能是否是完全独立完成这项发现。\n\n目前来看，人工智能依然只是工具，在我眼中，它是终极的科学研究工具，就像更先进的望远镜和显微镜。人类一直都在制造工具，让自己能更好地探索自然世界，人类本质上就是会制造工具的物种，这也是人类与其他动物的区别，而工具也让人类拥有了超越自身的能力，计算机当然也属于这类工具，人工智能则是这种能力的终极体现。\n\n所以在我看来，人工智能一直都是推动科学研究的终极工具，而且在可预见的未来，科学研究都将是顶尖科学家与人工智能的合作成果：科学家提出富有创意的想法和研究假设，而人工智能作为强大的工具，助力提升数据处理、模式识别的效率，推动科学探索的进程。\n\nAI 是否会取代人？\n\n我们将迎来后稀缺时代\n\n主持人：谷歌是 Anthropic 人工智能公司的主要投资方，Anthropic 联合创始人兼 CEO 达里奥・阿莫迪 (Dario Amodei) 今天早些时候也来到了达沃斯。他预测，未来五年内，人工智能会取代 50% 的初级白领岗位，你是否认同这一观点？\n\n哈萨比斯：我不认同，我认为这一过程会耗时更久。今年，我们或许能看到这一趋势的初步显现，比如初级岗位和实习岗位可能会受到影响，但要实现大规模取代，我们还需要解决人工智能系统的稳定性问题。\n\n我把目前人工智能的这种不均衡表现称为\"锯齿型智能\"，在某些领域表现出色，在另一些领域却不尽如人意。如果想将一整项工作完全交由人工智能代理完成，而非像现在这样，仅让其作为辅助工具，就需要让系统在各方面都保持稳定的表现。如果一个系统完成一项工作的成功率只有 95%，那是远远不够的，必须能圆满完成整个任务，才能让人放心地将工作交托给它。\n\n所以在出现这种大规模的岗位变革前，我们还有大量工作要做，但这种变革最终一定会到来。当然，一旦实现通用人工智能，整个经济体系都会发生改变，这早已超出了岗位变革的范畴。如果我们能打造出真正的通用人工智能，而且方向正确，我们或许会进入一个后稀缺时代，解决世界上一些根本性的难题，比如能源问题。借助人工智能，研发出全新的清洁、可再生的近乎免费的能源，比如实现核聚变。还有新材料的研发，我认为在实现通用人工智能后的五到十年，我们会进入一个彻底改变的世界。\n\n主持人：不过，在进入后稀缺时代之前，人们对这一过渡阶段充满了焦虑。我是一位母亲，我知道你也有孩子。你最担心孩子们未来会面临什么？你会和他们聊些什么？会告诉他们未来即将到来的变化吗？我听到很多人说，大学毕业生未来的就业会非常困难。\n\n哈萨比斯：我倒不这么认为。我觉得我们即将进入一个变革的时代，就像工业革命那样，或许变革的速度会是工业革命的十倍，甚至难以想象。准确来说，变革的规模和速度都会是工业革命的十倍，影响力会是百倍。\n\n但我想对所有人说，变革的背后，蕴藏着巨大的机遇。而且我始终坚信人类的创造力，我们的适应能力极强，因为人类的思维具有极强的通用性。\n\n人类的大脑无比强大，我们的祖先以狩猎采集为生，而我们凭借这样的大脑构建了现代文明，所以我相信我们能再次适应新的时代。当然，这次的变革是前所未有的，因为它的速度太快了。以往，这样的重大变革往往需要一两代人的时间才能完成，而这次人工智能技术的变革，规模和影响力都极为巨大。\n\n但对于如今的孩子，我会鼓励他们熟练掌握这些新工具，像使用母语一样运用它们，这些工具几乎能赋予他们超能力。比如在创意艺术领域，借助人工智能，一个人或许能完成过去十个人的工作。这意味着，如果你富有创业精神，在游戏设计、电影制作等创意领域有想法，就能完成更多工作，也能比以往更容易地跻身这些行业，成为新锐人才。\n\n主持人：一些人主张暂停人工智能的研发，让监管政策跟上技术发展的步伐，也让社会有时间适应这些变化。如果在理想情况下，所有企业、所有国家都同意暂停研发，你是否会支持这一做法？\n\n哈萨比斯：我会支持。我也曾公开表达过我的期望，这也是我十五年来的梦想。我接触人工智能研究已有二十五年，我一直希望，当我们接近实现通用人工智能的这一关键节点时，全球的科研人员能展开科学层面的合作。\n\n我有时会设想，成立一个类似欧洲核子研究中心的国际人工智能研究机构，让全球最顶尖的人才携手合作，以极为严谨的科学方式，推进通用人工智能研发的最后阶段，同时让全社会参与其中，不仅是技术人员，还有哲学家、社会科学家、经济学家，共同探讨我们希望从这项技术中获得什么，以及如何让它造福全人类。这才是我们当下的核心议题。\n\n但显然，这需要国际社会的通力合作，因为即便只有一家企业、一个国家，甚至整个西方世界决定暂停研发，倘若没有全世界的共同参与，没有制定统一的最低标准，这一做法也毫无意义。而目前，国际合作面临着不小的阻碍，所以如果想以严谨的科学方式推进通用人工智能的最后研发，就必须改变当下的国际合作现状。\n\n主持人：如果到 2030 年我们实现了通用人工智能，而相关的监管政策尚未出台，我们是否注定会面临困境？\n\n哈萨比斯：我依然乐观地认为，全球顶尖的人工智能研发机构会充分沟通，至少在安全和安保协议等方面展开合作，目前这方面的合作已经有了不少进展。比如我们和人工智能公司 Anthropic 在这些领域的合作就十分紧密。\n\n如果国际层面的合作难以推进，这种行业内的同行合作就尤为必要。我和其他顶尖人工智能实验室的负责人关系都很不错，我认为，当利害关系足够重大时，大家会意识到问题的严重性和潜在的风险，而在未来两到三年，这一点会变得更加清晰。\n\n主持人：你当初本可以把 Google DeepMind（谷歌深度思维）卖给任何一家企业，而如今，这些研发人工智能的企业都在寻求大众的信任。尤其是在监管政策难以跟上技术发展速度的情况下，历史经验也证明了这一点。我们为什么该信任你？为什么你认为谷歌，也是你内心所认可的，是最值得我们信任的企业？毕竟人工智能的研发存在不小的风险。\n\n哈萨比斯：我认为，评判一家企业，要看它的实际行动，也要看参与相关研发的领导者的初衷。\n\n我选择谷歌作为 Google DeepMind（谷歌深度思维）的归宿，有多个原因，最主要的是，谷歌的创始人创立谷歌的初衷，是打造一家以科学研究为核心的企业。很多人都忘了，谷歌最初其实是一个博士研究项目，是拉里・佩奇和谢尔盖・布林 的研究成果。所以我和他们一见如故。\n\n拉里・佩奇主导了 Google DeepMind（谷歌深度思维）的收购，而谷歌的董事会成员也都是各行各业的顶尖人才，比如董事会主席约翰・轩尼诗是图灵奖得主，弗朗西斯・阿诺德是诺贝尔奖得主，这样的阵容在企业董事会中并不多见。所以谷歌的整体环境充满了科学氛围，企业的发展以科学研究和工程技术为核心，这一文化早已根深蒂固。而追求最高水平的科学研究，就意味着做事要严谨、深思熟虑，在所有领域都践行科学方法。\n\n我认为这不仅适用于技术研发，也适用于企业的运营管理。所以我们始终努力做到深思熟虑、负责任，尽可能掌控我们推向市场的技术。当然，我们不可能做到尽善尽美，因为人工智能是一项全新、复杂且具有变革性的技术，但如果出现问题，我们会尽快调整修正。\n\n最后我想说，谷歌想要为世界做的事情，也是我当初选择谷歌的原因之一。谷歌的使命是整合全球信息，让人人皆可访问并从中受益，我认为这是一个非常崇高的目标。而Google DeepMind（谷歌深度思维）的使命是破解智能的奥秘，并利用智能解决其他所有问题，这两个使命高度契合。人工智能与整合全球信息的工作本就相辅相成，谷歌的各类产品，从谷歌地图、电子邮箱到搜索引擎，都是对世界有实际价值的产品，人工智能能很自然地融入这些产品，为所有人的日常生活提供助力，我认为这是一件造福世界的事，能为此贡献力量，我感到很荣幸。\n\n主持人：试想一下，在后稀缺时代，人们不再需要工作，当你实现了所有的技术目标后，你个人打算如何度过时间？毕竟到那时，科研工作本身或许也能实现自动化了。\n\n哈萨比斯：如果真的到了那个阶段，我想利用人工智能探索物理学的极限。\n\n上学时，我最感兴趣的就是那些终极问题：现实的本质是什么？意识的本质是什么？费米悖论的答案是什么？（费米悖论是宇宙学和天体生物学中最经典的未解之谜，由美籍意大利物理学家、1938 年诺贝尔物理学奖得主恩里科・费米（Enrico Fermi） 在 1950 年提出，核心是 \"理论上的地外文明存在性\" 与 \"人类实际观测证据为零\" 的尖锐矛盾 ，其最经典的表述就是费米的一句反问：\"他们都在哪儿呢？\"）时间是什么？引力是什么？\n\n我很惊讶，很多人每天忙于生活，却从未思考过这些重大问题，而这些问题一直萦绕在我心头，迫切想要找到答案。我想借助人工智能，去探索所有这些问题，或许还能在人工智能的助力下，利用新的能源和材料技术，实现星际旅行。\n\n主持人：如果人们不再需要工作，我们还能找到生活的意义和目标吗？\n\n哈萨比斯：说实话，这一点比经济层面的问题更让我担忧。经济层面的问题，更多是一个政治问题：当人工智能为我们带来巨大的效益和生产力提升时，我们能否确保这些成果为全人类共享，这也是我一直坚信的理念。\n\n但更核心的问题是，很多人从工作和科研中获得生活的意义和目标，在新的时代，我们该如何找到这些？我认为，我们需要新一代伟大的哲学家，来帮助我们思考这个问题。或许未来，我们的艺术创作会更加精妙，我们的探索之旅会更加深远，就像如今我们所做的极限运动等非经济目的的事情一样，未来或许会有更多更小众、更有深度的这类活动。\n\n主持人：在场的所有人都想知道，自己该如何应对人工智能带来的变革。比如现在坐在达沃斯的会场里，十年后该如何自处？你认为，在场的人在看待人工智能这件事上，最容易犯的重大错误是什么？\n\n哈萨比斯：我想从两个方面来说。\n\n第一，对于年轻人和我们的孩子而言，唯一可以确定的是，未来会发生巨大的变化。所以在学习技能方面，要做好持续学习的准备，学会学习，才是最重要的能力。要能快速适应新环境，利用现有工具吸收新信息。\n\n第二，对于在场的企业首席执行官和商界人士而言，当下最重要的是，目前市场上有很多顶尖的人工智能模型和服务提供商，未来还会更多。要选择那些以正确方式研发人工智能的合作伙伴，与这些企业携手，共同打造我们所期望的人工智能未来。\n\n技术人的年度仪式感！ 年度盘点与趋势洞察 启动！\n\n《2025 年度盘点与趋势洞察》由 InfoQ 技术编辑组策划。覆盖大模型、Agent、具身智能、AI Native 开发范式、AI 工具链与开发、AI+ 传统行业等方向，通过长期跟踪、与业内专家深度访谈等方式，对重点领域进行关键技术进展、核心事件和产业趋势的洞察盘点。\n\n力求以体系化视角帮助读者理解年度技术演化的底层逻辑、创新方向与落地价值，并为新一年决策提供参考。内容将在 InfoQ 媒体矩阵陆续放出，欢迎大家持续关注。\n\n今日荐文"
  },
  {
    "source": "Fortune",
    "company": "Google DeepMind",
    "title": "Google DeepMind researcher David Silver leaves to launch his own AI startup | Fortune",
    "date": "2026-01-30T14:04:20Z",
    "url": "https://fortune.com/2026/01/30/google-deepmind-ai-researcher-david-silver-leaves-to-found-ai-startup-ineffable-intelligence/",
    "content": "David Silver, a well-known Google DeepMind researcher who played a critical role in many of the company's most famous breakthroughs, has left the company to form his own startup.\n\nSilver is launching a new startup called Ineffable Intelligence, based in London, according to a person with direct knowledge of Silver's plans. The company is actively recruiting AI researchers and is seeking venture capital funding, the person said.\n\nOriol Vinyals, Google DeepMind's senior vice president for research, informed staff of Silver's departure earlier this month, the person said. Silver had been on sabbatical in the months leading up to his departure and never formally returned to his DeepMind role.\n\nA Google DeepMind spokesperson confirmed Silver's departure in an emailed statement to Fortune. \"Dave's contributions have been invaluable and we're grateful for the impact he's had on our work at Google DeepMind,\" the spokesperson said.\n\nSilver could not immediately be reached for comment.\n\nIneffable Intelligence was formed in November 2025 and Silver was appointed a director of the company on January 16, according to documents filed with U.K. business registry Companies House.\n\nIn addition, Silver's personal webpage now lists his contact as Ineffable Intelligence and provides an ineffable intelligence email address, although it continues to state that he \"leads the reinforcement learning team\" at Google DeepMind.\n\nIn addition to his work at Google DeepMind, Silver is a professor at University College London. He continues to maintain that affiliation.\n\nSilver was one of DeepMind's first employees when the company was established in 2010. He knew DeepMind cofounder Demis Hassabis from university. Silver played an instrumental role in many of the company's early breakthroughs, including its landmark 2016 achievement with AlphaGo, demonstrating that an AI program could beat the world's best human players at the ancient strategy game Go.\n\nHe also was a key member of the team that developed AlphaStar, an AI program that could beat the world's best human players at the complex video game Starcraft 2, AlphaZero, which could play chess and shogi as well as Go at superhuman levels, and MuZero, which could master many different kinds of games better than people even though it started out without any knowledge of the game, including not knowing the games' rules.\n\nMore recently, he worked with the DeepMind team that created AlphaProof, an AI system that could successfully answer the International Mathematics Olympiad questions. He is also one of the authors on the 2023 research paper that debuted the Google's original Gemini family of AI models. Gemini has now Google's leading commercial AI product and brand.\n\nSiliver has told friends he wants to get back to the \"awe and wonder of solving the hardest problems in AI\" and sees superintelligence -- or AI that would be smarter than any human and potentially smarter than all of humanity -- the biggest unsolved challenge in the field, according to the person familiar with his thinking.\n\nSeveral other well-known AI researchers have also left established AI labs in recent years to found startups dedicated to pursuing superintelligence. Ilya Sutskever, the former chief scientist at OpenAI, founded a company called Safe Superintelligence (SSI) in 2024. That company has raised $3 billion in venture capital funding to date and is reportedly valued at as much as $30 billion. Some of Silver's colleagues who worked on AlphaGo, AlphaZero, and MuZero have also recently left to found Reflection AI, an AI startup that also says it is pursuing superintelligence. Meanwhile, Meta last year reorganized its AI efforts around a new \"Superintelligence Labs\" that is headed by former Scale AI CEO and founder Alexandr Wang.\n\nSilver is well-known for his work on reinforcement learning, a way of training AI models from experience rather than historical data. In reinforcement learning, a model takes an action, usually in a game or simulator, and then receives feedback on whether those actions are productive in helping it achieve a goal. Through trial and error over the course of many actions, the AI learns the best ways to accomplish the goal.\n\nThe researcher was often considered one of reinforcement learning's most dogmatic proponents, arguing it was the only way to create artificial intelligence that could one day surpass human knowledge.\n\nOn a Google DeepMind-produced podcast that was released in April, he said that large language models (LLMs), the type of AI responsible for most of the recent excitement about AI, were powerful, but they were also constrained by human knowledge. \"We want to go beyond what humans know and to do that we're going to need a different type of method and that type of method will require our AIs to actually figure things out for themselves and to discover new things that humans don't know,\" he said. He has called for a new \"era of experience\" in AI that will be based around reinforcement learning.\n\nCurrently, LLMs have a \"pretraining\" development phase that uses what is called unsupervised learning. They ingest vast amounts of text and learn to predict which words are statistically most likely to follow which other words in a given context. They then have a \"post-training\" development phase that does use some reinforcement learning, often with human evaluators looking at the model's outputs and giving the AI feedback, sometimes just in the form of a thumbs up or thumbs down. Through this feedback, the model's tendency to produce helpful outputs is boosted.\n\nBut this kind of training is ultimately dependent on what humans know -- both because it depends on what humans have learned and written down in the past in the pre-training phase and because the way LLM post-training does reinforcement learning is ultimately based on human preferences. In some cases, though, human intuition can be wrong or short-sighted.\n\nFor instance, famously, in move 37 of the second game of AlphaGo's 2016 match against Go world champion Lee Sedol, AlphaGo made a move that was so unconventional that all the human experts commenting on the game were sure it was a mistake. But it wound up later proving to be a key to AlphaGo winning that match. Similarly, human chess players have often described the way AlphaZero plays chess as \"alien\" -- and yet its counterintuitive moves often prove to be brilliant.\n\nIf human evaluators were passing judgments on such moves though in the kind of reinforcement learning process used in LLM post-training, they might give such moves a \"thumbs down\" because they look to human experts like mistakes. This is why reinforcement learning purists such as Silver say that to get to superintelligence, AI will not just have to get beyond human knowledge, it will need to discard it and learn to achieve goals from scratch, working from first principles.\n\nSilver has said Ineffable Intelligence will aim to build \"an endlessly learning superintelligence that self-discovers the foundations of all knowledge,\" the person familiar with his thinking said."
  },
  {
    "source": "WebProNews",
    "company": "Google DeepMind",
    "title": "DeepMind's Ibrahim: AI's High-Stakes Classroom Revolution",
    "date": "2026-01-25T06:25:32Z",
    "url": "https://www.webpronews.com/deepminds-ibrahim-ais-high-stakes-classroom-revolution/",
    "content": "Google DeepMind Chief Operating Officer Lila Ibrahim posed a pivotal question in a recent CNBC appearance: Will AI make or break education? Her insights, drawn from ongoing pilots and research, suggest the technology holds transformative power if wielded responsibly. Speaking on \"The Tech Download,\" Ibrahim highlighted opportunities alongside risks, urging immediate education on ethical AI use (CNBC).\n\nIbrahim's perspective stems from her extensive background in education technology. As former president and COO of Coursera, she scaled online learning to millions worldwide. Now at DeepMind since 2018, she oversees operations, ethics, and partnerships, chairing initiatives on AI responsibility. Her personal drive traces to her sister with cerebral palsy, who earned two master's degrees despite barriers, and her neurodiverse twin daughters, fueling advocacy for personalized AI tutoring (CNBC).\n\nDeepMind's efforts emphasize pedagogy over rote answers. \"To realize this potential, AI learning tools must help learners cultivate deep understanding, not just deliver quick answers. They must ignite curiosity and engage learners in a process of discovery -- not offer a shortcut,\" Ibrahim wrote in a Google blog post (Google Blog).\n\nNorthern Ireland Pilot Reveals Real-World Gains\n\nA six-month trial with Northern Ireland's Education Authority C2k program equipped 100 teachers with Gemini and Google Workspace tools. Participants reported saving up to 10 hours weekly on administrative tasks, redirecting time to student interaction. Over 600 unique use cases emerged, from drafting parent letters to generating tailored lessons. Chris Lowe, Head of ICT at Ashfield Boys' High School, said, \"The time I saved using Gemini fundamentally allows me to do the job I want to do -- and that is to teach\" (Google DeepMind Blog).\n\nAlistair Hamill, Head of Geography at Lurgan College, leveraged NotebookLM's MindMap for a neurodivergent student, visualizing complex material. An ICT coordinator at Rowandale Primary called Gemini \"game-changing\" for personalized lessons and creative prompts. Damian Harvey, Interim Head of C2k, stressed partnership: \"I believe educators need to embrace the opportunity\" (Google DeepMind Blog).\n\nIbrahim, visiting the schools, praised teachers' ingenuity: \"I was struck by the teachers' ingenuity and the efficiencies they were able to achieve\" (Google DeepMind Blog).\n\nTools Tailored for Learning Science\n\nDeepMind's LearnLM family powers education-focused models integrated into Gemini, NotebookLM, and Google Classroom. These enable guided conversations, quizzes, and immersive experiences grounded in pedagogy. In a Les Echos interview, Ibrahim noted continuous improvement: \"More the product is used, more it improves, it's a cycle of continuous improvement\" (Les Echos).\n\nBen Gomes, Google's education lead, added that a \"large part of the use of information is learning.\" Tools like Experience AI, partnered with Raspberry Pi Foundation, have reached educators in 150 countries, equipping 11- to 14-year-olds with AI literacy. Google.org's $10 million expands it to millions in Europe, Middle East, and Africa (Google DeepMind).\n\nUK government collaboration scales these efforts. DeepMind supports randomized trials showing AI tutoring boosts problem-solving by 5.5 percentage points over human tutors alone. Plans tailor Gemini to England's curriculum via LearnLM, easing workloads (Google DeepMind Blog).\n\nScholarships Bridge Access Gaps\n\nDeepMind funds AI Masters scholarships via Martingale Foundation in the UK for 2026, alongside global postgraduate and postdoctoral fellowships at top universities like Oxford and Cambridge. The Pan-African AI for Science Masters with AIMS targets 160 students by 2027. Undergraduate Research Ready placements widen participation (Google DeepMind).\n\nTeam4Tech, co-founded by Ibrahim, delivers tech curricula to underserved communities. These initiatives counter equity risks, ensuring AI benefits diverse groups. Ibrahim stresses: \"We're never going to have as much time as we have now\" to upskill (CNBC).\n\nRisks loom large: academic integrity, cheating, bias, and over-reliance. Google addresses these through AI literacy, SynthID watermarks for generated content, and assessments like debates that AI can't replicate. Partnerships rethink evaluation for critical thinking (Google Blog).\n\nGoverning the Transformation\n\nIbrahim co-chairs DeepMind's Responsibility and Safety Council, evaluating projects against AI Principles. She balances opportunities and risks daily: \"Part of my job is to worry every day\" (CNBC). In five to ten years, AI ubiquity in education demands adaptation, from Global South pilots in Ghana to advanced markets.\n\nAt BETT 2026, Google unveiled Gemini updates for Classroom, including AI Writing Coach with Khan Academy and content verification, prioritizing thinking over answers. Irina Jurenka, DeepMind's AI education research lead, explores Gemini's pedagogical adaptations (The 74).\n\nIbrahim envisions AI maximizing human potential, as with her sister's achievements. Responsible deployment, she argues, positions education for unprecedented personalization and efficiency."
  },
  {
    "source": "bbntimes.com",
    "company": "Google DeepMind",
    "title": "From Chess Prodigy to Nobel Laureate: Sir Demis Hassabis's Visionary Leadership, DeepMind Legacy, and the Quest to Solve Intelligence",
    "date": "2026-02-17T22:10:20Z",
    "url": "https://www.bbntimes.com/companies/from-chess-prodigy-to-nobel-laureate-sir-demis-hassabis-s-visionary-leadership-deepmind-legacy-and-the-quest-to-solve-intelligence",
    "content": "Sir Demis Hassabis is a pioneering figure in artificial intelligence, known for his groundbreaking contributions to machine learning, neuroscience-inspired AI, and transformative technologies like AlphaGo and AlphaFold.\n\nAs the co-founder and CEO of Google DeepMind and Isomorphic Labs, he has bridged gaming, brain science, and advanced AI to achieve milestones that earned him the 2024 Nobel Prize in Chemistry. Born a chess prodigy in London, his path from child genius to leading one of the world's premier AI organizations reflects an unwavering pursuit of solving intelligence to tackle humanity's greatest challenges. Hassabis's career emphasizes ethical AI, scientific discovery, and the responsible development of artificial general intelligence (AGI), while warning of its profound societal impacts.\n\nThrough visionary leadership, interdisciplinary collaboration, and a commitment to profound impact over short-term gains, he has shaped the AI landscape and positioned Google as a leader in responsible innovation.\n\nAlso Read: Dario Amodei's Leadership Style, Business Success, OpenAI Leadership Role, and Anthropic: A Multifaceted Journey to Excellence\n\nSir Demis Hassabis's Leadership Style: Visionary, Scientific, and Mission-Driven\n\nSir Demis Hassabis's leadership blends rigorous scientific inquiry, bold ambition, and ethical responsibility, prioritizing long-term breakthroughs over immediate commercial pressures. Unlike many tech leaders focused on rapid scaling, he fosters a research-centric culture inspired by neuroscience and fundamental discovery, often describing AI's potential as ushering in a \"new golden era\" of productivity and abundance.\n\nAlso Read: Everything You Need to Know About Mark Zuckerberg: Leadership Style, Net Worth, Height, Wife and Personal Life\n\nSir Demis Hassabis's Business Success: From Prodigy to AI Titan\n\nSir Demis Hassabis's journey exemplifies transitioning from early entrepreneurial ventures to building world-changing AI enterprises, leveraging deep technical insight and strategic vision. A polymath with roots in chess, gaming, and neuroscience, he has driven innovations that redefine machine capabilities and amassed significant influence and wealth.\n\nAlso Read: Elon Musk's Leadership Style, Ethnicity, and Net Worth\n\nSir Demis Hassabis's Role in DeepMind: Founder, Visionary, and Long-Term Leader\n\nSir Demis Hassabis was the driving force behind DeepMind's creation and direction, serving as co-founder and CEO from inception. His leadership transformed the startup into a global AI powerhouse, embedding a culture of ambitious, neuroscience-grounded research that influenced the broader field.\n\nAlso Read: Jeff Bezos' Leadership at Amazon: The 1-Hour Rule and the Path to Business Success\n\nSir Demis Hassabis's Leadership at Google DeepMind: A Hub for AI Breakthroughs and Responsibility\n\nGoogle DeepMind, formed in 2023 from the merger of DeepMind and Google Brain, stands as one of the world's leading AI research entities under Hassabis's CEO leadership. Focused on AGI, scientific discovery, and tools like Gemini, it drives innovations in healthcare, energy, and beyond while prioritizing safety.\n\nAlso Read: Who is Sergey Brin? The Co-Founder of Google\n\nNet Worth of Sir Demis Hassabis\n\nSir Demis Hassabis's net worth is estimated in the range of $500 million to over $1 billion as of 2026, with some sources suggesting figures around $1 billion or higher due to his equity in Google (from the DeepMind acquisition) and leadership roles. This wealth stems primarily from the 2014 Google acquisition of DeepMind, subsequent Alphabet stock growth, his CEO compensation, and ventures like Isomorphic Labs. While not in the multi-billionaire tier of some tech peers, his fortune reflects profound impact rather than pure commercialization, and he focuses on using AI for societal benefit.\n\nAlso Read: Who is Larry Page? The Co-Founder of Google\n\nA Multifaceted Leader with a Lasting Legacy\n\nSir Demis Hassabis's journey from chess prodigy and game designer to Nobel laureate and CEO of Google DeepMind exemplifies versatility across science, entrepreneurship, and ethical leadership. Shaped by intellectual curiosity, interdisciplinary insight, and a sense of profound responsibility, he empowers teams to pursue transformative discoveries. Through Google DeepMind and Isomorphic Labs, he advances people-centric AI with global impact, inspiring a future where intelligence accelerates human progress responsibly and abundantly."
  },
  {
    "source": "developpez.net",
    "company": "Google DeepMind",
    "title": "0",
    "date": "2026-02-05T03:50:33Z",
    "url": "https://www.developpez.net/forums/d2181926/general-developpement/algorithme-mathematiques/intelligence-artificielle/demis-hassabis-pdg-deepmind-rendu-hommage-openai-createur-chatgpt-d-autres-startups-ia/",
    "content": "Demis Hassabis, PDG de Google DeepMind, a rendu hommage à OpenAI, le créateur de ChatGPT, et à d'autres startups IA pour avoir développé à grande échelle une technologie de transformateur de Google\n\nDemis Hassabis, fondateur et PDG de Google DeepMind, a rendu hommage à OpenAI, le créateur de ChatGPT, et à d'autres start-ups spécialisées dans l'IA pour avoir développé une technologie initialement mise au point par Google. Dans un podcast récent, Hassabis a reconnu que, bien que les chercheurs de Google aient développé en 2017 la technologie des transformateurs, qui constitue la base de presque tous les grands modèles linguistiques (LLM), l'entreprise a tardé à commercialiser et à développer cette innovation.\n\nSir Demis Hassabis, né le 27 juillet 1976, est un chercheur et entrepreneur britannique spécialisé dans l'intelligence artificielle (IA). Il est directeur général et cofondateur de Google DeepMind et d'Isomorphic Labs, ainsi que conseiller du gouvernement britannique en matière d'IA. En 2024, Hassabis et John M. Jumper ont reçu conjointement le prix Nobel de chimie pour leurs contributions à la recherche en IA dans le domaine de la prédiction de la structure des protéines.\n\nDeepMind est un pionnier de la recherche en IA depuis plus de dix ans. La société, basée à Londres, au Royaume-Uni, a fait ses preuves avec les systèmes tels qu'AlphaCode, AlphaGo et AlphaFold. Le système d'IA AlphaGo de DeepMind a réussi l'exploit de terrasser Lee Sedol, l'un des meilleurs joueurs du jeu de Go au monde. La société a été rachetée par Alphabet, la société mère de Google, le géant de la recherche et DeepMind ayant entretenu des liens étroits ces dernières années. Après des systèmes d'IA pour le jeu de Go (AlphaGo) et la programmation (AlphaCode), DeepMind a annoncé en 2023 le lancement de Gemini sur le marché des chatbots IA.\n\nGemini est un grand modèle de langage qui traite du texte et qui est similaire à GPT-4, la technologie qui alimente ChatGPT. L'objectif de Gemini est de jouer un rôle majeur dans la réponse de Google à la menace concurrentielle posée par ChatGPT et d'autres technologies d'IA génératives. Lors du lancement de Gemini, Demis Hassabis se dit confiant dans le fait que Gemini surpassera ChatGPT et les autres modèles de langage existants ou à venir. \" Nous pensons que Gemini sera le modèle de langage le plus avancé et le plus polyvalent jamais créé \", dit-il. \" Nous pensons qu'il ouvrira la voie à une nouvelle génération d'IA générative qui aura un impact positif sur le monde \".\n\nRécemment, Demis Hassabis, fondateur et PDG de Google DeepMind, a rendu hommage à OpenAI, le créateur de ChatGPT, et à d'autres start-ups spécialisées dans l'IA pour avoir développé une technologie initialement mise au point par Google. Dans un podcast récent, Hassabis a reconnu que, bien que les chercheurs de Google aient développé en 2017 la technologie des transformateurs, qui constitue la base de presque tous les grands modèles linguistiques (LLM), l'entreprise a tardé à commercialiser et à développer cette innovation.\n\nIl a rendu hommage à OpenAI, dirigée par Sam Altman, et à d'autres concurrents pour avoir utilisé la technologie de Google afin de développer leurs chatbots IA. Dans un épisode récent du podcast The Tech Download de CNBC, Hassabis a déclaré que le défi de Google n'était pas l'invention, mais l'exécution. Faisant référence à la commercialisation de la technologie, il a déclaré : \" C'est ce qu'OpenAI et d'autres ont très bien fait. \" L'entreprise a \" peut-être \" été \" un peu lente à la commercialiser et à la développer \", a ajouté Hassabis.\n\nGoogle a fusionné sa division de recherche Brain avec DeepMind en 2023 et a nommé Josh Woodward à la tête de son assistant IA Gemini, ce qui a permis à l'entreprise de rattraper son retard après la sortie de ChatGPT par OpenAI en novembre 2022. Les échecs des produits IA de Google en 2024 ont renforcé l'impression que l'entreprise avait du mal à rester compétitive. \" Au cours des deux ou trois dernières années, je pense que nous avons dû revenir à nos racines de start-up ou d'entreprise et être plus combatifs, plus rapides, livrer les produits très rapidement et faire des progrès très rapides \", a noté Hassabis.\n\nAu début de l'année 2025, les investisseurs se sont demandé si Google pourrait suivre le rythme d'OpenAI dans le domaine de l'IA. À la fin de l'année, l'action Alphabet a enregistré sa meilleure performance depuis 2009. Google a retrouvé sa force dans le domaine de l'IA. Cela est en grande partie dû à DeepMind, la société britannique rachetée par Google en 2014 pour environ 550 millions de dollars.\n\nHassabis l'a qualifiée de \" salle des machines \" du travail de Google dans le domaine de l'IA, affirmant que des changements avaient été apportés pour aider l'entreprise technologique à lancer rapidement des produits d'IA dans un \" environnement concurrentiel féroce \". Il a indiqué qu'il s'entretenait \" tous les jours \" avec le PDG de Google, Sundar Pichai, montrant à quel point les deux dirigeants travaillent en étroite collaboration pour faire avancer rapidement l'innovation.\n\n\" Toutes les technologies d'IA sont développées par ce groupe... puis diffusées dans tous ces produits incroyables de Google \", a déclaré Hassabis dans le podcast. \" Au cours des deux dernières années, nous avons construit cette infrastructure, non seulement les modèles, mais aussi... l'architecture complète de Google afin que... ces produits puissent être commercialisés très rapidement \", a-t-il poursuivi.\n\nCela pourrait être important pour Google, qui doit faire face à une nouvelle année de concurrence de la part d'OpenAI et de nombreuses autres entreprises, notamment Amazon, Perplexity et Anthropic. \" L'environnement concurrentiel est actuellement très féroce \", a fait remarquer Hassabis. Il a ajouté que \" de nombreux \" travailleurs du secteur technologique de longue date, présents dans l'industrie depuis \" 20 ou 30 ans \", lui avaient dit qu'il s'agissait de \" l'environnement le plus intense qu'ils aient jamais connu, peut-être même dans toute l'histoire de l'industrie technologique \".\n\nPourtant, Demis Hassabis a également annoncé au monde de l'IA que la voie vers la superintelligence empruntée par ChatGPT pourrait être une impasse. Le PDG de Google DeepMind affirme que les grands modèles de langage (LLM), la technologie à la base des produits phares d'OpenAI, ne peuvent pas aboutir à de véritables percées scientifiques car ils ne disposent pas de ce qu'il appelle \" un modèle de monde \". C'est une critique directe de la stratégie de développement de Sam Altman. Alors qu'OpenAI a misé des milliards pour rendre les LLM plus grands et plus rapides, Hassabis affirme que cette approche se heurte à un obstacle fondamental.\n\nHassabis a déclaré : \" Les grands modèles de langage actuels sont phénoménaux en matière de reconnaissance de formes. Mais ils ne comprennent pas vraiment la causalité. Ils ne savent pas vraiment pourquoi A mène à B. Ils se contentent de prédire le prochain token en se basant sur des corrélations statistiques. \" Selon Hassabis, une véritable invention scientifique nécessite quelque chose de plus profond : la capacité à mener des expériences de pensée, à simuler la physique avec précision et à raisonner à partir des principes fondamentaux. Cela nécessite un modèle de monde, c'est-à-dire un moteur de simulation interne qui comprend le fonctionnement réel de la réalité, et pas seulement la façon dont les mots s'enchaînent généralement.\n\nEn décembre 2025, Demis Hassabis a également mis en garde contre le caractère non viable des valorisations actuelles des start-ups spécialisées dans l'intelligence artificielle (IA), nombreuses étant celles qui lèvent des milliards avant même de lancer leurs produits. Selon lui, cette tendance au financement spéculatif révèle les failles du modèle de valorisation actuel et laisse présager une potentielle correction du marché. Demis Hassabis a également souligné l'importance de pousser l'IA actuelle à son maximum afin d'atteindre l'intelligence artificielle générale (AGI). \" Certaines start-ups n'ont même pas encore démarré leurs activités \", a déclaré Demis Hassabis. \" Il est assez intéressant de voir comment cela peut être viable. À mon avis, ce n'est probablement pas le cas, du moins pas en général. \"\n\nEt vous ?\n\nPensez-vous que cette déclaration est crédible ou pertinente ?\n\nQuel est votre avis sur le sujet ?\n\nVoir aussi :\n\nDemis Hassabis, PDG de Google DeepMind, a déclaré que DeepSeek n'apportait rien de nouveau et que ces travaux étaient \"impressionnants\" mais ne constituaient pas une \"véritable avancée scientifique\"\n\nTrois ans après avoir été pris au dépourvu par ChatGPT, Gemini est-il en train de gagner ? Des benchmarks et des chercheurs suggèrent que Gemini a ce qu'il faut pour détrôner OpenAI et tous ses concurrents\n\n\" L'AGI n'est pas à portée de main. L'incapacité des modèles d'IA actuels à apprendre de façon autonome et continue, comme les humains, freine leur évolution vers une superintelligence \", selon un critique"
  },
  {
    "source": "Developpez.com",
    "company": "Google DeepMind",
    "title": "Demis Hassabis~? PDG de Google DeepMind~? a rendu hommage à OpenAI~? le créateur de ChatGPT~? et à d'autres startups IA pour avoir développé à grande échelle une technologie de transformateur de Google",
    "date": "2026-02-05T03:35:50Z",
    "url": "https://intelligence-artificielle.developpez.com/actu/379938/Demis-Hassabis-PDG-de-Google-DeepMind-a-rendu-hommage-a-OpenAI-le-createur-de-ChatGPT-et-a-d-autres-startups-IA-pour-avoir-developpe-a-grande-echelle-une-technologie-de-transformateur-de-Google/",
    "content": "Demis Hassabis, PDG de Google DeepMind, a rendu hommage à OpenAI, le créateur de ChatGPT, et à d'autres startups IA pour avoir développé à grande échelle une technologie de transformateur de Google\n\nDemis Hassabis, fondateur et PDG de Google DeepMind, a rendu hommage à OpenAI, le créateur de ChatGPT, et à d'autres start-ups spécialisées dans l'IA pour avoir développé une technologie initialement mise au point par Google. Dans un podcast récent, Hassabis a reconnu que, bien que les chercheurs de Google aient développé en 2017 la technologie des transformateurs, qui constitue la base de presque tous les grands modèles linguistiques (LLM), l'entreprise a tardé à commercialiser et à développer cette innovation.\n\nSir Demis Hassabis, né le 27 juillet 1976, est un chercheur et entrepreneur britannique spécialisé dans l'intelligence artificielle (IA). Il est directeur général et cofondateur de Google DeepMind et d'Isomorphic Labs, ainsi que conseiller du gouvernement britannique en matière d'IA. En 2024, Hassabis et John M. Jumper ont reçu conjointement le prix Nobel de chimie pour leurs contributions à la recherche en IA dans le domaine de la prédiction de la structure des protéines.\n\nDeepMind est un pionnier de la recherche en IA depuis plus de dix ans. La société, basée à Londres, au Royaume-Uni, a fait ses preuves avec les systèmes tels qu'AlphaCode, AlphaGo et AlphaFold. Le système d'IA AlphaGo de DeepMind a réussi l'exploit de terrasser Lee Sedol, l'un des meilleurs joueurs du jeu de Go au monde. La société a été rachetée par Alphabet, la société mère de Google, le géant de la recherche et DeepMind ayant entretenu des liens étroits ces dernières années. Après des systèmes d'IA pour le jeu de Go (AlphaGo) et la programmation (AlphaCode), DeepMind a annoncé en 2023 le lancement de Gemini sur le marché des chatbots IA.\n\nGemini est un grand modèle de langage qui traite du texte et qui est similaire à GPT-4, la technologie qui alimente ChatGPT. L'objectif de Gemini est de jouer un rôle majeur dans la réponse de Google à la menace concurrentielle posée par ChatGPT et d'autres technologies d'IA génératives. Lors du lancement de Gemini, Demis Hassabis se dit confiant dans le fait que Gemini surpassera ChatGPT et les autres modèles de langage existants ou à venir. \" Nous pensons que Gemini sera le modèle de langage le plus avancé et le plus polyvalent jamais créé \", dit-il. \" Nous pensons qu'il ouvrira la voie à une nouvelle génération d'IA générative qui aura un impact positif sur le monde \".\n\nRécemment, Demis Hassabis, fondateur et PDG de Google DeepMind, a rendu hommage à OpenAI, le créateur de ChatGPT, et à d'autres start-ups spécialisées dans l'IA pour avoir développé une technologie initialement mise au point par Google. Dans un podcast récent, Hassabis a reconnu que, bien que les chercheurs de Google aient développé en 2017 la technologie des transformateurs, qui constitue la base de presque tous les grands modèles linguistiques (LLM), l'entreprise a tardé à commercialiser et à développer cette innovation.\n\nIl a rendu hommage à OpenAI, dirigée par Sam Altman, et à d'autres concurrents pour avoir utilisé la technologie de Google afin de développer leurs chatbots IA. Dans un épisode récent du podcast The Tech Download de CNBC, Hassabis a déclaré que le défi de Google n'était pas l'invention, mais l'exécution. Faisant référence à la commercialisation de la technologie, il a déclaré : \" C'est ce qu'OpenAI et d'autres ont très bien fait. \" L'entreprise a \" peut-être \" été \" un peu lente à la commercialiser et à la développer \", a ajouté Hassabis.\n\nGoogle a fusionné sa division de recherche Brain avec DeepMind en 2023 et a nommé Josh Woodward à la tête de son assistant IA Gemini, ce qui a permis à l'entreprise de rattraper son retard après la sortie de ChatGPT par OpenAI en novembre 2022. Les échecs des produits IA de Google en 2024 ont renforcé l'impression que l'entreprise avait du mal à rester compétitive. \" Au cours des deux ou trois dernières années, je pense que nous avons dû revenir à nos racines de start-up ou d'entreprise et être plus combatifs, plus rapides, livrer les produits très rapidement et faire des progrès très rapides \", a noté Hassabis.\n\nAu début de l'année 2025, les investisseurs se sont demandé si Google pourrait suivre le rythme d'OpenAI dans le domaine de l'IA. À la fin de l'année, l'action Alphabet a enregistré sa meilleure performance depuis 2009. Google a retrouvé sa force dans le domaine de l'IA. Cela est en grande partie dû à DeepMind, la société britannique rachetée par Google en 2014 pour environ 550 millions de dollars.\n\nHassabis l'a qualifiée de \" salle des machines \" du travail de Google dans le domaine de l'IA, affirmant que des changements avaient été apportés pour aider l'entreprise technologique à lancer rapidement des produits d'IA dans un \" environnement concurrentiel féroce \". Il a indiqué qu'il s'entretenait \" tous les jours \" avec le PDG de Google, Sundar Pichai, montrant à quel point les deux dirigeants travaillent en étroite collaboration pour faire avancer rapidement l'innovation.\n\n\" Toutes les technologies d'IA sont développées par ce groupe... puis diffusées dans tous ces produits incroyables de Google \", a déclaré Hassabis dans le podcast. \" Au cours des deux dernières années, nous avons construit cette infrastructure, non seulement les modèles, mais aussi... l'architecture complète de Google afin que... ces produits puissent être commercialisés très rapidement \", a-t-il poursuivi.\n\nCela pourrait être important pour Google, qui doit faire face à une nouvelle année de concurrence de la part d'OpenAI et de nombreuses autres entreprises, notamment Amazon, Perplexity et Anthropic. \" L'environnement concurrentiel est actuellement très féroce \", a fait remarquer Hassabis. Il a ajouté que \" de nombreux \" travailleurs du secteur technologique de longue date, présents dans l'industrie depuis \" 20 ou 30 ans \", lui avaient dit qu'il s'agissait de \" l'environnement le plus intense qu'ils aient jamais connu, peut-être même dans toute l'histoire de l'industrie technologique \".\n\nPourtant, Demis Hassabis a également annoncé au monde de l'IA que la voie vers la superintelligence empruntée par ChatGPT pourrait être une impasse. Le PDG de Google DeepMind affirme que les grands modèles de langage (LLM), la technologie à la base des produits phares d'OpenAI, ne peuvent pas aboutir à de véritables percées scientifiques car ils ne disposent pas de ce qu'il appelle \" un modèle de monde \". C'est une critique directe de la stratégie de développement de Sam Altman. Alors qu'OpenAI a misé des milliards pour rendre les LLM plus grands et plus rapides, Hassabis affirme que cette approche se heurte à un obstacle fondamental.\n\nHassabis a déclaré : \" Les grands modèles de langage actuels sont phénoménaux en matière de reconnaissance de formes. Mais ils ne comprennent pas vraiment la causalité. Ils ne savent pas vraiment pourquoi A mène à B. Ils se contentent de prédire le prochain token en se basant sur des corrélations statistiques. \" Selon Hassabis, une véritable invention scientifique nécessite quelque chose de plus profond : la capacité à mener des expériences de pensée, à simuler la physique avec précision et à raisonner à partir des principes fondamentaux. Cela nécessite un modèle de monde, c'est-à-dire un moteur de simulation interne qui comprend le fonctionnement réel de la réalité, et pas seulement la façon dont les mots s'enchaînent généralement.\n\nEn décembre 2025, Demis Hassabis a également mis en garde contre le caractère non viable des valorisations actuelles des start-ups spécialisées dans l'intelligence artificielle (IA), nombreuses étant celles qui lèvent des milliards avant même de lancer leurs produits. Selon lui, cette tendance au financement spéculatif révèle les failles du modèle de valorisation actuel et laisse présager une potentielle correction du marché. Demis Hassabis a également souligné l'importance de pousser l'IA actuelle à son maximum afin d'atteindre l'intelligence artificielle générale (AGI). \" Certaines start-ups n'ont même pas encore démarré leurs activités \", a déclaré Demis Hassabis. \" Il est assez intéressant de voir comment cela peut être viable. À mon avis, ce n'est probablement pas le cas, du moins pas en général. \"\n\nEt vous ?\n\nPensez-vous que cette déclaration est crédible ou pertinente ?\n\nQuel est votre avis sur le sujet ?\n\nVoir aussi :\n\nDemis Hassabis, PDG de Google DeepMind, a déclaré que DeepSeek n'apportait rien de nouveau et que ces travaux étaient \"impressionnants\" mais ne constituaient pas une \"véritable avancée scientifique\"\n\nTrois ans après avoir été pris au dépourvu par ChatGPT, Gemini est-il en train de gagner ? Des benchmarks et des chercheurs suggèrent que Gemini a ce qu'il faut pour détrôner OpenAI et tous ses concurrents\n\n\" L'AGI n'est pas à portée de main. L'incapacité des modèles d'IA actuels à apprendre de façon autonome et continue, comme les humains, freine leur évolution vers une superintelligence \", selon un critique Vous avez lu gratuitement 2 124 articles depuis plus d'un an.\n\nSoutenez le club developpez.com en souscrivant un abonnement pour que nous puissions continuer à vous proposer des publications."
  },
  {
    "source": "El Confidencial",
    "company": "Google DeepMind",
    "title": "¿Es la IA de Google que ayuda a leer el genoma una revolución digna de (otro) Nobel?",
    "date": "2026-01-31T04:04:32Z",
    "url": "https://www.elconfidencial.com/tecnologia/ciencia/2026-01-31/inteligencia-artificial-google-genoma-alphagenome-nobel_4294408/",
    "content": "AlphaGenome predice el impacto de las variaciones en el ADN y puede ayudar a comprender enfermedades complejas. Otro modelo de Google DeepMind se llevó la medalla de oro sueca hace dos años, pero esta nueva IA tiene competencia y limitaciones\n\nEl ser humano tiene unos 20.000 genes. Pueden parecer muchos, pero en realidad hasta un grano de arroz tiene más (unos 45.000) y ocupan solo el 2% del genoma. Nuestra complejidad no está ahí, sino en cómo se organizan y se activan. El sistema que los controla se guarda en el 98% restante, una parte del genoma tan inexplorada que era conocida como ADN basura. Interpretar toda esa información es uno de los grandes desafíos pendientes de la genómica moderna. Hacerlo es tan complejo que necesita potencia informática... y la inteligencia artificial puede ser clave.\n\nGoogle Deepmind lleva diez años queriendo ser referente en este frente, y ahora ha presentado ante la comunidad científica AlphaGenome, una herramienta de IA diseñada para entender ese 98% oscuro y entender cómo pequeñas variaciones en nuestro ADN pueden alterar la actividad de los genes. Conseguirlo no es solo resolver un enigma fascinante, también es crucial para diagnosticar enfermedades y desarrollar nuevos tratamientos.\n\nHace dos años, científicos de Google consiguieron el premio Nobel de Química por el desarrollo de una herramienta similar aplicada a proteínas, AlphaFold. Pero la nueva apuesta de IA aún tiene limitaciones técnicas y no parece tan sorprendente como para sumar una medalla más al historial de la big tech.\n\nSi el 2% de nuestro genoma está conformado por genes, el 98% restante está ocupado por diferentes familias de ADN repetitivo, elementos móviles, transposones y otras palabras que no nos son familiares. Entre ellas, los elementos reguladores. \"Son pequeñas secuencias de ADN a las que se pegan proteínas que van dirigiendo cómo funciona un gen, cuándo y dónde se va a activar, y cuándo y dónde se va a desactivar\", explica Lluís Montoliu, investigador científico en el centro Nacional de Biotecnología (CNB-CSIC), a El Confidencial. Estos elementos son muy relevantes porque \"si están alterados, el gen se activará o desactivará donde no toca y esto puede generar enfermedades congénitas\".\n\nEn 2003 se consiguió obtener la secuencia completa del genoma humano, pero saber leer ese libro y entender los elementos reguladores es otra historia muy difícil de conseguir sin tecnología. Hasta ahora había programas informáticos con los que se podía investigar mediante comparación de secuencias, pero \"se hacía sobre secuencias relativamente pequeñas de unos pocos miles de nucleótidos, con AlphaGenome se hace sobre hasta un millón de nucleótidos\".\n\nEsto puede ayudar a saber dónde poner la lupa buscar las mutaciones que provocan \"enfermedades asesinas genéticas\", añade Montoliu, que es referente internacional en el estudio de enfermedades raras. \"Es un desarrollo brutal y un avance espectacular\", considera.\n\nEste tipo de modelos son especialmente importantes para avanzar en investigación oncológica. \"Cuando secuenciamos a un paciente con un determinado tumor o buscamos qué mutación está causando un cáncer hereditario, muchas veces encontramos mutaciones que no se han descrito previamente, porque esto se hace a través de experimentos de laboratorio\", indica Dido Carrero, investigadora del Centro Nacional de Investigaciones Oncológicas (CNIO). Pero AlphaGenome permite inferir qué efecto pueden tener esas mutaciones y seguirles la pista. \"Con experimentos podemos validar si es una mutación que ocurre por azar o si es la causante de la enfermedad\", añade la investigadora.\n\nGoogle DeepMind acaba de publicar en Nature los primeros resultados de esta IA, un modelo de aprendizaje profundo que presentó en junio de 2025. Según los desarrolladores, logra mejorar la predicción de los modelos existentes en 25 de las 26 pruebas realizadas. La comunidad científica ha coincidido en que se trata de un hito que funciona de forma excepcional para algunas tareas, pero que aún dista de ser perfecto.\n\n\"El reto de hacer investigación con este tipo de modelos de IA es saber en qué te dan respuestas fiables y en qué no. En el caso de AlphaGenome, puede hacerlo sobre procesos de regulación genómica, pero de momento no es fiable para hacer medicina personalizada\", explica a este diario Mafalda Dias, que codirige un grupo de investigación sobre inteligencia artificial y genómica en el Centro de Regulación Genómica (CRG).\n\nNo hay solo limitaciones específicas del estudio del genoma, también las intrínsecas a los modelos de inteligencia artificial: la falta de explicabilidad para entender cómo funcionan los modelos, y los sesgos. Lo primero \"siempre ha sido un problema con todos los modelos matemáticos, pero como los de IA son cajas negras, el problema se amplifica\", detalla Dias.\n\nLos investigadores de AlphaGenome dicen que trabajan para evitarlo. \"Una de las principales formas en que lo hacemos es rastreando el origen: cuando el modelo predice el efecto de una variante específica, ¿en qué se está fijando exactamente? Para saberlo, simulamos mutaciones y recalculamos las puntuaciones del modelo\", explican Natasha Latysheva y Žiga Avsec, de Google DeepMind, a El Confidencial en una convocatoria de prensa. Se trata de hacer una especie \"disecciones genéticas\" para intuir en base a qué hace sus predicciones.\n\n\"Es una herramienta poderosa que ayuda a comprender mejor los fundamentos de la biología\", Mafalda Dias del CRG\n\nEn el caso de los sesgos, parte del problema viene de los datos. \"Hay muchísimos factores que pueden dar lugar a un modelo sesgado: edad, género, condicionantes ambientales... Imagina que recoges datos de personas de 80 años porque tienen una determinada enfermedad, pues estás ignorando los de personas de 30 y el modelo puede que no funcione\", explica Beatriz López, coordinadora del laboratorio de investigación en Inteligencia Artificial aplicada a la Medicina y la Salud del grupo de investigación eXiT de la Universidad de Girona, en conversación con este diario.\n\nLímites o no, \"hay que recordar que esto es una herramienta y que el científico especializado tiene después que interpretar sus resultados, no tomarlos como la solución\", subraya López. Dias llama a la ética y a \"usarla de forma responsable\". Sin duda, hacerlo puede redundar en que este descubrimiento científico aporte el \"mayor beneficio a la humanidad\"... La condición que puso Alfred Nobel para repartir sus premios.\n\nYa hace una década que Google Deepmind empezó a investigar la aplicación de inteligencia artificial a la biología molecular y la medicina. En 2016 comenzó a trabajar con hospitales en el análisis de datos clínicos y ese mismo año sucedió un hito: AlphaGo venció al jugador profesional de go Lee Sedol. El cofundador y CEO de Google DeepMind, Demis Hassabis, dijo entonces: \"Lo que realmente me entusiasma es aplicar este tipo de IA en ciencia y hacer que avance más rápido. Me gustaría ver una ciencia asistida por IA\".\n\nY comenzó la magia. La familia de los AlphaFold arrancó en 2018 y desde entonces han sacado cuatro versiones mejoradas. También están AlphaMissense (2023), que predice si un cambio en una sola letra del ADN causará una enfermedad, y AlphaProteo (2024), para crear proteínas desde cero. El propio Hassabis fue uno de los galardonados con el Nobel por AlphaFold.\n\nQue un descubrimiento científico reciba un premio Nobel es complicado. Implica un consenso internacional que puede requerir años y que depende de su novedad e impacto, pero es inevitable preguntarse si AlphaGenome también estará a la altura.\n\n\"AlphaFold fue una revolución por ser uno de los primeros modelos de aprendizaje profundo que surgieron con la explosión de la IA. Ahora, cada poco salen nuevos modelos predictores. Nosotros también estamos desarrollando uno\", apunta Carrero del CNIO. La investigadora considera que es un campo muy competitivo y cuesta destacar: \"Ojalá se lo lleve, pero creo que estos nuevos modelos van a pasar más desapercibidos para la comunidad internacional y para la opinión general\".\n\n\"No diría que esta investigación sea una revolución, porque es parte de una cadena de artículos y del trabajo de toda una comunidad que está desarrollando este tipo de modelos\", coincide Dias. La experta también señala que AlphaFold solucionó el problema de la predicción de la estructura de las proteínas, pero AlphaGenome no ha dado con la clave definitiva para leer el genoma.\n\n\"AlphaFold fue rompedor, porque predecir la estructura que tenían las proteínas con una fiabilidad así y solo a partir de la secuencia de ADN era algo que no sabíamos hacer de ninguna manera. Antes se usaban modelos que fallaban más que una escopeta de feria\", bromea Montoliú. En cambio, aunque AlphaGenome \"da un salto cuantitativo\", lo que consigue ya se sabía hacer, solo que en fragmentos más pequeños.\n\nLos investigadores están de acuerdo en que, aunque sea muy útil, de momento no está al mismo nivel de \"sorpresa\" que su predecesor y arrastra limitaciones. Si se lleva el Nobel o no es algo que sabremos con los años; esto no hay ningún modelo de IA desarrollado por Google que lo pueda predecir."
  },
  {
    "source": "The Times of India",
    "company": "Google DeepMind",
    "title": "Google CEO Sundar Pichai introduces Genie AI app that he says: Has been playing around with and ... - The Times of India",
    "date": "2026-01-30T15:39:19Z",
    "url": "https://timesofindia.indiatimes.com/technology/tech-news/google-ceo-sundar-pichai-introduces-genie-ai-app-that-he-says-has-been-playing-around-with-and-/articleshow/127804351.cms",
    "content": "Google DeepMind has unveiled Project Genie, an experimental AI tool allowing users to create and explore interactive digital worlds from text prompts. CEO Sundar Pichai described it as \"out of this world.\" This groundbreaking technology generates dynamic environments in real-time, simulating physics and interactions, and is now available to US Ultra subscribers. Google CEO Sundar Pichai has shared his views on the newly announced Project Genie AI tool. This new experimental tool from Google DeepMind allows users to create and explore interactive digital worlds using AI, and he says he's been testing it himself. In a post shared on the microblogging site X (formerly Twitter), Pichai wrote: \"Project Genie is a prototype web app powered by Genie 3, Nano Banana Pro + Gemini that lets you create your own interactive worlds. I've been playing around with it a bit, and it's...out of this world. Rolling out now for US Ultra subscribers.\" The announcement introduces the latest experimental offering from Google's AI research arm to end users as the company continues competing in the AI race. The company said this feedback will help guide future development in areas such as AI research, simulation, and generative media. Google said it plans to continue expanding access to Project Genie and improving the technology over time.Meanwhile,Google DeepMind CEO Demis Hassabis also took to X and wrote, \"Thrilled to launch Project Genie, an experimental prototype of the world's most advanced world model. Create entire playable worlds to explore in real-time just from a simple text prompt - kind of mindblowing, really! Available to Ultra subs in the US for now - have fun exploring!\"In an official statement, the company said: \"Starting today, we're rolling out access to Project Genie for Google AI Ultra subscribers in the U.S (18+). This experimental research prototype lets users create, explore and remix their own interactive worlds.\"What is Google's Project Genie, how it works and other detailsProject Genie is a research prototype built on Genie 3, a general-purpose world model that Google DeepMind first previewed in August. \"Unlike explorable experiences in static 3D snapshots, Genie 3 generates the path ahead in real time as you move and interact with the world. It simulates physics and interactions for dynamic worlds. At the same time, its breakthrough consistency enables the simulation of any real-world scenario -- from robotics and modelling animation and fiction, to exploring locations and historical settings,\" the company noted.With Project Genie, Google DeepMind is offering early access to this technology through a web-based prototype. Starting this week, the tool is being rolled out to Google AI Ultra subscribers in the US aged 18 or older. The company said access will expand to more regions over time.Project Genie generates interactive environments in real time. As users move through a world, the system predicts and creates what comes next based on their actions. This differs from traditional 3D experiences, which are usually pre-built and static.The prototype is powered by Genie 3, along with Gemini and Nano Banana Pro, two other Google AI systems. Together, these models allow users to build, explore, and modify digital worlds rather than simply view them. Google DeepMind said this work is part of its broader effort to develop artificial general intelligence, or AGI, which would require AI systems that can operate across many different real-world situations.Project Genie focuses on three main features. The first is world creation, also described as \"world sketching.\" Users can use text prompts or upload images to create environments. Characters, environments, and modes of transportation, such as driving, flying, or walking can all be specified. Prior to entering the setting, users can also select viewing perspectives, such as first or third-person point of views.World exploration is the second feature. After a world is created, users are free to explore it. The system generates the environment in real time as users navigate, adjusting in response to movement and camera changes.The third feature is world remixing. Users can build on existing worlds by modifying prompts or exploring examples provided in a gallery. Finished experiences can be recorded and downloaded as videos.Google DeepMind reported that Project Genie remains an early research prototype and has several limitations. The worlds generated may not always align with the prompt, the physics may not be realistic, and the characters may be difficult to control. Each generated experience is limited to 60 seconds.\n\nGN Awards 2025: Vote for your favorite Gadgets\n\nThe TOI Tech Desk is a dedicated team of journalists committed to delivering the latest and most relevant news from the world of technology to readers of The Times of India. TOI Tech Desk's news coverage spans a wide spectrum across gadget launches, gadget reviews, trends, in-depth analysis, exclusive reports and breaking stories that impact technology and the digital universe. Be it how-tos or the latest happenings in AI, cybersecurity, personal gadgets, platforms like WhatsApp, Instagram, Facebook and more; TOI Tech Desk brings the news with accuracy and authenticity."
  },
  {
    "source": "Inside The Star-Studded World",
    "company": "Google DeepMind",
    "title": "Exclusive: Google DeepMind Researcher David Silver Leaves To Launch His Own AI Startup Todayheadline | Today Headline",
    "date": "2026-01-30T14:48:40Z",
    "url": "https://todayheadline.co/exclusive-google-deepmind-researcher-david-silver-leaves-to-launch-his-own-ai-startup-todayheadline/",
    "content": "Silver is launching a new startup called Ineffable Intelligence, based in London, according to a person with direct knowledge of Silver's plans. The company is actively recruiting AI researchers and is seeking venture capital funding, the person said.\n\nGoogle DeepMind informed staff of Silver's departure earlier this month, the person said. Silver had been on sabbatical in the months leading up to his departure and never formally returned to his DeepMind role.\n\nA Google DeepMind spokesperson confirmed Silver's departure in an emailed statement to Fortune. \"Dave's contributions have been invaluable and we're grateful for the impact he's had on our work at Google DeepMind,\" the spokesperson said.\n\nSilver could not immediately be reached for comment.\n\nIneffable Intelligence was formed in November 2025 and Silver was appointed a director of the company on January 16, according to documents filed with U.K. business registry Companies House.\n\nIn addition, Silver's personal webpage now lists his contact as Ineffable Intelligence and provides an ineffable intelligence email address, although it continues to state that he \"leads the reinforcement learning team\" at Google DeepMind.\n\nIn addition to his work at Google DeepMind, Silver is a professor at University College London. He continues to maintain that affiliation.\n\nA key figure behind many of DeepMind's breakthroughs\n\nSilver was one of DeepMind's first employees when the company was established in 2010. He knew DeepMind cofounder Demis Hassabis from university. Silver played an instrumental role in many of the company's early breakthroughs, including its landmark 2016 achievement with AlphaGo, demonstrating that an AI program could beat the world's best human players at the ancient strategy game Go.\n\nHe also was a key member of the team that developed AlphaStar, an AI program that could beat the world's best human players at the complex video game Starcraft 2, AlphaZero, which could play chess and shogi as well as Go at superhuman levels, and MuZero, which could master many different kinds of games better than people even though it started without any knowledge of the game, including not knowing the games' rules.\n\nMore recently, he worked with the DeepMind team that created AlphaProof, an AI system that could successfully answer questions from the International Mathematics Olympiad. He is also one of the authors on the 2023 research paper that debuted the Google's original Gemini family of AI models. Gemini has now Google's leading commercial AI product and brand.\n\nLooking for a path to AI 'superintelligence'\n\nSiliver has told friends he wants to get back to the \"awe and wonder of solving the hardest problems in AI\" and sees superintelligence -- or AI that would be smarter than any human and potentially smarter than all of humanity -- the biggest unsolved challenge in the field, according to the person familiar with his thinking.\n\nSeveral other well-known AI researchers have also left established AI labs in recent years to found startups dedicated to pursuing superintelligence. Ilya Sutskever, the former chief scientist at OpenAI, founded a company called Safe Superintelligence (SSI) in 2024. That company has raised $3 billion in venture capital funding to date and is reportedly valued at as much as $30 billion. Some of Silver's colleagues who worked on AlphaGo, AlphaZero, and MuZero have also recently left to found Reflection AI, an AI startup that also says it is pursuing superintelligence. Meanwhile, Meta last year reorganized its AI efforts around a new \"Superintelligence Labs\" that is headed by former Scale AI CEO and founder Alexandr Wang.\n\nGoing beyond language models\n\nSilver is well-known for his work on reinforcement learning, a way of training AI models from experience rather than historical data. In reinforcement learning, a model takes an action, usually in a game or simulator, and then receives feedback on whether those actions are productive in helping it achieve a goal. Through trial and error over the course of many actions, the AI learns the best ways to accomplish the goal.\n\nThe researcher was often considered one of reinforcement learning's most dogmatic proponents, arguing it was the only way to create artificial intelligence that could one day surpass human knowledge.\n\nOn a Google DeepMind-produced podcast that was released in April, he said that large language models (LLMs), the type of AI responsible for most of the recent excitement about AI, were powerful, but they were also constrained by human knowledge. \"We want to go beyond what humans know and to do that we're going to need a different type of method and that type of method will require our AIs to actually figure things out for themselves and to discover new things that humans don't know,\" he said. He has called for a new \"era of experience\" in AI that will be based around reinforcement learning.\n\nCurrently, LLMs have a \"pretraining\" development phase that uses what is called unsupervised learning. They ingest vast amounts of text and learn to predict which words are statistically most likely to follow which other words in a given context. They then have a \"post-training\" development phase that does use some reinforcement learning, often with human evaluators looking at the model's outputs and giving the AI feedback, sometimes just in the form of a thumbs up or thumbs down. Through this feedback, the model's tendency to produce helpful outputs is boosted.\n\nBut this kind of training is ultimately dependent on what humans know -- both because it depends on what humans have learned and written down in the past in the pre-training phase and because the way LLM post-training does reinforcement learning is ultimately based on human preferences. In some cases, though, human intuition can be wrong or short-sighted.\n\nFor instance, famously, in move 37 of the second game of AlphaGo's 2016 match against Go world champion Lee Sedol, AlphaGo made a move that was so unconventional that all the human experts commenting on the game were sure it was a mistake. But it wound up later proving to be a key to AlphaGo winning that match. Similarly, human chess players have often described the way AlphaZero plays chess as \"alien\" -- and yet its counterintuitive moves often prove to be brilliant.\n\nIf human evaluators were passing judgments on such moves though in the kind of reinforcement learning process used in LLM post-training, they might give such moves a \"thumbs down\" because they look to human experts like mistakes. This is why reinforcement learning purists such as Silver say that to get to superintelligence, AI will not just have to get beyond human knowledge, it will need to discard it and learn to achieve goals from scratch, working from first principles.\n\nSilver has said Ineffable Intelligence will aim to build \"an endlessly learning superintelligence that self-discovers the foundations of all knowledge,\" the person familiar with his thinking said."
  },
  {
    "source": "Gadgets Now",
    "company": "Google DeepMind",
    "title": "Explained: What is Google DeepMind's Project Genie, how it works",
    "date": "2026-01-30T06:54:39Z",
    "url": "https://gadgetsnow.indiatimes.com/laptops-pc/explained-what-is-google-deepminds-project-genie-how-it-works/articleshow/127792521.cms",
    "content": "Google has announced Project Genie, a new experimental tool from its artificial intelligence (AI) research arm Google DeepMind that allows users to create and explore interactive digital worlds using AI. Available to select users, Project Genie is aimed to study how people use AI-generated world models. The company said this feedback will help guide future development in areas such as AI research, simulation, and generative media. Google said it plans to continue expanding access to Project Genie and improving the technology over time.\n\n\"Starting today, we're rolling out access to Project Genie for Google AI Ultra subscribers in the U.S (18+). This experimental research prototype lets users create, explore and remix their own interactive worlds,\" the company said in an official announcement.\n\nGoogle DeepMind CEO Demis Hassabis announced on X, writing \"Thrilled to launch Project Genie, an experimental prototype of the world's most advanced world model. Create entire playable worlds to explore in real-time just from a simple text prompt - kind of mindblowing really! Available to Ultra subs in the US for now - have fun exploring!\".\n\nProject Genie is a research prototype built on Genie 3, a general-purpose world model that Google DeepMind first previewed in August. \"Unlike explorable experiences in static 3D snapshots, Genie 3 generates the path ahead in real time as you move and interact with the world. It simulates physics and interactions for dynamic worlds, while its breakthrough consistency enables the simulation of any real-world scenario -- from robotics and modelling animation and fiction, to exploring locations and historical settings,\". the company said.\n\nWith Project Genie, Google DeepMind is offering early access to this technology through a web-based prototype. Starting this week, the tool is being rolled out to Google AI Ultra subscribers in the United States who are 18 years or older. The company said access will expand to more regions over time.\n\nProject Genie generates interactive environments in real time. As users move through a world, the system predicts and creates what comes next based on their actions. This differs from traditional 3D experiences, which are usually pre-built and static.\n\nThe prototype is powered by Genie 3, along with Gemini and Nano Banana Pro, two other Google AI systems. Together, these models allow users to build, explore, and modify digital worlds rather than simply view them.\n\nGoogle DeepMind said this work is part of its broader effort to develop artificial general intelligence, or AGI, which would require AI systems that can operate across many different real-world situations.\n\nProject Genie focuses on three main features.\n\nGoogle DeepMind said Project Genie is still an early research prototype and has several limits. Generated worlds may not always match prompts closely, physics may not behave realistically, and characters can sometimes be difficult to control. Each generated experience is also limited to about 60 seconds."
  },
  {
    "source": "DNyuz",
    "company": "Google DeepMind",
    "title": "Researchers Are Using A.I. to Decode the Human Genome",
    "date": "2026-01-28T16:38:31Z",
    "url": "https://dnyuz.com/2026/01/28/researchers-are-using-a-i-to-decode-the-human-genome/",
    "content": "In 2024, two scientists from Google DeepMind shared the Nobel Prize in Chemistry for an artificial-intelligence program called AlphaFold2.\n\nFor decades, scientists had struggled to understand how strings of molecular building blocks fold into the complex, three-dimensional structures of proteins. Demis Hassabis, John Jumper and their colleagues at Google DeepMind trained a program to predict the shapes; when AlphaFold2 was introduced in 2020, it performed so well at this task that scientists around the world adopted it.\n\n\"Everyone's using AlphaFold,\" said Alex Palazzo, a geneticist at the University of Toronto.\n\nScientists used the program to study how proteins normally work -- and how the failure to work can lead to disease. It helped them build entirely new proteins, some of which will soon be tested in clinical trials.\n\nNow another team of researchers at Google DeepMind is trying to do for DNA what the company did for proteins. AlphaFold, meet AlphaGenome.\n\nOn Wednesday, the researchers unveiled AlphaGenome in the journal Nature. They trained their A.I. on a vast wealth of molecular data, enabling it to make predictions about thousands of genes. For instance, AlphaGenome can predict whether a mutation will shut off a gene or switch it on at the wrong time -- a crucial question for understanding cancer and other diseases.\n\nPeter Koo, a computational biologist at Cold Spring Harbor Laboratory in New York who was not involved in the project, said that AlphaGenome represented an important step forward in applying artificial intelligence to the genome. \"It's an engineering marvel,\" he said.\n\nBut Dr. Koo and other outside experts cautioned that it represented just one step on a long road ahead. \"This is not AlphaFold, and it's not going to win the Nobel Prize,\" said Mark Gerstein, a computational biologist at Yale.\n\nAlphaGenome will be useful. Dr. Gerstein said that he would probably add it to his toolbox for exploring DNA, and others expect to follow suit. But not all scientists trust A.I. programs like AlphaGenome to help them understand the genome.\n\n\"I see no value in them at all right now,\" said Steven Salzberg, a computational biologist at Johns Hopkins University. \"I think there are a lot of smart people wasting their time.\"\n\nBefore the era of computers, biologists conducted painstaking experiments to uncover the rules that govern our genes. They discovered that genes are spelled out in a four-letter genetic alphabet called bases. To make a protein, a cell reads the sequence in a gene, which can run thousands of bases long.\n\nBut the more scientists studied the human genome, the more complicated and messy it turned out to be.\n\nAs cells read a gene, for example, they often skip over sections of its sequence. Through this process, known as splicing, cells can create hundreds of different proteins from a single gene.\n\nA number of diseases occur when cells splice their genes incorrectly. But there is no simple signature for the spots in genes where they should be spliced, so scientists have spent decades building up a catalog of them.\n\nAnother profound question about the genome is how cells choose which genes they use to make proteins. Scientists have discovered special molecules that grab hold of DNA and stretch it into intricate loops. In some cases, the loops expose a gene to the cell's protein-making machinery. In other cases, the gene ends up tucked away in a coil.\n\nThose molecules have to land precisely on tiny stretches of DNA to control genes. And these genetic locks can be hard to find since they often lie thousands or millions of bases away from the genes they control. In 2019, researchers at Google DeepMind embarked on a project that would evolve into AlphaGenome. By then, biologists had amassed huge amounts of data, including not only the three billion base pairs of the human genome but also the results of thousands of experiments measuring the activity of genes in many types of cells.\n\nThe researchers at Google DeepMind hoped that by training A.I. on these existing results, they could develop a program that could make accurate predictions about stretches of DNA it had never seen before.\n\n\"It was the right target for us,\" said Ziga Avsec, a research scientist at Google DeepMind.\n\nIn 2021, Dr. Avsec and his colleagues unveiled a preliminary A.I. called Enformer, which they have since expanded into AlphaGenome. They trained the program on an even greater expanse of biological data. \"It's really an industrial scale,\" Dr. Gerstein said.\n\nMany A.I. programs built to study the genome tackle only one aspect of it, like splicing. But AlphaGenome was trained to make predictions about 11 different processes. In the report on Wednesday, Dr. Avsec and his colleagues noted that AlphaGenome had performed as well or better than other programs across the board.\n\n\"It's state of the art,\" said Katherine Pollard, a data scientist at Gladstone Institutes, a research organization in San Francisco, who was not involved in the study.\n\nDr. Pollard and other researchers said that AlphaGenome was particularly adept with mutations, capable of predicting their effects, such as shutting down a nearby gene. In one performance test, the researchers added mutations to the stretch of DNA that includes a gene called TAL1.\n\nIn healthy people, TAL1 helps immune cells mature until they can fight pathogens. Once the cells have developed, the gene shuts down. But scientists have discovered that mutations 8,000 bases away from TAL1 can lead the gene to switch on permanently. That change can ultimately cause immune cells to multiply out of control, causing leukemia.\n\nDr. Avsec and his colleagues found that AlphaGenome had accurately predicted the impact of these mutations on TAL1. \"It has been really exciting to see, when these models work,\" he said. \"It feels like magic sometimes.\"\n\nThe AlphaGenome researchers shared their TAL1 predictions with Dr. Marc Mansour, a hematologist at University College London who spent years uncovering the leukemia-driving mutations with lab experiments.\n\n\"It was quite mind-blowing,\" Dr. Mansour said. \"It really showed how powerful this is.\"\n\nBut, Dr. Mansour noted, AlphaGenome's predictive powers fade the farther its gaze strays from a particular gene. He is now using AlphaGenome in his cancer research but does not blindly accept its results.\n\n\"These prediction tools are still prediction tools,\" he said. \"We still need to go to the lab.\"\n\nDr. Salzberg of Johns Hopkins is less sanguine about AlphaGenome, in part because he thinks its creators put too much trust in the data they trained it on. Scientists who study splice sites don't agree on which sites are real and which are genetic mirages. As a result, they have created databases that contain different catalogs of splice sites.\n\n\"The community has been working for 25 years to try to figure out what are all the splice sites in the human genome, and we're still not really there,\" Dr. Salzberg said. \"We don't have an agreed-upon gold-standard set.\"\n\nDr. Pollard also cautioned that AlphaGenome was a long way from being a tool that doctors could use to scan the genomes of patients for threats to their health. It predicts only the effects of a single mutation on one standard human genome.\n\nIn reality, any two people have millions of genetic differences in their DNA. Assessing the effects of all those variations throughout a patient's body remains far beyond AlphaGenome's industrial-strength power.\n\n\"It is a much, much harder problem -- and yet that's the problem we need to solve if we want to use a model like this for health care,\" Dr. Pollard said.\n\nCarl Zimmer covers news about science for The Times and writes the Origins column.\n\nThe post Researchers Are Using A.I. to Decode the Human Genome appeared first on New York Times."
  },
  {
    "source": "LaVanguardia",
    "company": "Google DeepMind",
    "title": "La IA AlphaGenome revela la información oculta en el ADN",
    "date": "2026-01-28T16:18:11Z",
    "url": "https://www.lavanguardia.com/ciencia/20260128/11450854/ia-alphagenome-revela-informacion-oculta-adn.html",
    "content": "El modelo de Google DeepMindi predice cómo las variantes en la secuencia del ADN afectan a las funciones biológicas\n\nMás de 3.000 investigadores de 160 países han empezado a utilizar la IA AlphaGenome de la compañía Google DeepMind desde que se puso a libre disposición de la comunidad científica hace seis meses. La nueva herramienta de investigación genómica predice cómo las variantes en la secuencia del ADN afectan a las funciones biológicas.\n\nAlphaGenome acelerará la investigación biomédica, afirman sus creadores en la revista Nature, donde hoy presentan los detalles técnicos del avance. Consideran que puede ser especialmente útil para diagnosticar enfermedades raras causadas por variantes genéticas infrecuentes, así como para investigar mutaciones involucradas en el cáncer y desarrollar nuevas terapias genéticas.\n\nLa nueva IA no solo descifra la información de los genes (que apenas representa el 2% del ADN humano y en gran parte ya se conoce), sino también el 98% de información no genética (que tiene un papel decisivo en la salud humana y en gran parte es territorio desconocido). Se trata de una IA de aprendizaje profundo entrenada con datos de genomas humanos y de ratones.\n\nComo ejemplo de su potencial, los investigadores citan la leucemia linfoblástica aguda de células T, que representa alrededor de un 2% de todas las leucemias. Estudios anteriores habían identificado mutaciones genéticas relacionadas con esta enfermedad pero no habían averiguado qué efecto tienen estas mutaciones. AlphaGenome ha revelado que activan el gen TAL1, involucrado en el desarrollo de las células sanguíneas.\n\n\"Hemos probado AlphaGenome con más de medio millón de nuevos experimentos y, sin duda, funciona muy bien\", declara Ben Lehner, investigador del Centre de Regulació Genòmica (CRG) en Barcelona y del Instituto Wellcome Sanger en Cambridge, a Science Media Centre. \"Es un excelente ejemplo de cómo la IA está desarrollando el descubrimiento biológico y el desarrollo de terapias\".\n\nGoogle DeepMind ya sorprendió a la comunidad biomédica en 2021 cuando presentó AlphaFold 2, una IA que predice la forma tridimensional de las proteínas a partir de la secuencia de ADN de los genes. Por aquel avance, que ha acelerado las investigaciones sobre proteínas y el desarrollo de fármacos, Demis Hassabis, cofundador y consejero delegado de Google DeepMind, recibió el premio Nobel de Química en 2024.\n\nPero AlphaFold solo analiza información de los genes, no el otro 98% del genoma humano, antiguamente llamado ADN basura porque se pensaba que no servía para nada, y ahora llamado ADN oscuro porque no se sabe qué hace exactamente, aunque se ha descubierto que tiene funciones importantes. De este 98% depende, por ejemplo, qué genes se activan en cada momento en cada célula.\n\nAlphaGenome no es la primera IA creada para predecir qué hace este ADN con funciones desconocidas. Pero es la mejor con diferencia, según los resultados presentados en Nature. La IA de Google DeepMind ha sido superior a los mejores modelos creados por otros investigadores en 25 de los 26 tests en que se han comparado sus prestaciones.\n\nLos creadores de AlphaGenome -entre los que se encuentra Demis Hassabis, aunque esta vez no ha dirigido la investigación como hizo con AlphaFold- han resuelto dos problemas técnicos que limitaban las capacidades de los modelos anteriores. Por un lado, han conseguido analizar secuencias de un millón de letras del ADN (o pares de bases) y aun así predecir los efectos de variantes de una sola letra. Los modelos anteriores, por el contrario, pierden resolución si analizan secuencias muy largas del ADN; y pierden capacidad de identificar efectos biológicos relevantes si se limitan a secuencias cortas.\n\nPor otro lado, AlphaGenome predice una amplia gama de efectos biológicos a partir de las secuencias de ADN. En cambio, los modelos anteriores son más precisos cuanto más se especializan en un efecto concreto. Estos efectos incluyen, entre otros, la expresión de genes, el inicio del proceso de producción de proteínas o mecanismos de regulación epigenética. Dado que todos estos efectos son importantes para las células, la especialización en un efecto concreto comporta renunciar a tener una visión general de lo que pasa en las células, limitación que AlphaGenome resuelve en gran parte.\n\nPara Google DeepMind, AlphaGenome no es un proyecto concluido sino un punto de partida. \"Desde que lanzamos el modelo hace seis meses para investigación no comercial, ha tenido un gran uso, con más de 3.000 usuarios en 160 países\", informó ayer en rueda de prensa Pushmeet Kohli, que ha gestionado el proyecto en Google DeepMind. \"Hemos tenido un feedback increíble de la comunidad académica\" para mejorar AlphaGenome.\n\n\"Aún nos queda un largo camino por delante\", añadió Ziga Avsec, que ha liderado el proyecto científico. Entre los retos pendientes, destacó mejorar la predicción los efectos de las variantes del ADN; comprender mejor los efectos de los variantes en cada célula y cada tejido; y avanzar hacia la predicción genómica personalizada. Para Avsec, AlphaGenome \"es una base sólida sobre la que la comunidad científica podrá construir; los investigadores podrán adaptar el modelo con sus propios conjuntos de datos para abordar mejor sus preguntas de investigación específicas\"."
  },
  {
    "source": "Science Media Centre",
    "company": "Google DeepMind",
    "title": "expert reaction to paper on Google DeepMind's AlphaGenome",
    "date": "2026-01-28T16:12:33Z",
    "url": "https://www.sciencemediacentre.org/expert-reaction-to-paper-on-google-deepminds-alphagenome/",
    "content": "A study published in Nature looks at Google Deepmind's AI AlphaGenome tool for predicting function from DNA sequences.\n\nProfessor Kristian Helin, CEO of The Institute of Cancer Research, London, said:\n\n\"AlphaGenome represents a major advance in computational genomics, matching or exceeding the performance of current state-of-the-art models for predicting functional elements directly from DNA sequence.\n\n\"The model shows strong predictive accuracy across key regulatory features, including RNA splice-site usage, the effects of genetic variants on gene expression, and chromatin accessibility. While it performs well in predicting overall gene expression levels, capturing cell type-specific regulation remains an important challenge.\n\n\"Despite these limitations, AlphaGenome establishes a powerful foundation for sequence-to-function modelling. In a rapidly evolving field, this work marks a significant step forward and helps lay the groundwork for more comprehensive approaches to functional genome interpretation.\n\n\"Even at this early stage, AlphaGenome has the potential to change how researchers generate hypotheses, prioritise experiments, and design more targeted and informative studies. Its immediate impact may be incremental, but its longer-term significance for the field is substantial.\n\n\"As seen previously with AlphaFold, the full impact of such approaches is likely to unfold over time. With continued methodological advances and the integration of increasingly rich experimental datasets, AlphaGenome could ultimately transform how we interpret genomes, understand disease mechanisms, and translate genetic variation into biological and clinical insight.\"\n\nDr Xianghua LI, Lecturer in Medical and Molecular Genetics, King's College London, said:\n\n\"The press release gives an accurate summary of the research. The main achievement is that this new AI can make many genetic predictions simultaneously, whereas most current tools handle only one. Bringing these abilities together in one system is a helpful advance.\n\n\"When we look at each prediction, this AI performs as well as the best existing tools, but not better. For important medical tasks, current AI models are still not reliable enough for patient care. Some 'best performing' tools often overstate the risks of certain genetic changes. These predictions are not yet ready for use in clinics.\n\n\"From a scientific point of view, the model does not uncover new findings about genes or biology. Some tough challenges remain, such as predicting very rare genetic variants. We still do not have enough good data for these cases.\n\n\"Still, this work is valuable as a starting point and a solid base for future progress. I hope the next version of Alphageome will show us deeper insights into how one genetic change, or several together, can affect many traits at once or in sequence. This understanding is crucial for learning about biology and making accurate predictions about genetic variants.\"\n\nDr Robert Goldstone, Head of Genomics at the Francis Crick Institute, said:\n\n\"DeepMind's AlphaGenome represents a major milestone in the field of genomic AI. This level of resolution, particularly for non-coding DNA, is a breakthrough that moves the technology from theoretical interest to practical utility, allowing scientists to programmatically study and simulate the genetic roots of complex disease.\n\n\"The model performs exceptionally well on tasks that might be expected to be governed by rigid 'grammatical' rules written in the DNA, such as splice site prediction. In these areas, it is poised to immediately replace older standard tools. And one of the most remarkable demonstrations is its ability to predict gene expression from DNA sequence alone. Whilst not perfect, given that gene expression is influenced by complex environmental factors that the model cannot see, achieving the level of accuracy demonstrated, based solely on 'local' DNA rules, is an incredible technical feat.\n\n\"AlphaGenome is not a magic bullet for all biological questions, but it is a foundational, high-quality tool that turns the static code of the genome into a decipherable language for discovery.\"\n\nProfessor Ben Lehner, Head of Generative and Synthetic Genomics, Wellcome Sanger Institute, Cambridge, said:\n\n\"AlphaGenome is a great example of how AI is accelerating biological discovery and the development of therapeutics. Identifying the precise differences in our genomes that make us more or less likely to develop thousands of diseases is a key step towards developing better therapeutics. AlphaGenome and models like it that help decipher the regulatory code of our genome will make it much easier to do this.\n\n\"As we have come to expect from Google Deepmind, AlphaGenome is a great piece of engineering that brings together ideas developed by many different scientists into a model that sets the standards. At the Wellcome Sanger Institute we have tested AlphaGenome using over half a million new experiments and it does indeed perform very well.\n\n\"However, AlphaGenome is far from perfect and there is still a lot of work to do. AI models are only as good as the data used to train them. Most existing data in biology is not very suitable for AI - the datasets are too small and not well standardized. The most important challenge right now is how to generate the data to train the next generation of even more powerful AI models. We need to do this fast, cost effectively and in a way that both the data and the resulting models are available for everyone to use.\n\n\"Interestingly, AlphaGenome was entirely developed in the UK by a very international team. This isn't luck but rather a direct consequence of years of government, charity and industry investment in science and attracting talent from across the world. It's a really exciting time with three areas where the UK is world-leading - genomics, biomedical research and AI - combining to transform biology and medicine. This magic combination of large-scale data generation and AI needs to be very strongly supported in the UK and Europe to make sure that the breakthroughs continue to happen here and also that they are used to benefit everyone.\"\n\nDr Fergal Martin, Eukaryotic Annotation Team Leader at EMBL's European Bioinformatics Institute (EMBL-EBI), said:\n\n\"AlphaGenome shows how far AI has come in helping us understand our genomes. It demonstrates that we can now predict many important biological features directly from DNA, without needing to run new experiments for every genome. This makes it possible to interpret the differences between human genomes faster. Further down the line, models like AlphaGenome could extend beyond humans to help interpret the DNA of plants, animals, and microbes that haven't been studied in detail before.\n\n\"AlphaGenome is one example among many AI models being developed in this area. The speed of progress will depend on sharing these models openly so the wider scientific community can test, improve, and build on them....\"\n\nProfessor Aldo Faisal, co-director of the School of Convergence Science in Human and Artificial Intelligence at Imperial College London, said:\n\n\"First of all, it is great to see progress in genome AI is being made by using public research data spanning national borders, I hope that the model will remain in the open and public domain going forward. The model sits in broad ecosystem of genome AI and is trained on major public functional genomics resources and uses an architecture that's a sensible evolution of what we already know works. The team reports broad benchmark coverage (e.g., outperforming specialized models on many tasks). As the impact of this model will be on research and development workflows, I would treat the results as promising rather than final until 1. more groups reproduce the findings and 2. we see robust, pre-specified evaluation panels.\"\n\nProfessor Rivka Isaacson, Professor of Molecular Biophysics in the Department of Chemistry at King's College London, said:\n\n\"This work is an exciting step forward in illuminating the 'dark genome'. We still have a long way to go in understanding the lengthy sequences of our DNA that don't directly encode the protein machinery whose constant whirring keeps us healthy. There are so many interwoven possibilities, and complex feedback mechanisms, that I doubt the whole thing will ever be fully untangled. AlphaGenome gives scientists whole new and vast datasets to sift and scavenge for clues.\"\n\n'Advancing regulatory variant effect prediction with AlphaGenome' by Demis Hassabis & Pushmeet Kohli et al. was published in Nature 16:00 UK time on Wednesday 28 January 2026.\n\nDOI: 10.1038/s41586-025-10014-0\n\nDeclared interests\n\nDr Robert Gladstone: No declarations of interest. There is a DeepMind lab at the Francis Crick Institute, but Robert has not worked on AlphaGenome.\n\nDr Fergal Martin: Fergal is involved in collaborative research with Google DeepMind. Fergal is not an author on, and was not involved in the development of the AlphaGenome model. EMBL-EBI has ongoing research collaborations with Google DeepMind, including work related to AlphaFold.\n\nProf Ben Lehner: has some research funding from Google DeepMind / a small collaboration with them. Ben is not an author on, and was not involved in the development of, the AlphaGenome model.\n\nProfessor Rivka Isaacson: No COIs.\n\nProf Aldo Faisal: I have no financial conflicts of interest with respect Google or Deepmind (unless pension funds type links).\n\nFor all other experts, no reply to our request for DOIs was received."
  },
  {
    "source": "Asianet News Network Pvt Ltd",
    "company": "Google DeepMind",
    "title": "Davos WEF: Google DeepMind CEO confirms AI Summit visit to New Delhi",
    "date": "2026-01-23T05:25:09Z",
    "url": "https://newsable.asianetnews.com/business/davos-wef-google-deepmind-ceo-confirms-ai-summit-visit-to-new-delhi-articleshow-dt2rxd1",
    "content": "Google DeepMind CEO Demis Hassabis confirmed his attendance at the AI Impact Summit in New Delhi in Feb 2026, following a meeting with Union Minister Ashwini Vaishnaw at the World Economic Forum in Davos, highlighting India's key role in AI.\n\nGoogle DeepMind CEO Confirms Summit Participation\n\nGoogle DeepMind CEO and Co-founder Demis Hassabis confirmed his participation in the upcoming AI Impact Summit in New Delhi following a meeting with Union Minister of Electronics and Information Technology Ashwini Vaishnaw at the World Economic Forum in Davos. The summit, scheduled for February 2026, focuses on India's role in the global development and implementation of artificial intelligence technologies.\n\nAdd Asianet Newsable as a Preferred Source\n\nThe announcement followed a series of diplomatic engagements at Davos, where the Union IT Minister Vaishnaw met with global technology leaders to discuss the sector's growth. Google DeepMind CEO Hassabis shared details of the interaction on the social media platform X, noting the significance of the upcoming summit. The discussions centered on the strategic positioning of India within the international AI landscape and the technical potential of these systems to address global challenges.\n\n\"Great to meet you Minister @AshwiniVaishnaw. Really enjoyed our discussion on AI's incredible potential to benefit humanity & India's important role in realising this - looking forward to continuing our conversation at the Summit!\" Hassabis stated on X.\n\nIndia's Push for AI Collaboration\n\nThe Google DeepMind executive's commitment to the February event aligns with the government's efforts to centralize AI policy discussions in the capital. Vaishnaw also met with OpenAI Chief Global Affairs Officer Chris Lehane during the forum. The Minister highlighted the necessity of collaborative frameworks in shaping technology for public benefit. He emphasized that the February summit serves as a platform for these international entities to engage directly with India's digital infrastructure.\n\n\"Met Mr. Demis Hassabis, CEO & Co-founder, Google DeepMind and Mr. Chris Lehane, Chief Global Affairs Officer, OpenAI, at WEF, Davos. Discussed India's growing role in shaping AI for global good. Encouraged active participation in the AI Impact Summit, to be held in New Delhi in February, 2026,\" the Minister said. (ANI)\n\n(Except for the headline, this story has not been edited by Asianet Newsable English staff and is published from a syndicated feed.)"
  },
  {
    "source": "Sifted",
    "company": "Google DeepMind",
    "title": "Ex-DeepMind scientist seeks $1bn for 'superhuman intelligence' startup",
    "date": "2026-02-18T10:43:14Z",
    "url": "https://sifted.eu/articles/david-silver-1bn-ineffable-intelligence/",
    "content": "The new company Ineffable Intelligence's raise is reportedly being led by Sequoia Capital\n\nIn a deal that could mark Europe's biggest ever seed round, former Google DeepMind scientist David Silver is reportedly raising $1bn for a London AI lab.\n\nThe round, which values the new company Ineffable Intelligence at around $4bn, is being led by Sequoia Capital, people with knowledge of the deal told the Financial Times. Nvidia, Google and Microsoft are also said to be considering investing.\n\nSilver, who left his research role at Google DeepMind late last year, hopes his new company will build on his work at the Big Tech research arm, particularly in reinforcement learning.\n\nIn this branch of machine learning, AI systems are trained to make decisions by interacting with an environment and receiving rewards or penalties based on their actions, enabling agents to learn through a trial-and-error process.\n\nAt Google DeepMind, Silver used reinforcement learning to train AI programs such as AlphaGo and AlphaStar to beat human players.\n\nSilver, who is also currently a professor at University College London and will serve as Ineffable Intelligence's CEO, is one of many top researchers in Europe who have left big AI labs in recent years to set up their own companies.\n\nFormer Meta chief scientist Yann LeCun is currently in talks to raise €500m at a €3bn valuation for a new world models startup, AMI Labs -- for which, Sifted understands, he has assembled a team of researchers from Google DeepMind, OpenAI, Elon Musk's xAI and Meta.\n\nMeanwhile, Paris-based AI giant Mistral was launched in 2023 by Google DeepMind alum Arthur Mensch and two former Meta researchers.\n\nAshish Patel, managing director at investment banking firm Houlihan Lokey Capital Solutions Group, tells Sifted Ineffable Intelligence's round is \"further evidence that the UK and wider European ecosystem can produce globally significant companies\", as \"well-informed investors are seeking the most capable individuals developing the most advanced technologies.\"\n\nSifted approached Ineffable Intelligence for comment."
  },
  {
    "source": "Fortune",
    "company": "Google DeepMind",
    "title": "How Demis Hassabis is leading Google through an innovator's dilemma -- and made OpenAI declare 'code red' | Fortune",
    "date": "2026-02-11T10:08:44Z",
    "url": "https://fortune.com/article/fortune-500-titans-and-disruptors-of-industry-google-deepmind-demis-hassabis-isomorphic-artificial-intelligence/",
    "content": "Fast forward to today, and Hassabis is still the one to beat. He runs all of Google's AI initiatives, including Gemini, which is quickly eating away at OpenAI's user base. In his spare time, Hassabis won a Nobel Prize, and he runs a startup called Isomorphic that wants to solve all disease with AI.\n\nIn a new episode of Fortune 500: Titans and Disruptors of Industry, Fortune's Editor-in-Chief Alyson Shontell sat down with Demis at the World Economic Forum in Davos to learn where he thinks the future is heading.\n\nHere's some of what they discussed during their 45-minute conversation:\n\nOn his early inspirations and the origin of DeepMind\n\nHow Hassabis manages his time between different teams\n\nOn Google's AI surge in 2025\n\nOn ensuring that tech advancements serve both business and society\n\nOn the near future of AI\n\nRead the transcript, which has been lightly edited for length and clarity, below.\n\nA quick message from our sponsor:\n\nJason Girzadas, CEO of Deloitte US: [Use cases for AI agents are] on the topic of every CXO conversation I'm a part of, and I think the thought process has to be looking for high impact areas that may not be necessarily the most glamorous or high profile functional areas, but are ripe for automation and use of this technology to create efficiencies as well as innovation. And over time, AI agents will be also in customer facing and growth oriented domains. In our case, Deloitte, we're using it within our financial organization, looking at very mundane processes like expense management and working capital management. We're seeing other organizations using it in call centers and with software development, that can be automated.\n\nYou've had a huge 2025. It sounds like you're gearing up for a great 2026, but before we get into both of those things, I want to just take a step back so people can get to know you a little bit better. One of the things you love is chess, you're a chess master. You also love astronomy. I'm curious how both of those things took you into AI or shape how you think about AI?\n\nHassabis: Yeah, well, I've always been interested in things like astronomy, cosmology, physics as a kid, because I've always been interested in the big questions. What's actually happening here in the universe? The nature of consciousness, all of these types of things. So you get sort of drawn to physics if you're interested in the big questions.\n\nAnd then for me, for chess, I also love games. I love strategy. I ended up training my own mind by playing chess as a kid, very seriously.\n\nAnd then that got me thinking about thinking -- and how does the brain work? And then I've combined all that together. That sort of led me to AI and computers, and AI being a way to understand our own minds, but also a perfect tool for science and understanding the universe out there.\n\nYou cofounded DeepMind a number of years ago, and in about 2014 you sold it to Google for about $500 million at the time. It was a hot deal. I know Meta wanted it too. And from my perspective, I think we're going to look back on that moment as one of the most transformative moments of business history.\n\nYou've given Google the foundation with which to build an incredible AI machine and really take it into the future. When you look back on that, how do you feel about that moment? How did you make that decision? Did you know it was going to be such a big moment at the time?\n\nWe did, actually, those of us that were involved in the science. We started DeepMind in 2010, which was 15+ years ago now, and nobody was talking about AI. But we knew, and we set out with the mission of solving intelligence and then using it to solve everything else.\n\nSo we wanted to be the first company to build artificial general intelligence. And the main thing we wanted to apply it to was solving scientific problems. So when Google came along in 2014 -- and it was actually driven by Larry at the time, Larry Page, who was the CEO -- we knew that in some ways we were sort of underselling.\n\nBut, on the other hand, what mattered to me was not the money, it was the mission, and being able to accelerate our progress towards artificial general intelligence and answering these scientific questions that we were trying to solve. And I felt that teaming up with Google would accelerate that, mostly because they had obviously enormous compute power, and we see today how important that is for developing intelligence.\n\nSo at the time, I did mention to Larry, and also the head of search at the time, who was driving the deal that it might turn out to be the most important acquisition Google has ever done. Which is saying something, because they've acquired YouTube and Android, they've got a good history of buying important things.\n\nNow, if you go back and you look at the origins of OpenAI -- Elon and Sam got together because they were afraid that Google might now have a monopoly in the AI space with the Deep Mind acquisition. So, really, that also kind of created a mega competitor at the time.\n\nYeah, I guess there's all these sort of butterfly effects that happen. And I think partly also was the success of things like AlphaGo, the first program to be the world champion at the game of Go. Using these kinds of learning systems that we're familiar with today -- reinforcement learning, deep learning, at the heart of it -- I think that was a big watershed moment as well. In 2016, it's actually the 10 year anniversary of that breakthrough. And I think that really started the starting gun for the modern AI era, including things like OpenAI. I know the founders of that watched that match, and wanted a piece of that action.\n\nUnder Google and Alphabet, you've been able to have a lot of moon shots, take risks, try things that haven't necessarily led to money immediately, but have been profound breakthroughs. And [for] one of them, you won a Nobel Prize. Congratulations, it's incredible.\n\nI was wondering if you could just tell me a little bit more about AlphaFold and why that's such a big deal in terms of how we could be looking at solving diseases moving ahead.\n\nI think this is one of the benefits of being a part of Google and Alphabet, having the resources and the time to really go after these sorts of deep scientific problems. And AlphaFold, I think, is the best example of that.\n\nIt's basically a solution to a 50-year-old grand challenge in biology -- can you determine the 3D structure of a protein just from its amino acid sequence, basically from its genetic sequence? And this is incredibly important, because proteins basically do everything in your body, from muscles to neurons firing. Everything depends on proteins. And if you know the 3D structure of a protein, what it looks like in your body, then you kind of partially know what the function it does, what it supports.\n\nObviously, it's important also for disease, because things can go wrong with proteins. They can fold in the wrong way, like in something like Alzheimer's, and then that can create a disease. So [it's} really important for drug discovery, as well as fundamental biology. And AlphaFold, it was a solution to this problem that was posed 50 years ago by another Nobel Prize winner -- actually Christian Anfinsen -- that this should be possible and to go directly from a one dimensional string of amino acid sequence to this 3D structure. How does it scrunch up into a ball?\n\nAnd AlphaFold was a solution, and it's so efficient. Not only is it accurate, we folded all 200 million proteins known to science, and then we put that on a huge database with the European Bioinformatics Institute. And for free, into the world for everyone to use. So now over 3 million researchers around the world make use of AlphaFold every day.\n\nWow. You're using some of it, I believe, for Isomorphic, which is a startup that you have, I want to say on the side. So you're doing two huge jobs at once. You've raised hundreds of millions of dollars with Google, of course, as a backer, for Isomorphic.\n\nCan you just explain the mission there? And you have some lofty goals, like you say, we're going to solve all disease. You don't say cure, you say solve. Also walk me through how hard it is to get a drug to trial, because that's historically been very difficult.\n\nSo that was always the idea behind AlphaFold. Obviously there's a lot of fundamental science that can be done if you understand the structures of proteins, including designing new proteins that do new things. So you can sort of use AlphaFold in reverse to sort of go, Okay, I want this particular shape. How do I get it from a genetic sequence?\n\nBut to do drug discovery, knowing the structure of protein is only one small part of that whole process. And usually it takes, like on average, 10 years to go from understanding a target for a disease, all the way to a drug that's ready for the market. So it's an enormous amount of time and cost. Billions of dollars, a decade or more. And most drugs fail along the way. It's only like a 10% success rate. So it's just incredibly inefficient, because biology is so complicated.\n\nSo what I've always dreamed about doing, and was the first thing I wanted to apply AI to, was human health, improving human health. What could be a more important use of AI? And AlphaFold was the proof point that this could be possible.\n\nAnd then Isomorphic, we spun that out after AlphaFold was done -- so three, four years ago -- to develop additional AlphaFold-level breakthroughs surrounding AlphaFold so you can think more in the chemistry space. So, if you now know the structure of a protein, you need to know where the chemical compound you're designing -- the drug, basically-is going to bind to the protein and what it is going to do. And so you need to build other AI systems that can predict all of that.\n\nSo that's what we've been doing in Isomorphic. It's going incredibly well. We have great partners with Eli Lilly and Novartis, the best pharmas in the world. We have like 17 drug programs active already, and we plan to eventually go to hundreds.\n\nAnd I think this is the way to make real step change progress in human health. You basically do your search and your hypothesis searching in silico, and that's hundreds, thousands of times more efficient than doing it in a wet lab. And you save the wet lab part just for the validation step.\n\nOf course, eventually you have to test it in trials, human trials, and all those types of things to make sure everything's safe. But you can do all of your search and design, or almost all of it, in silico. That's the plan.\n\n2026, you mentioned, is going to be a big year. I imagine, for both Google and for Isomorphic. Do you anticipate, in early 2026, this could be the moment that you get the first drug to trial? And might it be in cancer?\n\nYes, so we're working on, actually, several spaces. Cancer, cardiovascular, immunology, and then eventually we'd like to branch out to all therapeutic areas. We're building a general drug discovery engine platform, you can think. And we are already in pre-clinical trials, very early stage, for some cancer drugs. And then, hopefully by the end of the year, if those are successful, we'll start going towards clinical trials.\n\nHow do you manage yourself and your time and your teams? Because you're achieving really, really difficult things, whether it's the launch of Gemini 3 -- which was very successful and well received -- or it's getting drugs to trial. These are sounding like very different things, two different teams to run. You can't be in two places at once. How are you doing this? How are you running two companies?\n\nYou know, one of my skills is bringing together amazing, world class interdisciplinary teams. I've loved managing those teams. I love composing those management teams together. And I've got incredible teams both at Google DeepMind and Isomorphic.\n\nIf we take Isomorphic, for example, we've blended top biologists and chemists along with top machine learning and engineering. And I think there's a lot of magic that happens when you have these kinds of interdisciplinary groups.\n\nAnd then if we think about the Google DeepMind side, there we've tried to blend together the best of the startup worlds, like what we were doing at DeepMind originally. And then scale, in a kind of multinational scale, with all the advantages of having these amazing product surfaces that we can immediately deploy -- technologies like Gemini 3 and immediately get great feedback from users. And also help in the everyday lives of billions of users. So it's amazingly exciting and motivating, actually. And in terms of the way I manage my time, you know, I don't sleep very much.\n\nLike a couple of hours?\n\nYeah, well, a bit more than that. That would be bad for the brain. So I do try and get six, but I have unusual sleeping habits. I sort of manage during the day, and try and pack my day in the office with as many meetings as possible, back to back, almost no break between. Then I get home, spend a little bit of time with the family, have dinner, and then I sort of start a second day of work at about 10 P.M. and go to 4 A.M., where I do my thinking, more creative and research work. And it's worked out. I've done that for about a decade now, and it works well.\n\nI can't imagine being creative at four in the morning, but if it works for you.\n\nYeah, I come alive at about 1 A.M.\n\nYou're clearly good at motivating teams to do hard things. In 2023, a decision was made at Google to put two different AI teams under you. How did you work out management kinks there and get the team shipping again? Because there was this feeling that Google was a little bit asleep at the wheel for AI. And I'm curious if you think that's true, and how you got them to wake up?\n\nYeah, well, we had two world class groups in original DeepMind and Google Brain. And actually, I think often, as a collective, we don't get enough credit for the fact that I think about 90% of the modern AI industries are built on technology or discoveries made by one of those two groups. From Transformers to AlphaGo and deep reinforcement learning. So we have, and we still have, I think, the deepest and broadest research bench. So we have incredible talent. I think, better than anywhere else in the world by a long way.\n\nBut it was getting complicated having two groups, especially given the amount of compute needed in this scaling era. So that was really why we had to put the two groups together, so we could pull all of the talents together working on a single project in Gemini. But also, even [a company] like Google didn't have enough compute to have two frontier projects under one house, so we needed to combine all of our easels together.\n\nYou know, I'm a very collaborative person. I'm very open minded about different ways of working and I'm always looking to improve as well. One of the watch words I live by is this Japanese word, Kaizen, that I love. Which is sort of striving for continual self-improvement. And that's what I always try to do.\n\nI'm always in learning mode. Perhaps that's why I like building learning machines, because I like learning, and there's always something you can learn no matter how expert you are at what you do. And bringing the two groups together and trying to combine the best of both cultures has been great, and I think we're reaping the rewards of that now.\n\nGoogle DeepMind is the way we think about it, like the engine room of Google. It's like the nuclear power plant that's plugged into the rest of this amazing company in Google. And I think one of the things we did -- one of the things I'm very proud of -- is getting the shipping culture going and sort of rediscovering, I guess, the golden era of Google, back 10, 15 years ago and taking risks. Calculated risk, shipping things fast, and being innovative.\n\nAnd I think that's all working out really well now, whilst at the same time being thoughtful and scientific about and rigorous about what we put out in the world, whether that's engineering or scientifically. And I think, and I hope, we're getting that balance right.\n\nYou mentioned going back to the golden era of Google, so much so that the founders, at least Sergey, seems like he's back involved. How is it like working with him on AI and Google?\n\nIt's been great. And Larry is too, in different ways. Larry, more strategically. Sergey has been in the weeds, programming away on things like Gemini, and it's been fantastic seeing them engage.\n\nAre you putting him to work, are you like, Sergey, I need this code right now?\n\nNo, it's more like he chooses what to work on, but it's great seeing him in the office and pushing things in certain directions. And it's easier if the founders are heavily involved. And I still act as well like a cofounder of Google DeepMind, in terms of what we've got to do, and strategically, what we pick to do. And that's something I think I've learned to do well over the last, you know, 10-15, years.\n\nWhen you have some ambitious goal, like solve all disease or build AGI, what are the intermediate goals that are also very ambitious, but are waypoints? What are the right ones to pick? And I think we've done that historically pretty well with most of the Alpha projects, AlphaGo, AlphaFold, and so on, and then now Gemini. And I think that's really critical, actually, for any very ambitious scientific and engineering project, is breaking it down into manageable steps so that you can see you're in the right direction.\n\nAnd I think that we very clearly are with the technology that we're building. And it's been an incredible couple of years for us, and I think we're getting into our groove, I would say. And I think other people, and the external world, are starting to feel that, including things like Wall Street and the share price.\n\nIt definitely seems like there must have been some sort of KPI measurement, charge ahead, unifying moment, because the launch of Gemini 3 brought much fanfare, among other launches, that caused OpenAI to go to a code red, which they claim happens all the time. And then you have this huge monster deal with Apple that is monumental, I think, for the industry.\n\nSo I'm curious what happened internally, behind the scenes -- how did you set those KPIs for the team? And then how are you setting them to keep the momentum in 2026?\n\nWell, look, I think for me, it always starts with the research. Like having the best models in this case, and obviously fundamental research feeding into that. And I always believe you then need to reflect that obviously as quickly as possible in your products, and then you've got to get your marketing and distribution right. But none of it matters if your models aren't best in class, aren't state of the art.\n\nSo that's what we focused on, first with the Gemini models, but also our other models like Nano Banana, our image model, which went super viral. And that was a big part of their success last year, our video model, Veo, our world model. So there's more than just large language models, and we're kind of state of the art on all of those.\n\nAnd then it was about sorting things out internally, almost rebuilding the infrastructure in some way at Google, so that you could reflect very quickly the power of the latest models into the lighthouse products, including Search, YouTube and Chrome. All these amazing surfaces that we have, as well, of course, as the Gemini app. It was new for everyone in the industry. and I think it takes a little while to re-architect things around that.\n\nAnd very much, again, this idea of Google DeepMind being the engine room, providing the engine for the rest of the organization to use. I think that took a year, 18 months, to get right, but I think we're seeing the results of that now. I think there's still more to go, by the way, and we can have even faster velocity. And the other thing is just also instilling this culture of intensity and pace and focus and really focusing only on the things that matter, and cutting out distractions. Maybe the final thing I would say is, I think there's a lot to say, especially in today's very noisy world, to just consistently deliver good decisions, good rational decisions. And over time, minimal drama. I think it's just amazing how much that compounds over time. I think we're building a lot of momentum now, and I think hopefully we'll see that even more this year,\n\nSort of like we mentioned before, the decision to sell DeepMind to Google was a monumental moment, a transformational moment in business -- if you're successful now, I think that will be perhaps the biggest transformation in business.\n\nHow does that weigh on you to make sure that you, as a leader, are driving this in a direction that's good for society, good for the workforce, good for Google? Because it is a little bit of an innovator's dilemma where this is the search king, a huge business model based on ads, and if you're successful...\n\n...well look, it is a classic innovator's dilemma. I think we've navigated it pretty well so far, and search is more successful than ever. But also there's this aspect of, if we don't disrupt ourselves, someone else will. So you're better off being ahead of that, I think, and doing it on your terms. And so I think that's what we found. In terms of responsibility, I've felt that way not just at Google, but before DeepMind, even in my academic career.\n\nBecause myself and Shane especially, our chief scientist, when we started DeepMind, it seemed like a fanciful idea, but we really believed that it would be possible to create artificial general intelligence. And we understood what I think more and more people are understanding now -- how transformative to the world that would be. Amazing for things like science and human health and maybe helping with energy and so on.\n\nBut also, there are risks. It's a dual purpose technology. Harmful actors, bad actors could use it for harmful ends. And eventually, as technology becomes more autonomous, more agentic, and we get towards AGI, there's technological risk too. And so I worry a lot about all of those things.\n\nAnd we have to make sure that the engine and the economic engine works as well, so we have enough money to fund our research and fund things like AlphaFold and give it to the world for free. That's not easy. It costs a lot of money to create something like it and hire the researchers to create something like AlphaFold, but we do a lot of things like that, and I want to do more things like that for the world, but that requires us to be successful also on the commercial side. So I think there's a balance to be had there.\n\nBut the responsibility in part comes in as well, and I feel like we can do this at Google -- we have the platform to show how AI can be deployed in a responsible way and a beneficial way for all of society. And all of us who are frontier labs producing AI, we have choices about -- what should we use AI for? Are we going to use it for things like medicine and for alleviating administration and helping with things like poverty, or are we going to use it for exploitative things? And I think that we're going to try and be a role model for all of the good things that can come with AI.\n\nIt doesn't mean we won't make any mistakes. We will because it's such a nascent and complex technology, but we'll try to be as thoughtful as possible, and we'll try to be as scientific about it as possible, too. The scientific rigor we bring to our work, and always have, I think, is going to matter here a lot. I mean, it's a scientific endeavor in the end.\n\nAnd then I hope that the kind of reliability and security and safety that we like to work on will come through in our products. I think the market will reward that. Because if you think about enterprises that use these technologies, as they get more sophisticated, they're going to want to know -- if you're a big bank or insurance company, health company, medical company -- that you have some guarantees about what your AI systems that you're bringing in are going to do.\n\nSo I think that could be a good aspect of AI becoming very commercial, is that there'll be commercial incentives to be robust and reliable and secure, and all the things that you'd want actually in preparation for AGI coming into the world.\n\nSo when you look at the year ahead, what do you think the story of AI will be? What will we achieve?\n\nWell, I mean every year is pretty pivotal in AI. And it feels like, at least for those working at the coalface, that 10 years almost happens every year. And I think this year will be no different.\n\nIt's very intense, but you've also got to, every now and again, look up at the strategic picture. I think that, at least for us, with Gemini 3, we've crossed a watershed moment, in my opinion. And hopefully, those of you who've used it will feel that it's very capable now. And I'm certainly using it in my everyday life to help me with my research and summarizing things and doing some coding.\n\nSo I think that these systems are now ready to maybe build agents. The whole industry has talked a lot about agents and more autonomous systems and delegating whole tasks to them. But I think maybe by the end of this year, we'll really start seeing that.\n\nI'm very excited about assistance coming into the real world, maybe with glasses. We have a big project on smart glasses. I think that the AI technology is only just about there to make that actually viable. And I think that could be a kind of killer app for glasses. I think that part, bringing that into the world. Also robotics -- I still think there's more research to be done on robotics, but I think over the next 18 months or so, we're going to see breakthrough moments in robotics, too.\n\nSo all of these areas we're pushing very hard on, as well as, of course, improving Gemini itself.\n\nThose aren't the glasses, right? I would buy those if they were.\n\nNo, they're not.\n\nI was going to ask you about the future form of -- computers were not built for AI and all the things that AI can do. What do you think is the future for it? It sounds like glasses...\n\n...I think glasses would just be one of the solutions. I have this side notion -- we talk about this notion internally of a universal assistant, and what we mean by that is an assistant that's super helpful in everyday life. Recommending new things, enriching your life, dealing with admin, all of these types of things, but it goes across all of the surfaces. So it exists on your computer, on your browser, on your phone, and then I think there'll be new devices too, like glasses. And it will be the same assistant that understands your context across the different conversations you've had, whether that's in your car or in your office. And if you want it to, that can all be integrated together. And, I think, help you improve your life across all those different aspects.\n\nMaybe for Christmas next year, the holidays next year, we can all get our Google Glasses.\n\nThat's, that's the idea.\n\nYou had them just way too early, I think, before when they...\n\n...like a lot of things we've done at Google, we maybe pioneered all these spaces. Perhaps a little bit too early, in hindsight, with glasses. Both the technology of making them not too chunky and things, but also I think it was missing the killer app. And I think an AI digital system could be that.\n\nYeah, amazing. One last question for you. I want to ask your biggest, boldest prediction for how AI will transform the world. When you look ahead -- I know you said 10 years is one year now -- but when you're looking ahead, are you [seeing] the abundance world where AI can solve all of our problems? What does it look like?\n\nI think done right, in 10-15 years time, we'll be in a new golden era of discovery. That's what I hope, a kind of new renaissance. And I think human health will be revolutionized. Medicine won't look like it does today. I think that personalized medicine, for example, will be a real reality. And I think we'll have used these AI technologies to solve many big problems in science, and things like new materials, maybe help with fusion, or solar, or optimal batteries -- some way of solving the energy crisis. And then I think we'll be in a world of radical abundance, where we can use those energy sources to travel the stars and explore the galaxy. That's what I think our destiny is going to be."
  },
  {
    "source": "ai.zhiding.cn",
    "company": "Google DeepMind",
    "title": "AI鸟类训练模型揭开水下海洋奥秘",
    "date": "2026-02-11T02:19:18Z",
    "url": "https://ai.zhiding.cn/2026/0211/3179017.shtml",
    "content": "谷歌DeepMind发布的生物声学基础模型Perch 2.0虽然主要基于鸟类和陆地动物声音训练，但在海洋声学任务中表现出色。该模型能够有效分类鲸鱼发声，在区分不同须鲸物种和虎鲸亚群方面展现优异性能。研究显示，通过迁移学习，Perch 2.0在多个海洋数据集评估中consistently排名前列，为海洋生态系统研究提供了高效的声学分析工具。\n\n我们介绍了Google DeepMind的生物声学基础模型Perch 2.0如何在主要基于鸟类和其他陆地动物发声训练的基础上，成功迁移到水下声学挑战中，在鲸鱼相关任务上表现出色。\n\n水下声学的重要性\n\n水下声音对于理解海洋物种及其环境的不可见模式至关重要。海洋声景充满了神秘的噪音和未被发现的奥秘。例如，美国国家海洋和大气管理局最近将神秘的\"生物弦音\"归因于难以捉摸的布氏鲸，这说明了新声音类型和物种归属识别的持续挑战。\n\nGoogle在与外部科学家合作使用生物声学监测和保护鲸鱼方面有着悠久的历史，包括我们检测座头鲸分类的原始研究模型和2024年发布的多物种鲸鱼模型。为了跟上这一步伐，Google在生物声学AI方面的方法正在发展，以实现从新发现到科学见解的更高效连接。2025年8月，Google DeepMind发布了最新的Perch基础生物声学模型Perch 2.0，这是一个主要基于鸟类和其他陆地发声动物训练的生物声学基础模型。令人惊讶的是，尽管训练中不包含水下音频，Perch 2.0在海洋验证任务的迁移学习中作为嵌入模型表现出色。\n\n在我们最新的论文\"Perch 2.0在水下任务中的迁移学习\"中，Google Research和Google DeepMind合作在NeurIPS 2025非人类动物交流AI研讨会上展示了这些结果。我们深入研究了这些结果，展示了这个主要基于鸟类数据训练的生物声学基础模型如何用于支持和扩展水下海洋生态系统的洞察，特别是在鲸鱼发声分类方面。\n\n迁移学习的优势\n\n如果预训练的分类模型，如我们的多物种鲸鱼模型，已经具有必要的标签并在研究人员的数据集上表现良好，它可以直接用于为音频数据产生分数和标签。然而，为了为新发现的声音创建新的自定义分类器或提高新数据的准确性，我们可以利用迁移学习而不是从头构建新模型。这种方法大大减少了创建新自定义分类器所需的计算和实验量。\n\n在生物声学迁移学习中，预训练模型用于为每个音频窗口生成嵌入。这些嵌入将大量音频数据减少为更小的特征数组，作为简单分类器的输入。为了为任何标记音频数据集创建新的自定义模型，我们将预训练模型应用于音频数据以获得嵌入，这些嵌入用作逻辑回归分类器的输入特征。\n\n实验评估和结果\n\n我们使用少样本线性探测在海洋任务上评估了Perch 2.0，如区分不同须鲸物种或不同虎鲸亚群。其性能与我们Perch Hoplite存储库中支持的预训练模型进行比较，包括Perch 2.0、Perch 1.0、SurfPerch和多物种鲸鱼模型。\n\n对于水下数据评估，我们使用了三个数据集：NOAA PIPAN、ReefSet和DCLDE。\n\n我们的结果显示，每类更多示例可以提高所有模型的性能，除了ReefSet数据，在该数据上，即使每类只有四个示例，所有模型的性能也很高，多物种鲸鱼模型除外。值得注意的是，Perch 2.0在每个数据集和样本大小上始终是表现最佳或第二佳的模型。\n\n我们还将Perch 2.0与AVES-bird和AVES-bio的嵌入以及康奈尔鸟类学实验室的BirdNet v2.3进行了比较。Perch 2.0在大多数水下任务上超过了AVES-bird和AVES-bio。\n\n迁移性能分析\n\n我们为从主要基于鸟类训练的模型到水下声音的迁移性能提供了几个可能的原因。首先，先前的研究表明，具有大量训练数据的更大模型泛化能力更强。此外，分类相似鸟类叫声的挑战迫使模型学习详细的声学特征，这些特征可以为其他生物声学任务提供信息。最后，不同类型物种之间的特征迁移也可能与声音产生机制本身有关，各种物种包括鸟类和海洋哺乳动物已经进化出类似的声音产生方式。\n\n敏捷建模方法\n\nGoogle DeepMind Perch团队与Google Research和外部合作伙伴合作，开创了生物声学的敏捷建模方法，可以在几小时内从少量标记示例创建自定义分类器。为了支持Google Research合作伙伴以及更广泛的鲸类声学社区，我们为使用托管在Google Cloud上的被动声学档案数据集中的NOAA数据创建了端到端演示。\n\nA：Perch 2.0是Google DeepMind开发的生物声学基础模型，主要基于鸟类和其他陆地动物发声进行训练。其特殊之处在于，尽管训练中不包含水下音频，但在海洋生物声学任务中表现出色，能够有效分类鲸鱼发声。\n\nQ2：为什么基于鸟类训练的模型能够处理水下声音？\n\nA：有几个可能原因：首先，大型模型具有更强的泛化能力；其次，区分相似鸟类叫声的挑战迫使模型学习详细的声学特征，这些特征对其他生物声学任务也有帮助；最后，不同物种的声音产生机制可能存在相似性。\n\nQ3：迁移学习在生物声学中有什么优势？\n\nA：迁移学习可以大大减少创建新自定义分类器所需的计算和实验量。它使用预训练模型生成音频嵌入作为简单分类器的输入，而不需要从头训练深度神经网络的所有参数，这对研究人员的时间和计算资源都更加高效。"
  },
  {
    "source": "IEEE Spectrum: Technology, Engineering, and Science News",
    "company": "Google DeepMind",
    "title": "How AlphaGenome Is Changing the Genomic Research Landscape",
    "date": "2026-02-04T15:01:01Z",
    "url": "https://spectrum.ieee.org/alphagenome-ai-gene-regulation",
    "content": "The team at Google DeepMind behind that Nobel Prize-winning platform then turned their lens from from the structure of proteins to how these molecules function in the body. Applying similar machine-learning methods, they first developed AlphaMissense, an AI tool for predicting which changes in protein structure are likely to cause disease. AlphaProteo, a system for designing proteins that bind to specific molecular targets, came next.\n\nNow the architects of the Alpha platform are pushing beyond proteins into genomics, seeking to decipher how the vast regulatory regions of DNA shape when, where, and how genes are turned on and off.\n\nEnter AlphaGenome. Described as a \"Swiss Army knife for exploring non-coding DNA,\" the deep-learning tool offers a way to systematically interpret the 98 percent of the genome that does not encode instructions for making proteins, but instead orchestrates how those genetic instructions are used inside the cell.\n\n\"This allows us to model intricate processes... with unprecedented precision,\" Žiga Avsec, head of genomics at Google DeepMind, said in a press conference unveiling the new tool.\n\nAlphaGenome has its limitations. For instance, the tool's training data draw largely from bulk tissue datasets, curbing its reliability in rare cell types or specific developmental stages, notes Christina Leslie, a computational biologist at Memorial Sloan Kettering Cancer Center. \"Generalization to new cell types is a huge limitation,\" she says.\n\nIt also struggles to capture distant effects when regulatory regions are hundreds of thousands to millions of DNA letters away from their target genes, Leslie pointed out.\n\nEven so, the model is helping scientists to prioritize which genetic variants are most likely to matter, narrowing the search from across the genome to a manageable set of testable hypotheses. \"It is the state of the art right now,\" Leslie says.\n\nAccording to DeepMind, thousands of scientists around the world are already using AlphaGenome, which is freely available on GitHub for academic research purposes. It is being put to work across a range of applications, including pinpointing genetic drivers of cancer and rare diseases, discovering new drug targets, and designing synthetic strands of DNA with tailored regulatory functions.\n\n\"It's exciting to have things like AlphaGenome come out and perform much better than all the other dedicated algorithms that are exploring various aspects of genome biology,\" says Richard Young, a biologist at the Whitehead Institute for Biomedical Research who has collaborated with Google DeepMind on its AI co-scientist platform but was not involved in AlphaGenome. \"It's a huge accelerator.\"\n\nThe arrival of AlphaGenome marks another step in AI's steady advance into some of biology's most stubborn and consequential challenges.\n\nFor DeepMind, there is also a clear industrial logic at work. The company's growing stable of biological models -- spanning protein structure, mutation, and generation, and now genomic regulation -- amounts to a vertically integrated platform for molecular prediction. That platform, in turn, should help unlock new diagnostic capabilities and therapeutic strategies, according to Pushmeet Kohli, vice president of science and strategic initiatives at Google DeepMind.\n\n\"All these different models are solving key problems that are relevant for understanding biology,\" Kohli says.\n\nAlphaGenome is the latest -- and most expansive -- piece of that strategy. Trained on raw DNA, the model predicts 11 types of biological signals that help determine how genes are used inside cells. These include whether a gene is turned on or off, where gene activity begins, how genetic messages are edited, how tightly DNA is packed, which regulatory proteins bind to it, and how distant regions of the genome interact with one another.\n\nMany of these features already have their own specialty AI tools -- SpliceAI for splice site prediction, ChromBPNet for local chromatin accessibility, Orca for three-dimensional genome architecture. But such tools are typically used in isolation, requiring researchers to stitch together results from multiple sources.\n\n\"AlphaGenome replaces this fragmentation with a more unified framework, which is more convenient and user-friendly -- and we hope this will accelerate scientists' workflows,\" says Natasha Latysheva, a computational geneticist at Google DeepMind.\n\nAnd while there have been attempts to capture all manner of regulatory effects in a single model, earlier architectures such as Borzoi and Enformer typically traded fine-scale resolution for breadth of biological coverage.\n\nAlphaGenome tries to escape that trade-off. The model can ingest up to one million DNA letters at a time, preserving long-range regulatory context, while still making predictions at single-base-pair resolution. In practical terms, that means it can ask how a change in one nucleotide might reverberate across a vast swathe of the genome.\n\nThe new paper presents several demonstrations of this capability.\n\nIn one case, AlphaGenome correctly predicted how a tiny deletion disrupts a splice site in a gene involved in blood vessel biology, leading to reduced RNA output. In another, it captured how mutations near a cancer-linked gene boost its activity, helping to drive an aggressive form of leukemia.\n\nWhether this predictive power generalizes beyond well-studied genes remains an open question, though.\n\n\"This is obviously a potentially valuable tool -- but it's a tool,\" says Charles Mullighan, deputy director of the St. Jude Children's Research Comprehensive Cancer Center. \"It's not a final point of discovery, but it's going to be a very important tool for giving insights that then might guide further functional analyses and experiments.\"\n\nOne \"quirk\" of the system, notes Latysheva, is its bias toward false negatives over false positives, meaning it is more likely to miss a genuinely important DNA variant rather than incorrectly flag a harmless one. \"But the flip side of that is if it does predict a strong effect, it's actually very accurate,\" she says. So, when the model serves up a strong prediction, \"you can have a decent amount of confidence that it knows what it's doing.\"\n\nThat confidence proved useful for Y-h. Taguchi and Kenta Kobayashi from Chuo University in Japan when they set out to stress-test a data-driven link between sleep deprivation and specific neuronal cell types. Early adopters of AlphaGenome, the bioinformatics researchers used the AI tool as an independent cross-check, confirming that genes implicated by sleep loss were especially active in their neurons of interest -- just as their earlier analysis of gene-expression data from brain tissue had predicted.\n\n\"AlphaGenome succeeded in the cross validation,\" says Takuchi, who published the results January 1 in the journal Genes.\n\nThat sort of validation underscores AlphaGenome's role. Like AlphaFold before it, the system is not meant to explain biology in full, but to make its most opaque regions easier to explore."
  },
  {
    "source": "IT News zu den Themen Künstliche Intelligenz, Roboter und Maschinelles Lernen - IT BOLTWISE® x Artificial Intelligence",
    "company": "Google DeepMind",
    "title": "Neue KI-Ã\"ra fÃ¼r Atlas-Roboter: Boston Dynamics und Google DeepMind vereinen KrÃ¤fte",
    "date": "2026-02-02T09:46:23Z",
    "url": "https://www.it-boltwise.de/neue-ki-aera-fuer-atlas-roboter-boston-dynamics-und-google-deepmind-vereinen-kraefte.html",
    "content": "SAVANNAH / LONDON (IT BOLTWISE) - Die Partnerschaft zwischen Boston Dynamics und Google DeepMind markiert einen bedeutenden Schritt in der Robotik. Der Atlas-Roboter wird mit der Gemini-KI ausgestattet, um industrielle Aufgaben zu meistern. Erste Modelle sind bereits ausverkauft, was das große Interesse an dieser Technologie unterstreicht.\n\nDie erneute Zusammenarbeit zwischen Boston Dynamics und Google DeepMind könnte die Robotikbranche nachhaltig verändern. Mit der Integration der Gemini-KI in den humanoiden Atlas-Roboter zielen die Unternehmen darauf ab, die Fähigkeiten des Roboters von beeindruckenden akrobatischen Vorführungen hin zu praktischen industriellen Anwendungen zu erweitern. Diese Entwicklung könnte einen neuen Standard für die Flexibilität und Intelligenz von Robotern in der Industrie setzen.\n\nIm Mittelpunkt dieser Partnerschaft steht die Kombination von hochentwickelter Hardware und fortschrittlicher KI. Der vollelektrische Atlas-Roboter, der kürzlich auf der CES 2026 vorgestellt wurde, bildet die Grundlage für diese Innovation. Google DeepMinds Gemini-KI verleiht dem Roboter die Fähigkeit, nicht nur Befehle auszuführen, sondern auch seine Umgebung zu verstehen und auf unvorhergesehene Situationen flexibel zu reagieren. Diese Fähigkeit fehlt den meisten heutigen Industrierobotern, die oft nur für spezifische Aufgaben programmiert sind.\n\nDie Einführung von Robotern wie Atlas in Fabrikhallen bringt jedoch auch neue Herausforderungen mit sich, insbesondere im Bereich des Arbeitsschutzes. Die Kombination aus beweglichen humanoiden Robotern, Sensorik und KI-gesteuerten Aktionen erfordert neue Sicherheitsmaßnahmen und Gefährdungsbeurteilungen. Unternehmen, die solche Systeme implementieren oder testen, sollten daher auf umfassende Sicherheitsdokumentationen und Checklisten zurückgreifen, um einen reibungslosen Betrieb zu gewährleisten.\n\nDer Markt für humanoide Roboter wird zunehmend wettbewerbsintensiver. Neben Boston Dynamics und Google DeepMind drängen auch andere Unternehmen wie Tesla mit ihrem Optimus Gen 2 und Figure AI in diesen Bereich. Die Partnerschaft zwischen Boston Dynamics und Google DeepMind vereint zwei der bekanntesten Namen in der Robotik und KI, was den Druck auf die Konkurrenz erhöht, eigene Innovationen voranzutreiben. Branchenexperten sehen in der Entwicklung von 'physischer KI' einen entscheidenden Wendepunkt für die gesamte Robotik-Branche.\n\nDie ersten KI-gesteuerten Atlas-Modelle sollen in den kommenden Monaten an Google DeepMind und Hyundai ausgeliefert werden. Diese Tests in kontrollierten Umgebungen sind entscheidend, um die KI-Modelle mit realen Daten zu verfeinern. Die Nachfrage nach diesen Robotern ist bereits so groß, dass die Produktionskapazitäten für das Jahr 2026 vollständig ausgebucht sind. Langfristig könnte diese Technologie auch in Bereichen wie Logistik, Pflege oder im Haushalt Anwendung finden. Der erste Schritt in die Ära der lernenden Humanoiden in der Industrie ist jedoch bereits getan."
  },
  {
    "source": "developpez.net",
    "company": "Google DeepMind",
    "title": "Project Genie : Google DeepMind publiee son IA de création interactive de mondes et la bourse panique",
    "date": "2026-02-02T08:35:20Z",
    "url": "https://www.developpez.net/forums/d2181854/general-developpement/algorithme-mathematiques/intelligence-artificielle/project-genie-google-deepmind-publiee-ia-creation-interactive-mondes-bourse-panique/",
    "content": "Project Genie : Google DeepMind publie son IA de création de mondes générés en temps réel et la bourse panique,\n\nmême s'il ne peut générer qu'un monde interactif de 60 secondes plutôt qu'un monde entièrement jouable\n\nL'annonce d'un nouvel outil d'IA capable de générer des mondes de jeu interactifs en temps réel a suffi à provoquer une onde de choc sur les marchés financiers. En dévoilant Project Genie, Google DeepMind ne s'est pas contenté d'alimenter le débat sur l'avenir de la création vidéoludique : le laboratoire a déclenché une réaction quasi immédiate des investisseurs, entraînant une chute notable des actions de plusieurs acteurs majeurs du jeu vidéo. Derrière l'effet d'annonce, c'est toute la chaîne de valeur du gaming qui se retrouve interrogée, entre promesses technologiques, fantasmes de disruption totale et inquiétudes sur les modèles économiques existants.\n\nContexte\n\nProject Genie n'est pas un moteur de création classique. Il s'inscrit dans la lignée des \" world models \", ces modèles d'IA capables de simuler des environnements cohérents, persistants et explorables. Là où un générateur procédural traditionnel repose sur des règles écrites à l'avance, Genie produit des espaces jouables en temps réel, en fonction des actions du joueur. Selon la présentation technique, le système s'appuie sur plusieurs briques : un modèle de génération visuelle, une phase de \" world sketching \" permettant de définir l'ambiance et la structure, puis un moteur capable d'étendre le monde à mesure que le joueur avance.\n\nCette approche change radicalement la logique de conception. Le niveau n'est plus un objet figé, mais un flux continu, calculé à la demande. Pour les professionnels de l'informatique, cela évoque immédiatement des problématiques bien connues : latence, cohérence des états, consommation de ressources et contrôle de la qualité de sortie. Rien de magique ici, mais une démonstration impressionnante de ce que des modèles multimodaux avancés peuvent déjà produire.\n\nUn marché du jeu vidéo déjà sous tension\n\nDepuis plusieurs mois, l'industrie du jeu vidéo traverse une période délicate. Après l'euphorie post-pandémie, les éditeurs font face à un ralentissement de la croissance, à des coûts de production en hausse et à une pression accrue des actionnaires. Dans ce contexte fragilisé, l'irruption d'une IA présentée comme capable de concevoir des mondes jouables \" à la volée \" agit comme un révélateur des peurs latentes du secteur. Les marchés n'ont pas attendu pour trancher : des valeurs emblématiques associées à la création de contenus premium et à des cycles de développement longs ont immédiatement accusé le coup, comme si une partie de leur savoir-faire venait d'être brutalement remis en cause.\n\nGoogle a annoncé le lancement de Project Genie, un nouvel outil d'IA générative capable de créer des jeux entiers à partir de simples instructions. Il utilise les modèles Genie 3, Nano Banana Pro et Gemini pour générer un monde interactif de 60 secondes plutôt qu'un monde entièrement jouable. Malgré cela, de nombreux investisseurs ont été effrayés, imaginant que cela représentait l'avenir du développement des jeux vidéo, ce qui a entraîné une vente massive d'actions qui a fait chuter le cours des actions de plusieurs sociétés de jeux vidéo.\n\nLes entreprises concernées par cette situation comprennent Take-Two Interactive, propriétaire de Rockstar, des développeurs/distributeurs tels que CD Projekt Red et Nintendo, ainsi que Roblox, ce qui est tout à fait logique. La plupart des jeux disponibles sur la plateforme, y compris le tristement célèbre \" Steal a Brainrot \", ne sont pas très éloignés de l'IA, il est donc poétique que ce soit le produit d'un réseau neuronal qui ait nui à son action. Le cours de l'action Unity a enregistré la plus forte baisse, avec 20 %, car il s'agit d'un moteur de jeu très populaire.\n\nProject Genie, comment ça marche ?\n\nProject Genie s'inscrit dans une trajectoire de recherche de long terme chez Google DeepMind autour des modèles capables de comprendre, simuler et prédire des environnements complexes. Contrairement aux moteurs de jeu traditionnels, Genie ne se contente pas d'assembler des assets prédéfinis. Il repose sur un modèle de monde génératif capable de produire des environnements jouables cohérents, qui se déploient dynamiquement à mesure que l'utilisateur explore. L'objectif affiché n'est pas de remplacer les moteurs existants, mais d'explorer ce que pourrait devenir l'interactivité lorsque l'IA est capable de modéliser un monde plutôt que de simplement le représenter.\n\nDans son billet de blog, DeepMind insiste sur le caractère expérimental du projet. Genie est présenté comme un laboratoire à ciel ouvert permettant de tester de nouvelles formes d'interaction homme-machine, où le monde réagit en temps réel aux actions du joueur, sans script préétabli. Cette approche marque une rupture conceptuelle : le jeu n'est plus une suite d'événements conçus à l'avance, mais une simulation probabiliste qui évolue en continu.\n\nL'expérience s'articule autour de trois compétences clés que DeepMind présente comme suit :\n\nCe que montre réellement la démonstration vidéo\n\nLa vidéo de démonstration associée à Project Genie apporte un éclairage précieux, loin des slogans marketing. On y découvre des mondes explorables pendant des sessions limitées, avec des déplacements parfois approximatifs et une latence clairement perceptible. Les environnements, qu'il s'agisse d'un circuit miniature, d'une structure alien organique ou d'un monde flottant dans les nuages, sont intégralement générés par l'IA. Le joueur peut choisir une vue à la première ou à la troisième personne, définir un personnage et observer comment le décor se transforme dynamiquement à mesure de l'exploration.\n\nCes séquences montrent aussi les limites actuelles du système : contrôles imprécis, incohérences physiques, difficulté à maintenir une perspective stable. Autrement dit, Project Genie impressionne par le concept, mais reste très loin d'un outil prêt à remplacer des moteurs comme Unreal ou Unity dans un cadre de production industrielle. Cette nuance est essentielle pour comprendre le décalage entre la réaction des marchés et la réalité technique observable.\n\nPourquoi les investisseurs ont paniqué\n\nLa réaction boursière ne s'explique pas tant par l'état actuel de la technologie que par ce qu'elle symbolise. Pour les investisseurs, Project Genie incarne l'idée que la barrière à l'entrée de la création de jeux pourrait s'effondrer. Si des mondes jouables peuvent être générés à partir de simples descriptions textuelles, la valeur perçue des studios, de leurs équipes artistiques et de leurs pipelines complexes semble menacée. Cette lecture est évidemment simplificatrice, mais elle suffit à déclencher des ventes massives dans un contexte déjà anxiogène.\n\nOn retrouve ici un schéma désormais familier : chaque avancée de l'IA est interprétée comme une substitution immédiate du travail humain, alors qu'il s'agit le plus souvent d'un déplacement des compétences et des outils. Les marchés, eux, raisonnent à court terme et sanctionnent préventivement.\n\nEncore un outil expérimental\n\nEn général, c'est ainsi que fonctionnent la plupart des jeux : ils utilisent un framework logiciel, tel que Unity ou Unreal Engine, qui fournit des fonctionnalités de base telles que la physique, le rendu, la saisie et le son. Les studios construisent ensuite leur vision à partir de ces bases, et certains développeurs ont même leurs propres solutions internes personnalisées, telles que RAGE de Rockstar ou Decima de Guerrilla.\n\nLe projet Genie évite tout cela et gère lui-même ces éléments constitutifs, mais n'oubliez pas qu'il ne crée pas réellement de jeux en soi. Lorsque vous lui demandez de créer un clone de Super Mario 64, il le reproduit de manière assez impressionnante, mais tout ce que vous obtenez, ce sont des mouvements basiques avec une caméra libre qui permet de regarder autour de la carte. Il n'y a pas d'objectifs, et l'IA oublie souvent ce qu'elle a déjà généré lorsqu'elle comble les lacunes.\n\nLes routes de ces jeux générés de manière discutable comportent souvent des parcelles d'herbe entre elles, comme si le modèle pensait qu'il devait générer autre chose pendant un moment avant de se rattraper rapidement. Ce comportement hallucinatoire témoigne du caractère prototypique de la technologie, et Google a déclaré que Project Genie est pour l'instant un outil expérimental destiné à faciliter certaines tâches, comme la prévisualisation de jeux de grande envergure.\n\nCela rejoint le problème actuel du développement des jeux : certaines productions peuvent devenir extrêmement coûteuses et prendre un temps fou, pour finalement aboutir à un produit décevant. Des outils tels que Project Genie pourraient véritablement aider dans ce domaine, en permettant aux développeurs de gagner du temps qu'ils auraient autrement passé dans les premières phases du jeu, avant que la conception des niveaux ne soit finalisée.\n\nMais de cette manière, l'IA est la solution à un problème artificiel qui aurait dû être résolu sans elle ; quoi qu'il en soit, les développeurs pourraient toujours trouver un moyen de faire exploser les productions hors de contrôle, même avec Project Genie et une centaine d'autres modèles d'IA générative à la barre. Bien sûr, au lieu de s'inquiéter, les investisseurs sont clairement plus optimistes quant à un avenir assisté par l'IA, compte tenu des manuvres boursières qui en résultent.\n\nUne révolution créative... ou un nouvel outil pour les développeurs\n\nPour les professionnels de l'informatique et du jeu vidéo, la question centrale n'est pas de savoir si Project Genie va \" tuer \" les studios, mais comment ce type de technologie pourrait s'intégrer dans les workflows existants. On peut imaginer des usages très concrets : prototypage rapide d'environnements, exploration de concepts artistiques, tests de gameplay dans des mondes générés dynamiquement. Dans ce cadre, l'IA devient un accélérateur, pas un remplaçant.\n\nReste néanmoins un enjeu majeur : le contrôle. Un jeu commercial repose sur une expérience finement calibrée, un level design pensé dans le détail et une narration maîtrisée. Confier ces éléments à un modèle probabiliste pose des défis techniques, mais aussi créatifs. Qui est responsable d'un bug bloquant généré par l'IA ? Comment garantir une cohérence sur des dizaines d'heures de jeu ? Ces questions restent largement ouvertes.\n\nProject Genie n'est probablement pas la révolution instantanée que certains ont voulu y voir, mais il constitue un signal faible extrêmement puissant. Il montre que l'IA ne se limite plus à assister la création, mais qu'elle commence à toucher au cur même de l'interactivité. Pour le marché du jeu vidéo, déjà fragilisé, cette annonce agit comme un catalyseur de doutes et d'interrogations stratégiques.\n\nÀ moyen terme, il est peu probable que des mondes générés en temps réel remplacent les productions AAA. En revanche, il serait imprudent d'ignorer ce que révèle Project Genie : la capacité des géants de l'IA à redéfinir, par petites touches successives, les frontières entre création humaine et génération automatisée. Et comme souvent dans la tech, ce ne sont pas toujours les meilleures technologies qui gagnent, mais celles qui forcent tout un secteur à se réinventer plus vite qu'il ne l'aurait voulu.\n\nEssayer Project Genie\n\nSources : Google, DeepMind\n\nEt vous ?\n\nQue pensez-vous de Project Genie ? Comprenez-vous la réaction du marché ? Dans quelle mesure ?\n\nÀ partir de quand un outil d'IA cesse-t-il d'être une aide pour devenir une menace structurelle ?\n\nFaut-il réellement croire que des mondes générés en temps réel peuvent remplacer des années de direction artistique, de level design et de narration, ou assiste-t-on surtout à une surinterprétation financière d'une démo technologique encore immature ?\n\nLa panique des marchés traduit-elle une compréhension lucide de ce qui arrive, ou au contraire une méconnaissance profonde des réalités techniques du développement de jeux ?\n\nSi Google DeepMind est capable de produire des mondes jouables à la demande, qu'est-ce qui empêche demain un acteur du cloud ou de l'IA de court-circuiter totalement les studios traditionnels ?\n\nÀ l'inverse, pourquoi les éditeurs historiques ne pourraient-ils pas s'approprier ces outils pour réduire leurs coûts et accélérer leurs cycles de production ?"
  },
  {
    "source": "Napa Valley Register",
    "company": "Google DeepMind",
    "title": "Poetiq Raises $45.8M for AI Meta-System, Surpasses Top LLMs on Industry Benchmark",
    "date": "2026-01-30T11:00:09Z",
    "url": "https://napavalleyregister.com/online_features/press_releases/poetiq-raises-45-8m-for-ai-meta-system-surpasses-top-llms-on-industry-benchmark/article_5c9f46e8-10c5-5ccc-9d72-c6a9f2aa8301.html",
    "content": "Founded by former Google DeepMind scientists, Poetiq emerges from \"stealth\" after setting a new state-of-the-art (SOTA) on ARC-AGI-2\n\nMOUNTAIN VIEW, Calif., Jan. 29, 2026 /PRNewswire/ -- Poetiq, developer of an AI meta-system that makes LLMs work better, announced today that it raised $45.8 million in Seed funding co-led by FYRFLY Venture Partners and Surface Ventures with Y Combinator, 468 Capital, Operator Collective, Hico Ventures, and Neuron Venture Partners participating. The funding news follows Poetiq's commanding results on ARC-AGI-2, an industry benchmark for machine reasoning and progress towards artificial general intelligence (AGI).\n\nPoetiq can pair with any frontier LLM (OpenAI's Chat GPT, Anthropic's Claude, Google's Gemini, Meta's Llama, etc.) to make it learn faster and solve harder problems. Clients provide Poetiq with a problem and a few hundred examples instead of the thousands or millions required for fine-tuning or RL post-training. Poetiq's meta-system generates an agent that specializes in solving that problem and recursively improves the agent to become more accurate and cost-efficient.\n\nPoetiq was founded in June 2025 by co-CEOs Shumeet Baluja, PhD, and Ian Fischer, former AI researchers at Google DeepMind. Baluja was previously the CTO of Jamdat Mobile (IPO 2004) and spent the last 21 years with Google DeepMind, where he founded their mobile practice and started their fundamental computer vision research group. He has contributed to more than 170 patents in neural networks, machine learning, and applications and is one of the originators of YouTube's copyright system. Fischer joined Google DeepMind through its 2015 acquisition of Apportable, a platform that ported iOS games to Android, where he was co-founder and CTO.\n\nCollaborating at Google DeepMind, Fischer and Baluja noticed that frontier LLMs were struggling to solve most hard (or easy) problems. The current solution -- to pre-train and post-train LLMs through reinforcement learning (RL) -- takes weeks and is far too expensive for all but the biggest companies.\n\n\"LLMs are impressive databases that encode a vast amount of humanity's collective knowledge,\" said Shumeet Baluja, co-CEO of Poetiq. \"They are simply not the best tools for deep reasoning. That's why efforts to improve their problem-solving skills are so slow and expensive. For ARC-AGI 1 and 2, we used recursive self-improvement to produce specialized agents in a matter of hours. It demonstrates how much we can help with problems that have been too hard or too expensive for LLMs alone.\"\n\nAn MIT study of 300 public AI implementations, published in August 2025, underscores the need for Poetiq. Although enterprises have invested $30 to $40 billion in GenAI, 95% of organizations are \"getting zero return,\" according to the researchers. Use cases that have struggled to generate an ROI are ideal candidates for Poetiq, as it can improve the reasoning capabilities of any LLM, including proprietary, in-house models.\n\n\"That Poetiq managed to top ARC-AGI within six months of launching is remarkable,\" said Philipp Stauffer, General Partner at FYRFLY Venture Partners. \"Rather than compete against frontier models, their team of six found a way to coax more intelligence from every LLM available. Poetiq will be a must-have for companies trying to make AI work for real-world business applications.\"\n\n\"Poetiq is one of the rare AI startups that doesn't need to outcompete frontier models or pick sides,\" added Gyan Kapur, co-Managing Partner at Surface Ventures. \"It can enhance any combination of LLMs, any native AI platform, and any AI use case. Poetiq can provide better performance at lower costs across diverse use cases by sitting on top of foundation models, and that is a unique position to be in.\"\n\nThe Abstraction and Reasoning Corpus (ARC-AGI), developed in 2019 by AI researcher François Chollet, is a benchmark that measures an AI's \"human-like generalization\" of problem-solving skills. In early December, Poetiq established a SOTA on the ARC-AGI-2 semi-private evaluation set, topping Gemini 3 Deep Think, the previous leader, at half the cost per task; this was done using Poetiq's system on top of Gemini 3 Pro. Within a few days, OpenAI released GPT-5.2. Poetiq immediately incorporated this model into their system and showed a new SOTA at 75% accuracy (on the public evaluation set), a 16 percentage point improvement on the previous SOTA. OpenAI co-founder and President Greg Brockman tweeted in response to this feat, Poetiq is \"exceeding the human baseline on ARC-AGI-2 with gpt-5.2.\"\n\nFor technical details on Poetiq's ARC-AGI-2 results, read their blog posts at:\n\nhttps://poetiq.ai/posts/arcagi_verified/ https://poetiq.ai/posts/update_arcagi_gpt52/\n\nTo learn more about the company, visit poetiq.ai\n\nAbout Poetiq\n\nPoetiq is on the fastest path to AGI with a meta-system that makes frontier LLMs smarter and more cost-effective at solving real-world problems. Through recursive self-improvement, Poetiq generates AI agents to solve business problems that are too difficult or too expensive for LLMs alone. With each problem they solve, Poetiq agents become faster and more accurate at solving related problems. Visit poetiq.ai to learn more.\n\nPress Contact:\n\npress@poetiq.ai\n\nView original content to download multimedia:https://www.prnewswire.com/news-releases/poetiq-raises-45-8m-for-ai-meta-system-surpasses-top-llms-on-industry-benchmark-302674571.html\n\nSOURCE Poetiq"
  },
  {
    "source": "IT News Online",
    "company": "Google DeepMind",
    "title": "Poetiq Raises $45.8M for AI Meta-System, Surpasses Top LLMs on Industry Benchmark",
    "date": "2026-01-29T23:46:09Z",
    "url": "https://itnewsonline.com/PRNewswire/Poetiq-Raises-USD45.8M-for-AI-Meta-System-Surpasses-Top-LLMs-on-Industry-Benchmark/1108543",
    "content": "Founded by former Google DeepMind scientists, Poetiq emerges from \"stealth\" after setting a new state-of-the-art (SOTA) on ARC-AGI-2\n\nMOUNTAIN VIEW, Calif., Jan. 29, 2026 /PRNewswire/ -- Poetiq, developer of an AI meta-system that makes LLMs work better, announced today that it raised $45.8 million in Seed funding co-led by FYRFLY Venture Partners and Surface Ventures with Y Combinator, 468 Capital, Operator Collective, Hico Ventures, and Neuron Venture Partners participating. The funding news follows Poetiq's commanding results on ARC-AGI-2, an industry benchmark for machine reasoning and progress towards artificial general intelligence (AGI).\n\nPoetiq can pair with any frontier LLM (OpenAI's Chat GPT, Anthropic's Claude, Google's Gemini, Meta's Llama, etc.) to make it learn faster and solve harder problems. Clients provide Poetiq with a problem and a few hundred examples instead of the thousands or millions required for fine-tuning or RL post-training. Poetiq's meta-system generates an agent that specializes in solving that problem and recursively improves the agent to become more accurate and cost-efficient.\n\nPoetiq was founded in June 2025 by co-CEOs Shumeet Baluja, PhD, and Ian Fischer, former AI researchers at Google DeepMind. Baluja was previously the CTO of Jamdat Mobile (IPO 2004) and spent the last 21 years with Google DeepMind, where he founded their mobile practice and started their fundamental computer vision research group. He has contributed to more than 170 patents in neural networks, machine learning, and applications and is one of the originators of YouTube's copyright system. Fischer joined Google DeepMind through its 2015 acquisition of Apportable, a platform that ported iOS games to Android, where he was co-founder and CTO.\n\nCollaborating at Google DeepMind, Fischer and Baluja noticed that frontier LLMs were struggling to solve most hard (or easy) problems. The current solution -- to pre-train and post-train LLMs through reinforcement learning (RL) -- takes weeks and is far too expensive for all but the biggest companies.\n\n\"LLMs are impressive databases that encode a vast amount of humanity's collective knowledge,\" said Shumeet Baluja, co-CEO of Poetiq. \"They are simply not the best tools for deep reasoning. That's why efforts to improve their problem-solving skills are so slow and expensive. For ARC-AGI 1 and 2, we used recursive self-improvement to produce specialized agents in a matter of hours. It demonstrates how much we can help with problems that have been too hard or too expensive for LLMs alone.\"\n\nAn MIT study of 300 public AI implementations, published in August 2025, underscores the need for Poetiq. Although enterprises have invested $30 to $40 billion in GenAI, 95% of organizations are \"getting zero return,\" according to the researchers. Use cases that have struggled to generate an ROI are ideal candidates for Poetiq, as it can improve the reasoning capabilities of any LLM, including proprietary, in-house models.\n\n\"That Poetiq managed to top ARC-AGI within six months of launching is remarkable,\" said Philipp Stauffer, General Partner at FYRFLY Venture Partners. \"Rather than compete against frontier models, their team of six found a way to coax more intelligence from every LLM available. Poetiq will be a must-have for companies trying to make AI work for real-world business applications.\"\n\n\"Poetiq is one of the rare AI startups that doesn't need to outcompete frontier models or pick sides,\" added Gyan Kapur, co-Managing Partner at Surface Ventures. \"It can enhance any combination of LLMs, any native AI platform, and any AI use case. Poetiq can provide better performance at lower costs across diverse use cases by sitting on top of foundation models, and that is a unique position to be in.\"\n\nThe Abstraction and Reasoning Corpus (ARC-AGI), developed in 2019 by AI researcher François Chollet, is a benchmark that measures an AI's \"human-like generalization\" of problem-solving skills. In early December, Poetiq established a SOTA on the ARC-AGI-2 semi-private evaluation set, topping Gemini 3 Deep Think, the previous leader, at half the cost per task; this was done using Poetiq's system on top of Gemini 3 Pro. Within a few days, OpenAI released GPT-5.2. Poetiq immediately incorporated this model into their system and showed a new SOTA at 75% accuracy (on the public evaluation set), a 16 percentage point improvement on the previous SOTA. OpenAI co-founder and President Greg Brockman tweeted in response to this feat, Poetiq is \"exceeding the human baseline on ARC-AGI-2 with gpt-5.2.\"\n\nFor technical details on Poetiq's ARC-AGI-2 results, read their blog posts at:\n\nPoetiq is on the fastest path to AGI with a meta-system that makes frontier LLMs smarter and more cost-effective at solving real-world problems. Through recursive self-improvement, Poetiq generates AI agents to solve business problems that are too difficult or too expensive for LLMs alone. With each problem they solve, Poetiq agents become faster and more accurate at solving related problems. Visit poetiq.ai to learn more.\n\nPress Contact:\n\npress@poetiq.ai\n\nView original content to download multimedia:https://www.prnewswire.com/news-releases/poetiq-raises-45-8m-for-ai-meta-system-surpasses-top-llms-on-industry-benchmark-302674571.html"
  },
  {
    "source": "StreetInsider.com",
    "company": "Google DeepMind",
    "title": "Poetiq Raises $45.8M for AI Meta-System, Surpasses Top LLMs on Industry Benchmark",
    "date": "2026-01-29T23:04:47Z",
    "url": "https://www.streetinsider.com/PRNewswire/Poetiq+Raises+$45.8M+for+AI+Meta-System,+Surpasses+Top+LLMs+on+Industry+Benchmark/25918058.html",
    "content": "Founded by former Google DeepMind scientists, Poetiq emerges from \"stealth\" after setting a new state-of-the-art (SOTA) on ARC-AGI-2\n\nMOUNTAIN VIEW, Calif., Jan. 29, 2026 /PRNewswire/ -- Poetiq, developer of an AI meta-system that makes LLMs work better, announced today that it raised $45.8 million in Seed funding co-led by FYRFLY Venture Partners and Surface Ventures with Y Combinator, 468 Capital, Operator Collective, Hico Ventures, and Neuron Venture Partners participating. The funding news follows Poetiq's commanding results on ARC-AGI-2, an industry benchmark for machine reasoning and progress towards artificial general intelligence (AGI).\n\nPoetiq can pair with any frontier LLM (OpenAI's Chat GPT, Anthropic's Claude, Google's Gemini, Meta's Llama, etc.) to make it learn faster and solve harder problems. Clients provide Poetiq with a problem and a few hundred examples instead of the thousands or millions required for fine-tuning or RL post-training. Poetiq's meta-system generates an agent that specializes in solving that problem and recursively improves the agent to become more accurate and cost-efficient.\n\nPoetiq was founded in June 2025 by co-CEOs Shumeet Baluja, PhD, and Ian Fischer, former AI researchers at Google DeepMind. Baluja was previously the CTO of Jamdat Mobile (IPO 2004) and spent the last 21 years with Google DeepMind, where he founded their mobile practice and started their fundamental computer vision research group. He has contributed to more than 170 patents in neural networks, machine learning, and applications and is one of the originators of YouTube's copyright system. Fischer joined Google DeepMind through its 2015 acquisition of Apportable, a platform that ported iOS games to Android, where he was co-founder and CTO.\n\nCollaborating at Google DeepMind, Fischer and Baluja noticed that frontier LLMs were struggling to solve most hard (or easy) problems. The current solution -- to pre-train and post-train LLMs through reinforcement learning (RL) -- takes weeks and is far too expensive for all but the biggest companies.\n\n\"LLMs are impressive databases that encode a vast amount of humanity's collective knowledge,\" said Shumeet Baluja, co-CEO of Poetiq. \"They are simply not the best tools for deep reasoning. That's why efforts to improve their problem-solving skills are so slow and expensive. For ARC-AGI 1 and 2, we used recursive self-improvement to produce specialized agents in a matter of hours. It demonstrates how much we can help with problems that have been too hard or too expensive for LLMs alone.\"\n\nAn MIT study of 300 public AI implementations, published in August 2025, underscores the need for Poetiq. Although enterprises have invested $30 to $40 billion in GenAI, 95% of organizations are \"getting zero return,\" according to the researchers. Use cases that have struggled to generate an ROI are ideal candidates for Poetiq, as it can improve the reasoning capabilities of any LLM, including proprietary, in-house models.\n\n\"That Poetiq managed to top ARC-AGI within six months of launching is remarkable,\" said Philipp Stauffer, General Partner at FYRFLY Venture Partners. \"Rather than compete against frontier models, their team of six found a way to coax more intelligence from every LLM available. Poetiq will be a must-have for companies trying to make AI work for real-world business applications.\"\n\n\"Poetiq is one of the rare AI startups that doesn't need to outcompete frontier models or pick sides,\" added Gyan Kapur, co-Managing Partner at Surface Ventures. \"It can enhance any combination of LLMs, any native AI platform, and any AI use case. Poetiq can provide better performance at lower costs across diverse use cases by sitting on top of foundation models, and that is a unique position to be in.\"\n\nThe Abstraction and Reasoning Corpus (ARC-AGI), developed in 2019 by AI researcher François Chollet, is a benchmark that measures an AI's \"human-like generalization\" of problem-solving skills. In early December, Poetiq established a SOTA on the ARC-AGI-2 semi-private evaluation set, topping Gemini 3 Deep Think, the previous leader, at half the cost per task; this was done using Poetiq's system on top of Gemini 3 Pro. Within a few days, OpenAI released GPT-5.2. Poetiq immediately incorporated this model into their system and showed a new SOTA at 75% accuracy (on the public evaluation set), a 16 percentage point improvement on the previous SOTA. OpenAI co-founder and President Greg Brockman tweeted in response to this feat, Poetiq is \"exceeding the human baseline on ARC-AGI-2 with gpt-5.2.\"\n\nFor technical details on Poetiq's ARC-AGI-2 results, read their blog posts at:\n\nTo learn more about the company, visit poetiq.ai\n\nAbout Poetiq\n\nPoetiq is on the fastest path to AGI with a meta-system that makes frontier LLMs smarter and more cost-effective at solving real-world problems. Through recursive self-improvement, Poetiq generates AI agents to solve business problems that are too difficult or too expensive for LLMs alone. With each problem they solve, Poetiq agents become faster and more accurate at solving related problems. Visit poetiq.ai to learn more.\n\nPress Contact:\n\n[email protected]\n\nView original content to download multimedia:https://www.prnewswire.com/news-releases/poetiq-raises-45-8m-for-ai-meta-system-surpasses-top-llms-on-industry-benchmark-302674571.html"
  },
  {
    "source": "Weekly Voice",
    "company": "Google DeepMind",
    "title": "Poetiq Raises $45.8M for AI Meta-System, Surpasses Top LLMs on Industry Benchmark | Weekly Voice",
    "date": "2026-01-29T23:06:19Z",
    "url": "https://weeklyvoice.com/poetiq-raises-45-8m-for-ai-meta-system-surpasses-top-llms-on-industry-benchmark/",
    "content": "Founded by former Google DeepMind scientists, Poetiq emerges from \"stealth\" after setting a new state-of-the-art (SOTA) on ARC-AGI-2\n\nMOUNTAIN VIEW, Calif., Jan. 29, 2026 /PRNewswire/ -- Poetiq, developer of an AI meta-system that makes LLMs work better, announced today that it raised $45.8 million in Seed funding co-led by FYRFLY Venture Partners and Surface Ventures with Y Combinator, 468 Capital, Operator Collective, Hico Ventures, and Neuron Venture Partners participating. The funding news follows Poetiq's commanding results on ARC-AGI-2, an industry benchmark for machine reasoning and progress towards artificial general intelligence (AGI).\n\nPoetiq can pair with any frontier LLM (OpenAI's Chat GPT, Anthropic's Claude, Google's Gemini, Meta's Llama, etc.) to make it learn faster and solve harder problems. Clients provide Poetiq with a problem and a few hundred examples instead of the thousands or millions required for fine-tuning or RL post-training. Poetiq's meta-system generates an agent that specializes in solving that problem and recursively improves the agent to become more accurate and cost-efficient.\n\nPoetiq was founded in June 2025 by co-CEOs Shumeet Baluja, PhD, and Ian Fischer, former AI researchers at Google DeepMind. Baluja was previously the CTO of Jamdat Mobile (IPO 2004) and spent the last 21 years with Google DeepMind, where he founded their mobile practice and started their fundamental computer vision research group. He has contributed to more than 170 patents in neural networks, machine learning, and applications and is one of the originators of YouTube's copyright system. Fischer joined Google DeepMind through its 2015 acquisition of Apportable, a platform that ported iOS games to Android, where he was co-founder and CTO.\n\nCollaborating at Google DeepMind, Fischer and Baluja noticed that frontier LLMs were struggling to solve most hard (or easy) problems. The current solution -- to pre-train and post-train LLMs through reinforcement learning (RL) -- takes weeks and is far too expensive for all but the biggest companies.\n\n\"LLMs are impressive databases that encode a vast amount of humanity's collective knowledge,\" said Shumeet Baluja, co-CEO of Poetiq. \"They are simply not the best tools for deep reasoning. That's why efforts to improve their problem-solving skills are so slow and expensive. For ARC-AGI 1 and 2, we used recursive self-improvement to produce specialized agents in a matter of hours. It demonstrates how much we can help with problems that have been too hard or too expensive for LLMs alone.\"\n\nAn MIT study of 300 public AI implementations, published in August 2025, underscores the need for Poetiq. Although enterprises have invested $30 to $40 billion in GenAI, 95% of organizations are \"getting zero return,\" according to the researchers. Use cases that have struggled to generate an ROI are ideal candidates for Poetiq, as it can improve the reasoning capabilities of any LLM, including proprietary, in-house models.\n\n\"That Poetiq managed to top ARC-AGI within six months of launching is remarkable,\" said Philipp Stauffer, General Partner at FYRFLY Venture Partners. \"Rather than compete against frontier models, their team of six found a way to coax more intelligence from every LLM available. Poetiq will be a must-have for companies trying to make AI work for real-world business applications.\"\n\n\"Poetiq is one of the rare AI startups that doesn't need to outcompete frontier models or pick sides,\" added Gyan Kapur, co-Managing Partner at Surface Ventures. \"It can enhance any combination of LLMs, any native AI platform, and any AI use case. Poetiq can provide better performance at lower costs across diverse use cases by sitting on top of foundation models, and that is a unique position to be in.\"\n\nThe Abstraction and Reasoning Corpus (ARC-AGI), developed in 2019 by AI researcher François Chollet, is a benchmark that measures an AI's \"human-like generalization\" of problem-solving skills. In early December, Poetiq established a SOTA on the ARC-AGI-2 semi-private evaluation set, topping Gemini 3 Deep Think, the previous leader, at half the cost per task; this was done using Poetiq's system on top of Gemini 3 Pro. Within a few days, OpenAI released GPT-5.2. Poetiq immediately incorporated this model into their system and showed a new SOTA at 75% accuracy (on the public evaluation set), a 16 percentage point improvement on the previous SOTA. OpenAI co-founder and President Greg Brockman tweeted in response to this feat, Poetiq is \"exceeding the human baseline on ARC-AGI-2 with gpt-5.2.\"\n\nFor technical details on Poetiq's ARC-AGI-2 results, read their blog posts at:\n\nPoetiq is on the fastest path to AGI with a meta-system that makes frontier LLMs smarter and more cost-effective at solving real-world problems. Through recursive self-improvement, Poetiq generates AI agents to solve business problems that are too difficult or too expensive for LLMs alone. With each problem they solve, Poetiq agents become faster and more accurate at solving related problems. Visit poetiq.ai to learn more.\n\nPress Contact:\n\npress@poetiq.ai\n\nView original content to download multimedia:https://www.prnewswire.com/news-releases/poetiq-raises-45-8m-for-ai-meta-system-surpasses-top-llms-on-industry-benchmark-302674571.html"
  },
  {
    "source": "dpa International",
    "company": "Google DeepMind",
    "title": "Could a new AI model transform genetic testing and drug discovery?",
    "date": "2026-01-29T15:55:18Z",
    "url": "https://www.dpa-international.com/trends-and-features/urn:newsml:dpa.com:20090101:260129-99-325871/",
    "content": "A new AI model from Google DeepMind called AlphaGenome can predict how DNA mutations affect biological processes, helping pinpoint genes linked to diseases and speeding up drug discovery.\n\nA Google DeepMind invention that uses artificial intelligence (AI) to predict how DNA mutations behave could have a \"transformative impact\" on medicines discovery, according to its developers.\n\nAlphaGenome could also help experts pinpoint the genes associated with particular conditions or identify the cause of rare diseases.\n\nThousands of researchers around the world have used AlphaGenome to assist studies on \"neurodegenerative diseases, infectious diseases and cancer\" since its launch last June, experts said.\n\nThe model predicts how variants or mutations in DNA impact a range of biological processes that regulate genes.\n\nIt could help researchers pinpoint the cause of diseases more precisely, as well as improving genetic testing and driving the development of new treatments.\n\nIt could also help scientists accelerate their understanding of the human genome, which is the entire set of DNA instructions found in a cell.\n\nAlphaGenome was trained using human and mouse genomes.\n\nWriting in the journal Nature, researchers said the program can simultaneously predict 5,930 human or 1,128 mouse genetic signals.\n\nThese results matched or improved on the performance of existing state-of-the-art models in 25 out of 26 evaluations.\n\nNatasha Latysheva, a research engineer at GoogleDeepmind, said there are numerous applications where AlphaGenome \"could have a transformative impact\", including drug discovery.\n\n\"The idea here is by combining large genetic association studies, such as those from UK Biobank, with AlphaGenome predictions, scientists could better pinpoint the genes and the cell types associated with the particular trait or disease.\n\n\"This could add another piece of the puzzle for the discovery of drug targets and ultimately, the development of new drugs.\n\n\"In cancer, patients can harbour many different mutations simultaneously, and it's often challenging to differentiate between the large numbers of passenger non-causal mutations from the causal driver mutations.\n\n\"A model like AlphaGenome could help prioritise down lists of variants to those most likely to actually be functional and causal and contributing to the illness.\"\n\nElsewhere, AlphaGenome could help scientists pinpoint the potential causes of rare diseases, Latysheva said, and also has \"interesting applications\" in gene therapy.\n\n\"The idea here is that if you have a powerful DNA sequence to function model, you can actually start to use that model to design entirely new DNA sequences with specific desired properties.\n\n\"For example, you could try to design a sequence that activates certain gene only in nerve cells, but not in muscle cells.\"\n\nAlphaGenome has been accessible to researchers since June 2025 through an application programming interface (API), which opens a software system to interactions from the outside.\n\nAccording to Pushmeet Kohli, vice president of science and strategic initiatives at Google DeepMind, some 3,000 scientists have since used it, making one million API calls from 160 countries.\n\nThe team is now releasing the AlphaGenome model and rates for non-commercial research, with a commercial version of the program also in early testing.\n\nKohli said that researchers from many major academic labs, including UCL, are using the model \"to advance research into areas including neurodegenerative diseases, infectious diseases and cancer\".\n\nIn 2024, Demis Hassabis, co-founder and chief executive of Google DeepMind and Isomorphic Labs, and Google DeepMind director Dr John Jumper were co-awarded the 2024 Nobel Prize in Chemistry for their work developing AlphaFold, an AI system that predicts the 3D structure of proteins from their amino acid sequences.\n\nHowever, Kohli said \"proteins are only one chapter of the biological story\".\n\n\"If proteins are the ingredients of life, then DNA is the recipe,\" he added. \"While the Human Genome Project gave us the Book of Life, reading it remained a challenge.\n\n\"We have the text, but we are still deciphering the semantics. Understanding the grammar of this genome, what is encoded in our DNA and how it governs life, is the next critical frontier for research.\"\n\nProfessor Ben Lehner, head of generative and synthetic genomics at the Wellcome Sanger Institute in Cambridge, said: \"AlphaGenome is a great example of how AI is accelerating biological discovery and the development of therapeutics.\n\n\"Identifying the precise differences in our genomes that make us more or less likely to develop thousands of diseases is a key step towards developing better therapeutics.\n\n\"AlphaGenome and models like it that help decipher the regulatory code of our genome will make it much easier to do this.\"\n\nThe Wellcome Sanger Institute has tested AlphaGenome using 500,000 new experiments, according to Prof Lehner, who said it performs \"very well\".\n\nHowever, he added that there is still work to do.\n\n\"AI models are only as good as the data used to train them,\" he said. \"Most existing data in biology is not very suitable for AI - the datasets are too small and not well standardised.\n\n\"The most important challenge right now is how to generate the data to train the next generation of even more powerful AI models. We need to do this fast, cost effectively and in a way that both the data and the resulting models are available for everyone to use.\"\n\nDr Robert Goldstone, head of genomics at the Francis Crick Institute, said AlphaGenome \"represents a major milestone in the field of genomic AI\".\n\nHe added: \"This level of resolution, particularly for non-coding DNA, is a breakthrough that moves the technology from theoretical interest to practical utility, allowing scientists to programmatically study and simulate the genetic roots of complex disease.\n\n\"AlphaGenome is not a magic bullet for all biological questions, but it is a foundational, high-quality tool that turns the static code of the genome into a decipherable language for discovery.\""
  },
  {
    "source": "WinBuzzer",
    "company": "Google DeepMind",
    "title": "Google DeepMind Adds Agentic Vision to Gemini 3 Flash",
    "date": "2026-01-28T20:35:22Z",
    "url": "https://winbuzzer.com/2026/01/28/google-deepmind-adds-agentic-vision-to-gemini-3-flash-xcxwbn/",
    "content": "Availability: Agentic Vision is available through the Gemini API in Google AI Studio and Vertex AI, with rollout started in the Gemini app.\n\nGoogle DeepMind announced this week that it is adding Agentic Vision to Gemini 3 Flash, enabling the model to actively explore images by generating and running Python code that zooms, crops, and analyzes visuals step-by-step rather than processing them in a single, static glance.\n\nUntil now, multimodal models processed images in one pass, forcing them to guess when missing small details like serial numbers on microchips or distant street signs. Agentic Vision fundamentally changes how AI models process images by treating vision as an active investigation rather than passive observation.\n\nRohan Doshi, Product Manager at Google DeepMind, explained the problem the new feature solves.\n\n\"Frontier AI models like Gemini typically process the world in a single, static glance. If they miss a fine-grained detail -- like a serial number on a microchip or a distant street sign -- they are forced to guess.\"\n\nHow the Think-Act-Observe Loop Works\n\nTo solve this guessing problem, the system operates through a Think-Act-Observe loop where Gemini 3 Flash analyzes the request and image, formulates a plan, generates and executes Python code, then inspects the results before responding.\n\nRather than relying on probabilistic guessing, the architecture shifts to verifiable, step-by-step execution by employing code to actively manipulate or analyze images.\n\n\"Agentic Vision in Gemini 3 Flash converts image understanding from a static act into an agentic process. It treats vision as an active investigation.\"\n\nThe approach bypasses visual arithmetic hallucination by offloading complex calculations to a deterministic Python environment rather than relying on probabilistic inference.\n\nBy integrating deterministic tools into probabilistic vision models, Google DeepMind creates a hybrid system that delivers an important step forward toward verifiable AI output.\n\nThis shift from probabilistic to deterministic processing creates accountability paths that enterprise deployments require. When calculations occur in Python rather than neural networks, developers gain the ability to trace errors through readable code rather than opaque model weights.\n\nThis positions Google to address a persistent challenge in multimodal AI systems.\n\nPerformance Gains and Real-World Applications\n\nThe architectural changes translate into measurable improvements. Code execution delivers a 5 to 10 percent quality improvement across various vision benchmarks.\n\nPlanCheckSolver.com improved its accuracy by 5 percent by having Gemini 3 Flash iteratively inspect high-resolution construction blueprints, demonstrating practical value for specialized document analysis.\n\nThe model can draw bounding boxes and labels on images for annotation tasks like finger counting to avoid counting errors. For visual math problems, it can parse tables and run calculations in a Python environment and output results as charts, moving complex arithmetic operations from the neural network to deterministic code execution.\n\nThe shift from probabilistic inference to deterministic computation represents what analysts see as important for building trust. When models execute verifiable Python code, developers can inspect intermediate steps and verify reasoning aligns with requirements.\n\nCurrent Limitations and Competitive Context\n\nDespite these advances, Gemini 3 Flash faces implementation constraints. The model is trained to implicitly zoom when detecting fine-grained details, but features like rotating images or visual math require explicit prompts from users.\n\nThe automatic detection works for some scenarios but not others, creating an uneven user experience across different visual tasks.\n\nAgentic Vision is currently limited to the Flash model, with plans to expand to other model sizes and add tools like web search and reverse image search. Google DeepMind has not provided a timeline for these expansions.\n\nThe update builds on the global launch of Gemini 3 Flash in December 2025, which made the model the default for Google's AI Search.\n\nStartupHub.ai noted that the move from probabilistic inference to deterministic computation is \"essential for trust\" in AI vision systems.\n\nThe release arrives as competitors pursue similar capabilities. OpenAI introduced code-executing vision capabilities with its o3 model, positioning Google DeepMind's release as part of a broader industry shift toward agentic vision systems.\n\nMultiple labs are pursuing code-executing vision models as a way to reduce hallucinations and improve reliability.\n\nThe restriction to Flash-tier models indicates Google's cautious approach to code-executing vision capabilities. While the feature builds competitive parity with OpenAI's o3 offerings, the explicit prompt requirements for some functions and the absence of expansion timelines create an opening for competitors to ship more comprehensive implementations first.\n\nThe company recently partnered with Boston Dynamics to bring AI intelligence to Atlas humanoid robots, demonstrating its broader push into applied AI systems.\n\nAgentic Vision is available through the Gemini API in Google AI Studio and Vertex AI. Rollout has started in the Gemini app where users can select Thinking in the model dropdown to access the capability.\n\nDevelopers can integrate the feature into applications through standard API calls without requiring infrastructure changes."
  },
  {
    "source": "Wired",
    "company": "Google DeepMind",
    "title": "Google DeepMind Staffers Ask Leaders to Keep Them 'Physically Safe' From ICE",
    "date": "2026-01-27T17:01:09Z",
    "url": "https://www.wired.com/story/google-deepmind-staffers-ice-office-questions-safety/",
    "content": "Employees at Google DeepMind have asked the company's leadership for plans and policies to keep them \"physically safe\" from Immigration and Customs Enforcement (ICE) while on the company's premises, according to screenshots of internal messages obtained by WIRED.\n\nOn Monday morning, two days after federal agents shot and killed Minneapolis nurse Alex Pretti, a Google DeepMind employee sent the following message in an internal message board for the company's roughly 3,000-person AI unit:\n\n\"US focused question: What is GDM doing to keep us physically safe from ICE? The events of the past week have shown that immigration status, citizenship, or even the law is not a deterrent against detention, violence, or even death from federal operatives.\"\n\nIt continues: \"What kinds of plans and policies are in place to ensure our safety at the office? Coming to and from work? As we have seen, government agency tactics can change and escalate quite rapidly. With offices in many metro areas across the US, are we prepared?\"\n\nThe message received more than 20 \"plus emoji\" reactions from Google DeepMind staffers.\n\nBy Monday evening, no senior leaders from Google had responded to the message. In fact, Google's top brass -- including CEO Sundar Pichai and DeepMind CEO Demis Hassabis -- have remained silent on Pretti's killing even inside the company, sources say.\n\nThe messages show some of the latest divisions forming between AI firms and their employees over the Trump administration's deployment of federal immigration agents across America. While Silicon Valley CEOs have largely bent the knee to Trump, their employees have started raising concerns internally and externally about the federal government's actions.\n\nGoogle DeepMind's chief scientist, Jeff Dean, has been one of the industry's most outspoken critics of ICE. In a post on X Sunday, he responded to a video of Pretti's shooting saying, \"This is absolutely shameful.\"\n\nEmployees at the defense tech firm Palantir have questioned the company's decision to work with ICE. WIRED previously reported that one Palantir employee wrote in Slack, \"In my opinion ICE are the bad guys. I am not proud that the company I enjoy so much working for is part of this.\"\n\nEmployees of AI labs that partner with Palantir -- including OpenAI, Google, Anthropic, and Meta -- have also discussed whether to push leaders to cut ties with the defense tech firm, The New York Times reported.\n\nConcerns about ICE agents entering Google's offices are not unfounded. In a message obtained by WIRED, a separate Google DeepMind staffer raised concerns about a federal agent's alleged attempt to enter the company's Cambridge, Massachusetts, office in the fall.\n\nGoogle's head of security and risk operations responded to this message to clarify what had happened. They noted that an \"officer arrived at reception without notice\" and that the agent was \"not granted entry because they did not have a warrant and promptly left.\"\n\nGoogle did not respond to a request for comment prior to publication.\n\nGoogle is one of many Silicon Valley firms that relies on thousands of highly skilled foreign workers, many of whom are in the United States on visas. In light of the Trump administration's immigration crackdown, these firms have had to offer increased protections for many of their workers. Late last year, Google and Apple advised employees on visas not to leave the country after the White House toughened its vetting of visa applicants.\n\nAt that time, Silicon Valley leaders were not shy about defending visa programs, which have allowed the United States to bring in top talent from around the globe."
  },
  {
    "source": "MIT Technology Review",
    "company": "Google DeepMind",
    "title": "Inside OpenAI's big play for science",
    "date": "2026-01-26T18:53:57Z",
    "url": "https://www.technologyreview.com/2026/01/26/1131728/inside-openais-big-play-for-science/",
    "content": "An exclusive conversation with Kevin Weil, head of the firm's new AI for Science team.\n\nIn the three years since ChatGPT's explosive debut, OpenAI's technology has upended a remarkable range of everyday activities at home, at work, in schools -- anywhere people have a browser open or a phone out, which is everywhere.\n\nNow OpenAI is making an explicit play for scientists. In October, the firm announced that it had launched a whole new team, called OpenAI for Science, dedicated to exploring how its large language models could help scientists and tweaking its tools to support them.\n\nThe last couple of months have seen a slew of social media posts and academic publications in which mathematicians, physicists, biologists, and others have described how LLMs (and OpenAI's GPT-5 in particular) have helped them make a discovery or nudged them toward a solution they might otherwise have missed. In part, OpenAI for Science was set up to engage with this community.\n\nAnd yet OpenAI is also late to the party. Google DeepMind, the rival firm behind groundbreaking scientific models such as AlphaFold and AlphaEvolve, has had an AI-for-science team for years. (When I spoke to Google DeepMind's CEO and cofounder Demis Hassabis in 2023 about that team, he told me: \"This is the reason I started DeepMind ... In fact, it's why I've worked my whole career in AI.\")\n\nSo why now? How does a push into science fit with OpenAI's wider mission? And what exactly is the firm hoping to achieve?\n\nI put these questions to Kevin Weil, a vice president at OpenAI who leads the new OpenAI for Science team, in an exclusive interview last week.\n\nOn mission\n\nWeil is a product guy. He joined OpenAI a couple of years ago as chief product officer after being head of product at Twitter and Instagram. But he started out as a scientist. He got two-thirds of the way through a PhD in particle physics at Stanford University before ditching academia for the Silicon Valley dream. Weil is keen to highlight his pedigree: \"I thought I was going to be a physics professor for the rest of my life,\" he says. \"I still read math books on vacation.\"\n\nAsked how OpenAI for Science fits with the firm's existing lineup of white-collar productivity tools or the viral video app Sora, Weil recites the company mantra: \"The mission of OpenAI is to try and build artificial general intelligence and, you know, make it beneficial for all of humanity.\"\n\nThe impact on science of future versions of this technology could be amazing, he says: New medicines, new materials, new devices. \"Think about it helping us understand the nature of reality, helping us think through open problems. Maybe the biggest, most positive impact we're going to see from AGI will actually be from its ability to accelerate science.\"\n\nHe adds, \"With GPT-5, we saw that becoming possible.\"\n\nAs Weil tells it, LLMs are now good enough to be useful scientific collaborators, spitballing ideas, suggesting novel directions to explore, and finding fruitful parallels between a scientist's question and obscure research papers published decades ago or in foreign languages.\n\nAsk AI\n\nWhy it matters to you?BETA\n\nHere's why this story might matter to you, according to AI. This is a beta feature and AI hallucinates -- it might get weird\n\nAn industry I care about is.\n\nTell me why it matters\n\nLearn more about how we're using AI.\n\nThat wasn't the case a year or so ago. Since it announced its first reasoning model, o1, in December 2024, OpenAI has been pushing the envelope of what the technology can do. \"You go back a few years and we were all collectively mind-blown that the models could get an 800 on the SAT,\" says Weil.\n\nBut soon LLMs were acing math competitions and solving graduate-level physics problems. Last year, OpenAI and Google DeepMind both announced that their LLMs had achieved gold-medal-level performance in the International Math Olympiad, one of the toughest math contests in the world. \"These models are no longer just better than 90% of grad students,\" says Weil. \"They're really at the frontier of human abilities.\"\n\nThat's a huge claim, and it comes with caveats. Still, there's no doubt that GPT-5 is a big improvement on GPT-4 when it comes to complicated problem-solving. GPT-5 includes a so-called reasoning model, a type of LLM that can break down problems into multiple steps and work through them one by one. This technique has made LLMs far better at solving math and logic problems than they used to be.\n\nMeasured against an industry benchmark known as GPQA, which includes more than 400 multiple-choice questions that test PhD-level knowledge in biology, physics, and chemistry, GPT-4 scores 39%, well below the human-expert baseline of around 70%. According to OpenAI, GPT-5.2 (the latest update to the model, released in December) scores 92%.\n\nOverhyped\n\nThe excitement is evident -- and perhaps excessive. In October, senior figures at OpenAI, including Weil, boasted on X that GPT-5 had found solutions to several unsolved math problems. Mathematicians were quick to point out that in fact what GPT-5 appeared to have done was dig up existing solutions in old research papers, including at least one written in German. That was still useful, but it wasn't the achievement OpenAI seemed to have claimed. Weil and his colleagues deleted their posts.\n\nNow Weil is more careful. It is often enough to find answers that exist but have been forgotten, he says: \"We collectively stand on the shoulders of giants, and if LLMs can kind of accumulate that knowledge so that we don't spend time struggling on a problem that is already solved, that's an acceleration all of its own.\"\n\nHe plays down the idea that LLMs are about to come up with a game-changing new discovery. \"I don't think models are there yet,\" he says. \"Maybe they'll get there. I'm optimistic that they will.\"\n\nBut, he insists, that's not the mission: \"Our mission is to accelerate science. And I don't think the bar for the acceleration of science is, like, Einstein-level reimagining of an entire field.\"\n\nFor Weil, the question is this: \"Does science actually happen faster because scientists plus models can do much more, and do it more quickly, than scientists alone? I think we're already seeing that.\"\n\nIn November, OpenAI published a series of anecdotal case studies contributed by scientists, both inside and outside the company, that illustrated how they had used GPT-5 and how it had helped. \"Most of the cases were scientists that were already using GPT-5 directly in their research and had come to us one way or another saying, 'Look at what I'm able to do with these tools,'\" says Weil.\n\nThe key things that GPT-5 seems to be good at are finding references and connections to existing work that scientists were not aware of, which sometimes sparks new ideas; helping scientists sketch mathematical proofs; and suggesting ways for scientists to test hypotheses in the lab.\n\n\"GPT 5.2 has read substantially every paper written in the last 30 years,\" says Weil. \"And it understands not just the field that a particular scientist is working in; it can bring together analogies from other, unrelated fields.\"\n\n\"That's incredibly powerful,\" he continues. \"You can always find a human collaborator in an adjacent field, but it's difficult to find, you know, a thousand collaborators in all thousand adjacent fields that might matter. And in addition to that, I can work with the model late at night -- it doesn't sleep -- and I can ask it 10 things in parallel, which is kind of awkward to do to a human.\"\n\nSolving problems\n\nMost of the scientists OpenAI reached out to back up Weil's position.\n\nRobert Scherrer, a professor of physics and astronomy at Vanderbilt University, only played around with ChatGPT for fun (\"I used to it rewrite the theme song for Gilligan's Island in the style of Beowulf, which it did very well,\" he tells me) until his Vanderbilt colleague Alex Lupsasca, a fellow physicist who now works at OpenAI, told him that GPT-5 had helped solve a problem he'd been working on.\n\nLupsasca gave Scherrer access to GPT-5 Pro, OpenAI's $200-a-month premium subscription. \"It managed to solve a problem that I and my graduate student could not solve despite working on it for several months,\" says Scherrer.\n\nIt's not perfect, he says: \"GTP-5 still makes dumb mistakes. Of course, I do too, but the mistakes GPT-5 makes are even dumber.\" And yet it keeps getting better, he says: \"If current trends continue -- and that's a big if -- I suspect that all scientists will be using LLMs soon.\"\n\nDerya Unutmaz, a professor of biology at the Jackson Laboratory, a nonprofit research institute, uses GPT-5 to brainstorm ideas, summarize papers, and plan experiments in his work studying the immune system. In the case study he shared with OpenAI, Unutmaz used GPT-5 to analyze an old data set that his team had previously looked at. The model came up with fresh insights and interpretations.\n\n\"LLMs are already essential for scientists,\" he says. \"When you can complete analysis of data sets that used to take months, not using them is not an option anymore.\"\n\nNikita Zhivotovskiy, a statistician at the University of California, Berkeley, says he has been using LLMs in his research since the first version of ChatGPT came out.\n\nLike Scherrer, he finds LLMs most useful when they highlight unexpected connections between his own work and existing results he did not know about. \"I believe that LLMs are becoming an essential technical tool for scientists, much like computers and the internet did before,\" he says. \"I expect a long-term disadvantage for those who do not use them.\"\n\nBut he does not expect LLMs to make novel discoveries anytime soon. \"I have seen very few genuinely fresh ideas or arguments that would be worth a publication on their own,\" he says. \"So far, they seem to mainly combine existing results, sometimes incorrectly, rather than produce genuinely new approaches.\"\n\nI also contacted a handful of scientists who are not connected to OpenAI.\n\nAndy Cooper, a professor of chemistry at the University of Liverpool and director of the Leverhulme Research Centre for Functional Materials Design, is less enthusiastic. \"We have not found, yet, that LLMs are fundamentally changing the way that science is done,\" he says. \"But our recent results suggest that they do have a place.\"\n\nCooper is leading a project to develop a so-called AI scientist that can fully automate parts of the scientific workflow. He says that his team doesn't use LLMs to come up with ideas. But the tech is starting to prove useful as part of a wider automated system where an LLM can help direct robots, for example.\n\n\"My guess is that LLMs might stick more in robotic workflows, at least initially, because I'm not sure that people are ready to be told what to do by an LLM,\" says Cooper. \"I'm certainly not.\"\n\nMaking errors\n\nLLMs may be becoming more and more useful, but caution is still advised. In December, Jonathan Oppenheim, a scientist who works on quantum mechanics, called out a mistake that made its way into a scientific journal. \"OpenAI leadership are promoting a paper in Physics Letters B where GPT-5 proposed the main idea -- possibly the first peer-reviewed paper where an LLM generated the core contribution,\" Oppenheim posted on X. \"One small problem: GPT-5's idea tests the wrong thing.\"\n\nHe continued: \"GPT-5 was asked for a test that detects nonlinear theories. It provided a test that detects nonlocal ones. Related-sounding, but different. It's like asking for a COVID test, and the LLM cheerfully hands you a test for chickenpox.\"\n\nIt is clear that a lot of scientists are finding innovative and intuitive ways to engage with LLMs. It is also clear that the technology makes mistakes that can be so subtle even experts miss them.\n\nPart of the problem is the way ChatGPT can flatter you into letting down your guard. As Oppenheim put it: \"A core issue is that LLMs are being trained to validate the user, while science needs tools that challenge us.\" In an extreme case, one individual (who was not a scientist) was persuaded by ChatGPT into thinking for months that he'd invented a new branch of mathematics.\n\nOf course, Weil is well aware of the problem of hallucination. But he insists that newer models are hallucinating less and less. Even so, focusing on hallucination might be missing the point, he says.\n\n\"One of my teammates here, an ex math professor, said something that stuck with me,\" says Weil. \"He said: 'When I'm doing research, if I'm bouncing ideas off a colleague, I'm wrong 90% of the time and that's kind of the point. We're both spitballing ideas and trying to find something that works.'\"\n\n\"That's actually a desirable place to be,\" says Weil. \"If you say enough wrong things and then somebody stumbles on a grain of truth and then the other person seizes on it and says, 'Oh, yeah, that's not quite right, but what if we -- ' You gradually kind of find your trail through the woods.\"\n\nThis is Weil's core vision for OpenAI for Science. GPT-5 is good, but it is not an oracle. The value of this technology is in pointing people in new directions, not coming up with definitive answers, he says.\n\nIn fact, one of the things OpenAI is now looking at is making GPT-5 dial down its confidence when it delivers a response. Instead of saying Here's the answer, it might tell scientists: Here's something to consider.\n\n\"That's actually something that we are spending a bunch of time on,\" says Weil. \"Trying to make sure that the model has some sort of epistemological humility.\"\n\nAnother thing OpenAI is looking at is how to use GPT-5 to fact-check GPT-5. It's often the case that if you feed one of GPT-5's answers back into the model, it will pick it apart and highlight mistakes.\n\n\"You can kind of hook the model up as its own critic,\" says Weil. \"Then you can get a workflow where the model is thinking and then it goes to another model, and if that model finds things that it could improve, then it passes it back to the original model and says, 'Hey, wait a minute -- this part wasn't right, but this part was interesting. Keep it.' It's almost like a couple of agents working together and you only see the output once it passes the critic.\"\n\nWhat Weil is describing also sounds a lot like what Google DeepMind did with AlphaEvolve, a tool that wrapped the LLM Gemini inside a wider system that filtered out the good responses from the bad and fed them back in again to be improved on. Google DeepMind has used AlphaEvolve to solve several real-world problems.\n\nOpenAI faces stiff competition from rival firms, whose own LLMs can do most, if not all, of the things it claims for its own models. If that's the case, why should scientists use GPT-5 instead of Gemini or Anthropic's Claude, families of models that are themselves improving every year? Ultimately, OpenAI for Science may be as much an effort to a flag in new territory as anything else. The real innovations are still to come.\n\n\"I think 2026 will be for science what 2025 was for software engineering,\" says Weil. \"At the beginning of 2025, if you were using AI to write most of your code, you were an early adopter. Whereas 12 months later, if you're not using AI to write most of your code, you're probably falling behind. We're now seeing those same early flashes for science as we did for code.\"\n\nHe continues: \"I think that in a year, if you're a scientist and you're not heavily using AI, you'll be missing an opportunity to increase the quality and pace of your thinking.\""
  },
  {
    "source": "Numerama.com",
    "company": "Google DeepMind",
    "title": "Salaire, emploi, richesse : Google DeepMind se prépare déjà à la rupture de notre modèle économique - Numerama",
    "date": "2026-01-23T10:39:33Z",
    "url": "https://www.numerama.com/tech/2165881-salaire-emploi-richesse-google-deepmind-se-prepare-deja-a-la-rupture-de-notre-modele-economique.html",
    "content": "Dans un message publié sur X le 22 janvier 2026, Shane Legg, Chief AGI Scientist chez Google DeepMind, annonce recruter un économiste en anticipation de l'arrivée de l'intelligence artificielle générale. Une démarche qui interroge directement la pérennité de notre modèle économique actuel.\n\nEn pleine effervescence autour de l'intelligence artificielle, Google DeepMind se projette déjà dans l'après. Dans un message publié sur X le 22 janvier 2026, Shane Legg, cofondateur de DeepMind Technologies -- entreprise rachetée par Google en 2014 -- dévoile les ambitions de la firme. Le chercheur et entrepreneur annonce rechercher \" un économiste senior, directement rattaché à [lui], pour diriger une petite équipe travaillant sur l'économie post-IAG (intelligence artificielle générale) \".\n\nL'IAG (ou AGI en anglais) désigne une IA capable d'accomplir à peu près toutes les tâches intellectuelles qu'un humain peut réaliser, au moins aussi bien que lui. Une perspective que certains jugeront prématurée, quand d'autres estiment au contraire qu'il faut s'y préparer dès maintenant.\n\nShane Legg, lui, martèle depuis 2009 que ce basculement est proche, allant jusqu'à évoquer une échéance autour de 2028. Si les estimations quant à son arrivée divergent, une chose fait consensus : une telle avancée transformerait en profondeur l'économie.\n\nPourquoi Google DeepMind prépare-t-il déjà l'après IAG ?\n\nBien que l'on dispose aujourd'hui de modèles d'IA considérés comme très avancés -- à l'instar de GPT, Claude ou Gemini -- ces systèmes ne sont pas pour autant qualifiés d'intelligence artificielle générale. Une IAG devrait être capable d'accomplir à peu près toutes les tâches intellectuelles humaines : raisonner sur le long terme, s'adapter à de nouveaux contextes ou encore apprendre n'importe quel type de tâche. Ce niveau n'est pas encore atteint.\n\nLes systèmes actuels relèvent ainsi de l'IA dite \" étroite \" (ANI) : très performants dans des domaines spécifiques (texte, image, code...), ils restent spécialisés et peuvent se montrer fragiles dès qu'ils sortent de leur cadre d'entraînement. L'arrivée d'une IAG demeure donc, à ce stade, une hypothèse débattue, sur laquelle les experts affichent des avis très dispersés.\n\nShane Legg réfléchit pourtant à cette perspective depuis plus de vingt ans. Au sein de Google DeepMind, il occupe le poste de Chief AGI Scientist (scientifique en chef pour l'IAG) et pilote notamment les travaux de sûreté technique liés à cette dernière. Une part importante de son rôle consiste à orienter la recherche et à structurer les efforts de sécurité autour de l'IA avancée.\n\nDès le début des années 2000, il contribue, aux côtés de Ben Goertzel, chercheur en informatique et figure pionnière des travaux sur l'IAG, à remettre en circulation et à populariser le terme \" intelligence artificielle générale \", pour désigner une IA capable de réaliser la quasi-totalité des tâches cognitives humaines.\n\nL'IAG signe la fin du travail et de l'économie tels que nous les connaissons ?\n\nDans son message récemment publié sur X, Shane Legg affirme ainsi que l'AGI \" va profondément transformer beaucoup de choses, y compris l'économie \", ce qui suggère un changement de structure plutôt qu'un simple gain de productivité. Lors de son entretien The Arrival of AGI avec Hannah Fry en décembre 2025, il expliquait déjà que l'AGI allait \" structurer \" différemment l'économie et la société, en modifiant la manière dont la valeur est créée et répartie. Mais comment, concrètement ?\n\nIl anticipe notamment la remise en cause progressive du modèle économique classique fondé sur l'échange \" travail cognitif contre revenu \". Une large part du travail intellectuel pouvant, selon lui, être prise en charge par des systèmes d'IA avancés, de nombreux emplois deviennent structurellement vulnérables à l'automatisation.\n\nShane Legg avance ainsi une règle générale : tout travail pouvant être effectué à distance, sur un ordinateur et via Internet -- en particulier le travail purement cognitif -- est fortement automatisable. Cela concerne une grande partie des emplois de bureau, de services ou de back-office. Selon cette logique, des équipes entières de remote workers (support, back-office, analyse, une partie du développement logiciel) pourraient être fortement réduites.\n\nIl prend régulièrement l'exemple du développement logiciel : là où une entreprise emploie aujourd'hui une centaine de développeurs, elle pourrait demain n'en conserver qu'une vingtaine très qualifiés, s'appuyant sur des outils d'IA avancés pour produire le reste. Une mécanique qui, selon lui, pourrait s'étendre à de nombreux métiers \" au clavier \" : comptables, rédacteurs, analystes, ou encore certains profils du conseil.\n\nLe chercheur insiste alors sur un point central : l'économie repose aujourd'hui sur un contrat simple -- on échange son travail, mental ou physique, contre un salaire. Si les machines deviennent capables d'effectuer une grande partie du travail mental, plus efficacement et à moindre coût, ce contrat se fragilise. Le lien direct entre contribution individuelle et revenu devient flou, entraînant un changement structurel de l'économie.\n\nDans ce scénario, il devient nécessaire de repenser en profondeur des piliers entiers du système économique : fiscalité, protection sociale, mécanismes de redistribution, voire l'émergence de formes de revenus partiellement ou totalement détachées de l'emploi traditionnel.\n\nPour Shane Legg, l'IA avancée et l'AGI pourraient pourtant ouvrir un \" âge d'or \" de la productivité, avec davantage de science, d'innovation, de biens et de services produits avec moins de travail humain. Le véritable défi ne serait alors plus la création de richesse, mais sa distribution : comment éviter un monde où une poignée d'entreprises ou d'États captent l'essentiel des gains, laissant une large part de la population sans emploi stable ni filet de sécurité suffisant ? Autant de questions qui l'ont conduit à réfléchir sérieusement à un \" modèle post-AGI \".\n\nPourquoi Shane Legg cherche-t-il des économistes pour une époque post-IA ?\n\nPour Shane Legg, ces enjeux ne peuvent être laissés aux seuls informaticiens. Il appelle explicitement les économistes, éducateurs, éthiciens et décideurs publics à repenser leurs disciplines à l'aune d'un monde où l'AGI existerait réellement. C'est dans cette optique qu'il cherche à constituer, au sein de Google DeepMind, une équipe d'économistes spécialisés dans l'IA générale.\n\n\" Nous étudions les questions fondamentales qui se poseront dans un monde post-IA \", explique-t-il dans l'offre d'emploi, rappelant que \" l'une des plus cruciales et des moins bien comprises concerne l'économie \". Parmi les missions du profil recherché figurent la construction de simulations et de modèles économiques -- notamment à base d'agents -- afin d'explorer différents scénarios post-AGI, mais aussi la conduite de recherches fondamentales sur les impacts économiques de long terme de l'AGI, \" en remettant en question les hypothèses existantes sur la rareté, la richesse et la distribution \".\n\nPour autant, si Shane Legg est largement reconnu comme une figure sérieuse et respectée du monde de l'IA, ses prévisions -- notamment l'hypothèse d'une AGI autour de 2028 -- font l'objet de débats. Certains les jugent lucides et utiles pour anticiper, tandis que d'autres les estiment trop optimistes ou hautement spéculatives. Elles apparaissent néanmoins suffisamment crédibles pour justifier une préparation sérieuse, sans pour autant relever de la prophétie."
  },
  {
    "source": "LatestLY",
    "company": "Google DeepMind",
    "title": "Business News | Look Forward to Continuing Our Conversation at Summit: Google DeepMind CEO After Meeting with Ashwini Vaishnaw in Davos",
    "date": "2026-01-23T05:24:41Z",
    "url": "https://www.latestly.com/agency-news/business-news-look-forward-to-continuing-our-conversation-at-summit-google-deepmind-ceo-after-meeting-with-ashwini-vaishnaw-in-davos-7286410.html",
    "content": "Get latest articles and stories on Business at LatestLY. Google DeepMind CEO and Co-founder Demis Hassabis confirmed his participation in the upcoming AI Impact Summit in New Delhi following a meeting with Union Minister of Electronics and Information Technology Ashwini Vaishnaw at the World Economic Forum in Davos. The summit, scheduled for February 2026, focuses on India's role in the global development and implementation of artificial intelligence technologies.\n\nDavos [Switzerland], January 23 (ANI): Google DeepMind CEO and Co-founder Demis Hassabis confirmed his participation in the upcoming AI Impact Summit in New Delhi following a meeting with Union Minister of Electronics and Information Technology Ashwini Vaishnaw at the World Economic Forum in Davos.\n\nThe summit, scheduled for February 2026, focuses on India's role in the global development and implementation of artificial intelligence technologies.\n\nAlso Read | Dubai Gold Rate Today, January 23, 2026: Prices Jump to New Peak on Global Cues, Check 18K, 22K and 24K Gold Rates.\n\nThe announcement followed a series of diplomatic engagements at Davos, where the Union IT Minister Vaishnaw met with global technology leaders to discuss the sector's growth.\n\nGoogle DeepMind CEO Hassabis shared details of the interaction on the social media platform X, noting the significance of the upcoming summit. The discussions centered on the strategic positioning of India within the international AI landscape and the technical potential of these systems to address global challenges.\n\nAlso Read | Senorita Viral Video on TikTok: How to Take The 'Senorita' Challenge.\n\n\"Great to meet you Minister @AshwiniVaishnaw. Really enjoyed our discussion on AI's incredible potential to benefit humanity & India's important role in realising this - looking forward to continuing our conversation at the Summit!\" Hassabis stated on X. The Google DeepMind executive's commitment to the February event aligns with the government's efforts to centralize AI policy discussions in the capital.\n\nVaishnaw also met with OpenAI Chief Global Affairs Officer Chris Lehane during the forum. The Minister highlighted the necessity of collaborative frameworks in shaping technology for public benefit. He emphasized that the February summit serves as a platform for these international entities to engage directly with India's digital infrastructure.\n\n\"Met Mr. Demis Hassabis, CEO & Co-founder, Google DeepMind and Mr. Chris Lehane, Chief Global Affairs Officer, OpenAI, at WEF, Davos. Discussed India's growing role in shaping AI for global good. Encouraged active participation in the AI Impact Summit, to be held in New Delhi in February, 2026,\" the Minister said. (ANI)"
  },
  {
    "source": "Economic Times",
    "company": "Google DeepMind",
    "title": "Look forward to continuing our conversation at Summit: Google DeepMind CEO meets Ashwini Vaishnaw in Davos",
    "date": "2026-01-23T05:13:34Z",
    "url": "https://economictimes.indiatimes.com/tech/technology/look-forward-to-continuing-our-conversation-at-summit-google-deepmind-ceo-after-meeting-with-ashwini-vaishnaw-in-davos/articleshow/127234473.cms",
    "content": "Google DeepMind CEO and Co-founder Demis Hassabis confirmed his participation in the upcoming AI Impact Summit in New Delhi following a meeting with Union Minister of Electronics and Information Technology Ashwini Vaishnaw at the World Economic Forum in Davos. The summit, scheduled for February 2026, focuses on India's role in the global development and implementation of artificial intelligence technologies.\n\nGoogle DeepMind CEO and Co-founder Demis Hassabis confirmed his participation in the upcoming AI Impact Summit in New Delhi following a meeting with Union Minister of Electronics and Information Technology Ashwini Vaishnaw at the World Economic Forum in Davos.\n\nThe summit, scheduled for February 2026, focuses on India's role in the global development and implementation of artificial intelligence technologies.\n\nThe announcement followed a series of diplomatic engagements at Davos, where the Union IT Minister Vaishnaw met with global technology leaders to discuss the sector's growth.\n\nGoogle DeepMind CEO Hassabis shared details of the interaction on the social media platform X, noting the significance of the upcoming summit. The discussions centered on the strategic positioning of India within the international AI landscape and the technical potential of these systems to address global challenges.\n\n\"Great to meet you Minister @AshwiniVaishnaw. Really enjoyed our discussion on AI's incredible potential to benefit humanity & India's important role in realising this - looking forward to continuing our conversation at the Summit!\" Hassabis stated on X. The Google DeepMind executive's commitment to the February event aligns with the government's efforts to centralize AI policy discussions in the capital.\n\nVaishnaw also met with OpenAI Chief Global Affairs Officer Chris Lehane during the forum. The Minister highlighted the necessity of collaborative frameworks in shaping technology for public benefit. He emphasized that the February summit serves as a platform for these international entities to engage directly with India's digital infrastructure.\n\n\"Met Mr. Demis Hassabis, CEO & Co-founder, Google DeepMind and Mr. Chris Lehane, Chief Global Affairs Officer, OpenAI, at WEF, Davos. Discussed India's growing role in shaping AI for global good. Encouraged active participation in the AI Impact Summit, to be held in New Delhi in February, 2026,\" the Minister said."
  },
  {
    "source": "WebProNews",
    "company": "Google DeepMind",
    "title": "OpenClaw's Security Crisis: How a Promising AI Framework Became Too Dangerous for Its Own Creators to Share Freely",
    "date": "2026-02-20T16:04:55Z",
    "url": "https://www.webpronews.com/openclaws-security-crisis-how-a-promising-ai-framework-became-too-dangerous-for-its-own-creators-to-share-freely/",
    "content": "When Meta, Google DeepMind, and a coalition of leading artificial intelligence companies quietly began restricting access to OpenClaw -- an open-source AI framework originally designed to accelerate robotics and autonomous systems research -- the move sent shockwaves through the AI development community. The restrictions, which emerged in late January and early February 2026, represent one of the most significant reversals in the open-source AI movement to date, raising fundamental questions about how the industry balances transparency with the growing risks of powerful AI tools falling into the wrong hands.\n\nThe framework, which had been freely available on GitHub for roughly eight months before the restrictions took effect, was initially celebrated as a breakthrough in making advanced robotic manipulation and planning capabilities accessible to researchers worldwide. But a series of alarming security disclosures -- including demonstrations that OpenClaw's core modules could be repurposed for autonomous weapons targeting and critical infrastructure attacks -- forced the companies that contributed to its development to take the extraordinary step of pulling back, as reported by Ars Technica.\n\nFrom Open Innovation to Controlled Access in a Matter of Weeks\n\nThe speed of the reversal was remarkable even by the fast-moving standards of the AI industry. OpenClaw had been released under a permissive Apache 2.0 license, the gold standard for open-source software that allows virtually unrestricted commercial and research use. Within weeks of the security concerns surfacing, Meta moved the repository to a restricted-access model requiring institutional verification, while Google DeepMind withdrew several of its contributed modules entirely. Smaller contributors, including university labs and independent researchers, found themselves locked out of codebases they had helped build.\n\nThe trigger, according to the Ars Technica report, was a pair of research papers -- one from a team at ETH Zurich and another from researchers affiliated with the RAND Corporation -- that demonstrated how OpenClaw's spatial reasoning and object manipulation modules could be adapted with minimal modification to guide autonomous drones in identifying and engaging targets without human oversight. A separate proof-of-concept, circulated privately among AI safety researchers before being partially disclosed, showed how the framework's planning algorithms could be used to model attack sequences against power grid substations, optimizing for maximum disruption with minimal physical resources.\n\nThe Technical Heart of the Problem\n\nWhat made OpenClaw particularly potent -- and particularly dangerous -- was its integration of several capabilities that had previously existed only in isolation. The framework combined advanced 3D scene understanding, long-horizon task planning, dexterous manipulation control, and a novel transfer learning system that allowed models trained in simulation to be deployed on physical hardware with minimal fine-tuning. For legitimate robotics researchers, this combination was enormously valuable, collapsing months of integration work into days. For those with malicious intent, it provided a nearly turnkey system for building autonomous machines capable of operating in complex, unstructured environments.\n\nThe transfer learning component drew particular scrutiny. OpenClaw's approach allowed a model trained to, say, sort packages in a warehouse simulation to be rapidly adapted to handle entirely different objects in real-world settings. Security researchers pointed out that the same mechanism could allow a system trained on benign tasks to be quickly retrained for harmful ones -- assembling improvised explosive devices, for instance, or disabling safety systems on industrial equipment. The modularity that made the framework so appealing to researchers also made it dangerously adaptable.\n\nIndustry Reaction: A Community Divided\n\nThe restrictions have split the AI research community along familiar but increasingly bitter lines. Proponents of open-source AI development argue that restricting access to OpenClaw sets a dangerous precedent that will ultimately slow progress and concentrate power among a handful of large corporations. \"You cannot put this genie back in the bottle,\" said one prominent AI researcher at a major U.S. university, speaking on condition of anonymity because their institution receives funding from Meta. \"The knowledge embedded in OpenClaw is already distributed across thousands of forks and derivative projects. All the restrictions accomplish is preventing legitimate researchers from collaborating while doing nothing to stop bad actors who already have the code.\"\n\nOn the other side, AI safety advocates have largely applauded the move, even while acknowledging its imperfections. The Center for AI Safety, a San Francisco-based nonprofit, issued a statement calling the restrictions \"a necessary if belated recognition that capability and risk scale together, and that the open-source model as traditionally practiced may not be appropriate for all categories of AI systems.\" The statement noted that the dual-use concerns raised by OpenClaw were not hypothetical but had been demonstrated in concrete, reproducible experiments -- a threshold that, in the organization's view, obligated the developers to act.\n\nMeta's Delicate Balancing Act\n\nFor Meta, the OpenClaw episode represents a particularly awkward chapter in the company's broader AI strategy. Under CEO Mark Zuckerberg, Meta has positioned itself as the champion of open AI development, releasing the Llama family of large language models under increasingly permissive terms and arguing publicly that open-source approaches produce safer, more trustworthy AI systems. The company's decision to restrict OpenClaw -- a project in which Meta's FAIR (Fundamental AI Research) lab was the single largest contributor -- directly undercuts that narrative.\n\nMeta's public communications about the restrictions have been carefully worded. A spokesperson told Ars Technica that the company \"remains deeply committed to open science\" but acknowledged that \"certain combinations of capabilities require additional safeguards before broad distribution.\" The company has proposed a tiered access system in which verified academic institutions and approved commercial partners can obtain full access, while independent developers and researchers in countries subject to U.S. export controls would receive only a limited subset of the framework's modules. Critics have compared this approach to the \"open but not really open\" licensing terms that Meta has applied to some versions of Llama, arguing that it amounts to open-washing -- using the language and reputation of open source while maintaining corporate control.\n\nRegulatory Implications and the Shadow of Export Controls\n\nThe OpenClaw situation has also attracted the attention of policymakers in Washington, Brussels, and Beijing. In the United States, the Commerce Department's Bureau of Industry and Security (BIS) has been quietly evaluating whether AI frameworks with demonstrated dual-use potential should be subject to export control regulations similar to those applied to advanced semiconductor manufacturing equipment. The OpenClaw disclosures have reportedly accelerated those discussions, with some officials arguing that the voluntary restrictions imposed by Meta and Google DeepMind are insufficient and that mandatory controls are needed.\n\nIn the European Union, the OpenClaw case is being cited in ongoing debates about the implementation of the AI Act, which entered into force in stages beginning in 2024. The Act's provisions on \"general-purpose AI models\" with \"systemic risk\" could potentially apply to frameworks like OpenClaw, though the specific thresholds and enforcement mechanisms remain subjects of intense negotiation. European officials have pointed to the episode as evidence that self-regulation by AI companies cannot be relied upon, given that the security risks were identified not by the companies themselves but by independent researchers.\n\nThe Broader Question: Can Powerful AI Remain Open?\n\nAt its core, the OpenClaw controversy forces a reckoning with a question that the AI industry has been deferring for years: whether the open-source development model that has driven so much progress in software engineering over the past three decades is compatible with AI systems of increasing power and generality. Traditional open-source software -- operating systems, web servers, databases -- can certainly be misused, but the barrier between a database and a weapon is high. With AI systems that can reason about the physical world, plan complex sequences of actions, and adapt to novel situations, that barrier is dramatically lower.\n\nSome researchers have proposed intermediate models that attempt to preserve the benefits of openness while mitigating the risks. These include structured access programs, where researchers can run experiments on powerful models without downloading the weights; differential release strategies, where less sensitive components are made fully open while more dangerous modules are restricted; and mandatory red-teaming requirements, where new capabilities must be evaluated for misuse potential before release. OpenClaw's developers are reportedly considering a combination of these approaches for a future version of the framework.\n\nWhat Comes Next for OpenClaw and the Open AI Movement\n\nFor now, the restricted version of OpenClaw remains available to approved researchers, and development continues behind closed doors. Meta and Google DeepMind have both indicated that they intend to release updated versions of the framework with additional safety guardrails, though neither company has provided a timeline. The ETH Zurich team that first identified the weapons-related vulnerabilities has proposed a formal review process modeled on the biosecurity protocols used for dual-use research of concern in the life sciences -- a framework that would require independent safety review before the release of AI systems with demonstrated potential for catastrophic misuse.\n\nWhether the AI industry will adopt such protocols voluntarily, or whether governments will impose them, remains an open question. What is clear is that the OpenClaw episode has permanently altered the terms of the debate. The assumption that openness is an unalloyed good -- that more transparency always leads to better outcomes -- has been tested against a concrete case where the risks of unrestricted access proved too great for even the most committed advocates of open AI to accept. The challenge now is to build institutions and norms that can distinguish between the vast majority of AI research that benefits from openness and the narrow but growing category of capabilities that demand a more cautious approach. The stakes, as the OpenClaw case has made viscerally clear, are no longer merely academic."
  },
  {
    "source": "El HuffPost",
    "company": "Google DeepMind",
    "title": "El Nobel de Google frena la euforia de Silicon Valley y explica las tres razones por las que la IA aún no puede pensar ni planificar como un humano",
    "date": "2026-02-19T18:10:13Z",
    "url": "https://www.huffingtonpost.es/tecnologia/el-nobel-google-frena-euforia-silicon-valley-explica-tres-razones-iaun-pensar-planificar-humano-f202602.html",
    "content": "La Inteligencia Artificial ya está aquí y tanto es así que a veces cuesta distinguir lo que es realidad y lo que es IA. Este prodigio tecnológico ofrece tantas oportunidades y recursos como dudas genera sobre su utilización, potencialmente dañina como han demostrado recientes trabajos, y especialmente sobre sus efectos entre trabajadores.\n\nLas inquietudes alcanzan a los propios líderes del sector de la IA, pero son ellos mismos los que ponen cierto freno a las capacidades de esta tecnología tan en boga.\n\nEl CEO de Google DeepMind, Demis Hassabis, ha querido apuntar los tres elementos claves por los que la IA no puede igualar (aún) a la inteligencia humana ni tampoco pensar o planificar como lo hacemos los seres humanos.\n\nPara quien no le conozca, Demis Hassabis no es solo la mente pensante tras Google DeepMind, también es el Premio Nobel de Química 2024, junto a John Jumper y David Baker, por utilizar la inteligencia artificial para predecir la estructura de las proteínas.\n\nHassabis aseguró, en una reciente cumbre sobre IA en Nueva Delhi (India) que \"no creo que hayamos llegado ahí todavía\", pero se detuvo en las tres áreas donde la inteligencia artificial presenta deficiencias.\n\nPara el responsable de Google DeepMind, el primer aspecto es el \"aprendizaje continuo\", ya que entiende que los sistemas tecnológicos están congelados debido a la capacitación recibida antes de su implementación.\n\n\"Lo que nos gustaría es que esos sistemas aprendieran continuamente en línea a partir de la experiencia, que aprendieran del contexto en el que se encuentran, tal vez adaptándose a la situación y a las tareas que tenemos para ellos\", apuntó en su intervención en el acto.\n\nComo segundo elemento clave, Hassabis detalló los problemas para pensar a largo plazo de estos softwares. \"Pueden planificar a corto plazo, pero a largo plazo, como nosotros podemos planificar a lo largo de años, realmente no tienen esa capacidad en este momento\".\n\nEl gurú dejó para el final una carencia, la de la consistencia, algo que en este punto la inteligencia no es capaz de gestionar. A juicio del responsable de Google DeepMind, la IA es actualmente experta en algunas áreas, hasta niveles muy sofisticados, pero en paralelo no están formadas en otros sectores.\n\n\"Por ejemplo, los sistemas actuales pueden ganar medallas de oro en las Olimpiadas Internacionales de Matemáticas, con problemas realmente difíciles, pero a veces pueden cometer errores en matemáticas elementales si se plantea la pregunta de cierta manera\", ejemplificó Hassabis ante la audiencia."
  },
  {
    "source": "News9live",
    "company": "Google DeepMind",
    "title": "Google DeepMind partners with Indian Government to expand AI",
    "date": "2026-02-19T09:15:35Z",
    "url": "https://www.news9live.com/technology/artificial-intelligence/google-deepmind-partners-with-indian-government-to-expand-ai-2933164",
    "content": "New Delhi: Google on February 18 announced a new partnership between its AI research arm, Google DeepMind, and several Indian government bodies and local institutions. The aim is to support scientific research, education and public services in the country using advanced artificial intelligence tools. The announcement was made by Google CEO Sundar Pichai at a company event in New Delhi. The collaboration will give Indian researchers, schools and public institutions access to some of Google's most advanced AI systems and training programmes.\n\n\"I believe India is going to have an extraordinary trajectory with AI and we want to be a partner\" Pichai said. \"Google has a full-stack commitment to India and I have never been more excited about the future we are building together.\" The partnership is part of Google DeepMind's larger National Partnerships for AI programme. Under this initiative, the company works with governments to expand access to advanced AI tools for national priorities. Similar agreements were announced with the governments of the United States and the United Kingdom in December 2025.\n\nAccess to advanced AI tools\n\nAs part of the tie-up, Indian researchers and engineers will be able to use several AI models developed by Google DeepMind. Which include AlphaGenome, Earth AI and AI Co-scientist.\n\nGoogle DeepMind, Google Research and Google.org are also partnering with the Anusandhan National Research Foundation (ANRF) to help expand the use of AI models in scientific research.\n\nIndia is currently the fourth largest user of AlphaFold globally. More than 180,000 researchers in India are using it today, compared to 150,000 researchers in July 2025.\n\n\"We hope to see Indian scientists benefit even more from using AlphaGenome and the other AI systems we are now providing,\" the company noted.\n\n$30 million Science Impact Challenge\n\nAt the event, Pichai also announced a $30 million Google.org AI for Science Impact Challenge. The fund will support researchers around the world who are using AI to make scientific discoveries.\n\n\"AI is fundamentally shifting the base of discovery. I am excited to see how we can continue accelerating science for real-world impact,\" Pichai said.\n\nBringing AI and robotics to schools\n\nGoogle is also expanding its work in the education sector. The company has partnered with Atal Tinkering Labs to introduce generative AI tools in 10,000 schools, reaching around 11 million students. The plan includes adding robotics and coding into school learning.\n\nUnder this programme, Gemini AI assistant will be integrated into teacher workflows. A special \"guardrailed\" AI assistant will also be built for students. It will follow national curriculum standards and act as a study partner.\n\nIn another move, Gemini will convert two million static textbooks into interactive AI-powered learning tools. This will cover more than 250 titles across 2,000 schools, in partnership with K-12 publisher PM Publishers.\n\nEach book will have a QR code that students can scan to access a customised version of Gemini, called Gem. This assistant will provide summaries and answer questions related to that specific textbook.\n\nStrengthening US-India AI links\n\nGoogle also announced the America-India Connect initiative. This project will create new subsea cable routes to improve AI-related connectivity between the United States, India and several countries in the Southern Hemisphere.\n\nThis builds on Google's earlier $15 billion investment announced in October 2025 to set up its first AI hub in India. The hub is located in Visakhapatnam (Vizag), Andhra Pradesh.\n\nAnother $30 million Google.org Global AI for Government Innovation Impact Challenge was also announced. The aim is to support projects that use AI to improve public services.\n\nThe company introduced new skilling programmes as well. One of them is an AI Professional Certificate programme designed to help people learn how to use AI at work. In India, Google is working with Wadhwani AI to offer this programme to students and early career professionals.\n\nGoogle Cloud will also provide infrastructure support to iGOT Karmayogi, a digital learning platform used for training civil servants. The platform supports more than 20 million public servants across over 800 districts."
  },
  {
    "source": "Business Standard",
    "company": "Google DeepMind",
    "title": "Big Tech bets, Galgotias robodog row shape day 3 of India-AI Summit",
    "date": "2026-02-18T13:41:57Z",
    "url": "https://www.business-standard.com/technology/tech-news/big-tech-bets-galgotias-robodog-row-shape-day-3-of-india-ai-summit-126021801220_1.html",
    "content": "AI infrastructure bets, startup model launches and global funding commitments marked day three of the AI summit. | Image: Khalid Anzar\n\nThe India-AI Impact Summit entered its third day at Bharat Mandapam with Big Tech, defence leaders and India's startup ecosystem converging to define the country's next phase of AI expansion. Google DeepMind chief Demis Hassabis framed the scientific frontier, as Hassabis cautioned that current systems still lack consistency and continual learning.\n\nMajor infrastructure and investment announcements underscored India's growing strategic importance. Nvidia tied up with Yotta, L&T and NPCI to expand AI compute, data centres and payments infrastructure, while also backing Indian startups alongside Peak XV and Accel. Microsoft president Brad Smith outlined plans to expand AI across the Global South, warning of a widening global AI divide even as India accelerates adoption.\n\nThe focus moved from policy direction to concrete execution across compute infrastructure, startup funding, defence, healthcare and digital public systems, reinforcing India's attempt to scale domestic AI capability and deployment.\n\nNvidia anchors India's sovereign compute expansion\n\nCompute infrastructure dominated announcements, with Nvidia expanding its presence through multiple partnerships spanning cloud, startups, payments and data centre infrastructure. Nvidia and Yotta Data Services said they would deploy one of Asia-Pacific's largest DGX Cloud clusters in India, powered by advanced Blackwell Ultra GPUs, forming the backbone of sovereign AI compute capacity. The infrastructure is designed to support training and deployment of large AI models for enterprises, research institutions and startups.\n\nMicrosoft points to bridging AI divide\n\nMicrosoft announced plans to invest $50 billion to expand AI infrastructure and services across countries in the Global South, placing emerging economies, including India at the centre of its global strategy. Microsoft president Brad Smith said there was an urgent need to bridge the global AI divide, warning that unequal access to compute and AI capabilities could widen economic and technological gaps between countries.\n\nAlso Read\n\nNPCI partners with Nvidia to build sovereign AI infra for payments\n\nBharatGen unveils Param2 17B multilingual MoE model under sovereign AI push\n\nNPCI exploring multilateral routes to link UPI with global payment systems\n\nOpenAI partners with top Indian institutes to advance AI literacy, skills\n\nCAQM revokes Grap II curbs in Delhi-NCR after air quality improves\n\nThe investment will focus on expanding cloud infrastructure, enabling enterprise adoption and strengthening workforce readiness, as AI deployment becomes increasingly dependent on access to large-scale computing infrastructure and advanced models.\n\nGoogle DeepMind expands research access, flags model limitations\n\nCEO and cofounder of Google DeepMind, Demis Hassabis, said current AI systems remain powerful but inconsistent, with limitations in continual learning and reliability. He emphasised the need for research breakthroughs to improve reasoning and adaptability.\n\nSeparately, at a company event, Google DeepMind announced a partnership with the Indian government to provide researchers and institutions access to its frontier AI science models, expanding domestic research capability.\n\nSarvam launches domestic model as sovereign AI push accelerates\n\nDomestic startup Sarvam unveiled a new AI model designed specifically for Indian languages and enterprise use cases, marking continued progress in India's sovereign AI efforts. The model is aimed at improving performance across regional languages, enterprise workflows and local use cases, addressing limitations of global models that are primarily trained on Western datasets.\n\nDefence and aerospace leaders highlight operational deployment\n\nDefence experts said AI would play a critical role in improving military decision-making, logistics and operational readiness. Lt Gen Vinod Shinghal said AI would enhance operational capabilities but command authority would remain with human leadership. Military discussions focused on applications such as predictive maintenance, intelligence analysis and mission planning.\n\nAI will be workforce enabler: Airbus India chief\n\nAt a separate panel discussion, industry leaders also highlighted AI's role in augmenting professional expertise rather than replacing workers. Airbus executives said AI would enhance engineering, safety and operational efficiency while maintaining human oversight, reflecting broader industry emphasis on augmentation rather than automation-led displacement.\n\nHealthcare, workforce readiness and adoption capacity take focus\n\nHealthcare deployment emerged as another major theme, with discussions focusing on scaling AI in primary healthcare systems, workforce training and service delivery. The sessions examined how AI could improve diagnostics, workforce efficiency and system capacity, particularly in resource-constrained environments.\n\nAI literacy expansion is essential say industry leaders\n\nTechnology and enterprise leaders also emphasised workforce readiness and skills development. Founder and CEO of Vianai Systems, Vishal Sikka, said expanding AI literacy would be essential to ensure workers can adapt to technological shifts.\n\nMeanwhile, Zoho founder Sridhar Vembu said India could move faster in adopting AI than the US over the next decade, reflecting faster digital integration and population-scale deployment potential.\n\nRishi Sunak says AI optimism more in India than in West\n\nFormer UK prime minister Rishi Sunak said optimism around AI adoption was stronger in India than in Western economies, where concerns over job displacement and regulatory risk remain higher.\n\nStartup ecosystem and funding momentum for domestic innovation\n\nInvestor participation expanded alongside infrastructure and model announcements, with global technology firms and venture investors committing capital to Indian AI startups. Funding partnerships announced by Nvidia with Peak XV and Accel India are aimed at accelerating the development of domestic applications, infrastructure and enterprise-focused AI systems.\n\nNPCI said the next phase of its AI work will move towards a more general, scalable AI layer for the payments ecosystem, which will include exploring architectures to support high-volume, low-latency environments, and gradually expanding multilingual and agent-oriented capabilities.\n\nGalgotias robodog controversy draws scrutiny over authenticity claims\n\nThe summit also saw controversy after Galgotias University displayed a robotic dog presented as its own innovation, which was later identified as a commercially available Chinese product. Summit authorities asked the university to vacate its stall, and the power supply to the pavilion was reportedly cut. The incident drew attention to authenticity standards and verification requirements for research and innovation showcased at the summit.\n\nS Krishnan, secretary, Ministry of Electronics and Information Technology (MeitY), told reporters at the summit that the government was keen to avoid any controversy around the Expo displays, following a request to the university to take down its exhibit from the exhibition area.\n\nMore From This Section\n\nAI can transform trade by curbing fraud and aiding governance: DGFT\n\nGovt to push AI solutions to improve grid stability, RE integration: Joshi\n\n'India on extraordinary AI trajectory': Pichai outlines Google's big bets\n\nTech Wrap Feb 18: Google I/O 2026, Apple AI devices, Claude Sonnet 4.6\n\nOptimism for AI in India, anxiety in West, says Rishi Sunak at AI Summit"
  },
  {
    "source": "The Times of India",
    "company": "Google DeepMind",
    "title": "Google DeepMind CEO Demis Hassabis talks about his sleep routine that he agrees is unusual; but he also has a 'warning', sleeping ... - The Times of India",
    "date": "2026-02-12T12:15:16Z",
    "url": "https://timesofindia.indiatimes.com/technology/tech-news/google-deepmind-ceo-demis-hassabis-talks-about-his-sleep-routine-that-he-agrees-is-unusual-but-he-also-has-a-warning-sleeping-/articleshow/128256916.cms",
    "content": "Google DeepMind CEO Demis Hassabis has admitted that his sleep routine is far from typical in a video interview with Fortune. Hassabis said he sleeps 'very little' aiming for about six houses and added that he divides his waking hours into two distinct workdays. \"I do try and get six, but I have unusual sleeping habits. I sort of manage during the day,\" he explained. Hassabis added that his schedule includes packing his office hours with back-to-back meetings, then returning home for family time and dinner. Around 10 p.m., Hassabis begins what he calls a 'second day of work', often continuing until 4 a.m. This late-night block is reserved for creative thinking and research. \"I can't imagine being creative at four in the morning. But, I come alive at about 1 a.m.,\" he told Fortune.The warning on sleep\n\nDespite his unusual routine, Hassabis stressed on the importance of rest and sleep. He warned people that getting less than six hours of sleep can become harmful for the brain and could also lead to chronic sleep deprivation. The comments made by Google DeepMind CEO stress on the importance of balance between acknowledging his own habits and warning others about the cognitive downsides of insufficient sleep.The practice of decades\n\nHassabis said that he has followed this unusual schedule for about a decade. He said that this routine has now become a part of his rhythm as he leads Google DeepMind, the AI lab he co-founded in 2010, which Google acquired in 2014. DeepMind merged with Google Brain in 2023 to form Google DeepMind, the team behind AI systems like Gemini and Nano Banana. Hassabis and colleague John Jumper were awarded the 2024 Nobel Prize in Chemistry for their groundbreaking work on protein structure prediction.Tech leaders and their sleeping patterns\n\nHassabis' unusual schedule mirrors what other tech leaders have shared about their own sleep routines. Elon Musk has said he functions best with about six hours of sleep, warning that less affects his performance. During Tesla's production crunch in 2018, Musk even slept on the factory floor. By contrast, Salesforce CEO Marc Benioff has emphasized the importance of rest, saying in a 2023 interview that he averages about eight hours of sleep.\n\nGN Awards 2025: Vote for your favorite Gadgets\n\nThe TOI Tech Desk is a dedicated team of journalists committed to delivering the latest and most relevant news from the world of technology to readers of The Times of India. TOI Tech Desk's news coverage spans a wide spectrum across gadget launches, gadget reviews, trends, in-depth analysis, exclusive reports and breaking stories that impact technology and the digital universe. Be it how-tos or the latest happenings in AI, cybersecurity, personal gadgets, platforms like WhatsApp, Instagram, Facebook and more; TOI Tech Desk brings the news with accuracy and authenticity."
  },
  {
    "source": "Financial Times News",
    "company": "Google DeepMind",
    "title": "Transcript: What an economist eats for lunch (in 2026), with Tyler Cowen",
    "date": "2026-02-06T05:35:54Z",
    "url": "https://www.ft.com/content/8c313461-7622-4ec5-a333-0c26c7bf267f",
    "content": "Soumaya Keynes\n\nWhat does an economist eat for lunch? Powdered food for efficiency? Or maybe seasonal veg for the climate. And how is economics relevant for what we eat? My guest today has thought a lot about these questions, and the answers that tell us about incentives, globalisation and how the modern economy really works.\n\n[MUSIC PLAYING]\n\nThis is The Economics Show with Soumaya Keynes. I'm joined this week by Tyler Cowen, who was speaking to me from Fairfax, Virginia, in the suburbs of Washington DC. Tyler is a professor of economics at George Mason University, economics blogger and writer extraordinaire and host of the Conversations with Tyler podcast. Tyler, hello.\n\nTyler Cowen\n\nHi. Good to be chatting with you. I've only had a yoghurt for breakfast, let me warn you.\n\nSoumaya Keynes\n\n(Laughter) OK, that's fine. You're allowed to stay on the podcast. OK, on this show, we always start with a question on a scale from one to 10. So, on a scale of one to 10, how important do you think food is to the average person?\n\nTyler Cowen\n\nWell, as economists we're trained to think at the margin. If you mean food at all, it's important at the level of a 10. Without food, you die. Even to go hungry for a day feels pretty grim. But at the margin, I don't know. People like to complain about current high food prices, but I don't think they are much less happy. And you could eat 10 per cent less or eat different things. And if your life was fine to begin with, it still would be fine.\n\nSoumaya Keynes\n\nOK. Very specific, I like it. OK, so on that same scale, how important is food for you?\n\nTyler Cowen\n\nMuch higher than 3.8. So a lot of my travel is organised around food or trying out new things. I eat food to learn about supply chains, to learn how immigration works, to think about globalisation, to better understand the histories of countries, how they relate to their neighbours, the importance of discovering the new world. I don't know, when it comes to food, can I be at a 9.246? You wanted specificity.\n\nSoumaya Keynes\n\nAnd you have given me that. Yes, I'm very satisfied with that answer.\n\nTyler Cowen\n\nAnd I've learned how to eat very well, very cheaply. That's important. I don't feel I can do that in London, but most of the places I go, I feel I can eat superb meals very often for less than $10 or £10 a meal.\n\nSoumaya Keynes\n\nOK, great. Well, I wanna get to those tips on how to replicate that shortly. But first of all, can I just ask, do you think that you approach food differently because you're an economist? Or is it that economics explains the approach to food that you would have even if you weren't an economist?\n\nTyler Cowen\n\nI don't think in terms of those boxes so clearly. So in my view, there's stuff. And you're out there in the world and you encounter stuff. And some of that stuff is food, which for me is 9.246 on the scale you constructed, and you try to make sense of it. Maybe anthropology is always coming before economics, but economics is part of the worldview. You apply to the stuff, but it's not just food. It's really any kind of stuff you encounter. Like you walk into Daunt Books in London and you look at what's on the front table and you start thinking, well, why is that there? And you can think in terms of, well what's the mark-up on that book? Is buying front table space for it more likely to make it go viral? Since Daunt branches are determined by local demand, how does that, you know, feed into what's on the front table? And you just start thinking -- and same with food. So I guess it all springs from being this obsessive who wants to anthropologically dissect stuff all the time.\n\nSoumaya Keynes\n\nOK, but what you're describing sounds more diagnostic, right? It sounds more like using the economic framework to explain what's there, rather than say, use economics to make different choices.\n\nTyler Cowen\n\nSo once you understand certain things about food, you can eat much, much better. So, for instance, the simple fact that a lot of the biggest food problems in Italy will come in Venice, Florence, and Rome follows pretty readily. Once you know even a small amount about tourism, you don't need to be a very sophisticated economist. So Venice is maybe the worst place to eat in all of Italy. I think it's about 60,000 -- you could call them native Venetians, or maybe they're not native -- but they're living and working in Venice. It's not many people. The number of tourists who visit Venice each year must be in the millions. A lot of them are people who don't know much about Italian food. They're not return customers. They're mainly in Venice for other reasons. But if you go to Veneto, you know, which is right outside of Venice, which is ugly and hot, the food is incredible. So a lot of the key lessons, I think they're quite simple. It's merely the willingness to think systematically about what you're doing that's scarce, not some deep understanding of economics.\n\nSoumaya Keynes\n\nSo you've now said this a few times, right. You've said, well the food here is good and the food there is less good. So can you define what counts as good food?\n\nTyler Cowen\n\nWell, I could define it if I brought you there. I'd sit you down and I'd say, here, try this, and you would nod your head and agree with me. But I believe that if a person has some basic degree of food openness that there's a lot of consensus as to what's good.\n\nSoumaya Keynes\n\nBut can't we be any more precise than that? I mean, freshness, unusualness of the ingredients, variety. You know, we could talk about protein, fat, fibre. I mean, there are all sorts of properties of food that you can pin down.\n\nTyler Cowen\n\nSure. But I'm reluctant to set up a scale, so take freshness. I mean, most cases, that's a good thing. But if it's one of those stinky eggs in Taiwan, or if it's fermented Korean foods, you're not looking for freshness, you're not quite looking for the opposite of freshness. But if you're just thinking in terms of freshness, you're gonna miss what's special about it. So the mere notion that having some basic openness and going around and just trying different things and talking about them, I think gets you furthest.\n\nAnd again, I think you and I, if we went to food stalls in Oaxaca or Puebla, we'd agree 87 per cent on what's best. And I don't really know anything about you and food. I just figure if you wanna do this episode, you're high in openness, you're super smart, you're curious. So we're mostly gonna agree. But if it ends up like you like sandwiches more than I do, I like very spicy more than you do, that's fine. We know some of that might be just a genetic difference like in spice tolerance.\n\nSoumaya Keynes\n\nI mean, I've spent a lot of my life eating essentially powdered food. OK, maybe not a lot of my life, but a bunch of time eating powdered food.\n\nTyler Cowen\n\nBut I don't think you and I would so much disagree as to how good the powder is. If you had a pretty good powder, I think I'd say, oh, that was a pretty good powder.\n\nSoumaya Keynes\n\nOK. So I mean, what you're saying is that basically you think that there is some kind of scale of good to bad, but we're not really able to say what the kind of parameters of that are. It's just a sort of sense. Do you have a sense of what counts as a good food culture? Like some places have a better food culture than others.\n\nTyler Cowen\n\nAbsolutely. Italy is one of my favourites. Malaysia is incredible. Mexico is one of the top for me.\n\nSoumaya Keynes\n\nHow does economics contribute to that? To where has a good food culture and where doesn't?\n\nTyler Cowen\n\nLet's take Mexico for instance. So they have ingredients such as chiles, corn, you know, corn turned into masa, possibly tortillas that go back centuries, indeed, millennia, that they have finally honed and experimented with. And they're sold in competitive markets and they have incredible ingredients to work with. And Mexican people really care about food, and typically have strong opinions about food. Not every culture has that, like rural America often does not have that.\n\nSoumaya Keynes\n\nHow do you think economic factors like inequality, or I suppose you know, female labour force participation contribute to a kind of local food culture?\n\nTyler Cowen\n\nThere can be destructive inequalities that simply ruin a whole country and that's the food. But putting that aside, when you have a fair number of customers who have enough money to buy the food, and then a lot of people who are much poorer and are quite keen to cook the food, they may have few other good opportunities in countries such as India and China, historically, that has given rise to excellent food and a lot of experimentation and very vibrant restaurant cultures, 'cause wages don't drive places out of business very readily. So that can be a positive. I'm not saying it's a positive for the whole country. It's not a positive for the people who are poor, but it can make it a very good place to eat.\n\nNow, female labour force participation, it's often better for the food when older women become cooks, and this you see a great deal in Mexico, you know, abuelas, the grandmas, and they carry these old food traditions. They know how to use ingredients. They started cooking when supply chains were super short and before refrigeration was too common. And they just know everything about the food stuff they have access to. If you're in a culture where those women have better opportunities, they grow up to be doctors, lawyers, whatever. Again, much better for the society as a whole. But if you want that $2 incredible taco at the street food stand, you're probably not gonna get it.\n\nSoumaya Keynes\n\nWhat do you think is the single most important technology when it comes to contributing and kind of improving our current food system? And you're not allowed to say heating, which is just cooking?\n\nTyler Cowen\n\nWell, to take refrigeration, that is a mixed blessing. There's an optimal amount of refrigeration. So one problem I think with Russian food, historically, is they had refrigerated boxcars quite early. There are vast distances in Russia in some parts of the country. Because of the extreme cold, it's hard to grow things that are not wheat or relatively small number of other items. So you have things being frozen put on trains and shipped across the country. And then when it arrives, it might be fine, but it's not really gonna be all that interesting or special.\n\nThe United States also -- with great distances, so much trucking, interstate highway system -- we have frozen so many things. It makes a lot of our food, again, maybe fine, but just average. I think of the United States as specialising in a particular kind of variety, and only rarely being top of the line with respect to ingredients. And that has to do with too much refrigeration and too long supply chains. But let's not damn refrigeration entirely.\n\nIt's also the reason why things can be shipped from China, India, Mexico, whatever, and you get this incredible variety that you have, of course, in your London, in my northern Virginia. And just to think through the economics of this, you figure out pretty quickly, well, where should I be eating? What should I be getting? If you're in the United States, in most cases, do not be looking for the best ingredients. Be looking for the chefs who know the best, how to construct an interesting and complex sauce. And for many things in London, I would say broadly the same principles hold.\n\nSoumaya Keynes\n\nWhen thinking about food, what do you think is the single biggest technological constraint to delivering good food?\n\nTyler Cowen\n\nThe biggest technological constraint, I think just the difficulty of scaling many of the best things. So if you look at US corn, it's shipped in great bulk all over the world. That process works. It does not arrive super fresh. It's spread to be durable and shippable and just cheap. And it's fine. I like it, but it's very far from my favourite corn in the world, which might be in the Andes. The farms there are super inefficient, very small. There's some very local variety of corn that's grown. It may have a strange colour. There's not a corn like it anywhere else in the world, and it's amazing. So, problems of scale, I think we won't really overcome. You have to go to these places and consume the non-scalable items.\n\nSoumaya Keynes\n\nA lot of your restaurant dining advice seems to be associated with leisure dining, right? So I've got an evening, I'm gonna go for a great meal. I can kind of travel anywhere. But what about if you were an office worker trying to work out where to get your office lunch? What's the economist's guide to that?\n\nTyler Cowen\n\nIs this London or somewhere else?\n\nSoumaya Keynes\n\nGive me London and how would that be different in, say, New York?\n\nTyler Cowen\n\nI think London, for a city of its wealth, has relatively good, quick takeaway food. None of which is great, but compared to other cities of a similar sort is above average. And there's such density of office space in London, you do have a lot of choices, and I find this quite encouraging about London. It's a very good place to get pretty good lunch food. So if you're out, say, by Google DeepMind where I was not too long ago, I walked out of the office, I had given a talk, and I saw a Turkish place. It's the first thing you see as you head to the main road. It was quite good, and I knew it would be quite good, 'cause there's people with some wealth, very high intelligence, nearby and they need something quick, but they want it to be good. And I ate there and I was quite confident and it delivered.\n\nSoumaya Keynes\n\nYou just said very high intelligence. Do you think that that's correlated with your ability to discern good food?\n\nTyler Cowen\n\nOf course. You know, it correlates with many things. Your ability to discern what are good books or good newspaper articles. It's correlated with intelligence. And if there's anywhere that has high intelligence, it's Google DeepMind. Now, they're not the only people who work there, but in general, it's a neighbourhood with a lot of finance, a lot of people who are highly analytical. XTX is nearby. And yeah, that makes the Turkish place better on average.\n\nSoumaya Keynes\n\nThat's not something I had thought about before. And I guess my assumption would be that, actually, your ability to enjoy or engage with food or kind of decide what's really, really good food is actually not very strongly correlated with, you know, intelligence, along the dimension that, say, Google DeepMind would value. Do we have any evidence for that? Or is that just, you know, your hunch meeting lots of people?\n\nTyler Cowen\n\nWell anecdotal evidence, but I'd say there's a sizeable research literature showing that higher smarts are correlated with skill in many, many, many things. I've never seen a paper on smarts and taste in food, but it would be shocking if that were somehow an exception.\n\nSoumaya Keynes\n\nI suppose I also think of taste in food as something that can be developed and learned, right?\n\nTyler Cowen\n\nOf course.\n\nSoumaya Keynes\n\nAnd so, I haven't invested in developing a real taste for, you know, fine food, partly because I don't have time, partly 'cause it would be ruinously expensive.\n\nTyler Cowen\n\nKeep in mind, in most of the world, the best food is quite cheap. It's not expensive.\n\nSoumaya Keynes\n\nIn most of the world, but I don't live . . .\n\nTyler Cowen\n\nTwo-thirds of the world. Now you may not be there, you may have other obligations. But, you can eat very, very well, fairly cheaply. And in the United States, which is never a super cheap country, but still often, say, Asian food at a lower price point is gonna be better than non-Asian food at a higher price point.\n\nSoumaya Keynes\n\nCan I ask about the environment? Economists think about externalities. There are lots in the food that we eat that is terrible for the environment. What are your favourite examples of things that seem really great for the environment or seem terrible for the environment, but are actually the opposite?\n\nTyler Cowen\n\nIt depends a great deal how it's made. And what seems to be good for the environment can, in the larger picture, not be great for the environment. So in poorer countries, you have all kinds of foods that are produced pretty simply with short supply chains and often do not involve meat. And a lot of people think, well, that's great for the environment. And in the short run maybe. But if it's not addressing, say, problems of malnutrition -- you have stunted children who grow up a little shorter than they should be -- it keeps the country poorer. And the country then is likely to pollute for longer, and it's not clear it's really good for the environment after all. So figuring out which foods are good or bad for the environment, it's not always entirely simple, but it does seem to me, if you want one basic rule most of us could follow, if you're in a wealthy country, eat somewhat less meat. It's a pretty safe rule that I think will help the environment, and not harm animals, plausibly help them, depending on the conditions under which they're raised.\n\nSoumaya Keynes\n\nWhat about local food vs imported food? Are there any supply chains that are actually surprisingly efficient relative to something that's made nearby?\n\nTyler Cowen\n\nWell, very often having your food shipped in is more energy-efficient and lower-cost than having it grown locally. So locavorism makes no sense. It's very often bad for the environment. If things are being shipped in, they're being shipped in because they're cheaper, all things considered for the most part. And if they're cheaper, that could mean a lower environmental burden. So don't be a locavore.\n\nSoumaya Keynes\n\nI guess the counter to that would be, though, that some externalities are not being priced, right? And so the externality associated with the pollution isn't being priced and that's why it's cheaper to import food.\n\nTyler Cowen\n\nSure, but the externalities associated with the local costs, well, the person drives a truck for seven miles and it's sledded gas that's also not being priced. So those can go either ways. But to think that it's gonna favour the local on net is unjustified, typically.\n\nSoumaya Keynes\n\nOK. But it's, we don't know, rather than it's always better to import than to buy locally.\n\nTyler Cowen\n\nSure. But if you have locavorism as a ruling principle, that's a big mistake. And I would just say, as a first-cut approach, I think that the method with a lower cost might have fewer externalities, is not a crazy starting point. It shouldn't be your final conclusion, but lower cost means you're expending fewer resources, right?\n\nSoumaya Keynes\n\nOK. Can I ask now about international differences in dining? So if you were to do a ranking, right, so we've got the US, let's say France, Germany, Italy, UK, how would you rank those countries in terms of the quality of their food, the food culture?\n\nTyler Cowen\n\nWell, the wonderful thing about Italy, Venice aside, is you can go almost anywhere and it's amazing. So in that regard, you could say Italy has the best food culture, but variety in Italy is terrible. It's very hard to find decent Chinese food or good Indian food. So, Italy is specialising in all these local, regional monocultures, which are amazing. But if I had to live there, I would love them much less. France is pretty rapidly moving into more refrigeration, less freshness, fewer traditional foods. So France is in transition. It's changing more and more rapidly than Italy is. German food, I think, is quite underrated. I certainly would not put it above either Italy or France, but I think basic German dishes and with vegetables are excellent. And especially if you're eating in southwestern Germany, they're very excellent. And for foods from around the world, if you're in Berlin or Cologne or Frankfurt, the choice is really pretty good. Not as good as London, but clearly better than most other places.\n\nThe United States was the fourth country. Oh, it so depends. I would say if you don't know what you're doing, it's one of the worst places in the world to eat. If you know what you're doing, it's wonderful. So one thing I do, for instance, is I order frozen sausages from Texas Country BBQ, and they arrive at my house and I have a big stock of them. And when I want one, I thaw. And these frozen sausages, they actually freeze and thaw pretty well and they're excellent. And I can just do that. And then in my local neighbourhood, I have amazing regional Chinese food, great Afghan food, Pakistani food. But if I were doing what most people do, it would be very mediocre. But I feel no anxiety having to live here. I think I have a wonderful food life at reasonable expense. If you live in the middle of nowhere, it's a lot tougher though.\n\nSoumaya Keynes\n\nHow do you think economics explains the international differences in the quality of food?\n\nTyler Cowen\n\nWell, if you're trying to think about Italy. Italy has had a less successful immigration patterns than, say, Germany has. It attracts a lower diversity of people and they don't do as well. Ethiopian food is quite good in Italy, but most foreign cuisines you just wouldn't eat in Italy flat out, and you shouldn't. So that's one economic reason why Italian food has stayed Italian. Italy unified quite late, as you know. So a lot of Italian food is still quite regional. Dishes from one region you simply cannot get in another region. They tend to be very excellent in the regions of their origin. And that again, limits your choice, but boosts your quality. And that too is for the economic reason that both Italian taste and Italian supply chains have stayed pretty local for the most part.\n\nSo those would be some simple economic ideas you would apply to Italy. And France, I think the story is one where just processed and refrigerated foods. They're way more convenient. French food, which again, can be amazing. It's pretty slow, and French labour law makes it harder to hire and fire cheap labour, so you're waiting forever for the waiter. And French people have done a workaround of just eating much more rapid foods 'cause their wages are higher, they're less patient. There's other better things they can do and at the margin that's made French food less special for economic, legal and regulatory reasons.\n\nSoumaya Keynes\n\nOK. I wanna ask you to engage with me in a thought experiment, right? So what we're doing is we've got a new country, it's called Thailaland, and your job as the, the leader of this country is to design a really amazing food culture where the food is incredible. What would you do? What levers do you pull? How would you make it happen?\n\nTyler Cowen\n\nWell, the food won't be good right away. So we've run versions of this experiment. So Brasília in Brazil is a pretty new city, right? And I would say it's actually relatively successful as a city, but it's quite hard to find amazing food there. They don't have a cuisine of their own. They import other, mostly Brazilian, Italian, what you would expect cuisines from elsewhere. They're not bad. There's enough purchasing power in Brasília to keep it all going. But you would never look forward to eating in Brasília or in India, Bengaluru. It's not literally a new city, but it's been quite large and prosperous only lately. There's not quite a food of its own in Bengaluru. So if you have a new city, I would just say it's tough going. You're not gonna do very well. You leave. That's what Tyler does is he leaves.\n\nSoumaya Keynes\n\nNo, I'm not gonna let you leave. It can be a slow project. That's OK. The project can take a few years.\n\nTyler Cowen\n\nInvite in smart people from around the world, give them other things to do. And just like this part of London near Google DeepMind, let smarts and wealth work their magic and you'll get a diverse set of offerings through good immigration policy. And after a long time, the food will be quite good.\n\nSoumaya Keynes\n\nOK. I think that is a good moment to cut to a break and when we get back, I'm gonna ask Tyler about what is changing in the economics of food.\n\n[MUSIC PLAYING]\n\nWe are back from the break. OK, so you wrote your book, An Economist Gets Lunch, in 2012. If you were writing it today, would you write anything differently?\n\nTyler Cowen\n\nWell, I would have a whole chapter on what is being called food price inflation, which has been a big story ever since the pandemic. When I wrote the book, it was before all that happened, so there was nothing to say about it. But why do people perceive food prices as so especially high when often they're not adjusting for inflation? Now it's interesting. In the UK food prices have gone up, I think about 7 per cent more than the general rate of inflation. In the US I think it's around only 2 per cent more. So on net there is some extra price inflation in food. Not that high in my country, much higher in yours. But I think as with gasoline, the fact that we buy it so often means it really bothers us. I think also in the US at least, I'm not sure about London, but there's been a relative price shift.\n\nDining out compared to home dining is much more expensive, so eating at home is more economical than it used to be. People get upset at this. I think it's sort socialising. Also, alcohol sales are down. That has also made, you know, in relative terms, dining out more expensive because the food is not cross-subsidised by the drinks. And those changes, you notice their import. So now is not an amazing time where I live to be out there exploring new restaurants. It makes more sense to get to know your old familiar ones very well and optimise with those and to perfect your home cooking. And those are all new developments.\n\nSoumaya Keynes\n\nThe thing about home dining that you mentioned, there was this recent study I saw that found that restaurants had become much more productive since the pandemic. Do you know the one I'm talking about?\n\nTyler Cowen\n\nYes. Austan Goolsbee who runs the Chicago Fed, that's a very interesting paper -- 30 per cent more productive, they claim. I'm not sure they're defining productivity properly, 'cause the thing that is doing the work according to them is people spending less time at the restaurant. So you can serve more people. The measured output goes up, but the unmeasured, how much did people enjoy themselves in the conversation? Possibly that's down. Now maybe people just wanna get out and go to the movies, but it could be some of that increase in productivity is an illusion.\n\nSoumaya Keynes\n\nI guess there's also potentially the issue that in a restaurant you're paying someone to clean up. And that work is now moving into the household and not being paid for as far as they can observe.\n\nTyler Cowen\n\nThat's right. But I think the general trend of people spending less time at the table, which again, will also correlate with less drinking of alcohol. That is true. And it does matter for restaurants, so there's just more turnover.\n\nSoumaya Keynes\n\nOK. Now I wanna talk about immigration briefly, which is something you've already mentioned, but just in the context of what's going on with the Trump administration and changes in migration policy. You know, and obviously, you know, how this affects the food environment is not the most important consequences of those. But in the context of this conversation, are you seeing any effects already? Or how do you think those changes will play out?\n\nTyler Cowen\n\nSome bad, dark things are going on with immigration policy in my country, and I'm quite opposed to them. I would note that the change in the aggregate numbers is much less than you might think given what perhaps you're seeing on the evening news. The biggest problem I think is with Latinos. I sometimes like to say every restaurant in the US is a Mexican restaurant. That is, the food is cooked by Mexicans in most restaurants, no matter what cuisine they're serving.\n\nSo a lot of these workers, whether they're legal or not, they're now afraid to show up for work 'cause ICE agents are targeting different places where illegal and also legal immigrants show up, and they stay at home more. Or in some cases they don't come to the US at all. Or in a much smaller number of cases, they're being sent back or otherwise penalised. So it's just made it harder to run any kind of restaurant at all. And you said, well, you know, food isn't the main issue in assessing immigration. I agree it's not the main issue, but I think it's quite an illustrative issue. The point is, we have a lot of the migration we do because there are gains from trade and you see those gains from trade in the food sector. And you see those gains from trade and other parts of the economy, and those gains from trade are going down. From my point of view, it's bad both for food and more generally.\n\nSoumaya Keynes\n\nOK. So I saw this really interesting paper on the rise of niche consumption. So there's this idea, you know, that on the one hand, taste consumption has been becoming more homogenous around the world. But actually in the US, if you look at what people are buying, you see the rise of these niches, right? So we are not all buying the same flavour of Doritos anymore. Some people are buying roast chicken-flavoured Doritos, and some people are buying sausage-flavoured Doritos, and it's kind of falling into these smaller and smaller niches. Have you seen that paper? Do you recognise this, and what's your opinion of it?\n\nTyler Cowen\n\nWe see this in food markets. So now, say, food from Laos has become a quite normal category. Most people don't eat it or maybe don't even know about it. But for those of us who do, it's like, oh, Laotian food, whatever. It's like a hamburger pizza, Chinese food. That's a good place to be. My worry for myself is just I've run out of new things to try, at least in the US. I have to travel to do so.\n\nSo it used to be, I'd see a Tibetan restaurant, I'd get all excited. I'm not excited anymore. I still like it. I might go to it, but, oh, Tibetan, yeah. You know, like here's some more pizza, right. That's not exciting, per se. So I think it's one reason some food markets have slowed down. That the people who love the niches, they're at corners where they've tried everything they can, they're waiting for the next new thing. It will be coming, but so far, I don't quite see it. For the last five years, in part due to pandemic, but not only, I haven't seen the next new thing in my country.\n\nSoumaya Keynes\n\nIt's funny. I feel like you might be quite unusual. I think a lot of people are much more habitual than you are when it comes to food. Am I wrong?\n\nTyler Cowen\n\nI'm clearly at the extreme end of the spectrum of not being so habitual, but there's a lot of people like me, and we're all sitting around pinging each other online. What's the next new thing? Where can I go? And someone will say, oh, Eritrean. And I say oh, Eritrean. Yeah, I had that last week. You know? So there's plenty of countries we don't get to eat the food. So say food from the south-west region of China from Yunnan. Very hard to find here. It's amazing. There are supply chain problems. Maybe it will come, and I'm waiting.\n\nSoumaya Keynes\n\nOK. Last question. In a decade's time, or maybe two decades' time, what do you think is gonna be the biggest change in the way that we eat?\n\nTyler Cowen\n\nWe -- that's a tricky we there. If you mean people in London, I suspect your government at some point will have a backlash against immigration, and your food choices will become somewhat worse and food will continue in relative terms to be more expensive.\n\nIn the United States, I think our immigration will become more new world-oriented, with or without President Trump or someone like him in charge. And our food will be more Latino. And GLP-1s, especially in the United States, will be a big, big deal 'cause we're the nation with one of the highest obesity rates. But it will affect you as well. And I guess I'm a little bearish on food markets and quality food because of immigration and GLP-1s.\n\nSoumaya Keynes\n\nDo you have anything more optimistic we could end on?\n\nTyler Cowen\n\nWell, it's super optimistic. In fact, on average we will live more years. So any given meal you might be eating less. But say you live for seven years longer, that's seven extra years to get to south-west China, Malaysia, rural Mexico, wherever it is you need to go. And perhaps when you are 91 years old, you can join us on one of these food tours.\n\nSoumaya Keynes\n\nI look forward to it. Tyler, thank you so much for joining me. This has been a real treat.\n\nSoumaya Keynes\n\nThat is all for this week. You have been listening to The Economics Show with Soumaya Keynes. If you enjoyed the show, then I would be eternally grateful if you could rate and review us wherever you listen. This episode was produced by Mischa Frankl-Duval and Josh Gabert-Doyon with original music and sound design from Breen Turner. The broadcast engineer was Andrew Georgiades. Our executive producer is Manuel Saragosa. Cheryl Brumley is the FT's head of audio. I'm Soumaya Keynes. Thanks for listening."
  },
  {
    "source": "WebProNews",
    "company": "Google DeepMind",
    "title": "When AI Becomes the Game Developer: How Google's Genie 2 Triggered a Market Reckoning for Video Game Giants",
    "date": "2026-01-30T21:08:59Z",
    "url": "https://www.webpronews.com/when-ai-becomes-the-game-developer-how-googles-genie-2-triggered-a-market-reckoning-for-video-game-giants/",
    "content": "The video game industry experienced a sharp tremor on January 30, 2025, as investors confronted an unsettling question: What happens when artificial intelligence can generate playable game worlds from simple text prompts? Google DeepMind's unveiling of Genie 2, an AI model capable of transforming written descriptions into interactive 3D environments, sent shockwaves through gaming stocks, with major publishers seeing billions wiped from their market capitalizations in a single trading session.\n\nAccording to Reuters, shares of leading game developers tumbled following the announcement, as market participants grappled with the implications of AI-generated content for an industry built on the labor-intensive process of manual game development. The selloff reflected deeper anxieties about whether traditional game studios can maintain their competitive moats in an era where sophisticated AI tools threaten to democratize content creation at unprecedented speed and scale.\n\nGenie 2 represents a significant leap beyond its predecessor, which Google DeepMind introduced in 2024. While the original Genie could generate simple 2D platformer-style games from images, the second iteration produces fully navigable 3D worlds complete with physics, lighting, and interactive elements -- all from text descriptions that might take mere seconds to compose. The technology uses advanced machine learning techniques to predict how environments should respond to player actions, creating coherent game experiences without traditional programming or asset creation.\n\nAt its core, Genie 2 employs what researchers call a \"world model\" -- an AI system trained on vast datasets of existing video games to understand the fundamental rules governing interactive virtual spaces. The model has learned patterns of how objects behave, how lighting changes with time of day, how characters move through environments, and countless other details that human developers typically spend months or years perfecting. When given a text prompt like \"a misty forest clearing with ancient ruins,\" the system draws upon this learned knowledge to generate a playable space that feels internally consistent.\n\nThe implications extend far beyond novelty demonstrations. Google DeepMind researchers demonstrated Genie 2 creating diverse environments ranging from alien planets to underwater cities, each with distinct visual styles and interactive mechanics. The generated worlds persist for extended gameplay sessions, maintaining consistency as players explore -- a technical achievement that addresses one of the primary limitations of earlier generative AI systems, which often produced incoherent results when users ventured beyond initial parameters.\n\nIndustry analysts note that Genie 2's capabilities arrive at a particularly sensitive moment for traditional game publishers. Development costs for AAA titles have soared to $200 million or more, with production timelines stretching five years or longer for flagship releases. Meanwhile, player expectations for content volume and variety have intensified, creating a cost-quality squeeze that has already forced consolidation and layoffs across the sector. An AI tool that could dramatically reduce development time and expense threatens to upend the economic foundations of the industry.\n\nThe stock market's response reflected immediate concern about competitive positioning. Major publishers saw significant declines, with investors questioning whether companies commanding premium valuations based on their development expertise and intellectual property portfolios might face margin compression or market share erosion. The selloff was particularly pronounced among mid-tier developers without the diversified revenue streams of industry giants, suggesting investors view smaller studios as especially vulnerable to AI-driven disruption.\n\nHowever, not all market observers share the pessimistic outlook. Some analysts argue that Genie 2, while impressive, remains far from replacing human creativity and game design expertise. The generated worlds, they note, lack the narrative depth, carefully balanced progression systems, and artistic vision that define commercially successful games. Creating a playable environment differs fundamentally from crafting a compelling player experience -- the latter requiring human judgment about pacing, difficulty curves, emotional resonance, and countless other factors that resist algorithmic optimization.\n\nThe technology also faces practical limitations that may constrain its near-term impact. Generated content currently lacks the polish and optimization that players expect from commercial releases. Frame rates can be inconsistent, textures sometimes appear muddy or repetitive, and the AI occasionally produces physically impossible geometries or nonsensical object placements. Google DeepMind has not announced plans to commercialize Genie 2, leaving unclear when or whether developers might access the technology for production use.\n\nThe anxiety surrounding Genie 2 echoes previous technological disruptions that ultimately reshaped rather than destroyed the gaming industry. When sophisticated game engines like Unity and Unreal became widely available in the 2000s, observers predicted they would eliminate the competitive advantages of established studios by democratizing development tools. Instead, the industry expanded dramatically, with new studios emerging while incumbents adapted by focusing on areas where technology alone provided no advantage -- original IP, player communities, live service operations, and marketing expertise.\n\nSimilarly, the rise of user-generated content platforms like Roblox and Fortnite Creative initially sparked concerns about cannibalization of traditional game sales. Yet these platforms ultimately grew the overall market, attracting different player segments and creating new revenue opportunities through platform fees and content monetization. The pattern suggests that AI-generated content might expand gaming's addressable market rather than simply redistributing existing revenue among fewer participants.\n\nNevertheless, the current AI wave differs in important respects from previous technological shifts. Earlier tools required significant skill to master, creating natural barriers that preserved some competitive differentiation. Genie 2's text-based interface, by contrast, requires no specialized knowledge -- a characteristic that could genuinely democratize content creation in ways previous technologies did not. If the barrier to creating playable game content drops to the level of writing a sentence, the industry's fundamental economics might indeed face unprecedented disruption.\n\nForward-looking publishers are already exploring how to incorporate rather than resist AI-assisted development. Some studios have begun experimenting with AI tools for generating background assets, procedural content, and rapid prototyping -- using the technology to accelerate early-stage development while preserving human control over core creative decisions. This hybrid approach could allow traditional developers to maintain quality standards while capturing efficiency gains, potentially improving rather than threatening their competitive positions.\n\nThe technology might also enable entirely new business models. Imagine games that generate personalized content based on individual player preferences, creating unique experiences that traditional development pipelines could never economically support. Or consider educational applications where teachers could instantly create custom learning environments, or therapeutic uses where clinicians could generate tailored exposure therapy scenarios. These applications might create new revenue streams that offset any cannibalization of traditional game sales.\n\nLegal and ethical questions surrounding AI-generated content add further complexity to the outlook. Genie 2 was trained on existing games, raising unresolved questions about intellectual property rights, attribution, and compensation for the human creators whose work informed the AI's training. Regulatory frameworks for AI-generated content remain undeveloped, creating uncertainty about ownership, liability, and monetization rights that could significantly impact commercial viability.\n\nPerhaps the most important consideration is what players actually value in gaming experiences. While Genie 2 can generate playable spaces, it cannot yet create the narrative arcs, character development, emotional beats, and carefully orchestrated moments that define memorable games. Titles like The Last of Us, Red Dead Redemption, or Baldur's Gate 3 succeed not because of technical virtuosity alone but because human designers made thousands of deliberate choices about story, pacing, and player agency. These creative decisions require empathy, cultural understanding, and artistic judgment that current AI systems do not possess.\n\nThe technology might ultimately prove most valuable not as a replacement for human developers but as a tool that amplifies their capabilities. Just as digital audio workstations didn't eliminate musicians but enabled new forms of musical expression, AI content generation might free developers from tedious technical tasks to focus on higher-level creative decisions. The studios that thrive will likely be those that most effectively integrate AI tools while preserving the human creativity that players value.\n\nThe market selloff triggered by Genie 2's announcement may prove either prescient or premature, depending on how quickly the technology matures and how effectively traditional publishers adapt. What seems certain is that the gaming industry faces a period of significant transformation, with AI-generated content likely playing an increasingly important role in how games are conceived, developed, and experienced. The companies that navigate this transition successfully will be those that recognize AI as a powerful tool rather than an existential threat -- leveraging its capabilities while doubling down on the irreplaceable human elements that make games culturally resonant and commercially successful.\n\nFor investors, the challenge lies in distinguishing between short-term market volatility and fundamental shifts in competitive dynamics. The gaming industry has repeatedly demonstrated resilience in the face of technological change, adapting business models and creative approaches to incorporate rather than resist innovation. Whether Genie 2 represents a genuine inflection point or simply the latest in a long series of overhyped disruptions will become clearer as the technology moves from research demonstrations to practical applications -- a transition that may take considerably longer than the market's immediate reaction suggests."
  },
  {
    "source": "Asianet News Network Pvt Ltd",
    "company": "Google DeepMind",
    "title": "Apple's AI Researchers Are Reportedly Leaving For Google, Meta",
    "date": "2026-01-30T20:57:29Z",
    "url": "https://newsable.asianetnews.com/markets/apple-s-ai-researchers-are-reportedly-leaving-for-google-meta-articleshow-ugqia0o",
    "content": "According to a report from Bloomberg, at least four of Apple's AI researchers and one top Siri executive have quit the company in recent weeks.The latest executives to depart Apple include Yinfei Yang, Haoxuan You, Bailin Wang, and Zirui Wang, according to the report. Yang has reportedly moved on to start a new company, while You and Bailin Wang have found new positions at Meta.As per the report, another Apple executive, Stuart Bowers, among the most senior executives on the Siri team, had earlier left the company to join Google DeepMind.\n\nApple Inc. (AAPL) is reportedly losing artificial intelligence (AI) researchers and Siri executives to competitors, including Meta Platforms Inc. (META) and Alphabet Inc.'s Google (GOOG).\n\nAdd Asianet Newsable as a Preferred Source\n\nAccording to a report from Bloomberg, which cited people familiar with the matter, at least four of the iPhone maker's AI researchers and one top Siri executive have quit the company in recent weeks and moved to Meta and Google DeepMind.\n\nThe departures come as Apple works on improving its AI strategy to compete with other big tech companies.\n\nExecutive Reshuffle\n\nThe latest executives to depart Apple include Yinfei Yang, Haoxuan You, Bailin Wang, and Zirui Wang, according to the report.\n\nYang has reportedly moved on from the tech giant to start a new company. Meanwhile, You and Bailin Wang have found new positions at Meta, according to the people cited in the report.\n\nYou will join Meta's Superintelligence research arm while Bailin Wang will join Meta recommendations. Wang has found a position at Google DeepMind.\n\nAs per the report, another Apple executive Stuart Bowers, among the most senior executives on the Siri team, had earlier left the company to join Google DeepMind.\n\nApple's AI Push\n\nEarlier this month, Apple and Google announced a multi-year collaboration under which Apple Foundation Models will leverage Google's Gemini models and cloud technology to power Apple Intelligence features in the future, including developing Siri for more personalized features, expected to launch this year.\n\nOn Thursday, ahead of its strong first-quarter (Q1) earnings results, Apple reportedly acquired Israeli artificial intelligence (AI) audio startup Q.ai for an undisclosed amount.\n\nWhile Apple reportedly did not comment on how it plans to use Q.ai's technology, the company said that the startup had previously worked on new machine learning applications to help devices comprehend whispered speech and also improve audio in difficult environments.\n\nEarlier reports also indicated that Apple is planning to turn Siri into an AI chatbot, code-named Campos. The release could come later this year, as per reports.\n\nHow Did Stocktwits Users React?\n\nOn Stocktwits, retail sentiment around AAPL shares jumped to 'extremely bullish' territory from 'bullish' over the past day.\n\nMeanwhile, message volumes increased to 'extremely high' from 'high' levels.\n\nOne bullish user said that Apple is an AI enabler, adding that its processors allow faster AI responses. The user also highlighted Apple's partnership with Google, as well as the Q.ai acquisition.\n\nShares of AAPL have gained nearly 9% in the past year.\n\nFor updates and corrections, email newsroom[at]stocktwits[dot]com.<\n\nAlso Read: Unity Shares Tumble After Google Unveils AI World-Building Tool - CEO Plays Down The Threat\n\nRead Full Article"
  },
  {
    "source": "IT News zu den Themen Künstliche Intelligenz, Roboter und Maschinelles Lernen - IT BOLTWISE® x Artificial Intelligence",
    "company": "Google DeepMind",
    "title": "David Silver verlÃ¤sst Google DeepMind fÃ¼r neues KI-Startup",
    "date": "2026-01-30T18:15:31Z",
    "url": "https://www.it-boltwise.de/david-silver-verlaesst-google-deepmind-fuer-neues-ki-startup.html",
    "content": "LONDON (IT BOLTWISE) - David Silver, ein führender Forscher bei Google DeepMind, hat das Unternehmen verlassen, um sein eigenes KI-Startup Ineffable Intelligence zu gründen. Silver, bekannt für seine Arbeit an AlphaGo und anderen bahnbrechenden KI-Projekten, plant, sich auf die Entwicklung von Superintelligenz zu konzentrieren.\n\nDavid Silver, ein prominenter Forscher bei Google DeepMind, hat das Unternehmen verlassen, um ein eigenes KI-Startup namens Ineffable Intelligence zu gründen. Diese Entscheidung markiert einen bedeutenden Schritt in seiner Karriere, da er sich nun auf die Entwicklung von Superintelligenz konzentrieren möchte, einer Form von KI, die intelligenter als der Mensch sein könnte. Silver war maßgeblich an der Entwicklung von AlphaGo beteiligt, einem KI-Programm, das die besten menschlichen Spieler im Spiel Go besiegte.\n\nSilver, der auch Professor am University College London ist, hat seine neue Firma in London gegründet und sucht aktiv nach KI-Forschern sowie nach Risikokapital, um seine Vision zu verwirklichen. Ineffable Intelligence wurde im November 2025 gegründet, und Silver wurde im Januar 2026 als Direktor des Unternehmens eingetragen. Seine persönliche Webseite weist nun auf seine neue Rolle hin, obwohl sie weiterhin seine frühere Position bei Google DeepMind erwähnt.\n\nWährend seiner Zeit bei Google DeepMind war Silver an mehreren bedeutenden Projekten beteiligt, darunter AlphaStar, AlphaZero und MuZero. Diese Projekte demonstrierten die Fähigkeit von KI, komplexe Spiele auf einem Niveau zu spielen, das weit über dem menschlichen liegt. Besonders bemerkenswert war AlphaGo's Sieg über den Go-Weltmeister Lee Sedol im Jahr 2016, ein Ereignis, das die Möglichkeiten der KI eindrucksvoll unter Beweis stellte.\n\nSilvers Entscheidung, ein eigenes Unternehmen zu gründen, spiegelt einen Trend wider, bei dem führende KI-Forscher etablierte Labore verlassen, um sich auf die Entwicklung von Superintelligenz zu konzentrieren. Andere Forscher, wie Ilya Sutskever von OpenAI, haben ähnliche Schritte unternommen. Diese Entwicklungen könnten die KI-Landschaft erheblich verändern, da neue Ansätze und Technologien entstehen, die über die derzeitigen Möglichkeiten hinausgehen.\n\nSilver ist ein Verfechter des Reinforcement Learning, einer Methode, bei der KI-Modelle durch Erfahrung lernen, anstatt sich auf historische Daten zu stützen. Er argumentiert, dass diese Methode entscheidend für die Entwicklung von Superintelligenz ist, da sie es der KI ermöglicht, über das hinauszugehen, was Menschen wissen, und neue Erkenntnisse zu gewinnen. Diese Philosophie wird wahrscheinlich die Arbeit von Ineffable Intelligence prägen, da das Unternehmen bestrebt ist, eine KI zu entwickeln, die die Grundlagen allen Wissens selbst entdeckt."
  },
  {
    "source": "DCD",
    "company": "Google DeepMind",
    "title": "Google DeepMind seeks team lead for growing AI chip design effort",
    "date": "2026-01-30T14:41:08Z",
    "url": "https://www.datacenterdynamics.com/en/news/google-deepmind-seeks-team-lead-for-growing-ai-chip-design-effort/",
    "content": "Google DeepMind is hiring a team lead to head AI chip design research engineering efforts.\n\nThe company claims that its AlphaChip system has already been used in the development of the past five generations of Tensor Processing Unit (TPU) as well as Axion Arm-based CPUs.\n\nThe job listing also notes that Google \"Gemini has been adopted by the hardware team in multiple workstreams accelerating the chip design process.\"\n\nGoogle DeepMind's offices in London - Google DeepMind\n\nResearch initially began at Google Brain (which was merged into DeepMind in 2023), with the project co-led by Anna Goldie.\n\nGoldie was the lead co-author (alongside Azalia Mirhoseini) of a 2020 AlphaChip research paper, which said \"dramatically shortening the chip design cycle would allow hardware to better adapt to the rapidly advancing field of AI. We believe that it is AI itself that will provide the means to shorten the chip design cycle, creating a symbiotic relationship between hardware and AI with each fueling advances in the other.\"\n\nThe group claimed that AlphaChip was able to save thousands of hours of human effort for each new generation of Google TPU AI accelerator, with the work open-sourced in 2022.\n\nHowever, in 2023, two papers cast doubt on the success of the effort - one from Chang et al., and one from Igor Markov. Chang's paper said that they were unable to reproduce Google's methods, while Markov, a scientist at competitor Synopsys, published a meta-analysis that called the approach a 'false dawn.'\n\nNature investigated the claims, with Google in 2024 noting that it \"found entirely in our favor.\" AlphaChip has since expanded, with MediaTek announcing it will use the AI as part of its chip development.\n\nHowever, the team has also faced high-profile departures. Both Goldie and Mirhoseini left in September 2025 to found Ricursive Intelligence, an AI chip development company. Other AlphaChip staffers, including Ebrahim Songhori, Hao Chen, and Jiwoo Pak, came with them. This week, the firm raised $300 million at a $4 billion valuation.\n\nAnother key departure was that of Richard Ho. One of the original paper's authors, he left in 2022 for photonic chip company Lightmatter. He later joined OpenAI as head of hardware, where he works with Broadcom on the company's custom AI chips.\n\nAnother author, Young-Joon Lee, left for Apple in 2024 - following AlphaChip colleague Emre Tuncer, who left for Apple in 2022. Will Hang left in 2019 to found a stealth startup.\n\nOthers remain, and the company has regularly hired since, including bringing over neuromorphic computing researcher Maxence Ernoult from Rain AI last year. Google DeepMind has multiple job listings for hardware engineers, part of a team conducting research and engineering towards automatically designing hardware/chips using AI.\n\nThe team lead role, meanwhile, adds that the group will be expected to \"develop ML breakthroughs that will have a big impact for Google and for the whole chip design industry,\" as well as \"use LLMs and transformer models to accelerate chip design,\" and \"solve some of the most complex tasks in Chip Design (RTL generation, RTL verification, Logic Synthesis, Physical Design, PPA prediction).\"\n\nMore in The Compute, Storage & Networking Channel"
  },
  {
    "source": "VP Land",
    "company": "Google DeepMind",
    "title": "Google Premieres AI-Animated Short at Sundance, Demonstrates New Workflow for Creative Control",
    "date": "2026-01-29T16:58:44Z",
    "url": "https://www.vp-land.com/p/google-premieres-ai-animated-short-at-sundance-demonstrates-new-workflow-for-creative-control",
    "content": "Google DeepMind premiered Dear Upstairs Neighbors at the 2026 Sundance Film Festival, a 6-minute animated short directed by Pixar story artist Connie He. The film follows Ada, a sleep-deprived woman whose noisy neighbors trigger increasingly unhinged hallucinations, rendered through fine-tuned versions of Veo and Imagen. that maintain artistic control while scaling hand-crafted animation.\n\nA 45-person crew developed entirely new AI capabilities for this project. The result challenges assumptions about AI's role in creative work by positioning generative tools as a stylization layer rather than a replacement for human artistry.\n\nThe expressionistic visual styles director Connie He envisioned were central to the storytelling but extremely difficult to achieve in traditional animation. Her storyboards called for a series of hallucinations that shift through multiple painterly styles as Ada's night progresses, from muted bedroom tones to neon expressionism.\n\nThe team quickly discovered their vision was too specific for existing tools. Production designer Yingzong Xin (character designer on Turning Red, Soul; director of Nini) created concept art with extruded proportions and angular shape language that required the AI to learn deep artistic concepts, not just surface-level style transfer.\n\nGoogle's researchers built tools allowing artists to fine-tune custom Veo and Imagen models on their artwork, teaching the models new visual concepts from just a few example images. What the AI learned surprised the team: not just superficial details like color and texture, but principles like two-point perspective and how to maintain character silhouettes that follow 2D animation rules even as forms rotate in 3D space.\n\nText prompting alone couldn't control the rhythm of Ada's sleepy fingers typing, the comedic timing of her facial expressions, or the exact framing of a camera reveal. Using text-to-video with the fine-tuned Veo model produced scenes that looked like Ada, but their movement was random, uncontrolled, and often bizarre.\n\nThe solution was video-to-video workflows inspired by how animators naturally communicate: by drawing or acting out scenes. Animators created rough animation in their preferred tools, which AI then transformed into fully stylized video with an adjustable balance between firm control and improvisation.\n\nThis approach kept motion and timing in human hands while offloading the labor-intensive stylization process to AI.\n\nDifferent animators used different tools, all feeding into the same AI transformation process:\n\nThe workflow allowed switching freely between Veo and traditional tools like Premiere during iteration, treating AI as one tool among many rather than an all-or-nothing approach.\n\nNo final shots were \"one-click\" generations. Like any film production, the team critiqued each shot in dailies reviews, going through several rounds of feedback.\n\nTo iterate without regenerating from scratch every time, researchers built tools for localized refinement. When Ada's hair silhouette didn't work in one scene, researcher Erika Lu added a rough mask indicating where more hair was needed, and Veo improvised an extra tuft that fit seamlessly into the rest of the shot.\n\nThis mirrors the note-based revision process familiar to anyone who's worked in animation production, just with AI handling the execution of adjustments to specific regions rather than requiring manual rework.\n\nVeo upscaled final shots to 4K resolution while preserving stylistic nuances. This upscaling tool is currently available in Flow and will be included in Google AI Studio and Vertex AI.\n\nThe production brought together animation industry veterans and Google DeepMind researchers:\n\nThe animation team included Ben Knight, Mattias Breitholtz, and Steven Chao. Google DeepMind researchers Andy Coenen, Forrester Cole, Ellen Jiang, and Erika Lu developed the technical workflows.\n\nAlongside the film premiere, Google.org announced $2 million in funding for an AI Literacy Initiative to train over 100,000 artists through Sundance Collab. The initiative establishes an AI Literacy Alliance in collaboration with The Gotham and Film Independent.\n\nThe funding will support:\n\nThe timing is strategic. According to Google, only 25% of media companies are currently investing in AI training, creating a skills gap as tools advance faster than workforce capabilities. By funding education through established film institutions, Google positions itself as enabling creators rather than replacing them.\n\nThe \"Dear Upstairs Neighbors\" workflow offers a template that sidesteps many current AI filmmaking concerns:\n\nThis contrasts with text-to-video approaches where prompts drive generation from scratch. We've previously covered how Flow consolidates Google's AI filmmaking tools, and how Veo 3.1 added more control features for production workflows. \"Dear Upstairs Neighbors\" demonstrates these capabilities applied to a complete short film with a professional animation team.\n\nThe model also builds on Google's collaboration-first approach. Their partnership with Darren Aronofsky's Primordial Soup on Ancestra forced their generative models to solve real-world production hurdles, developing advanced capabilities like personalized video for character consistency and motion matching to replicate complex 3D camera paths.\n\nThe \"Dear Upstairs Neighbors\" workflow won't work for every project. It requires substantial animation expertise, access to fine-tuning capabilities not yet publicly available, and a team large enough to handle both traditional animation and AI iteration.\n\nBut it establishes a proof of concept for AI as amplifier rather than replacement. Hand-painted frames of this complexity would require thousands of artist-hours; the AI stylization pipeline scaled the visual ambition without sacrificing directorial control.\n\nFor filmmakers watching AI tools evolve, the practical question is whether similar workflows will become accessible. Google's Veo 4K upscaling is already available in Flow, with broader availability coming to Google AI Studio and Vertex AI. The fine-tuning tools demonstrated in \"Dear Upstairs Neighbors\" represent the next frontier, currently limited to research partnerships but likely to expand as the technology matures.\n\nThe film will be released publicly following its Sundance premiere. Watch the trailer and behind-the-scenes on Google DeepMind's YouTube channel."
  },
  {
    "source": "dpa International",
    "company": "Google DeepMind",
    "title": "Could a new AI model transform genetic testing and drug discovery?",
    "date": "2026-01-29T16:08:35Z",
    "url": "https://www.dpa-international.com/trends-and-features/urn%3Anewsml%3Adpa.com%3A20090101%3A260129-99-325871/",
    "content": "A Google DeepMind invention that uses artificial intelligence (AI) to predict how DNA mutations behave could have a \"transformative impact\" on medicines discovery, according to its developers.\n\nAlphaGenome could also help experts pinpoint the genes associated with particular conditions or identify the cause of rare diseases.\n\nThousands of researchers around the world have used AlphaGenome to assist studies on \"neurodegenerative diseases, infectious diseases and cancer\" since its launch last June, experts said.\n\nThe model predicts how variants or mutations in DNA impact a range of biological processes that regulate genes.\n\nIt could help researchers pinpoint the cause of diseases more precisely, as well as improving genetic testing and driving the development of new treatments.\n\nIt could also help scientists accelerate their understanding of the human genome, which is the entire set of DNA instructions found in a cell.\n\nAlphaGenome was trained using human and mouse genomes.\n\nWriting in the journal Nature, researchers said the program can simultaneously predict 5,930 human or 1,128 mouse genetic signals.\n\nThese results matched or improved on the performance of existing state-of-the-art models in 25 out of 26 evaluations.\n\nNatasha Latysheva, a research engineer at GoogleDeepmind, said there are numerous applications where AlphaGenome \"could have a transformative impact\", including drug discovery.\n\n\"The idea here is by combining large genetic association studies, such as those from UK Biobank, with AlphaGenome predictions, scientists could better pinpoint the genes and the cell types associated with the particular trait or disease.\n\n\"This could add another piece of the puzzle for the discovery of drug targets and ultimately, the development of new drugs.\n\n\"In cancer, patients can harbour many different mutations simultaneously, and it's often challenging to differentiate between the large numbers of passenger non-causal mutations from the causal driver mutations.\n\n\"A model like AlphaGenome could help prioritise down lists of variants to those most likely to actually be functional and causal and contributing to the illness.\"\n\nElsewhere, AlphaGenome could help scientists pinpoint the potential causes of rare diseases, Latysheva said, and also has \"interesting applications\" in gene therapy.\n\n\"The idea here is that if you have a powerful DNA sequence to function model, you can actually start to use that model to design entirely new DNA sequences with specific desired properties.\n\n\"For example, you could try to design a sequence that activates certain gene only in nerve cells, but not in muscle cells.\"\n\nAlphaGenome has been accessible to researchers since June 2025 through an application programming interface (API), which opens a software system to interactions from the outside.\n\nAccording to Pushmeet Kohli, vice president of science and strategic initiatives at Google DeepMind, some 3,000 scientists have since used it, making one million API calls from 160 countries.\n\nThe team is now releasing the AlphaGenome model and rates for non-commercial research, with a commercial version of the program also in early testing.\n\nKohli said that researchers from many major academic labs, including UCL, are using the model \"to advance research into areas including neurodegenerative diseases, infectious diseases and cancer\".\n\nIn 2024, Demis Hassabis, co-founder and chief executive of Google DeepMind and Isomorphic Labs, and Google DeepMind director Dr John Jumper were co-awarded the 2024 Nobel Prize in Chemistry for their work developing AlphaFold, an AI system that predicts the 3D structure of proteins from their amino acid sequences.\n\nHowever, Kohli said \"proteins are only one chapter of the biological story\".\n\n\"If proteins are the ingredients of life, then DNA is the recipe,\" he added. \"While the Human Genome Project gave us the Book of Life, reading it remained a challenge.\n\n\"We have the text, but we are still deciphering the semantics. Understanding the grammar of this genome, what is encoded in our DNA and how it governs life, is the next critical frontier for research.\"\n\nProfessor Ben Lehner, head of generative and synthetic genomics at the Wellcome Sanger Institute in Cambridge, said: \"AlphaGenome is a great example of how AI is accelerating biological discovery and the development of therapeutics.\n\n\"Identifying the precise differences in our genomes that make us more or less likely to develop thousands of diseases is a key step towards developing better therapeutics.\n\n\"AlphaGenome and models like it that help decipher the regulatory code of our genome will make it much easier to do this.\"\n\nThe Wellcome Sanger Institute has tested AlphaGenome using 500,000 new experiments, according to Prof Lehner, who said it performs \"very well\".\n\nHowever, he added that there is still work to do.\n\n\"AI models are only as good as the data used to train them,\" he said. \"Most existing data in biology is not very suitable for AI - the datasets are too small and not well standardised.\n\n\"The most important challenge right now is how to generate the data to train the next generation of even more powerful AI models. We need to do this fast, cost effectively and in a way that both the data and the resulting models are available for everyone to use.\"\n\nDr Robert Goldstone, head of genomics at the Francis Crick Institute, said AlphaGenome \"represents a major milestone in the field of genomic AI\".\n\nHe added: \"This level of resolution, particularly for non-coding DNA, is a breakthrough that moves the technology from theoretical interest to practical utility, allowing scientists to programmatically study and simulate the genetic roots of complex disease.\n\n\"AlphaGenome is not a magic bullet for all biological questions, but it is a foundational, high-quality tool that turns the static code of the genome into a decipherable language for discovery.\""
  },
  {
    "source": "Chemistry World",
    "company": "Google DeepMind",
    "title": "Google's AlphaGenome wants to do for DNA what AlphaFold did for ...",
    "date": "2026-01-28T16:18:17Z",
    "url": "https://www.chemistryworld.com/news/googles-alphagenome-wants-to-do-for-dna-what-alphafold-did-for-proteins/4022824.article",
    "content": "Google's new deep learning model can predict the effect of small changes to DNA sequences up to one million base pairs in length and is particularly good with non-coding DNA, which has proven especially difficult to understand. The artificial intelligence (AI) tool - called AlphaGenome - offers researchers a way to better understand the human genome and may help scientists develop treatments for disease.\n\nAlphaGenome is 'a foundational, high-quality tool that turns the static code of the genome into a decipherable language'\n\nRobert Goldstone, Francis Crick Institute\n\nSmall variations in the human genome can have a big impact on a person's health, causing genetic disorders like cystic fibrosis or certain cancers. Most changes occur in the genome's non-coding regions that make up 98% of the total DNA. These regions influence the expression of genes, rather than coding for proteins, and alterations can often have a range of biological effects, making it hard to predict their impact.\n\nAlphaGenome, developed by Google DeepMind, can predict the molecular impact of single base pair variations across whole DNA sequences up to a million base pairs in length. This builds on Google's earlier model, AlphaMissense, which was only able to understand the effects of variations in the coding region of DNA sequences.\n\nThe new model - trained on human and mouse genome data - takes a DNA sequence as an input and gives predictions on various genetic signals that relate to specific biological functions. This includes gene expression, DNA's accessibility to proteins and where gene splicing occurs.\n\n'The key [benefit] is that you can introduce a mutation to the sequence, changing for example a C [base pair] to a T, and then use the model to compare these differences,' says Google DeepMind researcher Žiga Avsec.\n\nWhat do we mean when we say AI?\n\nAlphaGenome matched or outperformed other state-of-the-art models in 25 out of 26 tasks predicting the effects of genetic variations. The team were also able to simulate known DNA mutations responsible for a type of leukaemia, predicting the same results as those observed in the lab.\n\n'Previously, the field required separate models for separate tasks,' says Avsec, adding that earlier models also often had a trade-off between sequence length and resolution. 'AlphaGenome unifies these under one roof.'\n\nNatasha Latysheva, a senior research engineer at DeepMind, explains that AlphaGenome may help improve fundamental knowledge about the genome, improve understanding of rare diseases and cancers or help scientists design new DNA sequences to treat specific conditions.\n\nAlphaGenome adds to the collection of other AI tools developed by Google DeepMind, which includes the 2024 Nobel prize winning AlphaFold that predicts the 3D shape of proteins. Pushmeet Kohli, who led the work, explains that 'the genome is the recipe and understanding the effect of changing any part of the recipe is what AlphaGenome looks at'.\n\nAlphaGenome turns genetic code into 'decipherable language of discovery'\n\nRobert Goldstone, head of genomics at the Francis Crick Institute in the UK, believes that AlphaGenome is 'a foundational, high-quality tool that turns the static code of the genome into a decipherable language for discovery', but warns that it 'is not a magic bullet for all biological questions'.\n\nDespite the improvements, AlphaGenome still has a number of limitations. Like other models, it struggles to predict the influence of genetic alterations that are more than 100,000 base pairs apart and can only make predictions about DNA sequences from the cell types used to train the model - namely human and mouse.\n\nAnother issue is interpreting results from the model, explains Jian Zhou, a genomics machine learning researcher at the University of Chicago in the US. 'Even when the model makes accurate predictions, it does not always directly inform us of the underlying biological processes,' he adds.\n\nGoogle DeepMind released a preview of the model for non-commercial research in June last year. Since then, Kohli explains that nearly 3000 scientists in 160 different countries have used AlphaGenome, submitting around 1 million requests each day.\n\nHe hopes that 'AlphaGenome will continue to be a valuable resource for the scientific community and help scientists better understand genome function and disease biology, and ultimately drive new biological discoveries and ... new treatments'."
  },
  {
    "source": "Der Tagesspiegel",
    "company": "Google DeepMind",
    "title": "KI löst Erbguträtsel: AlphaGenome sagt Funktion von Genen voraus",
    "date": "2026-01-28T16:15:34Z",
    "url": "https://www.tagesspiegel.de/wissen/ki-lost-erbgutratsel-alphagenome-sagt-funktion-von-genen-voraus-15186370.html",
    "content": "Das Erbgut ist entziffert, aber welche Aufgaben haben einzelne DNA-Abschnitte? Bislang brauchten Forschende Jahre, um das herauszubekommen. Jetzt nimmt eine Maschine ihnen die Arbeit ab - teilweise.\n\nIm Jahr 2000 präsentierten zwei Genforscher, Craig Venter und Francis Collins, und der damalige US-Präsident Bill Clinton stolz die erstmalige Entzifferung des menschlichen Erbguts. Es hatte Milliarden US-Dollar gekostet, Tausende Wissenschaftler hatten dem Projekt Jahre und Jahrzehnte ihres Lebens gewidmet. All das in der Hoffnung, mit dem Bauplan für den menschlichen Körper in Händen endlich auch verstehen zu können, welche Fehler im Plan Erkrankungen auslösen oder dazu beitragen.\n\nDoch die Sache hatte einen Haken.\n\nDas Wissen über die Abfolge der DNA-Bausteine des Erbguts allein reichte nicht. So wie man den Sinn eines Buches in einer fremden Sprache nicht erfassen kann, obwohl man alle Buchstaben kennt, so verrät die pure Sequenz der Erbgutbausteine noch nichts über die Funktion der darin versteckten Gene: welche Rolle sie im Stoffwechsel spielen, ob sie die Entwicklung von Nervenzellen steuern oder lediglich Haare wachsen lassen. Bis heute kennen Genforscher nur von wenigen Prozent des Erbguts dessen Aufgaben, denn dafür sind aufwendige, oft Jahre beanspruchende Experimente vieler Wissenschaftler nötig.\n\nDoch jetzt könnte eine Maschine diesen Job übernehmen und erheblich beschleunigen: Die künstliche Intelligenz AlphaGenome des Unternehmens Google DeepMind könne große DNA-Abschnitte vollständig entschlüsseln, schreibt das Fachjournal \"Nature\". Damit sollen die Funktionen ganzer Genregionen analysier- und vorhersagbar sein.\n\nEin neues Standardwerkzeug für Genforscher\n\nVollkommen neu ist das nicht. Schon seit Jahren experimentieren Genforscher mit diversen KI-Programmen, mit denen einzelne Gene oder die DNA-Abschnitte vor und hinter Genen auf ihre Funktion hin untersucht werden können. \"Für die meisten Features von AlphaGenome gibt es etwas bessere Tools\", sagt Klaus Meyer vom Helmholtz Zentrum München für Gesundheit und Umwelt.\n\nAlphaGenome vereint jedoch viele Fähigkeiten verschiedener KIs und könne, so das Autorenteam von DeepMind, viel größere DNA-Abschnitte von bis zu einer Million Erbgut-Bausteinen gleichzeitig auf elf verschiedene Eigenschaften hin untersuchen, etwa welche Erbgutabschnitte in Proteine übersetzt werden oder welche als Ein- und Ausschalter für Gene fungieren. Auch zum Erkennen der Auswirkungen von Punktmutationen, also Veränderungen einzelner DNA-Bausteine, die häufig im Zusammenhang mit Krankheiten stehen, sei AlphaGenome \"besonders nützlich\", sagt Simon Maria Zumkeller vom Forschungszentrum Jülich. \"Hierbei übertrifft es andere Genomikmodelle.\"\n\nTatsächlich habe AlphaGenome ein \"breiteres Analysespektrum als bereits existierende, spezialisiertere Analysetools\", sagt Klaus Meyer. \"In den meisten Kategorien liefert AlphaGenome bessere oder sogar deutlich bessere Ergebnisse als die existierenden Werkzeuge.\"\n\nPrinzipiell seien die Ergebnisse der KI \"robust\", meint auch Martin Kircher, Bioinformatiker an der Uniklinik Lübeck und der Berliner Charité. Insgesamt bleibe die Treffsicherheit der Vorhersagen aber noch weit hinter dem zurück, was nötig wäre, um etwa die Bedeutung einer bestimmten Genvariante für Diagnose und Behandlung der Erkrankung eines Patienten korrekt einschätzen zu können.\n\nEine Besonderheit gegenüber anderen Genomanalyse-KIs ist allerdings, dass AlphaGenome die Funktion von Genen in unterschiedlichsten Zelltypen vorhersagen kann. Die meisten bisherigen KI-Modelle können solche Aussagen nur für einen oder wenige Gewebetypen treffen. Das liegt daran, dass das gleiche Gen in einer Nervenzelle stillgelegt sein kann, in einer Bindegewebszelle jedoch aktiv und in wichtige Lebensprozesse eingebunden ist, während es in der Leber nur zeitweise eingeschaltet wird. Offenbar kann das AlphaGenome-KI-Modell diese zelltypischen Unterschiede berücksichtigen, da es mit Erbgutdaten und Genfunktionen von Hunderten verschiedener Zelltypen und Geweben von Mensch und Maus trainiert wurde.\n\nKI wird Experimente nicht ersetzen\n\nDoch komplett ersetzen kann die KI die Arbeit von Genforschern natürlich nicht. Es werden weiter echte Experimente und Untersuchungen nötig sein, um zu bestätigen, ob AlphaGenome mit der Vorhersage, eine bestimmte Genvariante könne etwa in die Entstehung von Krebs involviert sein, tatsächlich richtig liegt. Dennoch ist schon jetzt abzusehen, dass sich AlphaGenome zu einem Standardwerkzeug entwickeln wird, mit dem sich Genforscher einen ersten Überblick verschaffen, welche Funktionen eine Genregion im Erbgut höchstwahrscheinlich erfüllt, und mit welchen Experimenten sie das am besten überprüfen können.\n\nWenn es dann in die detailliertere Genanalyse geht, werden andere KIs und Versuchsmethoden besser geeignet sein: \"Für präzise, experimentell zu überprüfende Fragestellungen bleiben spezialisierte Tools oft effizienter als AlphaGenome\", sagt Martin Kircher von der Arbeitsgruppe Computational Genome Biology des Berlin Institute of Health der Uniklinik Charité. - zumal bei einer Gesamtlänge des menschlichen Genoms von etwa 3,2 Milliarden Bausteinen die Obergrenze von AlphaGenomes Analysekapazität von einer Million DNA-Bausteinen nur \"ein Bruchteil des Genoms oder aber auch einzelner Chromosomen\" untersucht werden könne. Damit sei AlphaGenome am oberen Limit aktueller Großrechnerleistung, meint Kircher. \"Wir müssen davon ausgehen, dass in Zukunft effizientere Architekturen gefunden werden, welche biologisches Wissen und Konzepte besser kodieren.\"\n\nGerät die Forschung in Abhängigkeit von einer KI-Firma?\n\nAllerdings beruhen die Fähigkeiten aktueller KIs wie AlphaGenomes sowie zukünftiger Modelle immer auf \"einem großen und robusten Datensatz, der in öffentlichen Datenbanken über die vergangenen Jahre kuratiert und verfügbar gemacht wurde\", betont Meyer. Die privat finanzierte Firma Google DeepMind nutzt also mit öffentlichen Geldern bereitgestelltes Wissen. \"Die Konzentration großer Modelle und Recheninfrastruktur bei einer privaten Firma kann den Zugang, die Transparenz und die Reproduzierbarkeit\" der Erbgutanalysen einschränken, gibt Kircher zu bedenken. \"Das ist aus wissenschaftlicher Sicht bedenklich.\"\n\nZwar benenne Google Deepmind die Urheber der Datengrundlage, mit der AlphaGenome trainiert wurde, mache den Code der KI verfügbar und stelle so \"Wissen und IT-Macht in den Dienst der Genomanalytik\", so Meyer. \"Aber natürlich besteht die Gefahr, dass zukünftig ein genomisches Dienstleistungsunternehmen darauf aufbaut.\"\n\nAuch Simon Maria Zumkeller sieht die Gefahr, dass \"Spitzenforschung von der Infrastruktur privater Unternehmen abhängig wird\". Allerdings habe die KI AlphaFold von Google DeepMind gezeigt, wie sehr die öffentliche Forschung von den Entwicklungen der Firma profitieren könne. Für diese KI, mit der Proteinstrukturen vorhergesagt werden können, wurden 2024 David Baker und Demis Hassabis mit dem Nobelpreis für Chemie 2024 ausgezeichnet, AlphaFold ist Forschenden frei zugänglich.\n\nOb das auch bei AlphaGenome der Fall sein wird, ist noch offen. Den Code der dritten, weiterentwickelten Version von AlphaFold wollte DeepMind zunächst nicht veröffentlichen, sondern gab ihn erst nach Protesten der internationalen Forschungsgemeinschaft frei."
  },
  {
    "source": "The Times of India",
    "company": "Google DeepMind",
    "title": "In 'internal message board' Google DeepMind employees ask top execs about ICE, call it 'US focused question' - The Times of India",
    "date": "2026-01-28T13:08:21Z",
    "url": "https://timesofindia.indiatimes.com/technology/tech-news/in-internal-message-board-google-deepmind-employees-ask-top-execs-about-ice-call-it-us-focused-question/articleshow/127706415.cms",
    "content": "Google DeepMind employees have asked the company's leadership for plans and policies to keep them \"physically safe\" from Immigration and Customs Enforcement (ICE) while on company premises, Wired reported.In an internal message board for Google's roughly 3,000-person AI unit, an employee posted what they called a \"US focused question\" on Monday morning -- two days after federal agents fatally shot Minneapolis nurse Alex Pretti.\"What is GDM doing to keep us physically safe from ICE? The events of the past week have shown that immigration status, citizenship, or even the law is not a deterrent against detention, violence, or even death from federal operatives,\" the employee wrote, according to screenshots of internal messages obtained by Wired.The message received more than 20 \"plus emoji\" reactions from other Google DeepMind staffers. By Monday evening, no senior leaders from Google had publicly responded, the report said.Employee concerns aren't unfounded. A separate Google DeepMind staffer raised concerns about a federal agent's alleged attempt to enter the company's Cambridge, Massachusetts office in the fall, Wired reported.Google's head of security and risk operations clarified that an \"officer arrived at reception without notice\" and was \"not granted entry because they did not have a warrant and promptly left.\"Google CEO Sundar Pichai and DeepMind CEO Demis Hassabis have remained silent on Pretti's killing, even internally, sources told Wired. However, Google DeepMind's chief scientist Jeff Dean has been outspoken on X, calling the shooting \"absolutely shameful.\"The DeepMind employee concerns come as other tech leaders have begun speaking out -- carefully. OpenAI CEO Sam Altman told employees in an internal Slack message that \"what's happening with ICE is going too far,\" according to The New York Times. He also called President Trump \"a very strong leader\" and expressed hope he would \"rise to this moment.\"Apple CEO Tim Cook sent a memo to staff on Tuesday saying he was \"heartbroken\" by the Minneapolis events and had a \"good conversation\" with Trump about his views, Bloomberg reported.Anthropic CEO Dario Amodei called the events in Minnesota a \"horror\" and confirmed his company has no ICE contracts.Google, like many other Silicon Valley firms, relies heavily on thousands of highly skilled foreign workers on visas. Google did not respond to a request for comment prior to publication."
  },
  {
    "source": "My Mobile",
    "company": "Google DeepMind",
    "title": "India to Launch Homegrown Smartphone Brands Within 12-18 Months, Says IT Minister Ashwini Vaishnaw ~ My Mobile India",
    "date": "2026-01-27T04:21:02Z",
    "url": "https://www.mymobileindia.com/india-to-launch-homegrown-smartphone-brands-within-12-18-months-says-it-minister-ashwini-vaishnaw/",
    "content": "* India to launch homegrown smartphone brands within 12-18 months with groundwork completed.\n\n* IT Minister Ashwini Vaishnaw met Google DeepMind CEO Demis Hassabis and OpenAI's Chris Lehane to discuss AI's potential and future collaborations.\n\n* India emphasised \"AI for global good,\" inviting DeepMind and OpenAI to contribute.\n\nUnion IT Minister Ashwini Vaishnaw has said that India is on track to introduce its own smartphone brands with launches expected over the next 12 to 18 months. Speaking on the sidelines of the World Economic Forum (WEF) 2026, the minister stated that the government has completed all the necessary groundwork to make this possible. Here's more on that.\n\nIndian Smartphone Brands on the Way\n\nVaishnaw highlighted that India now has a strong and mature electronics manufacturing ecosystem, making this the right time to push for domestic smartphone brands.\n\n\"Now we have a very substantial electronics ecosystem in our country, this is the time when we will be going for getting our own Indian brands in mobile phones. We have done all the homework that is required to be done,\" he said.\n\nHe added that the ecosystem has progressed significantly, with the government engaging closely with component manufacturers involved in mobile phone production.\n\n\"Today and yesterday, we had significant meetings with the entire ecosystem, which produces the thousands of things that are required to go into a mobile phone. And it's very happy, very satisfying progress. Very soon, maybe another one year from now, or maybe maximum 18 months from now, we should have our own Indian Brands coming out,\" Vaishnaw noted.\n\nMeetings With Global Tech Leaders at WEF 2026\n\nDuring the WEF 2026 event, the IT Minister also met Google DeepMind CEO and co-founder Demis Hassabis, along with OpenAI's Chief Global Affairs Officer Chris Lehane. The discussions focused on artificial intelligence and its broader impact.\n\nFollowing the meeting, Hassabis shared his thoughts on X (formerly Twitter), expressing optimism about collaboration with India.\n\n\"Great to meet you Minister @AshwiniVaishnaw. Really enjoyed our discussion on AI's incredible potential to benefit humanity & India's important role in realising this,\" Hassabis wrote, while also confirming his participation in an upcoming summit in New Delhi.\n\nFocus on 'AI for Global Good'\n\nVaishnaw also hosted OpenAI's Chris Lehane to discuss the theme of \"AI for global good.\" He encouraged both Google DeepMind and OpenAI to actively participate in shaping India's upcoming AI policy frameworks, underlining the government's focus on responsible and inclusive AI development.\n\nFAQs\n\nQ1. When will India launch its own smartphone brands?\n\nAnswer. India is expected to introduce homegrown smartphone brands within the next 12 to 18 months, as the government has already completed the necessary groundwork.\n\nQ2. Why does the IT Minister believe now is the right time for Indian smartphone brands?\n\nAnswer. Ashwini Vaishnaw highlighted that India now has a mature electronics manufacturing ecosystem, with strong engagement from component makers, making this the ideal time to push for domestic brands.\n\nQ3. What global tech leaders did the IT Minister meet at WEF 2026?\n\nAnswer. At the World Economic Forum 2026, Vaishnaw met Demis Hassabis (CEO & Co‑founder of Google DeepMind) and Chris Lehane (Chief Global Affairs Officer of OpenAI) to discuss artificial intelligence and its role in shaping India's upcoming AI policy frameworks."
  },
  {
    "source": "Ad Hoc News",
    "company": "Google DeepMind",
    "title": "Boston Dynamics und Google DeepMind starten KI-Allianz für humanoide Roboter",
    "date": "2026-01-25T17:56:14Z",
    "url": "https://www.ad-hoc-news.de/boerse/ueberblick/boston-dynamics-und-google-deepmind-starten-ki-allianz-fuer-humanoide/68518800",
    "content": "Die strategische Allianz kombiniert führende Roboter-Hardware mit leistungsstarker KI für den Einsatz humanoider Roboter in der Automobilfertigung, beginnend bei Hyundai.\n\nRoboterpionier Boston Dynamics und KI-Labor Google DeepMind gehen eine strategische Partnerschaft ein, um den nächsten Atlas-Roboter mit künstlicher Intelligenz zu versehen. Ziel ist der Einsatz in Fabriken, beginnend in der Automobilindustrie.\n\nDie Zusammenarbeit kombiniert die weltweit führende Roboter-Hardware von Boston Dynamics mit den leistungsstarken KI-Modellen von Google DeepMind. Im Zentrum steht die Integration der Gemini Robotics AI-Modelle in die nächste Generation des humanoiden Atlas-Roboters. Das Ziel: Maschinen zu schaffen, die komplexe, reale Aufgaben in industriellen Umgebungen eigenständig bewältigen können. Die Automobilbranche soll das erste Einsatzfeld werden.\n\nHardware trifft auf KI-Intelligenz\n\nDie Allianz bringt zwei Schwergewichte ihrer jeweiligen Felder zusammen. Boston Dynamics liefert mit dem kürzlich neu entwickelten, vollelektrischen Atlas-Roboter eine der agilsten und leistungsfähigsten humanoiden Plattformen der Welt. Google DeepMind steuert seine auf der Gemini-Architektur basierenden KI-Modelle bei.\n\nPassend zum Thema Mensch‑Roboter‑Kollaboration: Der Einsatz humanoider Roboter auf Montagelinien bringt neue Gefährdungen und erfordert rechtssichere Gefährdungsbeurteilungen. Zahlreiche Arbeitgeber unterschätzen den Dokumentationsaufwand - das kann bei Kontrollen teuer werden. Ein kostenloser Leitfaden bietet praxisnahe Vorlagen, Checklisten und Schritt‑für‑Schritt‑Anleitungen, damit Sie Gefahren systematisch erfassen und Schutzmaßnahmen rechtssicher umsetzen. Er ist speziell auf industrielle Umgebungen und Mensch‑Roboter‑Interaktionen zugeschnitten. Ideal für Sicherheitsfachkräfte, Betriebsräte und Produktionsleiter. Kostenlose GBU‑Vorlagen und Checklisten herunterladen\n\nDiese multimodalen KI-Modelle verarbeiten visuelle, sprachliche und Handlungsdaten. Dadurch soll Atlas in der Lage sein, neue Fähigkeiten aus menschlichen Vorführungen zu lernen und auf neue Situationen zu übertragen. Statt für jede einzelne Bewegung programmiert zu werden, entwickelt der Roboter so ein grundlegendes Verständnis der physischen Welt. Er kann mit Unvorhersehbarkeit umgehen und ein breiteres Aufgabenspektrum bewältigen.\n\nErste Fabriktests bei Hyundai\n\nDer konkrete Fokus der Partnerschaft liegt zunächst auf der Automobilfertigung. Hyundai Motor Group, Mehrheitseigentümer von Boston Dynamics, wird als erster Großkunde und Testumgebung fungieren. Die intelligente Atlas-Generation soll gefährliche, repetitive und körperlich anstrengende Aufgaben auf Montagelinien übernehmen.\n\nDie Strategie von Hyundai zielt auf eine harmonische Mensch-Roboter-Kollaboration ab. Menschen sollen sich vermehrt auf Überwachung, komplexe Problemlösung und wertschöpfende Entscheidungen konzentrieren, während Roboter die physische Arbeit verrichten. Dies soll die Produktivität steigern und dem Fachkräftemangel in der Fertigungsindustrie entgegenwirken.\n\nWettlauf um den Fabrikroboter der Zukunft\n\nDie Partnerschaft verändert das Wettbewerbsfeld für humanoide Roboter deutlich. Sie schafft ein mächtiges Bündnis gegen andere Player wie Tesla mit seinem Optimus-Roboter. Während Tesla Hardware und KI in Eigenentwicklung vorantreibt, vereint diese Kooperation die Spitzenkompetenz zweier spezialisierter Weltmarktführer.\n\nDer Vorteil: Die Unterstützung durch den globalen Fertigungsriesen Hyundai bietet einen direkten Weg zur praktischen Erprobung und Skalierung. Analysten sehen in der Kombination aus erprobter Hardware, leistungsstarker KI und industriellem Maßstab ein umfassendes Ökosystem, das für Konkurrenten schwer zu replizieren sein dürfte.\n\nRoadmap bis 2030\n\nDie Zusammenarbeit folgt einem klaren Zeitplan. Noch in diesem Jahr soll die gemeinsame Forschung mit einer neuen Atlas-Flotte beginnen. Hyundai plant eine schrittweise Einführung: Ab 2028 sollen die Roboter zunächst Teilesortieraufgaben im neuen Werk Hyundai Motor Group Metaplant America (HMGMA) übernehmen.\n\nBis 2030 sollen sie zu anspruchsvolleren Montagearbeiten fähig sein. Der CEO von Google DeepMind rechnet damit, dass zuverlässige, universell einsetzbare Robotersysteme in 18 bis 24 Monaten bedeutende Demonstrationserfolge zeigen könnten. Langfristig könnten die KI-gesteuerten Humanoiden auch in Logistik, Bauwesen und eventuell sogar im häuslichen Bereich zum Einsatz kommen.\n\nPS: Sie planen, Atlas‑Roboter in der Fertigung einzusetzen? Dann sollten Gefährdungsbeurteilungen von Anfang an Teil der Einführung sein. Unser kostenloses Download‑Paket enthält anerkannte Vorlagen und eine Risikomatrix, mit der Sicherheitsverantwortliche in kurzer Zeit rechtskonforme Dokumente erstellen können. So schützen Sie Beschäftigte, vermeiden Betriebsunterbrechungen und erfüllen Prüfanforderungen der Aufsichtsbehörden. Jetzt Gefährdungsbeurteilung‑Paket sichern"
  },
  {
    "source": "Economic Times",
    "company": "Google DeepMind",
    "title": "Google DeepMind CEO Demis Hassabis plays down AI bubble fears, warns of excess startup funding",
    "date": "2026-01-25T11:08:49Z",
    "url": "https://economictimes.indiatimes.com/tech/artificial-intelligence/google-deepmind-ceo-demis-hassabis-plays-down-ai-bubble-fears-warns-of-excess-startup-funding/articleshow/127469485.cms",
    "content": "According to Google DeepMind CEO Demis Hassabis, AI's impact is only just beginning, and the technology is in great demand, so it cannot be dismissed as a bubble. However, supply constraints, especially the shortage of advanced chips, remain a big impediment, he said.\n\nGoogle DeepMind CEO Demis Hassabis said the artificial intelligence (AI) industry is not in a bubble but warned that overheated funding in emerging startups looks unsustainable.\n\nIn an interview with the Financial Times, Hassabis said the question of an AI bubble is \"not binary,\" emphasising that demand for AI technology is stronger than ever.\n\n\"We are seeing more usage than ever, incredible demand for our models and AI features,\" he said, adding that supply constraints, particularly the global shortage of advanced chips, remain a major bottleneck.\n\nFrom DeepMind's vantage point inside Alphabet, Hassabis said AI's impact is only just beginning. \"This is going to be the most transformative technology probably ever invented,\" he noted, suggesting that the scale and depth of adoption make it difficult to characterise the broader industry as a bubble.\n\nHowever, Hassabis acknowledged that certain pockets of the market are showing clear signs of excess. He pointed specifically to early-stage funding trends, calling out \"multi-billion-dollar seed rounds in new startups that don't have a product or technology or anything yet\" as potentially unsustainable.\n\n\"There may be some corrections in some parts of the market,\" he said.\n\nDespite the uncertainty, Hassabis stressed that Google DeepMind is well insulated from any potential downturn.\n\n\"I don't worry too much about that... I focus on our technology and delivering that,\" he said. \"My job is to head Google DeepMind to make sure we are well-positioned no matter what happens.\"\n\nIf investor enthusiasm cools and valuations correct, Hassabis said DeepMind would still benefit from Alphabet's core businesses, where AI can drive productivity gains at scale. Conversely, if the bullish scenario continues, the company is also betting heavily on AI-first, AI-native products such as the Gemini app, Hassabis said.\n\nHis comment came a few months after Google CEO Sundar Pichai told the BBC that \"no firm is immune if an AI bubble bursts,\" including Google.\n\nIn September last year, Alphabet pledged 5 billion pounds over two years for UK AI infrastructure and research, including a new data centre and investment in DeepMind.\n\nRecently, Microsoft CEO Satya Nadella also weighed in on this topic at the Davos conference and said the benefits of AI need to be \"much more evenly distributed\" to avoid it from becoming a \"bubble\". He added that a key warning sign of an AI bubble would be if the discussion remains limited to tech firms and supply-side advances.\n\nAccording to JP Morgan Chase, big tech firms will spend around $5 trillion till 2030 on AI data centres. However, revenues needed to make those investments worthwhile should be $650 billion.\n\nCurrently, they are at $50 billion. Sam Altman-led OpenAI made a series of large investment deals with the likes of Nvidia, AMD, and Oracle totalling almost $1.4 trillion over the next eight years, while revenue for 2025 is expected to be just about $20 billion, estimates suggest.\n\nAlso Read: The big debate: Is AI a bubble?"
  },
  {
    "source": "Financial Times News",
    "company": "Google DeepMind",
    "title": "Google DeepMind chief warns AI investment looks 'bubble-like'",
    "date": "2026-01-24T08:56:20Z",
    "url": "https://www.ft.com/video/d8575873-33c2-43a8-ba8b-6a22723e3a9c",
    "content": "Demis Hassabis says the level of investment in some parts of the tech industry had become detached from commercial realities\n\nYou can enable subtitles (captions) in the video player\n\nDemis, Google launched its most powerful model, Gemini 3, just a few months ago. It was received with a lot of excitement. Where do you think Google is right now on the AI race?\n\nWell, we're very happy, as you say, with the last model we released, Gemini 3. It's topping pretty much all the leaderboards. So it's a great model. Feedback's been great from our users and enterprise customers.\n\nBut I think, overall, we have had a really good year last year when we look back on it. I think the trajectory of progress we've been making is the fastest of anyone in the industry. If you look at, Gemini 2.5, the previous version that we released in April/May last year, that was already becoming very competitive, I think, at the frontier.\n\nAnd then I think we cemented that with Gemini 3. But of course, it's a ferocious, intense competition, as you know. And everyone's pushing as hard as they can. And we've got to make sure we deliver this year too.\n\nIs it why Sam Altman declared a code red?\n\nWell, apparently that's what was being reported. And...\n\nHow do you feel about it?\n\nIt's fine. We just focus on ourselves. And I think that's what we've got to do is block out the noise, and just execute, focus on the quality of our research, and then making sure we're shipping that quality fast enough into our product services. And I think that's what you've seen with our share of the chatbot space with Gemini app has gone up, 650mn monthly users now.\n\nAnd then things like AI Overview, two billion users. It's the most used AI product in the world. So we're really proud and pleased with how that's going. But I think we're just scratching the surface of what we can really do when we're fully in our groove.\n\nAnd we're going to get to that. But when you look at the industry, what do you think rivals are doing best? What do you think is really interesting right now?\n\nWell, I think what Anthropic's doing with code is very interesting with their Claude code. There's a lot of excitement around that in the developer market. We're pleased with the performance of Gemini 3. But they've done something special there, I think other than that, I'm very excited about the stuff that we're doing on multimodal. I feel like...\n\nDo you want to explain?\n\nYes, multimodal, being... Gemini, from the beginning, has been multimodal. And by that, I mean being able to deal with more than just language and text, but actually image, video, audio as a native input and output. And we're bringing that all together.\n\nThat's always been our strength. And the reason we want to do that, and I think that's what I'm excited about this year, is that's what you would need for a kind of an assistant that travels around with you in the real world, maybe on your glasses or your phone.\n\nIt needs to understand the world, the context around you, the physical world. And of course, for robotics, that's critical too. And I think I've been spending quite a lot of time on that last year. And I think that's going to have some big moments in the next couple of years.\n\nCan you talk a little bit about these big moments? Is it a question of trying to create devices that would... the new iPhone, or the glasses?\n\nThere's actually so many simultaneous things one has to do, which is why it's very exciting, but also quite daunting at the moment is, at least from our perspective, as Google DeepMind, as kind of like... we like to think of ourselves, we describe ourselves internally as the kind of engine room of Google. And we're providing the engine, which is these models, like Gemini, and Veo, and Nano Banana, all these state-of-the-art models.\n\nAnd then we got to figure out how do we want to incorporate them into features in products that are really useful to the end user? So there's that whole aspect of work, which is enhancing what already exists, from email, to your Chrome browser, to search.\n\nBut then there are also all these very exciting new greenfield areas of digital assistants like the Gemini app. But what does that become over time, including new devices? And we're working on, and we've announced recently, partnerships with Warby Parker and Gentle Monster on new types of smart glasses.\n\nObviously, Google has a long history with smart glasses. But I think maybe we were a bit too ahead of our time when we first started this 10-plus years ago at Google with the devices. But now I think what was missing was a killer app for that. And I think a universal digital assistant that helps you in your everyday life could well be that killer app for things like smart glasses that's connected to your phone.\n\nThis is an area where a lot of your competitors are also working on. Why do you think you will be able to compete very effectively?\n\nWell, I think it starts with the quality of your research and models. So I think we have, by far, the deepest and broadest research bench. I think we have the most talent in the industry. And I think that then will translate to the quality of our breakthroughs and research innovations. And then that underpins what you can do with these new products.\n\nSpeaking of talent, there's a real talent war in the industry. Some researchers are getting offers for $100mn. How are you holding on to your researchers? Are you having to pay more than that?\n\nLook, I mean, of course, another part of the ferociousness of the competition is the talent wars. But I think that most top researchers, they're, of course, fabulously well paid. But it's then, beyond that, is the mission.\n\nWhat are you trying to do with your skills? These are phenomenally smart people. They could do anything with their skills. Are you doing good in the world? Are you building products, or applying, in our case as well, AI for science, scientific ends that actually you'd be proud of, and your friends and family would be proud of, and you're overall benefiting society?\n\nAnd I think we're very lucky at Google that we have that, those product services that people love and use every day, from maps to email, that we are enhancing with our AI work. So it's very motivating to when you make a research breakthrough, you can ship it. And then immediately, a billion users can take advantage of that.\n\nSo my expectation is that this year we're going to hear a lot more about a techlash because there are growing concerns in society, but there are also safety misuse issues. And we've seen several examples of that. How concerned are you? And how do you protect against it?\n\nLook, I think society is right to be worried about these things. And I think, of course, as you know, I spent my whole career working on AI because I really believe in all the benefits that are going to come from science and medicine advances, things like AlphaFold that we've done.\n\nBut we need to also worry about these harmful use cases. We've tried to get ahead of that with things like SynthID, like watermarking technology for things like deep fakes, getting the right guardrails around the usage of Gemini.\n\nAnd we take that responsibility very seriously for all the users that we have. And we try to be role models beyond what we do. So there's what we can control. And then beyond that, we try to be role models for what responsible use of these kind of... deployment of these technologies looks like.\n\nAnd then as far as society goes and the average person is we need to show, as an industry, as a scientific field, what the unequivocal benefits are more clearly, more quickly. And I think for us, that's doubling down on our AI for science and AI for medicine work and things like that are kind of unequivocal goods in the world.\n\nI'm going to get to that with Isomorphic, but just staying with the misuse and safety issue, whatever happens in this industry, and if there is a growing techlash it will affect all the companies. So is that something... I mean, are you all not getting together to discuss this? Is there any effort under way to address it as an industry?\n\nThere are some industry groups. And of course, most of the lab heads know each other quite well. But I think you're seeing different frontier labs do different things. And I think we'll have to see how that works out.\n\nWhat we can control is what we do at Google DeepMind. And we try to broadcast that at places like this and show the way forward that, I think, gets most of the benefits but mitigates the risks. And we hope others will follow in that path. But it would need something governmental, I think, to create the whole of the industry to do that. And then there's also the international co-operation question too.\n\nThe other big risk this year is the bubble bursting. Are we in an AI bubble?\n\nWell, yeah, look, for me, it's not a binary question, yes or no. The AI industry is very big now, as you know. And it's sort of multifactorial. So I think my guess is... I mean, from our point of view we're seeing more usage than ever, incredible demand for our models and the AI features. We can barely satisfy them. There aren't enough chips to go around.\n\nAnd so I think from that perspective, and also overall, there's not going to be... it's going to be the most transformative technology probably ever invented. So I think from that perspective, there can't really be a bubble.\n\nBut on the other hand, I think there are parts of the industry that do look bubble like, for example, seed rounds, multi-billion dollar seed rounds in new start-ups that don't have a product, or technology, or anything yet does seem a little bit unsustainable. So there may be some corrections in some parts of the market. And then we have to see.\n\nI don't worry too much about that from our day to day. I focus on our technology and delivering that. And my job as head of Google DeepMind is to make sure we're well positioned no matter what happens. If the bubble bursts, we'll be fine. We've got an amazing business that we can add AI features to and get more productivity out of. And also, if the bull case continues, then we've also got these amazing AI first, AI native products like the Gemini app.\n\nYou've also spoken about the AI race and the competition with China. From what I can see, in China there is no AI race. It's very different from what you hear. There's no sort of race to reach AGI [artificial general intelligence]. There is a lot more focus on applications and finding efficiencies. Is that, perhaps, the more realisitic approach?\n\nLook, it's perhaps the more... I don't know about realistic, but the less risky approach, perhaps. And I think, by the way, I think the Chinese market, from what I understand, is just as intensely competitive as the western companies are with each other.\n\nIt's just that I think you're right, they're more focused on the near-term applications, what can you concretely do right now, rather than maybe these more research heavy frontier capabilities that would get you to AGI. I think that's fine.\n\nI started DeepMind. And our job is now at Google DeepMind and Alphabet as a whole, we want to build AGI. We think that's the ultimate goal. And then that will unlock so many opportunities and possibilities in the world that we've talked about many times.\n\nSo I think that's really the North Star. And on the way, we'll create lots of useful technologies. But I think you've got to have that as a North Star if you want to progress the research in as innovative way as possible. And I think that's why, in my opinion, the western companies are still in the lead on that.\n\nHow many months are you ahead? Is it a matter of months?\n\nI think probably it's only a matter of months now, would be my guess. Although interestingly, some of the Chinese leaders and entrepreneurs I talked to, they feel like they're further behind than that.\n\nI'm not sure that's the case. Maybe it's only a matter of six months or so now. But I think it's important that because I think what even things like DeepSeek, which was I think was a bit of an overreaction in the West to that, actually, it's a bit overblown, the Chinese labs haven't proven they can innovate beyond the frontier yet.\n\nThey're getting faster and faster at catching up to the frontier, what the frontier labs are doing. But they haven't innovated beyond that, the next transformers or something like that. They haven't proven they have that capability yet.\n\nDo you think they are as focused on it, though?\n\nThey're probably not. And that might be one reason why.\n\nThere was, in the last few months, a debate about AGI. And you disagreed with Yann LeCun, who said that there is no such thing as general intelligence. You're a real expert on the brain. So explain to me why you disagreed with him.\n\nYes, yeah, we have many fun debates, Yann and I, at conferences and things. But this was an online one. Yeah, I just think it's kind of ridiculous, his argument on that. I think he's confusing two things, which is general intelligence, which I think clearly we have as humans, and our brain has that, and universal intelligence, something that can understand anything that could be possible.\n\nAnd the thing is, it's obvious our brains are very general because look at modern civilisation we've built. And we're basically tool-making creatures. That's what separates us from other animals is we build tools, from all the modern things around us - vehicles, 747s, but also computers. And I'd include AI in that, as well as the ultimate expression of the computational tool.\n\nSo if you include all of that and the science that we do, it's unbelievably general. It's not everything that could possibly happen to your retina and all these arguments he makes. But it's clearly general. And then the other argument I make is more from Alan Turing, who's one of my all-time scientific heroes. And he proved that Turing machines could compute anything that was computable.\n\nSo that's super general class of machine. And all modern computers are based around that. But also, I think most neuroscientists would agree that our brains are an approximate Turing machine, or approximately Turing powerful, which means that we can do, in theory, understand almost anything. So that's computable.\n\nAnd so the idea is that we have... our brains are a general system that can, in theory, learn almost anything. Not that we already know that. So it's a question of... do you have the learning capability versus the actual full knowledge. Obviously, our brains are limited, we can't know everything, a single human. But in totality, our brains are very, very powerful.\n\nAnd very flexible.\n\nAnd extremely flexible.\n\nSo what is it going to take to get to AGI, recursive self-improvement, where essentially AI models can teach themselves? We're not there yet. How far are we from it?\n\nYeah so...\n\nAnd is that the main breakthrough?\n\nWell, that's one. I mean, I think there are... I think there are quite a few capabilities missing from today's systems that will be needed for something that could probably pass as AGI. And continual learning is one of those things, like online learning after you've been - after it's been trained. Can it learn new things from the user or from experience? So it's sometimes called continual learning or online learning.\n\nAnd for that, you need the personalisation, right?\n\nYes, that would be part of it. So if you want it to personalise, then that would be a form of online learning. But also self learning and self-improvement can also be part of that. So that's a closed loop version of like experiencing something in the world and then updating your knowledge base directly and automatically.\n\nAnd we actually pioneered a lot of that work back 10-plus years ago now with AlphaGo and AlphaZero, our games playing programmes. But of course, the question is, in games, that's much simpler. The real world is much messier, much more complex. So the question is, can you translate some of those techniques to the messy real world?\n\nLet's get to Isomorphic, because originally, I think the company has said that you'd be going to clinical trials in Q4 of 2025, but then it was preclinical. So what happened? And when will the drugs go to...\n\nSo nothing happened. I misspoke it. I think it was one interview I gave a couple of years ago. It was preclinical we were entering last year was the plan. And so I misspoke then. And basically, we're in preclinical trials with a few of our drug programmes. It's going very well. We're advancing very well. And then as soon as that's ready, we'll move that into clinical.\n\nSo when will we have the first AI-designed drug?\n\nWell, I hope in the next few years, but it depends on how the preclinical trials go and the clinical trials.\n\nHas it been harder than you had expected?\n\nNot at all. It's been we're actually doing phenomenally well. And we just announced a new deal with - a new partnership with J&J yesterday. So we now work with J&J, Eli Lilly, and Novartis, three of the best pharmas in the world. And we also have our own internal programmes.\n\nSo we have about 17 programmes in total. And we're going to talk a lot more about that. You'll see a lot more news from us this year, first half of this year on our progress, which is going very well.\n\nYou also announced that you were building a materials science lab in the UK. Can you give me some more details about that?\n\nA little bit more. I mean, we're still quite early stages with that. But I think materials science, and AI designing new materials, semiconductors, superconductors, batteries, these kind of things is going to be a huge part of the benefits AI will bring to the world.\n\nAnd I think we're at maybe like an AlphaFold 1 level, some promising research prototypes. But we need to go further. And for part of that is we need to be able to test our materials that our AIs are designing quickly. And so we're thinking about creating a kind of automated lab in the UK to test these theoretical compounds that the AI systems are coming up with.\n\nEvery time I see you, I ask you, what is your expectation of... what's the timeline for AGI? And I've noticed that lately all of you are not talking so much about timelines. In fact, Sam Altman even says there is no... we are almost at that stage. So I am going to ask you, what is your timeline?\n\nWell, mine's been very consistent. I think we're about 5 to 10 years away. So maybe it's now about four to nine years. So now it's... so now it's like four to eight years. So I think 2030 is probably the earliest it could be. Maybe 50 per cent chance over that kind of time zone.\n\nSo I'm still sticking with my timeline. I think others who've had more aggressive timelines maybe are updating to be a little bit longer and a little bit more realistic. But for me, things always take a little bit longer than one assumes, even at the pace that we're all going at. I mean, that's still phenomenally... that's still extremely soon. I just think it's not going to be like next year.\n\nAnd your job has evolved a lot from DeepMind to actually handling all of Google's AI. I'm just wondering where you see your future is. Do you want to be CEO?\n\nLook, I love the... I'm very happy with what I'm doing. I love being close to the science and the research. So I still try and carve out time to do that, even though I'm running a lot of things, including some products now.\n\nBut look, I can get pretty excited about... I'm very general in my interests. And I can get very excited about anything that's cutting edge, especially if it has... especially if it has a leaderboard attached to it. Yeah, I think there's only so much one can do in the day and still leave enough time for serious thinking, which I do at night time. And I quite like that routine. So I hope to be able to stick with that.\n\nYou didn't say no.\n\nNo. OK, there we go.\n\nThank you. Thank you, Demis."
  },
  {
    "source": "Gizmodo",
    "company": "Meta AI",
    "title": "Rokid AI Glasses Style Review: Surprisingly Tough Competition for Meta's Ray-Ban",
    "date": "2026-01-31T15:02:00Z",
    "url": "https://gizmodo.com/rokid-ai-glasses-style-review-surprisingly-tough-competition-for-metas-ray-ban-2000716034",
    "content": "Thanks to Meta, everyone wants a piece of the AI glasses pie. While Ray-Ban Meta AI glasses aren't quite at iPhone levels of popularity, they've sold a lot more pairs than most were expecting, and as a result, competitors are cropping up left and right.\n\nNo, really, smart glasses competition is getting serious: In the pipeline are forthcoming entrants from Google, Samsung, and maybe (just maybe) Apple. Needless to say, when (or if) those competitors enter the chat, Meta is going to have its work cut out for it. In the meantime, there are other options, and some of them have come surprisingly close to challenging Meta already.\n\nI've tried quite a few last year, but for me, Rokid's latest pair, the $299 AI Glasses Style, might be the closest to giving Meta's Ray-Bans a run for their money so far.\n\nSo here's the deal: the Rokid AI Glasses Style, despite their word-garbled name, are nicer than I expected, though I have some serious gripes about how they're being marketed.\n\nOne of the things that immediately drew my attention to these display-less smart glasses was the weight. Rokid's AI Glasses Style are advertised as weighing just north of 38g, which is notably less than the Ray-Ban Meta AI Gen 2 glasses. Meta's smart glasses weigh between 48g and 50g, for context. Lighter smart glasses? Hell yeah.\n\nThere's just one problem: that weight difference actually has a gigantic caveat. As I learned upon using them, the AI Glasses Style weigh 38.5g without the lenses.Â I bothered to weigh them myself at home, and they actually clock in at 45g, which is just 3g lighter than the lightest pair of Ray-Ban Meta AI glasses. Womp, womp.\n\nWhy Rokid chose to list its smart glasses' weight without the lenses is beyond me (I think I know why, though), but I was aggravated to learn that the figure was a stretch of the truth. That's like listing a backpack's weight without the straps or a phone's weight without the chassis. It doesn't make sense since you're never, under any circumstance, going to be using your smart glasses without the lenses in them.\n\nWeight makes a difference, too. A difference of 10g (which is actually only 3 to 5g) may not look like a lot on paper, but it makes a difference on your face, where gadgets can turn uncomfortable fast. That brings me to my next point of comparison: despite the small difference in weight, the AI Glasses Style don't feel nearly as comfortable as the Ray-Ban Meta AI glasses.\n\nFor me, that's mostly because of the frame. While Rokid's AI Glasses Style may look like a Ray-Ban dupe, they don't always feel like one after longer periods of wear. The bridge really took a toll on my nose after a while, and having worn Meta's Ray-Ban smart glasses for hours upon hours collectively, I can't say that I felt the same fatigue wearing them.\n\nThat's partially because I have a big nose, but also because of a difference in design. While Rokid opts for a bridge with nose pads (presumably for a tighter fit), the Ray-Ban Meta AI glasses are smooth. Even just running your finger through the bridge of both, Rokid's smart glasses feel a lot more angular than the Ray-Ban Meta AI glasses. The actual frame design is where I feel the lack of EssilorLuxottica (the maker of Ray-Bans) the most, and while glasses are a specific thing to each person that wears them, the Ray-Ban Meta AI glasses still have my vote in terms of comfort.\n\nIn appearance, Rokid's look fairly similar to Meta's, which is to say, like a Ray-Ban knockoff. That's not a bad thing, necessarilyâ€\"EssilorLuxottica's Ray-Ban frames are iconic and popular for a reason. That reason is they fit well on lots of head shapes, they look generally stylish, but not obnoxious, and they mesh with lots of different styles of clothing.\n\nPersonally, I think the AI Glasses Style look pretty alright, but your take may vary. The frames, I've noticed, are a little wider than the Ray-Ban Meta AI glasses, which isn't ideal for me since I don't have a particularly wide face, but they may look better on your head. The plastic is a bit cheap-feeling, but I've never been a shiny plastic kind of guy. I still prefer the matte option of the Ray-Ban Meta AI glasses.\n\nLike the Ray-Ban Meta AI glasses, you get almost the exact same set of tap and swipe controls on the glasses arm. One tap on the arm for things like play/pause, two taps for things like skipping songs, three taps to go back, and a tap and hold to launch the voice assistant. Swiping forward or backward controls volume. Nothing groundbreaking here, but they work as well as they should, though the Ray-Ban Meta AI glasses might be a tad more sensitive.\n\nOne thing I do love about Rokid's AI Glasses Style is that they have loads of lens options. You can get prescription lenses, progressive lenses, polarized lenses, transition lenses, blue light-blocking lenses, or just regular tinted lenses like the pair. tried. Rokid has Meta beat here for sure.\n\nOverall, the AI Glasses Style look similar, but feel a little worse than the Ray-Ban Meta AI glasses. You should be wary of deceptive marketing, too. These smart glasses are not significantly lighter than Meta's.\n\nAll of that said, these are smart glasses, and the smart part is where Rokid really competes.\n\nRokid's glasses don't just look like the Ray-Ban Meta AI glasses; they're also intended to be used in similar ways. One of the first things people will associate smart glasses with right now is audio, video, and pictures, so let's get into that first.\n\nThe Rokid AI Glasses Style have a 12-megapixel Sony IMX681 sensor, which shoots at a max resolution of 3,024 x 4,032 pixels for still photos, and videos at a 3K resolution. One difference between Meta here is that Rokid, with its companion app, allows you to shoot photos and videos in quite a few ratios (9:16, 3:4, and 4:3) to fit a number of different scenarios, including posting straight to social media.\n\nI shot with the AI Glasses Style for a week, and they do a fairly decent job. Photos were fairly sharp for smart glasses, and comparable to the Ray-Ban Meta AI glasses (they both use a 12-megapixel Sony sensor, so no surprise here). While I wouldn't want to do any professional photography with this camera, it's serviceable for snapping stuff on the fly. Surprisingly, despite the higher resolution of Rokid's sensor, I still prefer the Ray-Ban Meta AI glasses' photos, maybe because of the difference in image processing. It's hard to put my finger on why, but I find Meta's version of slightly soft-looking photos more appealing. Here's a comparison: on the left is Meta's Ray-Ban AI glasses and on the right is Rokid's AI Glasses Style.\n\nVideo is also solid and comparable quality-wise to the Ray-Ban Meta AI glasses, with both topping out with a 3K resolution at 30 frames per second. Rokid gets the point here because its smart glasses can record up to a maximum of 10 freaking minutes as opposed to Ray-Ban Meta's AI glasses that top out at 3 minutes. I found this out by accidentally recording a 5-minute video after I tried and failed to use the voice assistant to stop recording. A happy bug, I guess?\n\nEither way, if you're going to use the AI Glasses Style for action sports, or capturing video while riding your bike, or anything where you want to record for longer periods, that 10-minute recording limit is kind of great, especially with the option to shoot natively in different aspect ratios. Which pair of glasses you like for shooting pictures and videos may depend on your preference for image processing like me, but there's no denying that the AI Glasses Style blow Ray-Bans out of the water in terms of recording length.\n\nIt's worth noting here that, like the Ray-Ban Meta AI glasses, there's an LED light that turns on when you're recording or taking a picture to let people know you've captured something. Whether anyone notices the light or knows what it means is another question entirely, but it's there at least.\n\nThose videos wouldn't be complete without audio, and Rokid is also competitive in that arena. One of the best uses of smart glasses is as an open-ear audio product, so audio is more important than you might think on the surface, and Rokid's AI Glasses Style seem to understand that. I was pleased with the quality of sound coming from Rokid's new smart glasses as well as the volume.\n\nI put these smart glasses through the same gauntlet as every audio product that I test, including using them on the New York subway, and they performed well. Sound is fairly clear, though a little more tinny-sounding than Meta's glasses, and the volume was loud enough, though I couldn't hear it as well as Meta's glasses (maybe Meta's glasses are more directional). I still think Meta takes this category, but Rokid doesn't leave the fight without landing some punches.\n\nOne thing I'll give Rokid here is its inclusion of more than one audio modeâ€\"one for general music listening and one for podcasts that EQs the sound to accentuate voices. I personally love the podcast setting since it does make voices more audible. I still wouldn't want to use the AI Glasses Style on a loud subway, but you could say that for pretty much any open-ear audio product. It's a nice flourish, and one that I'm going to assume Meta will look to replicate in the future.\n\nAs decent as the audio sounds, I have some notes on calling. I did 10 minutes of calling cumulatively with multiple people, and all of them could hear themselves on the call to a disruptive degree. I'm pretty sure that's because the microphones on the AI Glasses Style are picking up what the speakers are putting out because when I turned the volume down, the problem went away. For the record, I've had no such issue on the Ray-Ban Meta AI glasses, so I'm pretty sure this issue is solvable with software. If you're big on taking calls with smart glasses like I am, you may want to wait on using the AI Glasses Style for now.\n\nThe AI Glasses Style don't break the mold entirely when it comes to display-less smart glasses, but they do come with a few things that I wish Ray-Ban Meta AI glasses had.\n\nOne of those things is navigation. Unlike the Ray-Ban Meta AI glasses, you can use the AI Glasses Style to get walking, biking, or driving directions to locations, all by using the onboard voice assistant. It's kind of great. By saying \"Hi Rokid, give me walking directions to the Brooklyn Museum,\" the glasses start the navigation and give you turn-by-turn directions in your ear. If you pull open the companion app, you can also tap open a screen with a map to follow along. So many times while biking, I wished that Ray-Ban Meta AI glasses could launch navigation, so it's nice to see Rokid making that wish a reality.\n\nLike other smart glasses, you can use AI translation, and there's a twist here, too. While most smart glasses are reliant on an internet connection to translate speech, Rokid's AI Glasses style come with a downloadable local model that allows you to load six different languages onto the glasses for offline translation. Those languages are English, French, German, Japanese, Spanish, and simplified Chinese. I tested the local translation by listening to a YouTube video in French, and the results were not bad.\n\nThe AI Glasses Style have similar problems to other AI models (translation can be a bit literal and doesn't always get sentence construction right), but it performed its duty better than I expected. I will say, however, that the local translation mode was a bit slow, so if you're using this feature in the real world, you may have to ask the person you're speaking with to slow down. Using online translation with Alibaba's Qwen AI model was much, much, faster for me (impressively so), but that will depend on your internet connection, obviously.\n\nLike other smart glasses, the AI Glasses Style will convert speech into a translated voice on the speakers, but also gives you an option to convert your speech, which is displayed in text on the companion app. To use this \"DuoTalk\" feature, as Rokid calls it, you'll need to be online. I tested it by translating my speech into German, and it was equally as snappy as the other singular translation mode. Another nice touch on the translation front is the ability to direct your microphone to hear someone in front of you, or a 360-degree hearing mode to capture ambient noise. I'm going to assume most people will want to hear one person at a time, but maybe the 360-degree mode could be good for groups? Either way, the option is interesting.\n\nIn addition to translation and navigation, there are your usual smart glasses features like calling, though I wasn't able to get Rokid's companion app to recognize my contacts despite enabling permissions in iOS. There are also reminders, which work fineâ€\"I set a reminder to go grocery shopping and a voice popped up on time to remind me after I set it. Lastly, there's a recording feature that allows you to capture audio and then use the companion app to generate a summary or word-for-word transcription. This feature also worked fairly well and was able to capture audio in a quiet room and transcribe it accurately.\n\nLike the Ray-Ban Meta AI glasses, there's also computer vision that you can use for stuff like image-based translation. I'd say there's parity here between Meta and Rokid. While Rokid's AI was able to translate some stuff from a Japanese poster I pulled up online, it couldn't read some of the smaller text. I don't find this to be a particularly important battleground for smart glasses, though, if I'm being honest, since the use case feels a little niche.\n\nIf there's a bright spot that I was hoping for software-wise, it's in the voice assistant department and Rokid did an admirable job here, too. The last time I used a pair of Rokid smart glasses, they failed to recognize my \"Hey, Rokid\" wake command over and over, but the company seems to have fixed that this time around. Using the voice assistant on the AI Glasses Style is snappy and pretty efficient. It doesn't get everything right all of the time, but it's definitely less sloppy than Meta AI.\n\nI do have one problem with Rokid's voice assistant, and that's its tendency to shut down the microphone too early. If you're asking Rokid's AI something, I would suggest getting the whole thing out in one fell swoop, since it seemingly gets bored with your yammering quickly.\n\nOne thing that any person willing to wear smart glasses all the time seeks is battery life, and Rokid's AI Glasses Style push the envelope here. The AI Glasses Style claim a 12-hour battery life, which I was skeptical of at first, but after using the smart glasses for a week, I think Rokid may actually have achieved it. As of writing these words, I've had the AI Glasses Style on for several hours with intermittent use and have only seen a 15% dip in battery.\n\nObviously, the battery will depend on what you're doing. Rokid lists its battery life as a \"typical use,\" which is a gray area, and if you're constantly playing music on the AI Glasses Style, I would expect a lot less. Still, if you're listening sometimes, taking short calls, and using them incrementally throughout the day, you may manage to squeeze 12 hours out of the AI Glasses Style. For reference, the Meta Ray-Ban AI Glasses Gen 2 advertise a maximum of 8 hours of battery, so four more hours is nothing to scoff at.\n\nSpeaking of battery, one thing that I wish Rokid included was a charging case. With the purchase of the AI Glasses Style, all you get is a regular case. If you want one that charges, you'll have to spend an additional $99. To charge the smart glasses without a charging case, you'll need to use the included magnetic contact charger (a small dongle) that connects to USB-C and then clips onto the side of the glasses. I don't love the idea of having to rely on a cheap, glasses-specific dongle for charging (what if you lose it or it frays?), but that's what you'll get from Rokid in the standard purchase.\n\nOne last thing to be aware of: I did have some Bluetooth issues with the AI Glasses Style, which caused them to disconnect abruptly, though after I downloaded an OTA update, there haven't been any problems. Still, it's worth mentioning since other smart glasses I've tested, like the Even Realities Even G2 glasses, have had persistent problems with that sort of thing.\n\nAs I mentioned previously, I was surprised at how functional Rokid's AI glasses Style really are. They look decent, the AI features are functional and sometimes a cut above those offered by Meta, and audio, video, and picture are all competitive.\n\nWhether these are the smart glasses for you will depend on what you want out of a pair. If you're looking for the most comfortable and sleekest look, the Ray-Ban Meta Gen 2 AI glasses still win in my opinion, but if you're in it for the AI then the AI Glasses Style might be the better option.\n\nIf there's one thing I'm sure of, it's that Rokid isÂ competition, and while it probably won't unseat Meta from its pedestal any time soon, it goes to show you just how heated things are about to get. If Rokid can give Meta a scare, just imagine what Google or Samsung could do."
  },
  {
    "source": "Gonzales Inquirer",
    "company": "Meta AI",
    "title": "What is the Meta AI app? Everything you need to know - The Gonzales Inquirer",
    "date": "2026-02-19T06:40:38Z",
    "url": "https://gonzalesinquirer.com/premium/stacker/stories/what-is-the-meta-ai-app-everything-you-need-to-know,175449",
    "content": "What is the Meta AI app? Everything you need to know\n\nThe Meta AI app is an all-in-one assistant that remembers your preferences, generates content, and continues conversations across devices -- including Ray-Ban Meta glasses, phones, and desktops. It adapts to your workflow for personalized productivity.\n\nLaunched in April 2025 and powered by Llama 4, the Meta AI app helps businesses with content ideation, customer support, and AI-driven insights, making it easier to create, collaborate, and engage with audiences. Here, WebFX breaks down what the Meta AI app offers and how businesses can use it.\n\nWhat is the Meta AI app?\n\nThe Meta AI app is a free, standalone personal AI assistant app that allows users to engage in voice or text conversations to complete tasks, generate content, and get recommendations. Built on Llama 4 and enhanced with full-duplex speech technology, it creates a more natural, real-time voice experience for users.\n\nThe app is Meta's answer to other AI assistants like Siri, Google Gemini, and ChatGPT -- but with an emphasis on personalization and social connectivity. It syncs with Meta platforms like Facebook and Instagram, and offers features tailored to how users interact with content across these channels.\n\nKey features of the Meta AI app\n\nThe Meta AI app offers a mix of features designed to make digital tasks feel more natural and personalized. These capabilities make it easy to multitask on the go or find fresh inspiration for your content.\n\nHow the Meta AI app supports business users\n\nWhile this AI assistant app is designed for everyday users, using Meta AI for business holds potential -- especially for marketing applications, customer service, and internal productivity.\n\nThinking beyond casual use? Here's how the Meta AI app can help your business, too:\n\nQuick content ideation\n\nStruggling to write a headline or social caption? The Meta AI app can suggest, revise, or optimize your content in seconds -- perfect for marketers juggling multiple platforms.\n\nProductivity on the go\n\nUse voice commands while multitasking to brainstorm ideas, prep meeting notes, or outline blog posts. It's like having a creative partner in your pocket.\n\nCustomer service support\n\nTrain the app for Meta AI to simulate customer inquiries so you can draft better responses. It's a tool for small businesses without a full-scale support team.\n\nAI-powered research and insights\n\nUse the Meta AI app to quickly synthesize topics, summarize articles, or generate images and insights using its existing knowledge base -- saving time when you need fast, actionable takeaways.\n\nQuick Note: This AI app does not currently access real-time web data. It may also experience hiccups in voice recognition as Meta collects feedback to refine the tech.\n\nTraining and onboarding support\n\nCreate onboarding scripts, internal FAQs, or quick-reference guides for new team members. It's an efficient way to scale internal knowledge.\n\nAttracting leads\n\nThe Meta AI app's Discover Feed isn't just a place to explore fun prompts -- it's also a subtle tool for visibility and AI lead generation. By sharing unique use cases, creative prompts, or value-driven content, businesses and creators can spark conversations, gain followers, or even attract leads organically.\n\nPrivacy and data: What you should know\n\nWith personalization at the core of the Meta AI experience, it's important to understand how your data is collected, processed, and stored. No matter how you use this AI assistant app -- casually or for business -- reviewing its privacy settings and controls can help you stay informed and protected.\n\nWhat Meta AI remembers\n\nMeta AI customizes responses based on data from your Facebook or Instagram profile (if connected). It may also recall things you've explicitly told it, such as your professional interests, recent purchases, or personal routines you've shared over time.\n\nData transparency and controls\n\nMeta AI keeps things visible -- a microphone icon appears on-screen when voice input is active. You can also adjust whether Meta AI remembers personal details and even wipe your memory history.\n\nPotential concerns\n\nSome users worry about over-personalization and behavioral tracking. Since the AI draws from your Meta ecosystem activity, being aware of how your data is used is crucial. Businesses, especially, should weigh the convenience of personalization with responsible data use and AI ethics.\n\nMeta AI app vs. other AI assistants\n\nHow does Meta AI compare to other AI assistants?\n\nThe Meta AI app stands out for its full-duplex voice conversations, tight integration with Meta's social platforms, and a Discover feed that lets users explore and share AI-generated content. These features amplify personalization and boost user engagement across devices.\n\nConsiderations for use\n\nIf you're curious about AI or already using other assistants, the Meta AI app is an option for both casual and business users.\n\nIt is designed to be intuitive and helpful for various tasks. Just be mindful of what you share, especially if you manage sensitive data.\n\nFor most users, the benefits of personalized support and content ideation are factors to consider alongside data privacy.\n\nThis story was produced by WebFX and reviewed and distributed by Stacker."
  },
  {
    "source": "Jujuy al día®",
    "company": "Meta AI",
    "title": "WhatsApp: cómo desactivar Meta AI de la aplicación y por qué hacerlo",
    "date": "2026-02-20T16:23:34Z",
    "url": "https://www.jujuyaldia.com.ar/2026/02/20/whatsapp-como-desactivar-meta-ai-de-la-aplicacion-y-por-que-hacerlo/",
    "content": "Este asistente de IA es una función integrada en la app, por lo que no puede eliminarse\n\nJujuy al día ® - Cómo desactivar Meta AI en WhatsApp es una de las dudas más comunes entre los usuarios. Sin embargo, es importante saber que Meta AI es una función integrada en la plataforma, al igual que las llamadas o los stickers, y no se puede desactivar por completo.\n\nAnte esto, los usuarios pueden reducir su presencia tomando algunas acciones: eliminar el chat con Meta AI, evitar mencionarla en grupos o conversaciones individuales y borrar sus datos del servidor de Meta. De esta forma, es posible limitar la interacción con la inteligencia artificial dentro de la aplicación.\n\nPara reducir la presencia de Meta AI en WhatsApp, los usuarios pueden:\n\nBorrar el chat con Meta AI como si se tratara de cualquier otra conversación.\n\nEn caso de querer eliminar los datos almacenados por Meta AI, se puede utilizar el comando \"/reset-ai\". Al hacerlo, la herramienta se restablece a su estado inicial y la copia de la conversación guardada en los servidores de Meta se elimina.\n\nEvitar mencionar a la IA en grupos o chat individuales. Esto hace referencia al comando \"@meta ai\".\n\nAlgunos usuarios proponen usar versiones anteriores de WhatsApp. No obstante, esto no es recomendable, ya que las versiones antiguas suelen tener vulnerabilidades que pueden comprometer la seguridad del dispositivo.\n\nPor qué no se puede desactivar Meta AI en WhatsApp\n\nNo es posible desactivar Meta AI en WhatsApp, ya que es una función integrada en la plataforma. Al respecto, Joshua Breckman, director de Comunicaciones Internacionales de WhatsApp, explicó al medio británico Standard que la herramienta opera \"como cualquier otra función\" dentro de la aplicación.\n\nEsta imposibilidad ha generado preocupación en Europa. La parlamentaria eslovaca Veronika Cifrová Ostrihoňová expresó en X (antes Twitter) que la falta de control sobre la inteligencia artificial plantea \"serias dudas sobre el control del usuario y la seguridad digital\".\n\nSegún indicó, ha recibido numerosas consultas de ciudadanos y ya abordó el tema en una reunión del Comité de Mercado Único del Parlamento Europeo, planteando el asunto directamente al vicepresidente ejecutivo y comisario Henna Virkkunen.\n\nAnte esta situación, los usuarios solo pueden optar por reducir la presencia y las interacciones con la inteligencia artificial dentro de WhatsApp.\n\nPor qué desactivar Meta AI es importante para algunas personas\n\nPara muchos usuarios, la decisión de reducir o desactivar la presencia de Meta AI en WhatsApp está motivada principalmente por cuestiones de privacidad, ya que al interactuar con la inteligencia artificial, parte de la información puede ser procesada y almacenada en los servidores de Meta.\n\nQuienes prefieren mantener sus conversaciones completamente privadas suelen limitar esta función para evitar una mayor recopilación de datos.\n\nOtra razón habitual es la preferencia personal: algunos usuarios no desean la intervención de una inteligencia artificial en sus chats, ya que pueden considerarla invasiva o sentir que afecta la experiencia tradicional de mensajería.\n\nSin embargo, Meta AI también puede resultar útil para quienes deciden utilizarla. Ofrece respuestas instantáneas, sugerencias de contenido y asistencia en la redacción de mensajes, lo que facilita y agiliza las conversaciones.\n\nCómo utilizar Meta AI de forma segura\n\nPara utilizar Meta AI de forma segura en WhatsApp, es importante tomar algunas precauciones básicas:\n\nEvita compartir información sensible, como contraseñas, datos bancarios o información personal, en tus interacciones con Meta AI.\n\nAjusta la configuración de privacidad en WhatsApp para controlar qué datos pueden recopilarse y almacenarse.\n\nSi deseas limitar la presencia de Meta AI, elimina el chat, no la menciones en grupos y borra la información asociada desde el servidor de Meta.\n\nMantén actualizada la aplicación para contar con las últimas funciones de seguridad y protección de datos.\n\nUtiliza Meta AI solo para consultas generales o asistencia, evitando compartir información confidencial.\n\nRecuerda que Meta AI es opcional y puedes restringir su uso según tus preferencias de privacidad."
  },
  {
    "source": "NewsChannel 3-12",
    "company": "Meta AI",
    "title": "What is the Meta AI app? Everything you need to know",
    "date": "2026-02-18T23:44:31Z",
    "url": "https://keyt.com/stacker-ai/2026/02/18/what-is-the-meta-ai-app-everything-you-need-to-know/",
    "content": "The Meta AI app is an all-in-one assistant that remembers your preferences, generates content, and continues conversations across devices -- including Ray-Ban Meta glasses, phones, and desktops. It adapts to your workflow for personalized productivity.\n\nLaunched in April 2025 and powered by Llama 4, the Meta AI app helps businesses with content ideation, customer support, and AI-driven insights, making it easier to create, collaborate, and engage with audiences. Here, WebFX breaks down what the Meta AI app offers and how businesses can use it.\n\nThe Meta AI app is a free, standalone personal AI assistant app that allows users to engage in voice or text conversations to complete tasks, generate content, and get recommendations. Built on Llama 4 and enhanced with full-duplex speech technology, it creates a more natural, real-time voice experience for users.\n\nThe app is Meta's answer to other AI assistants like Siri, Google Gemini, and ChatGPT -- but with an emphasis on personalization and social connectivity. It syncs with Meta platforms like Facebook and Instagram, and offers features tailored to how users interact with content across these channels.\n\nThe Meta AI app offers a mix of features designed to make digital tasks feel more natural and personalized. These capabilities make it easy to multitask on the go or find fresh inspiration for your content.\n\nWhile this AI assistant app is designed for everyday users, using Meta AI for business holds potential -- especially for marketing applications, customer service, and internal productivity.\n\nThinking beyond casual use? Here's how the Meta AI app can help your business, too:\n\nQuick content ideation\n\nStruggling to write a headline or social caption? The Meta AI app can suggest, revise, or optimize your content in seconds -- perfect for marketers juggling multiple platforms.\n\nProductivity on the go\n\nUse voice commands while multitasking to brainstorm ideas, prep meeting notes, or outline blog posts. It's like having a creative partner in your pocket.\n\nTrain the app for Meta AI to simulate customer inquiries so you can draft better responses. It's a tool for small businesses without a full-scale support team.\n\nAI-powered research and insights\n\nUse the Meta AI app to quickly synthesize topics, summarize articles, or generate images and insights using its existing knowledge base -- saving time when you need fast, actionable takeaways.\n\nQuick Note: This AI app does not currently access real-time web data. It may also experience hiccups in voice recognition as Meta collects feedback to refine the tech.\n\nTraining and onboarding support\n\nCreate onboarding scripts, internal FAQs, or quick-reference guides for new team members. It's an efficient way to scale internal knowledge.\n\nAttracting leads\n\nThe Meta AI app's Discover Feed isn't just a place to explore fun prompts -- it's also a subtle tool for visibility and AI lead generation. By sharing unique use cases, creative prompts, or value-driven content, businesses and creators can spark conversations, gain followers, or even attract leads organically.\n\nWith personalization at the core of the Meta AI experience, it's important to understand how your data is collected, processed, and stored. No matter how you use this AI assistant app -- casually or for business -- reviewing its privacy settings and controls can help you stay informed and protected.\n\nWhat Meta AI remembers\n\nMeta AI customizes responses based on data from your Facebook or Instagram profile (if connected). It may also recall things you've explicitly told it, such as your professional interests, recent purchases, or personal routines you've shared over time.\n\nData transparency and controls\n\nMeta AI keeps things visible -- a microphone icon appears on-screen when voice input is active. You can also adjust whether Meta AI remembers personal details and even wipe your memory history.\n\nPotential concerns\n\nSome users worry about over-personalization and behavioral tracking. Since the AI draws from your Meta ecosystem activity, being aware of how your data is used is crucial. Businesses, especially, should weigh the convenience of personalization with responsible data use and AI ethics.\n\nThe Meta AI app stands out for its full-duplex voice conversations, tight integration with Meta's social platforms, and a Discover feed that lets users explore and share AI-generated content. These features amplify personalization and boost user engagement across devices.\n\nIf you're curious about AI or already using other assistants, the Meta AI app is an option for both casual and business users.\n\nIt is designed to be intuitive and helpful for various tasks. Just be mindful of what you share, especially if you manage sensitive data.\n\nFor most users, the benefits of personalized support and content ideation are factors to consider alongside data privacy."
  },
  {
    "source": "El Output",
    "company": "Meta AI",
    "title": "Meta AI en WhatsApp: privacidad, controles y cambios clave",
    "date": "2026-02-04T02:41:40Z",
    "url": "https://eloutput.com/noticias/aplicaciones/meta-ai-en-whatsapp-como-afecta-a-los-usuarios-y-que-puedes-hacer-si-no-quieres-usarla/",
    "content": "Meta explora nuevos modelos de negocio con IA en WhatsApp: investigación regulatoria en Europa, pagos por bots en Italia y futuros servicios avanzados.\n\nLa llegada de Meta AI a WhatsApp se ha convertido en uno de los cambios más visibles -- y polémicos -- de la aplicación de mensajería. El asistente de inteligencia artificial ha empezado a aparecer en la barra de búsqueda y en la parte superior de la app, incluso en cuentas que nunca lo habían activado ni lo habían solicitado.\n\nEste movimiento de Meta no solo implica un cambio estético en la interfaz, sino que abre un debate más amplio sobre privacidad, control del usuario y futuro del modelo de negocio de WhatsApp, especialmente en regiones como Europa, donde las autoridades ya vigilan con lupa cualquier novedad relacionada con datos personales y competencia.\n\nEn los últimos meses, miles de usuarios de WhatsApp en todo el mundo se han encontrado con un nuevo icono o botón de IA integrado en la parte superior de la app o dentro de la barra de búsqueda. Ese acceso directo abre un chat específico con Meta AI, desde el que se pueden hacer preguntas, pedir resúmenes, redactar textos o solicitar ayuda para tareas cotidianas.\n\nAunque en su documentación oficial Meta sostiene que se trata de una función \"opcional\" y basada en la decisión del usuario, en la práctica no existe un interruptor claro dentro de los ajustes de WhatsApp que permita eliminar por completo ese botón o desactivar el asistente de forma definitiva.\n\nMedios internacionales como la BBC recogieron la postura de la compañía: Meta reconoció que \"no es posible desactivar el icono\" de Meta AI, si bien insistió en que nadie está obligado a usar el asistente ni a escribirle. Es decir, el acceso está siempre visible, pero la interacción depende de cada persona.\n\nLa situación ha generado un malestar creciente entre quienes usan WhatsApp para trabajo, estudios o comunicaciones sensibles. Muchos consideran que la app ha cambiado sin darles margen real para decidir si quieren convivir o no con funciones avanzadas de IA integradas en su mensajería diaria.\n\nUno de los focos de preocupación es la privacidad y el uso de datos. Los mensajes convencionales de WhatsApp siguen protegidos con cifrado de extremo a extremo, pero varios análisis especializados señalan que esta protección no se aplica exactamente igual a todo lo que el usuario escribe dentro del chat de Meta AI.\n\nPortales como AtomicMail han subrayado que las conversaciones directas con el asistente podrían tratarse de forma diferente, lo que abre la puerta a que parte de ese contenido se utilice para mejorar modelos, perfilar intereses o alimentar sistemas internos de recomendación. Aunque Meta no comparte todos los detalles de sus procesos, la simple duda basta para inquietar a quienes manejan datos delicados.\n\nEn Europa existen ya mecanismos legales para oponerse a que Meta utilice cierta información con fines de entrenamiento de IA, pero estas herramientas se han aplicado sobre todo a Facebook e Instagram. En el caso de WhatsApp, las opciones para gestionar de manera específica cómo se usan las interacciones con Meta AI todavía no están tan claras ni tan desarrolladas a ojos de muchos usuarios.\n\nMás allá de la privacidad, otro punto delicado es el control real que tiene la persona sobre la herramienta. Distintos medios tecnológicos, como The Sun y otros portales especializados, recomiendan directamente evitar abrir el chat del asistente para que no quede anclado de forma más visible en la interfaz ni se refuercen sus recomendaciones dentro de la app.\n\nPara quienes trabajan con información sensible -- periodistas, sanitarios, abogados, funcionarios o consultores -- la presencia constante del botón de IA puede generar ruido y la sensación de que el entorno de mensajería ya no es tan limpio ni tan separado de otros sistemas de análisis y perfiles automatizados.\n\nEl modo en que Meta ha incorporado Meta AI también ha sido cuestionado por la falta de un consentimiento realmente claro. Publicaciones como Wired han descrito el cambio con una idea sencilla: el usuario no puede quitar Meta AI del diseño general de WhatsApp, incluso si nunca piensa utilizarlo en su día a día.\n\nEsta forma de desplegar nuevas funciones choca con la sensibilidad regulatoria europea, donde se exige cada vez más que las empresas tecnológicas expliquen de manera comprensible qué activan, con qué datos trabajan y qué opciones reales de rechazo tienen los usuarios. Cuando un asistente de IA se instala de facto en una herramienta usada para todo -- desde coordinar trabajos hasta hablar con la familia -- , cualquier opacidad genera desconfianza.\n\nEn paralelo al debate público, Reuters informó de que Meta se ha convertido en objeto de investigación por parte del regulador italiano de competencia, que analiza la forma en que se introdujo Meta AI en WhatsApp y hasta qué punto se comunicó con claridad la imposibilidad de desactivarlo completamente.\n\nWhatsApp, además, ha sido incluida por las instituciones de la Unión Europea entre las plataformas con \"poder de mercado significativo\" en el marco de la Ley de Servicios Digitales, lo que implica un mayor nivel de supervisión sobre cómo integra nuevas capas de IA, publicidad y servicios de pago.\n\nA pesar de que Meta AI no se puede apagar por completo, sí hay ciertas acciones que permiten reducir su visibilidad y minimizar su impacto en el uso diario de la app. Diversas guías publicadas por medios como AP News y portales tecnológicos coinciden en varios pasos prácticos.\n\nEl primero es archivar o eliminar el chat del asistente. En la lista de conversaciones, basta con mantener pulsado el chat llamado \"Meta AI\" y elegir \"Archivar\" o \"Eliminar\". Con esto, desaparece de la vista principal, aunque el botón de acceso puede seguir apareciendo en la parte superior o en la barra de búsqueda.\n\nTambién es posible silenciar las notificaciones del chat de IA. Al entrar en la conversación, se puede abrir el menú de los tres puntos, seleccionar la opción de silenciar y evitar cualquier aviso sonoro o emergente asociado al asistente.\n\nAlgunos tutoriales sugieren incluso enviar comandos al propio bot, como /reset-ia, para borrar copias de conversaciones anteriores con la IA. No es una desactivación global de la función, pero sí un modo de limpiar el historial de lo que se ha intercambiado con el asistente hasta ese momento.\n\nUna recomendación constante de expertos en seguridad digital es, sencillamente, no interactuar en absoluto con Meta AI si no se quiere alimentar el sistema. Cuanto menos se escriba en ese chat, menos material habrá que pueda utilizarse para entrenar modelos o personalizar publicidad y contenido en el ecosistema de Meta.\n\nMientras se discute el grado de control que tienen los usuarios, Meta ha adelantado que, a partir de una fecha marcada en su calendario interno, las conversaciones con su asistente de IA se utilizarán para ajustar la publicidad y las recomendaciones de contenido en otras plataformas de la compañía.\n\nLa empresa ha señalado que este sistema se aplicaría solo a quienes interactúen realmente con la IA, y que no habrá un botón único para desactivar dicho uso de los datos. En la práctica, esto significa que cada pregunta lanzada a Meta AI puede convertirse en una señal más dentro del perfil publicitario de cada persona en Facebook, Instagram u otros servicios del grupo.\n\nEn el caso concreto de WhatsApp, el impacto podría ser algo menor para quienes tengan su cuenta menos vinculada con las demás aplicaciones de Meta, pero la tendencia general es clara: la inteligencia artificial deja de ser un complemento aislado y pasa a integrarse como pieza central del modelo de negocio basado en publicidad personalizada.\n\nLa compañía también ha comunicado que cualquier cambio relevante en ese sentido llegará acompañado de notificaciones dentro de las aplicaciones, aunque la experiencia demuestra que no todos los usuarios leen con detalle esos avisos, muchas veces extensos y redactados en lenguaje jurídico o técnico.\n\nEl papel de Europa es especialmente relevante en esta historia. En Italia, la Autoridad de Competencia (AGCM) obligó a Meta a levantar la prohibición de chatbots de terceros en WhatsApp, una restricción que la empresa había aplicado a nivel global a partir de enero con el argumento de proteger sus sistemas.\n\nTras la intervención del regulador, Meta reabrió el acceso a estos bots externos, pero lo hizo bajo un nuevo modelo de pago para empresas: las compañías que quieran mantener sus propias soluciones de IA dentro de WhatsApp deberán abonar una tarifa por cada respuesta generada a través de la API empresarial.\n\nSegún cálculos de sitios especializados, la cifra se sitúa en torno a los 0,0572 euros por respuesta automatizada. Multiplicado por miles de interacciones diarias, esto puede suponer un coste importante para servicios de atención al cliente, asistentes conversacionales o proyectos que dependen de grandes volúmenes de mensajes.\n\nEste giro ilustra cómo Meta busca equilibrar la carga que supone mantener infraestructuras de IA dentro de WhatsApp con un modelo que traslada parte de los costes a los desarrolladores externos. La compañía ya ha adelantado que podría adoptar enfoques similares en otras regiones si las autoridades imponen condiciones comparables.\n\nAl mismo tiempo, grandes actores del sector de la IA -- como OpenAI, Perplexity o Microsoft -- han tenido que adaptar, limitar o retirar parte de sus bots en WhatsApp ante los cambios de política de Meta, redirigiendo a los usuarios hacia otras plataformas donde tienen un control más directo del entorno.\n\nLa integración de Meta AI no se entiende aislada, sino como parte de una transformación más amplia de WhatsApp. La aplicación lleva años ampliando sus funciones: llamadas y videollamadas, canales para creadores, herramientas para empresas, pagos en algunos mercados y mayor control de privacidad. Con la llegada de la IA, esa evolución se acelera.\n\nMeta ha ido reforzando su apuesta por contratar y adquirir tecnología especializada, como la compra de Manus AI, con la idea de desarrollar agentes capaces de realizar tareas más complejas, automatizar procesos y ofrecer servicios avanzados dentro de sus redes sociales y apps de mensajería.\n\nBuena parte de estas capacidades apuntan a estar ligadas, al menos en parte, a planes de suscripción o servicios de pago. La estrategia pasa por diversificar ingresos más allá de la publicidad clásica y segmentar la experiencia entre usuarios que se mantienen en el nivel gratuito y quienes contratan funciones adicionales.\n\nEn ese contexto, Meta estudia diferentes fórmulas de suscripción en Facebook, Instagram y WhatsApp, con herramientas exclusivas para creadores, empresas o usuarios que necesiten un uso más intensivo de la IA. Las funciones más potentes podrían reservarse para quienes estén dispuestos a pagar una cuota mensual.\n\nLa suma de todos estos movimientos -- integración de IA, monetización de bots empresariales, posibles suscripciones y uso publicitario de las conversaciones con Meta AI -- dibuja un escenario en el que WhatsApp deja de ser solo una app de mensajería gratuita para convertirse en una plataforma más compleja, donde la inteligencia artificial y los modelos de negocio se entrelazan cada vez más.\n\nTodo apunta a que Meta AI se asentará como una pieza fija dentro de WhatsApp, al menos a corto y medio plazo. Para los usuarios, la clave pasa por conocer qué hace exactamente el asistente, qué datos puede utilizar y qué margen hay para limitar su presencia. Entre regulaciones europeas, presiones comerciales y nuevas suscripciones, el uso cotidiano de la app más popular de mensajería se mueve hacia un terreno en el que la tecnología de IA gana peso, y donde cada persona tendrá que decidir hasta qué punto quiere -- o no -- convivir con ella dentro de sus chats."
  },
  {
    "source": "Gizbot",
    "company": "Meta AI",
    "title": "WhatsApp May Soon Add a Dedicated Meta AI Tab to Centralise AI-Powered Features: Report",
    "date": "2026-02-12T11:01:47Z",
    "url": "https://www.gizbot.com/apps/whatsapp-may-soon-add-a-dedicated-meta-ai-tab-to-centralise-ai-powered-features-123263.html",
    "content": "WhatsApp is reportedly working on a dedicated Meta AI tab to centralize all AI-powered features in one place. This feature is currently in development and expected to be available for beta testing in the coming days.\n\nWhatsApp May Soon Add a Dedicated Meta AI Tab\n\nAs per the shared screenshot (via wabetainfo), WhatsApp will add a dedicated Meta AI tab within the bottom navigation bar. The instant messaging app is said to remove the Communities tab, which will be replaced by a new tab offering Meta AI tools.\n\nWith the new dedicated Meta AI tab, WhatsApp is also expected to remove the floating action button that currently opens the Meta AI from the Chats tab. This means there won't be a Meta AI shortcut in the future.\n\nThe Meta AI tab will bring together the existing tools already available through the chatbot. It will just organise them in one place for more convenient access. Users will get an option to quickly generate images, a shortcut to animate photos using Meta AI. In addition, there will be a few more options, namely, learn something, shopping help, and write anything. Moreover, you can find the \"Ask Meta AI\" search bar located at the bottom of the screen.\n\nMeta AI will also support voice calls to receive responses in real time. It will include a dedicated button within the search bar to quickly start a call with the chatbot, which can be accessed from the overflow menu.\n\nAdditionally, the overflow menu will offer shortcuts to manage chatbot memory and view all media shared in conversations with Meta AI. This tab will also let users access individual conversations with Meta AI. Each interaction with the chatbot will be saved separately with its own title, allowing users to revisit conversations anytime, similar to ChatGPT and Gemini.\n\nAs noted, the Meta AI tab is currently in the development phase. Once testing is complete, it will gradually roll out to beta testers, followed by a wider release in the future."
  },
  {
    "source": "The Motley Fool",
    "company": "Meta AI",
    "title": "Meta (META) Q4 2025 Earnings Call Transcript",
    "date": "2026-01-29T00:07:23Z",
    "url": "https://www.fool.com/earnings/call-transcripts/2026/01/28/meta-meta-q4-2025-earnings-call-transcript/",
    "content": "Jan. 28, 2026, 4:30 p.m. ET\n\nCall participants\n\n* Chief Executive Officer -- Mark Elliot Zuckerberg\n\n* Chief Financial Officer -- Susan Li\n\n* General Counsel and Secretary -- Kenneth J. Dorell\n\nTakeaways\n\n* Total Revenue -- $58.9 billion, up 25% year over year, driven by robust ad demand and product improvements.\n\n* Family of Apps Ad Revenue -- $58.1 billion, increasing 24%, with a constant currency growth of 23%.\n\n* Ad Impressions -- Rose 18% across services, primarily due to engagement and user growth, with some contribution from ad load optimization.\n\n* Average Price per Ad -- Increased 6% year over year, attributed to improved ad performance and higher advertiser demand.\n\n* Family of Apps Other Revenue -- $801 million, up 54%, led by WhatsApp paid revenue and MetaVerified subscriptions.\n\n* Reality Labs Revenue -- $955 million, declining 12%, attributed to lapping Quest 3s launch in 2024 and prior retail channel stocking.\n\n* Daily Active People -- Exceeded 3.5 billion in December, reflecting sustained global engagement growth.\n\n* Free Cash Flow -- $14.1 billion, supporting ongoing capital investments.\n\n* Cash and Marketable Securities -- $81.6 billion on hand, compared to total debt of $58.7 billion.\n\n* Employee Headcount -- Exceeded 78,800, increasing 6%, largely due to infrastructure and AI hires.\n\n* Capital Expenditures -- Totaled $22.1 billion, allocated to data centers, servers, and network infrastructure.\n\n* Product Engagement -- Facebook feed/video post views increased 7% from optimizations, generating the largest quarterly revenue effect from new launches in two years.\n\n* Facebook Content Recommendation -- Over 25% more same-day Reels surfaced sequentially from Q3, driving higher user engagement.\n\n* Instagram Original Content -- Prevalence grew 10 percentage points, now at 75% of US recommendations in Q4.\n\n* Threads Time Spent -- Increased 20%, directly attributed to recommendation system enhancements.\n\n* Meta AI Media Generation -- Number of daily actives tripled year over year, with media creation tool usage on Reels nearly tripling sequentially.\n\n* WhatsApp Paid Messaging -- Surpassed $2 billion annual run rate in Q4.\n\n* US Click-to-Message Ads -- Revenue grew over 50%, reflecting widespread adoption of website-to-message formats.\n\n* AI Coding Tools Productivity -- Output per engineer increased 30% since the start of 2025, with power users up 80% year over year.\n\n* First Quarter 2026 Revenue Guidance -- Projected in the $53.5 billion to $56.5 billion range, with a 4% foreign currency tailwind compared to prior year.\n\n* 2026 Full-Year Expense Guidance -- Estimated between $162 billion and $169 billion, led by infrastructure investment and technical talent hiring.\n\n* 2026 Capital Expenditure Outlook -- Anticipated at $115 billion to $135 billion, aimed at Meta Superintelligence Labs and core infrastructure capacity.\n\n* Operating Income -- Management expects \"to deliver operating income above 2025 operating income.\"\n\n* Tax Rate Outlook -- 2026 full-year tax rate expected in the 13%-16% range.\n\n* EU Ad Offering Changes -- New less-personalized ads rollout to begin this quarter per regulatory alignment.\n\nNeed a quote from a Motley Fool analyst? Email [email protected]\n\nRisks\n\n* Management cited ongoing \"legal and regulatory headwinds in the EU and the US that could significantly impact our business and financial results,\" referencing scrutiny on youth-related issues and \"a number of trials scheduled for this year in the US, which may ultimately result in a material loss.\"\n\n* Expense and capital expenditure growth in 2026 will be substantial, with Reality Labs operating losses expected to remain similar to 2025 levels before improvement in subsequent years.\n\n* Susan Li identified diminishing currency tailwinds and potential headwinds from revised EU ad offerings as factors that may reduce growth rates later in the year.\n\nSummary\n\nMeta Platforms (META 0.43%) reported substantial year-over-year revenue growth in advertising and increased engagement across all major platforms, supported by AI-driven product enhancements and significant capital investment. Management outlined detailed 2026 expense and capital expenditure projections driven by continued commitment to infrastructure and technical talent, particularly for Meta Superintelligence Labs, while also guiding for absolute operating income growth versus 2025. The company disclosed a measured approach to product innovation in AI and immersive media, with ongoing regulatory and legal challenges flagged as explicit risks to near-term financial results.\n\n* Meta AI adoption accelerated in Q4, with daily media-generating actives tripling and the platform now supporting nine languages for video dubbing.\n\n* The rollout of ads in Threads and WhatsApp is being expanded globally, with WhatsApp ads and paid messaging each flagged for continued inventory and product optimization.\n\n* Strategic capital allocation prioritizes reinvestment in core business, recommendation systems, and capacity, with the company maintaining a strong net cash balance despite multi-year infrastructure spending plans.\n\n* Large language models (LLMs) and model unification efforts, including GEM and Lattice, drove measurable increases in conversion rates and ad quality, positioning Meta to consolidate more models in 2026 than in the prior two years.\n\n* MetaCompute and infrastructural advances aim to decrease cost per gigawatt over time, with strategic partnerships and new ownership structures cited to enhance long-term operational flexibility.\n\nIndustry glossary\n\n* MetaVerified: Subscription product offering additional account features and support across Meta's Family of Apps.\n\n* Meta AI: Meta's suite of artificial intelligence products, including large language models and AI assistants for consumer and business applications.\n\n* Threads: Meta's standalone social app, positioned as a text-based, conversation-driven complement to Instagram.\n\n* GEM: Meta's foundational ads model used for training and knowledge transfer to lighter-weight inference models powering ads ranking.\n\n* Lattice: A unified modeling architecture consolidating separate content ranking models within Meta's apps for efficiency and higher ad quality.\n\n* Andromeda: Meta's proprietary ads performance and ranking framework, referenced in model scaling and unification efforts.\n\n* Reality Labs: Meta's segment focused on augmented and virtual reality hardware, software, and content including Quest headsets and AR glasses.\n\n* MetaCompute: Initiative for building and managing Meta's AI compute infrastructure and associated strategic partnerships.\n\nFull Conference Call Transcript\n\nKenneth J. Dorell: Thank you. Good afternoon, and welcome to Meta Platforms' Fourth Quarter and Full Year 2025 Earnings Conference Call. Joining me today to discuss our results are Mark Elliot Zuckerberg, CEO, and Susan Li, CFO. Our remarks today will include forward-looking statements which are based on assumptions as of today. Actual results may differ materially as a result of various factors, including those set forth in today's earnings press release, and in our quarterly report on Form 10-Q filed with the SEC. We undertake no obligation to update any forward-looking statements. Performed very well, thanks to record-breaking holiday demand and AI-driven performance gains.\n\nMark Elliot Zuckerberg: We are now seeing a major AI acceleration. I expect 2026 to be a year where this wave accelerates even further on several fronts. We're starting to see agents really work. This will unlock the ability to build completely new products and transform how we work. In '25, we rebuilt the foundations of our AI program. Over the coming months, we're going to start shipping our new models and products. I expect our first models will be good, but more importantly, we'll show the rapid trajectory that we're on. And then I expect us to steadily push the frontier over the course of the year as we continue to release new models.\n\nI'm very excited about the products that we're building. Our vision is building personal superintelligence. We're starting to see the promise of AI that understands our personal context, including our history, our interests, our content, and our relationships. A lot of what makes agents valuable is the unique context that they can see. And we believe that Meta will be able to provide a uniquely personal experience. We're also working on merging LLMs with the recommendation systems that power Facebook, Instagram, Threads, and our ad system. Our world-class recommendation systems are already driving meaningful growth across our apps and ads business, but we think that the current systems are primitive compared to what will be possible soon.\n\nToday, our systems help people stay in touch with friends, understand the world, and find interesting and entertaining content. But soon, we'll be able to understand people's unique personal goals and tailor feeds to show each person content that helps them improve their lives in the ways that they want. This also has implications for commerce. Our ads today help businesses find just the right very specific people who are interested in their products. New agentic shopping tools will allow people to find just the right very specific set of products from the businesses in our catalog. We're focused on making these experiences work across both our feeds and across business messaging, significantly increasing the capabilities of WhatsApp over time.\n\nNew kinds of content will soon be possible as well. People want to express themselves and experience the world in the most immersive and interactive way possible. We started with text and then moved to photos when we got phones with cameras. Then moved to video when mobile networks got fast enough. Soon, we'll see an explosion of new media formats that are more immersive and interactive, only possible because of advances in AI. Our feeds will become more interactive overall. Today, our apps feel like algorithms that recommend content.\n\nSoon, you'll open our apps, and you'll have an AI that understands you and also happens to be able to show you great content or even generate great personalized content for you. Glasses are the ultimate incarnation of this vision. They're going to be able to see what you see, hear what you hear, talk to you, and help you as you go about your day. And even show you information or generate custom UI right there in your vision. Sales of our glasses more than tripled last year, and we think that they're some of the fastest-growing consumer electronics in history.\n\nBillions of people wear glasses or contacts for vision correction, and I think that we're at a moment similar to when smartphones arrived. It was clearly only a matter of time until all those flip phones became smartphones. It's hard to imagine a world in several years where most glasses that people wear aren't AI glasses. For Reality Labs, we are directing most of our investment towards glasses and wearables going forward, while focusing on making Horizon a massive success on mobile and making VR a profitable ecosystem over the coming years.\n\nI expect Reality Labs losses this year to be similar to last year, and this will likely be the peak as we start to gradually reduce our losses going forward while continuing to execute on our vision. As we plan for the future, we will continue to invest very significantly in infrastructure to train leading models and deliver personal superintelligence to billions of people and businesses around the world. I recently announced MetaCompute with the belief that being the most efficient at how we engineer, invest, and partner to build our infrastructure will become a strategic advantage. Dina Powell McCormick also joined us as president and vice chairman.\n\nShe will lead our efforts to partner with governments, sovereigns, and strategic capital partners to expand our long-term capacity, including ensuring positive economic impact in the communities that we operate in around the world. An important part of MetaCompute will be making long-term investments in silicon and energy. We will continue working with key partners while advancing our own silicon program. We're architecting our systems so that we can be flexible in the systems that we use, and we expect the cost per gigawatt to decrease significantly over time through optimizing both our technology and supply chain.\n\nThe last thing that I want to mention is that I think that 2026 is going to be the year that AI starts to dramatically change the way that we work. As we navigate this, our North Star is building the best place for individuals to make a massive impact. So to do this, we're investing in AI-native tooling so individuals at Meta can get more done. We're elevating individual contributors and flattening teams. We're starting to see projects that used to require big teams now be accomplished by a single very talented person.\n\nI want to make sure that as many of these very talented people as possible choose Meta as the place that they can make the greatest impact. To deliver personalized products to billions of people around the world. And if we do this, then I think that we're going to get a lot more done, and I think it's going to be a lot more fun. Alright. That's everything I wanted to cover. This is going to be a big year for delivering personal superintelligence, accelerating our business, building infrastructure for the future, and shaping how our company will work going forward.\n\nAs always, I am grateful for all of the hard work of our teams and to all of you for being on this journey with us. And now here's Susan.\n\nSusan Li: Thanks, Mark, and good afternoon, everyone. Let's begin with our segment results. All comparisons are on a year-over-year basis unless otherwise noted. Our community across the family of apps continues to grow. We estimate more than 3.5 billion people used at least one of our family of apps on a daily basis in December. Q4 total family of apps revenue was $58.9 billion, up 25% year over year. Q4 family of apps ad revenue was $58.1 billion, up 24% or 23% on a constant currency basis. In Q4, the total number of ad impressions served across our services increased 18%.\n\nImpression growth was healthy across all regions, driven primarily by engagement and user growth and to a lesser degree ad load optimizations. The average price per ad increased 6% year over year, benefiting from increased advertiser demand largely driven by improved ad performance. Family of apps other revenue was $801 million, up 54%, driven by WhatsApp paid revenue growth as well as MetaVerified subscriptions. Within our Reality Labs segment, Q4 revenue was $955 million, down 12% year over year. As we noted on the last call, the year-over-year decline in Reality Labs revenue is due to us lapping the introduction of Quest 3s in 2024 as well as retail partners procuring Quest headsets during the year. Added this year.\n\nParticularly AI talent. Legal expense growth was due to both lapping legal accrual reversals in '24 and charges recorded in Q4 2025. Infrastructure expense growth was driven by higher depreciation, cloud spend, and other operating expenses. We ended Q4 with over 78,800 employees, up 6% year over year, driven by hiring in priority areas of infrastructure, Meta Superintelligence were $22.1 billion, driven by investments in data centers, servers, and network infrastructure. Free cash flow was $14.1 billion. We ended the quarter with $81.6 billion in cash and marketable securities and $58.7 billion in debt. Turning now to the business performance. There are two primary factors that drive our revenue performance.\n\nOur ability to deliver engaging experiences for our community, our effectiveness at monetizing that engagement over time. Product efforts on both feed and video surfaces. The optimizations we made in Q4 drove a 7% lift in views of organic feed and video posts on Facebook. Resulting in the largest quarterly revenue impact from Facebook product launches in the past two years. We're continuing to increase the freshness and originality of content recommendations as well. On Facebook, our systems are surfacing over 25% more reels published that day than the prior quarter. On Instagram, we grew the prevalence of original content in the US by 10 percentage points in Q4 with 75% of recommendations now coming from original posts.\n\nThreads is also seeing strong momentum again, benefiting from recommendation improvements. The optimizations we made in Q4 drove a 20% lift in Threads time spent. Turning to 2026, we see a lot of opportunity to drive additional gains. This includes scaling the complexity and amount of training data we use in our models, while continuing to make our systems more responsive to people's real-time interests. We're also focused on incorporating LLMs to understand content more deeply across our platform, which will enable more personalized recommendations. Another big area of investment this year is developing the generation of our recommendation systems.\n\nWe have several big bets on this front, including building new model architectures from the ground up that will work on top of LLMs, leveraging the world knowledge and reasoning capabilities of an LLM to better infer people's interests. Beyond improvements to our recommendation systems, we expect to use the models developed by Meta Superintelligence Labs to deliver compelling and differentiated AI products. One area we're already seeing promise is with AI dubbing of videos into local languages. We are now supporting nine different languages with hundreds of millions of people watching AI-translated videos every day. This is already driving incremental time spent on Instagram, and we plan to launch support for more languages over the course of this year.\n\nWe are also seeing strong traction with our media creation tools. Nearly 10% of the reels people view each day are now created in our Edits app, almost tripling from last quarter. Within Meta AI, the number of daily actives generating media tripled year over year in Q4. This year we expect to advance the capabilities of our underlying media generation models and ship new features to further enhance the product experience. Another area we're focused on for Meta AI is personalization. We're seeing in our early testing that personalized responses drive higher levels of engagement. And we expect to significantly advance the personalization of Meta AI this year.\n\nThis dovetails with our investments in content understanding, which will enable our systems to develop a deeper understanding of each person's interests and preferences while also identifying the most relevant content across our platform to pull into responses. Turning to the second driver of our revenue performance, increasing monetization efficiency. The first part of this work is optimizing the level of ads within organic engagement. Load increases. We also continue to make progress on bringing ads to our newer services. Within Threads, we're beginning to expand ads to all remaining countries this month, including the UK, European Union, and Brazil.\n\nOn WhatsApp, we expect to complete the rollout of ads and status throughout the year with the level of ads remaining low in the near term while we follow our standard approach of optimizing ad formats and performance before ramping inventory. Moving to the second part of increasing monetization efficiency, improving performance for the businesses who use our tools. We're seeing very strong results from the ad performance investments we made throughout 2025 with year-over-year conversion growth accelerating through the fourth quarter. We expect the set of investments we're making in 2026 will enable us to drive further gains as we continue to integrate AI across all layers of the marketing and customer engagement funnel.\n\nThe first area is our ad system, where we're continuing to scale the data complexity and compute we use in our future ranking models to deliver performance gains. As we scale up our foundational ads models like GEM, we are also developing more advanced models to use downstream of them at runtime for ads inference. In Q4, we launched a new runtime model across Instagram feed stories, reels, resulting in a 3% increase in conversion rates in Q4. We continue to progress on our model unification efforts under Lattice as well.\n\nAfter seeing strong success with the consolidation of Facebook feed and video models in 2025, in Q4, we consolidated models for Facebook stories and other surfaces into the overall Facebook model. This, along with a series of back-end improvements, drove a 12% increase in ad quality. And in 2026, we expect to consolidate more models than we had in the prior two years as we continue to evolve our systems towards running a smaller number of highly capable models. Moving to the next area, ads products. Continue investing in ways to help businesses leverage AI to reduce the friction of setting up and optimizing an ad campaign.\n\nIn Q4, we started testing our Meta AI business assistant with advertisers, which helps with tasks like campaign optimization and account support. In the coming months, we'll make it available to more advertisers so each business has an AI assistant they can chat with that remembers their business's goals and provides personalized recommendations on how to improve performance. Another area we're deploying AI to improve performance is ad creative. The work I'll cover is business messaging, where we're seeing strong momentum across our portfolio of solutions.\n\nClick-to-message ads revenue growth accelerated in Q4 with the US up more than 50% year over year, driven by strong adoption of our website-to-message ads, which direct people to a business's website for more information before choosing to launch a chat. Paid messaging within WhatsApp continues to scale as well, crossing a $2 billion annual run rate in Q4. Finally, we're seeing good early traction with our business AIs in Mexico and the Philippines, with over 1 million weekly conversations between people and business AIs now happening on our messaging platforms.\n\nThis year, we will expand the availability of our business AIs to more markets, while also extending their capabilities so they not only answer questions on topics like product availability but can help people get things done right within WhatsApp. We speak a lot about how AI is improving our products, but I'd like to take a moment to give an update on how it's changing the way we work. Mark mentioned our focus on making Meta a place where individuals can have significant impact. A big focus of this is to enable the adoption and advancement of our AI coding tools where we're seeing strong momentum.\n\nSince the beginning of 2025, we've seen a 30% increase in output per engineer, but the majority of that growth coming from the adoption of agentic coding, which saw a big jump in Q4. We're seeing even stronger gains with power users of AI coding tools, whose output has increased 80% year over year. We expect this growth to accelerate through the next half. Next, I would like to discuss our approach to capital allocation. We have significant opportunities to improve our core business in 2026. We plan to continue to prioritize investing in the business to support these opportunities recommendation training workloads. In addition to the inference workloads it currently runs.\n\nMore broadly, as we invest in infrastructure to meet our business needs, we continue to prioritize maintaining long-term flexibility so we can adapt to how the market develops. We're doing so in several ways, including changing how we develop data center sites, establishing strategic partnerships, contracting cloud capacity, and establishing new ownership structures for some of our large data center sites. We have a strong net cash balance and expect our business will continue to generate sufficient cash to fund our infrastructure investments in 2026, which is reflected in our expectations.\n\nNonetheless, we will continue to look for opportunities to periodically supplement our strong operating cash flow with prudent amounts of cost-efficient external financing, which may lead us to eventually maintain a positive net debt balance. Moving to our financial outlook. We expect our first quarter 2026 total revenue to be in the range of $53.5 billion to $56.5 billion. Our guidance assumes foreign currency is an approximately 4% tailwind to year-over-year total revenue growth based on current exchange rates. Turning to the expense and CapEx outlooks. Expect full-year 2026 total expenses to be in the range of $162 to $169 billion.\n\nThe majority of expense growth will be driven by infrastructure costs, which includes third-party cloud spend, higher depreciation, and higher infrastructure operating expenses. The second largest contributor to total expense growth is compensation driven by investments in technical talent. This includes 2026 hires to support our priority areas, particularly AI, as well as a full year of expenses from 2025 hires. At a segment level, we expect expense growth to be driven by the family of apps with Reality Labs operating losses remaining similar to 2025 levels.\n\nAnticipate 2026 capital expenditures, including principal payments on finance leases, to be in the range of $115 to $135 billion with year-over-year growth driven by increased investment to support our Meta Superintelligence Labs efforts and core business. Despite the meaningful step-up in infrastructure investment, in 2026, we expect to deliver operating income that is above 2025 operating income. Absent any changes to our tax landscape, we expect our full-year 2026 tax rate to be 13% to 16%. Finally, we recently aligned with the European Commission on further changes to our less personalized ads offering, which we will begin rolling out this quarter.\n\nHowever, we continue to monitor legal and regulatory headwinds in the EU and the US that could significantly impact our business and financial results. For example, we continue to see scrutiny on youth-related issues and have a number of trials scheduled for this year in the US, which may ultimately result in a material loss. In closing, 2025 was another strong year for our company. The investments we've made to improve our business are continuing to drive strong growth, and we have an exciting roadmap this year to deliver new experiences and services for our global community. As always, thank you to our teams for their hard work and commitment to our mission.\n\nWith that, Krista, let's open up the call for questions.\n\nKrista: Thank you. We will now open the lines for the question and answer session. Please pick up your handset before asking your question to ensure clarity. If you are streaming today's call, please mute your computer speakers. And your first question comes from the line of Brian Thomas Nowak with Morgan Stanley. Please go ahead.\n\nBrian Thomas Nowak: Thanks for taking my questions. I have one for Mark, one for Susan. Mark, one is a long-term question. As you think about ramping all this investment, and the personal intelligence opportunity, the MetaCompute opportunity, can you walk us through a little bit how you think about the largest revenue or ROI long-term opportunities you're trying to unlock with those over the next, call it, three, five, ten years through all the investment? And then, Susan, a little more near term, more like 2026. I think the guide is the fastest growth you've had in almost five years. I know you have a lot of improvements on recommendations and monetization efficiency.\n\nBut can you just sort of help us a little bit understand two or three of the biggest drivers of this inflection you're seeing on revenue in '26?\n\nMark Elliot Zuckerberg: Yeah. I guess I can start with the first one. Although, I have to say upfront that I think my answers to a lot of your questions on this particular call may be somewhat unfulfilling because we're in this interesting period where we've been rebuilding our AI effort. And we're six months into that. And I'm happy with how it's going. But we are going to be rolling out our initial set of models and products and businesses around that over the coming months. And will have a lot more to share on all of those fronts at that point.\n\nSo I'm happy to offer kind of a high-level view of some of the stuff, but I apologize in advance that not much of this is going to be particularly detailed, but it will be exciting as we roll it out. I think the theme on the business, I mean, this is and, effectiveness of the core business, both for people who use it organically and there's going to be, you know, several for businesses. So I think that will have a compounding effect. And then many, I think, new business opportunities that come up. I mean, we have been working on Meta AI for a while.\n\nI think you're starting to see the way that products like that get monetized across the industry when we get that to a scale and depth that we want. We think that there are going to be opportunities both in terms of subscriptions and advertising and all of the different things that you see on that. And I mean, yeah, I think, you know, there's a number of things on shopping and commerce I'm quite excited about that I alluded to in the comments upfront. And as the models launch and we demonstrate some of the capabilities, both in the first set of models and over the year. I think the models are going to get a lot better too.\n\nWe'll be able to have different products paired with those that I think will facilitate different businesses for, you know, businesses who use us and our platforms as well as direct consumer businesses. I guess it's probably also worth flagging because I didn't I don't think we either of us mentioned the Manus acquisition in the upfront comments. I mean, that is going to is a good example of you do have a significant number of businesses that already pay a subscription to basically use their tool to accelerate their business results.\n\nAnd integrating that kind of thing into our ads and business manager, so that way we can just offer more integrated solutions for the many, many millions of businesses that use and rely on our platforms is going to be really powerful, both for accelerating their results using the existing products that we have and, I think, adding new lines as well. So, you know, a somewhat high-level answer and I think the picture will become clearer and I think more exciting if we do our jobs well over the course of the year.\n\nSusan Li: Brian, on your second question, there's obviously a range of outcomes captured in the Q1 2026 revenue outlook. It overall reflects our expectation for a strong quarter of growth. The range embeds an outlook for accelerated growth, and that's really underpinned by the strong demand that we saw through the end of Q4 and continuing into the start of 2026. Now I will say we also expect foreign currency to be a four-point benefit to year-over-year growth, so that is a three-point larger tailwind than it was in Q4 2025. As we lap the strengthening of the US dollar a year ago. But overall, you know, we see that advertisers are responding to ad performance improvements that we made.\n\nThey're driving strong conversion growth. We've made a lot of these investments over the course of 2025, including advances to our ads ranking and delivery systems, the more effective redistribution of ad load, new features and ad products, like Advantage Plus, better measurement, and just a lot of great work. That has helped to drive the continued performance of our ads.\n\nKrista: Your next question comes from the line of Eric James Sheridan with Goldman Sachs. Please go ahead.\n\nEric James Sheridan: Thanks so much for taking the question. Maybe two, if I could. In prior periods, you've talked about being capacity constrained internally and not having enough compute to sort of achieve the goals you have on a platform and a product standpoint. I want to know if we get any update on currently how you think about your own internal needs for compute against that roadmap? And the second part of the question would be, as we continue to see the ads business sort of scale, especially in terms of dollar growth year on year? Have we yet seen the full first-order effects of scaling the business against applying more compute to it?\n\nOr how should investors think about the directional relationship between applying more compute and rate of change in terms of outcomes on the monetization side? Thank you.\n\nSusan Li: On your first question, we do continue to be capacity constrained. Our teams have done a great job ramping up our infrastructure through the course of seeing our infrastructure efficiency in several ways, including by optimizing workloads, improving infrastructure utilization, diversifying our chip supply, and just investing in efficiency improvements as part of our core technology development efforts in areas like content and ads ranking. So that was your first question. The second question about how the ads business scales. I think we don't I don't have an extremely precise answer to this question. What I'd say is, you know, one of the ways that we are working to drive ads performance improvements is by improving our larger scale models.\n\nAlong with our lighter weight ones that we use for ads inference at runtime. You know, we don't typically use our larger model architectures like GEM for inference because their size and complexity would make it too cost prohibitive. So the way that we drive performance from those models is by using them to transfer knowledge to smaller lightweight models used at runtime. But I would say that we think that there is room for our larger models to benefit from having more compute.\n\nAnd I think as we scale up the compute available to those models, and the foundational models in different areas that power the different stages of ads ranking and recommendation, you know, we expect that we will see gains coming from that.\n\nKrista: Your next question comes from the line of Mark Elliott Shmulik with Bernstein. Please go ahead.\n\nMark Elliott Shmulik: Yes. Thanks for taking the questions. I think the first question was asking about when do I expect the product impact to be. I mean, we're going to roll out products over the course of the year. I think the important thing is we're not just launching one thing, and we're building a lot of things. I think AI is going to enable a lot of new experiences. I outlined thematically a bunch of these in the upfront comments around personal AI, around LLMs, combining with the recommendation systems.\n\nI think that's a somewhat longer-term research project that I think will yield dividends over a long period of time, but we're already definitely seeing optimizations of the recommendation systems as we're including more of the AI research improvements and advances into that. The content is going to improve. There are going to be new formats. There are going to be improvements on the glasses. There are all these different things as well as several things that we think are new that we're going to try that are not just extensions of the current things that we're doing.\n\nSo yeah, I mean, I would expect that we'll roll these out over the course of the year and that, you know, sometimes it takes a few iterations for things to really hit and reach the kind of product market fit that you need. But I think we have enough time, hopefully, to, you know, we're starting off early enough in the year that I would expect that we'll see some successes by the end of the year on this as well as on the work side.\n\nWhat we were talking about is I think it's very hard for anyone exactly to predict what the shape of, you know, how organizations working is going to feel, but I just think the fact that agents are really starting to work now is quite profound. And I think it is going to allow we're already starting to see the people who adopt them are just being significantly more productive. And there's a big delta between the people who do it and do it well, and the people who don't.\n\nAnd I think that's going to just be a very profound dynamic for I think, across the whole sector and probably the whole economy going forward in terms of the productivity and efficiency with which we can run these companies. Which I think, you know, my hope is that we can use that to just get a lot more done than we were able to before. And I'm most focused on making sure that Meta is a great company to have a big impact. Right?\n\nYou'll be able to use these kind of agentic tools anywhere, but you will only be able to come and ship things to billions of people if you join a company like Meta nor that many companies like Meta. So I think if we make it so that we can harness these kind of tools, I think that you we should over some period of time start to see a real acceleration in the amount of output that we could have. Now how to predict exactly the time frame for adopting that, somewhat hard. Right? I'm not going to predict a specific quarter or something like that. But the trend seems like unmistakably, like, this is going to happen.\n\nAnd that to me is something that is very exciting and like I said in my comments upfront, also, like, honestly, kind of fun. Right? If you just makes it more fun to be able to build a lot of things. And, you know, that's what we're here to do.\n\nSusan Li: Mark, on your second question, I want to make sure and clarify something. So I think in the question you had said that operating income growth in '26 would be higher than '25, and I want to make sure my comments were super clear. In 2026, we expect to deliver operating income above 2025 operating income. So this is comparing absolute dollars, not over year growth. So to give some context on that, you know, we are going into 2026 with strong revenue growth at the start of course, we are just a few weeks in, set against, you know, a healthy macro backdrop.\n\nSo, obviously, hard to extrapolate the current trends to the full year, and, you know, there are many moving variables in the current landscape. We're really taking advantage of the current business strength to reinvest a lot of the revenue into what we see as very attractive investment opportunities in AI infrastructure and talent. It's hard to assess, you know, what all of those investment opportunities will be over the course of the year as we continue to work through our capacity options. And, of course, it remains a very competitive hiring market but we'd like to invest aggressively where we can.\n\nWe continue to use our framework that we shared, at this point several years ago of growing consolidated operating profit over time to guide those investments. And based on where our plans are rolling up today, again, in '26, we expect to deliver more operating income than we did in 2025.\n\nKrista: Please go ahead. Your next question comes from the line of Douglas Till Anmuth with JPMorgan.\n\nDouglas Till Anmuth: Thanks so much for taking the questions. One for Mark and one for Susan. Mark, could you just provide more detail on the progress of the MSL team several months in? And more on your view on the path to a frontier model this year. And then, Susan, I know you expect to grow operating income in '26. Do you also expect to have positive free cash flow? Just how should we think about the current and any future JVs for data center and compute build out? Thanks.\n\nMark Elliot Zuckerberg: I'm not sure I have anything else to add. On the current progress on this. I mean, that's why I said upfront that I think this is somewhat of an unfulfilling time to be answering some of these questions. We're about six months in to building MSL. I'm very pleased with the quality of the team. I think we have the most talent-dense research effort in the industry, and some of the early indicators look positive. But, look, this is going to this is a long-term effort. Right? We're not here to do this to ship like, one model or one product. We're doing a lot of models over time and a lot of different products.\n\nAnd I want to make sure that the work can speak for itself and also that, you know, we all internalize that this is a journey that we're on and the first set of things that we put out, I think, are going to be more about showing the trajectory that we're on. Rather than being a single moment in time. So yeah, I'm quite optimistic, but don't have anything else particularly concrete to share.\n\nSusan Li: Doug, on the first part of your question, you know, we are making very significant investments in infrastructure capacity this year to support our AI efforts. And we believe we're in a strong position to support them with the cash generation of our business this year. And, you know, at the same time, we'll continue to explore different paths as we build out our infrastructure capacity that help us provide, you know, that help provide us the long-term flexibility and option value that we look for as we support our future capacity needs against the backdrop of a very wide range of possible capacity demand over the years to come.\n\nSo we don't have anything additional to announce at this point. You know, we are looking at, you know, all of the different opportunities to stand up, to stand up capacity. Across kind of the different time frames that we need them.\n\nKrista: Your next question comes from the line of Justin Post with Bank of America. Please go ahead.\n\nJustin Post: Great. A couple, maybe one for Mark and one for Susan. It just seems like you're going to have a tremendous amount of capacity. How do you think about expanding your opportunities beyond ads? Things like subscriptions or licensing cloud models. Just with all the interesting things you're building. I don't expect any product announcements. But can you do things beyond ads? And then for Susan, it's really interesting to see the acceleration even ex FX and advertising. I'm just wondering if you're seeing a general acceleration in e-commerce activity. Where do you think the dollars are coming from? And is the entire Internet ecosystem accelerating? I'm just wondering your thoughts on that.\n\nMark Elliot Zuckerberg: So, yes, we are focused on things beyond ads. I think the numbers make it so that for the next couple of years, ads are going to be by far the important driver of growth in our business. So that's why as we're working on this, we have a balance of new things that we're trying to do while also investing very heavily in making sure that all of the work that we're doing in AI improves both the quality and business performance of the core apps and businesses that we run there. But yeah. I mean, we'll have more to share on that.\n\nBut, I mean, all these things, even if they scale very quickly, are going to take some time to be meaningful at the scale of what the ads business is and while we're doing that, we're just very focused on also delivering more value to businesses and more quality in the apps that we run ads at.\n\nSusan Li: Justin, on your second question, we saw healthy year-over-year growth across all verticals in Q4 with the exception of politics as we lap the US presidential election last year. The online commerce vertical was the largest contributor to year-over-year growth. That was followed by professional services and technology. So in online commerce, year-over-year growth was strong. It was actually relatively consistent with Q3 levels, and that was broad-based across averages, regions, and sizes. In general, we saw that the demand leading up to the holiday shopping period that sustained through Cyber Five and into the end of the year, you know, was very healthy for us.\n\nProfessional services, in this category, we saw strong broad-based growth with nice contributions from lead generation ads. Due to product improvements we've made, including from Advantage Plus lead campaigns that we fully rolled out at the start of Q4. And, you know, the tech vertical continues to be strong for us too, again, broad-based across advertiser regions and sizes. So in general, I would say it was very healthy, broadly driven growth.\n\nKrista: Your next question comes from the line of Ross Sandler with Barclays. Please go ahead.\n\nRoss Sandler: Yeah. Mark, you mentioned bringing Horizon World into mobile. We haven't heard much from the Horizon World squad on these calls. So interesting that's making it in. It seems like the combo of AI and what you guys have built with Horizon might open up the door to a bunch of new potential areas in gaming or new forms of kind of communication. So could you just elaborate on what the plan is there? Thank you.\n\nMark Elliot Zuckerberg: Yeah. So let me talk about the basic theme here. One core idea that I've talked about on some of these calls over the years is that people always want to express themselves and experience the world in whatever the richest format is that they can. So I talked about this upfront today. It's when we started, a lot of this was text. Right? That was kind of the best we could do. Then we all got phones. They had cameras. Like, a lot of this medium, visual, but with photos, we went through a period where the mobile networks were kind of weak and every time you wanted to watch a video, it would buffer.\n\nAnd once that got worked out, now the majority of the content is video. And one of the core ideas that we have had for a while is that is not the end of the line. Right? Video will continue to be here for a long time. It's going to continue growing. It's not going away. But there are going to be more immersive and interactive formats. Right. And you can engage in it and there are 3D versions of that, and there are 2D versions of that. And Horizon, I think, fits very well with the kind of immersive 3D version of that.\n\nBut there's definitely a version of the future where, you know, any video that you see, you can, like, tap on and jump into it and, like, engage and, like, and be kind of, like, experience it in a more meaningful way. And I think that the investments that we've done in both a lot of the virtual reality software and Horizon as well as a number of other areas around the company, are actually going to pair well with these AI advances to be able to bring some of those experiences to hundreds of millions and billions of people through mobile.\n\nSo anyway, that's the thing that I'm quite excited about, but it's just sort of one flavor of a theme that I think is going to be very interesting. I think there are going to be lots of different types of interactive and immersive content that become possible. And I think Horizon is going to be one very interesting example that I'm quite excited to see how this unfolds.\n\nKrista: Your next question comes from the line of Ronald Victor Josey with Citi. Please go ahead.\n\nRonald Victor Josey: Great. Thanks for taking the question. I wanted to drill down maybe, Susan, on your comments around ranking recommendation model changes. You know, clearly, lots of tailwinds here given the results from GEMS and Dramadel Lattice, consolidation of models, etcetera. So can you help us understand a little bit more just about the roadmap and where we stand within ranking recommendation model changes? There's a thesis out there that maybe we're, you know, there's a limiting factor or maybe we're waiting on newer models, but any insights there would be very helpful as we think about the next as the future going forward. Thank you.\n\nSusan Li: Yeah. Thanks for the question, Ron. You know, we have I'm just sorting out if your question was more specific to ads or if it was more specific to kind of the engagement side, but I'll try to talk a little bit about both. So on the sort of core engagement piece, you know, we launched several ranking improvements in Q4 on Facebook and Instagram that drove incremental engagement. And there isn't really one single launch, you know, that is driving most of the gains. It's, you know, multiple optimizations to our recommendation systems that are helping us make more accurate predictions about what will be interesting to each person.\n\nAnd I talked a little bit about some of these, the specific instantiations on both Facebook and on Instagram. And we see, you know, a lot of headroom to improve recommendations in 2026, which we expect will drive additional engagement growth on both apps. First, we plan on to continue scaling up our models and increase the amount of data we use, including a longer history of content interactions. To further improve the overall quality of recommendations. We're also going to start validating the use of ads signals and organic content recommendations as we continue to work towards having a more shared platform for organic and ads recommendations over time.\n\nSecond, we're going to continue to make recommendations even more to what a person is engaging with during their session so the recommendations we surface are more relevant to what they're interested in at that moment. And finally, we will work on more deeply incorporating LLMs into our existing recommendation systems given their capability to more deeply understand content. And so this will, I think, in particular, be useful for content that has been more recently posted since there's engagement data to base recommendations off of. On the ad side, again, we have we've talked about a lot of the sort of model work in the ads world across Andromeda and Lattice and GEM.\n\nI'll touch maybe specifically on GEM in Q4. We extended GEM to cover Facebook reels. Now it covers all major surfaces across Facebook and Instagram. We also doubled the size of the GPU cluster we used to train it. In 2026, we're expecting to meaningfully scale up GEM training to an even larger cluster, increasing the complexity of the model, expanding the data that we train it on, leveraging new sequence learning architecture that we had begun deploying in Q4. And we're also going to further how we transfer the learnings from our GEM foundation models to the runtime models that we're using.\n\nSo, you know, there is a lot more headroom, I think, across many, many components of the stack. This is the first time we have found a recommendation model architecture that can scale with similar efficiency as LLMs. And, you know, we're hoping that this will unlock the ability for us to significantly scale up the size of our ranking models while preserving an attractive ROI.\n\nKrista: Your next question will come from the line of Kenneth Gawrelski with Wells Fargo. Please go ahead.\n\nKenneth Gawrelski: Thank you very much. Two, if I may, please. First, for Mark, how critical is it for Meta to have a leading general-purpose model or is there a sufficient capability in a model that really excels at specific use cases? Maybe similar to what you see at Anthropic in coding today. If we'd love to if you could opine on that. And then second, maybe I just want to push again maybe on this last question, Susan. On the visibility you have. You talked about the improvements you're making in '26 on the models. The fine-tuning of the core, both in engagement and ad relevance. Could you talk about are you seeing any signs of diminishing returns to those investments?\n\nAnd do you think do you have visibility beyond '26 into further opportunities there? Thank you.\n\nMark Elliot Zuckerberg: I think the question was around how important is it for us to have a general model. You know, the way that I think about Meta is we're a deep technology company. Some people think about us as we build these apps and experiences, but the thing that allows us to build all these things is that we build and control the underlying technology that allows us to integrate and design the experiences that we want and not just be constrained to what others in the ecosystem are building or us to build.\n\nSo I think that this is a really fundamental thing where my guess is that Frontier AI for many reasons, some competitive, some safety-oriented, are not going to always be available through an API to everyone. So I think, like, it's very important, I think, to be able to have the capability to build the experiences that you want if you want to be one of the major companies in the world that helps to shape the future of these products.\n\nSo that, I think, is it's going to be, I think, important from a business perspective, and I think it's just important from, like, a creative and mission perspective to be able to actually design and build the experiences that we believe that we should be building for people. But yeah, I mean, I think it's quite important. Otherwise, we wouldn't be so focused on this. We're clearly extremely focused on this.\n\nSusan Li: On your second question, you know, interestingly, a year ago on this call, I think I talked about the set of investments we were making in 2025. As part of our 2025 budgeting process. Across our ads performance and organic engagement initiatives. You know, and those investments low the levels in Q1, for a few reasons. First, we would expect that currency tailwinds will dissipate later in the year based on current rates. Second, we'll be lapping stronger periods of growth later in the year that benefited from our 2025 ad performance investments and the strong macro landscape.\n\nAnd finally, we expect there could be some headwinds from our introduction of the revised, less personalized ads offering in the EU that begins rolling out later in Q1. But, again, similar to '25, we feel good about the process by which we identified opportunities with attractive ROIs and funded them as part of our budget to support, you know, key initiatives across our ranking and recommendation systems and to increase the capacity efficiency of our models, all of which are key to sort of driving growth for us.\n\nKrista: Your next question comes from the line of Mark Stephen Mahaney with Evercore. Please go ahead.\n\nMark Stephen Mahaney: Okay. Two questions, please. Meta AI, any update on what you're seeing there in terms of engagement and usage and do you think you're just starting to be able to apply improvements to that specific functionality? And then just real quickly on share repurchase Susan, I don't think you bought any stock back in the quarter. It's been a while, maybe a year since you haven't bought anything back. You talked about capital allocation a little bit into the year. It didn't sound like you're going to be buying back stock anytime soon, but just do want to clarify that. Thanks a lot.\n\nSusan Li: Yes. I'm happy to take both of those. So Meta AI, the quick update there is, you know, it's now available in over 200 markets. The largest daily active user markets for Meta AI align with our app where aligned with where our app are also very popular, though the apps people engage most with Meta AI differ, in some places, you know, it's primarily WhatsApp driven, for example. India or Indonesia. In the US, Facebook is a stronger driver of engagement. And in general, we see a lot of opportunity to make it easier for people to accomplish the task that they already come to our services for every day.\n\nAnd if we do that well, then the way people use our products will continue to expand. So we're focused on making Meta AI the most personalized assistant while tapping into the vast amount of, you know, information, trends, content from our platform to offer differentiated insights. And think we have a very strong track record in building highly personalized experiences and we're bringing just other uses of cache. Great.\n\nKenneth J. Dorell: Think we will wrap it here. Thank you everyone for joining us today. We look forward to speaking with you again soon.\n\nKrista: This concludes today's conference call. Thank you for your participation and you may now disconnect."
  },
  {
    "source": "Investing.com",
    "company": "Meta AI",
    "title": "Earnings call transcript: Meta Platforms beats expectations with strong Q4 2025 By Investing.com",
    "date": "2026-02-03T10:50:32Z",
    "url": "https://www.investing.com/news/transcripts/earnings-call-transcript-meta-platforms-beats-expectations-with-strong-q4-2025-93CH-4481034",
    "content": "Meta Platforms Inc. reported robust financial results for Q4 2025, exceeding market expectations. The company posted an earnings per share (EPS) of $8.88, surpassing the forecasted $8.19, reflecting a significant earnings surprise of 8.42%. Revenue reached $59.9 billion, outperforming the anticipated $58.35 billion. Following the announcement, Meta's stock surged by 9.71% in after-hours trading, closing at $738.31, up from $672.97.\n\nKey Takeaways\n\n* Meta reported a 24% year-over-year increase in total revenue for Q4 2025.\n\n* The company's EPS exceeded forecasts by 8.42%.\n\n* Meta's stock price increased by 9.71% in after-hours trading.\n\n* Strong growth was observed in ad revenue and user engagement metrics.\n\n* The company continues to invest heavily in AI and infrastructure.\n\nCompany Performance\n\nMeta Platforms demonstrated strong financial performance in Q4 2025, with revenue and net income showing substantial year-over-year growth. The company's strategic focus on AI and infrastructure investments appears to be paying off, as evidenced by increased user engagement and ad revenue. Meta's performance outpaced many of its industry peers, reflecting its competitive edge in AI research and digital advertising.\n\nFinancial Highlights\n\n* Revenue: $59.9 billion, up 24% year-over-year\n\n* Earnings per share: $8.88, up from previous forecasts\n\n* Operating income: $24.7 billion, with a 41% operating margin\n\n* Net income: $22.8 billion\n\n* Free cash flow: $14.1 billion\n\n* Cash and marketable securities: $81.6 billion\n\nEarnings vs. Forecast\n\nMeta's EPS of $8.88 significantly exceeded the forecasted $8.19, resulting in an 8.42% earnings surprise. The company also outperformed revenue expectations, bringing in $59.9 billion compared to the anticipated $58.35 billion. This positive earnings surprise reflects Meta's strong operational execution and market positioning.\n\nMarket Reaction\n\nFollowing the earnings announcement, Meta's stock price rose by 9.71% in after-hours trading, closing at $738.31. This reaction indicates strong investor confidence in Meta's growth prospects and operational strategy. The stock's movement is notable within its 52-week range, which saw a high of $796.25 and a low of $479.8.\n\nOutlook & Guidance\n\nLooking ahead, Meta expects Q1 2026 revenue to range between $53.5 billion and $56.5 billion. The company projects full-year 2026 expenses to be between $162 billion and $169 billion, with capital expenditures estimated at $115 billion to $135 billion. Meta plans to continue its focus on AI infrastructure and talent acquisition, anticipating significant changes in work processes driven by AI advancements.\n\nExecutive Commentary\n\nMark Zuckerberg, CEO, emphasized the transformative potential of AI, stating, \"We're starting to see agents really work. This will unlock the ability to build completely new products and transform how we work.\" CFO Susan Li highlighted the company's commitment to scaling AI capabilities, noting, \"We expect to meaningfully scale up GEM training to an even larger cluster.\"\n\nRisks and Challenges\n\n* Supply chain constraints could impact future growth.\n\n* Market saturation in certain regions may limit user growth.\n\n* Increased regulatory scrutiny could pose operational challenges.\n\n* Rising competition in the digital ad space may pressure margins.\n\n* Economic downturns could affect advertising budgets.\n\nQ&A\n\nDuring the earnings call, analysts inquired about Meta's AI model development strategy and potential revenue streams beyond advertising. The company addressed concerns about compute capacity constraints and outlined improvements in ranking and recommendation systems. These discussions highlighted Meta's strategic focus on innovation and diversification.\n\nFull transcript - Meta Platforms Inc (META) Q4 2025:\n\nKrista, Conference Operator: Good afternoon. My name is Krista, and I will be your conference operator today. At this time, I would like to welcome everyone to the Meta Fourth Quarter and Full Year 2025 Earnings Conference Call. All lines have been placed on mute to prevent any background noise. After the speaker's remarks, there will be an opportunity to ask questions. If you would like to ask a question, please press star one on your telephone keypad. To withdraw your question, again, press star one. We ask that you limit yourself to one question, and this call will be recorded. Thank you very much. Kenneth Dorell, Meta's Director of Investor Relations, you may begin.\n\nKenneth Dorell, Director of Investor Relations, Meta: Thank you. Good afternoon, and welcome to Meta Platforms' Fourth Quarter and Full Year 2025 Earnings Conference Call. Joining me today to discuss our results are Mark Zuckerberg, CEO, and Susan Li, CFO. Our remarks today will include forward-looking statements, which are based on assumptions as of today. Actual results may differ materially as a result of various factors, including those set forth in today's earnings press release and in our quarterly report on Form 10-Q filed with the SEC. We undertake no obligation to update any forward-looking statement. During this call, we will present both GAAP and certain non-GAAP financial measures. A reconciliation of GAAP to non-GAAP measures is included in today's earnings press release. The earnings press release and an accompanying investor presentation are available on our website at investor.meta.com. And now I'd like to turn the call over to Mark.\n\nMark Zuckerberg, CEO, Meta: All right. Hey, everyone. Thanks for joining us. We ended 2025 strong, with more than 3.5 billion people now using at least one of our apps every day. That includes more than 2 billion daily actives each on Facebook and WhatsApp, and just shy of that on Instagram. Our business also performed very well, thanks to record-breaking holiday demand and AI-driven performance gains. We are now seeing a major AI acceleration. I expect 2026 to be a year where this wave accelerates even further on several fronts. We're starting to see agents really work. This will unlock the ability to build completely new products and transform how we work. In 2025, we rebuilt the foundations of our AI program. Over the coming months, we're going to start shipping our new models and products.\n\nI expect our first models will be good, but more importantly, we'll show the rapid trajectory that we're on. And then I expect us to steadily push the frontier over the course of the year as we continue to release new models. I'm very excited about the products that we're building. Our vision is building personal superintelligence. We're starting to see the promise of AI that understands our personal context, including our history, our interests, our content, and our relationships. A lot of what makes agents valuable is the unique context that they can see, and we believe that Meta will be able to provide a uniquely personal experience. We're also working on merging LLMs with the recommendation systems that power Facebook, Instagram, Threads, and our ad system.\n\nOur world-class recommendation systems are already driving meaningful growth across our apps and ads business, but we think that the current systems are primitive compared to what will be possible soon. Today, our systems help people stay in touch with friends, understand the world, and find interesting and entertaining content. But soon, we'll be able to understand people's unique personal goals and tailor feeds to show each person content that helps them improve their lives in the ways that they want. This also has implications for commerce. Our ads today help businesses find just the right, very specific people who are interested in their products. New agentic shopping tools will allow people to find just the right, very specific set of products from the businesses in our catalog.\n\nWe're focused on making these experiences work across both our feeds and across business messaging, significantly increasing the capabilities of WhatsApp over time. New kinds of content will soon be possible as well. People want to express themselves and experience the world in the most immersive and interactive ways possible. We started with text and then moved to photos when we got phones with cameras and then moved to video when mobile networks got fast enough. Soon, we'll see an explosion of new media formats that are more immersive and interactive and only possible because of advances in AI. Our feeds will become more interactive overall. Today, our apps feel like algorithms that recommend content. Soon, you'll open our apps, and you'll have an AI that understands you and also happens to be able to show you great content or even generate great personalized content for you.\n\nGlasses are the ultimate incarnation of this vision. They're going to be able to see what you see, hear what you hear, talk to you, and help you as you go about your day, and even show you information or generate custom UI, right there in your vision. Sales of our glasses more than tripled last year, and we think that they're some of the fastest-growing consumer electronics in history. Billions of people wear glasses or contacts for vision correction, and I think that we're at a moment similar to when smartphones arrived, and it was clearly only a matter of time until all those flip phones became smartphones. It's hard to imagine a world in several years where most glasses that people wear aren't AI glasses.\n\nFor Reality Labs, we are directing most of our investment towards glasses and wearables going forward, while focusing on making Horizon a massive success on mobile and making VR a profitable ecosystem over the coming years. I expect Reality Labs losses this year to be similar to last year, and this will likely be the peak as we start to gradually reduce our losses going forward while continuing to execute on our vision. As we plan for the future, we will continue to invest very significantly in infrastructure to train leading models and deliver personal superintelligence to billions of people and businesses around the world. I recently announced Meta Compute with the belief that being the most efficient at how we engineer, invest, and partner to build our infrastructure will become a strategic advantage.\n\nDina Powell McCormick also joined us as President and Vice Chairman, and she will lead our efforts to partner with governments, sovereigns, and strategic capital partners to expand our long-term capacity, including ensuring positive economic impact in the communities that we operate in around the world. An important part of Meta Compute will be making long-term investments in silicon and energy. We will continue working with key partners while advancing our own silicon program. We're architecting our systems that we can be flexible in the systems that we use, and we expect the cost per gigawatt to decrease significantly over time through optimizing both our technology and supply chain. The last thing that I wanna mention is that I think that 2026 is going to be the year that AI starts to dramatically change the way that we work.\n\nAs we navigate this, our North Star is building the best place for individuals to make a massive impact. So to do this, we're investing in AI-native tooling, so individuals at Meta can get more done. We're elevating individual contributors and flattening teams. We're starting to see projects that used to require big teams now be accomplished by a single, very talented person. I wanna make sure that as many of these very talented people as possible choose Meta as the place that they can make the greatest impact, to deliver personalized products to billions of people around the world. And if we do this, then I think that we're gonna get a lot more done, and I think it's gonna be a lot more fun. All right, that's everything I wanted to cover.\n\nThis is gonna be a big year for delivering personal superintelligence, accelerating our business, building infrastructure for the future, and shaping how our company will work going forward. As always, I am grateful for all the hard work of our teams and to all of you for being on this journey with us. Now, here's Susan.\n\nSusan Li, CFO, Meta: Thanks, Mark, and good afternoon, everyone. Let's begin with our segment results. All comparisons are on a year-over-year basis, unless otherwise noted. Our community across the Family of Apps continues to grow, and we estimate more than 3.5 billion people used at least one of our Family of Apps on a daily basis in December. Q4 total Family of Apps revenue was $58.9 billion, up 25% year-over-year. Q4 Family of Apps ad revenue was $58.1 billion, up 24% or 23% on a constant currency basis. In Q4, the total number of ad impressions served across our services increased 18%. Impression growth was healthy across all regions, driven primarily by engagement and user growth and, to a lesser degree, ad load optimizations.\n\nThe average price per ad increased 6% year-over-year, benefiting from increased advertiser demand, largely driven by improved ad performance. Family of Apps other revenue was $801 million, up 54%, driven by WhatsApp paid messaging revenue growth, as well as Meta Verified subscriptions. Within our Reality Labs segment, Q4 revenue was $955 million, down 12% year-over-year. As we noted on the last call, the year-over-year decline in Reality Labs revenue is due to us lapping the introduction of Quest 3S in Q4 of 2024, as well as retail partners procuring Quest headsets during the third quarter of 2025 to prepare for the holiday season, which was recorded as revenue in Q3. Moving now to our consolidated results.\n\nQ4 total revenue was $59.9 billion, up 24% or 23% on a constant currency basis. Q4 total expenses were $35.1 billion, up 40% compared to last year. Year-over-year growth was driven primarily by employee compensation expenses, legal expenses, and infrastructure costs. Growth in employee compensation expenses reflects the technical hires we've added this year, particularly AI talent. Legal expense growth was due to both lapping legal accrual reversals in Q4 of 2024 and charges recorded in Q4 2025. Infrastructure expense growth was driven by higher depreciation, cloud spend, and other operating expenses. We ended Q4 with over 78,800 employees, up 6% year-over-year, driven by hiring in priority areas of monetization, infrastructure, Meta Superintelligence Labs, as well as regulation and compliance.\n\nFourth quarter operating income was $24.7 billion, representing a 41% operating margin. Q4 interest and other income was $609 million, driven primarily by unrealized gains on our equity investments. Our tax rate for the quarter was 10%, slightly lower than our outlook of 12%-15% due to the settlement of matters with tax authorities. Net income was $22.8 billion or $8.88 per share. Capital expenditures, including principal payments on finance leases, were $22.1 billion, driven by investments in data centers, servers, and network infrastructure. Free cash flow was $14.1 billion. We ended the quarter with $81.6 billion in cash and marketable securities and $58.7 billion in debt. Turning now to the business performance.\n\nThere are two primary factors that drive our revenue performance: our ability to deliver engaging experiences for our community and our effectiveness at monetizing that engagement over time. On the first, we're continuing to drive incremental engagement from ranking and product improvements. Instagram Reels had another strong quarter, with watch time up more than 30% year-over-year in the U.S. Engagement is benefiting from several optimizations we made to improve the quality of recommendations, including simplifying our ranking architecture to enable more efficient model scaling. This unlocked the ability for our systems to consider longer interaction histories to better identify a person's interests. On Facebook, video time continued to grow double digits year-over-year in the U.S., and we're seeing strong results from our ranking and product efforts on both feed and video surfaces.\n\nThe optimizations we made in Q4 drove a 7% lift in views of organic feed and video posts on Facebook, resulting in the largest quarterly revenue impact from Facebook product launches in the past two years. We're continuing to increase the freshness and originality of content recommendations as well. On Facebook, our systems are surfacing over 25% more Reels published that day than the prior quarter. On Instagram, we grew the prevalence of original content in the U.S. by 10 percentage points in Q4, with 75% of recommendations now coming from original posts. Threads is also seeing strong momentum, again, benefiting from recommendation improvements. The optimizations we made in Q4 drove a 20% lift in Threads time spent. Turning to 2026, we see a lot of opportunity to drive additional gains.\n\nThis includes scaling the complexity and amount of training data we use in our models, while continuing to make our systems more responsive to people's real-time interests. We're also focused on incorporating LLMs to understand content more deeply across our platform, which will enable more personalized recommendations. Another big area of investment this year is developing the next generation of our recommendation systems. We have several big bets on this front, including building new model architectures from the ground up that will work on top of LLMs, leveraging the world knowledge and reasoning capabilities of an LLM to better infer people's interests. Beyond improvements to our recommendation systems, we expect to use the models developed by Meta Superintelligence Labs to deliver compelling and differentiated AI products. One area we're already seeing promise is with AI dubbing of videos into local languages.\n\nWe are now supporting nine different languages, with hundreds of millions of people watching AI-translated videos every day. This is already driving incremental time spent on Instagram, and we plan to launch support for more languages over the course of this year. We are also seeing strong traction with our media creation tools. Nearly 10% of the Reels people view each day are now created in our Edits app, almost tripling from last quarter. Within Meta AI, the number of daily actives generating media tripled year-over-year in Q4. This year, we expect to advance the capabilities of our underlying media generation models and ship new features to further enhance the product experience. Another area we're focused on for Meta AI is personalization. We're seeing in our early testing that personalized responses drive higher levels of engagement, and we expect to significantly advance the personalization of Meta AI this year.\n\nThis dovetails with our investments in content understanding, which will enable our systems to develop a deeper understanding of each person's interests and preference, preferences, while also identifying the most relevant content across our platform to pull into responses. Turning to the second driver of our revenue performance, increasing monetization efficiency. The first part of this work is optimizing the level of ads within organic engagement. Here, our focus remains on tuning our systems to identify the right time and place to deliver ads. In some cases, this enables us to grow the overall level of ad load while preserving the user experience. However, an increasingly important part of this work is finding opportunities to drive incremental conversions within the same overall level of ad load by determining when a person is more interested in seeing an ad.\n\nIn fact, in the second half of 2025, our initiatives on Facebook to redistribute ads across users and sessions delivered a nearly four times larger revenue impact than Facebook ad load increases. We also continue to make progress on bringing ads to our newer services. Within Threads, we're beginning to expand ads to all remaining countries this month, including the U.K., European Union, and Brazil. On WhatsApp, we expect to complete the rollout of ads in Status throughout the year, with the level of ads remaining low in the near term while we follow our standard approach of optimizing ad formats and performance before ramping inventory. Moving to the second part of increasing monetization efficiency, improving performance for the businesses who use our tools. We're seeing very strong results from the ad performance investments we made throughout 2025, with year-over-year conversion growth accelerating through the fourth quarter.\n\nWe expect the set of investments we're making in 2026 will enable us to drive further gains as we continue to integrate AI across all layers of the marketing and customer engagement funnel. The first area is our ad system, where we're continuing to scale the complexity and size of our models to better select which ads to show. In Q4, we doubled the number of GPUs we use to train our GEM model for ads ranking. We also adopted a new sequence learning model architecture, which is capable of using longer sequences of user behavior and processing much richer information about each piece of content. The GEM and sequence learning improvements together drove a 3.5% lift in ad clicks on Facebook, and a more than 1% gain in conversions on Instagram in Q4.\n\nThis new sequence learning architecture is significantly more efficient than our prior architectures, which should enable us to further scale up the data, complexity, and compute we use in our future ranking models to deliver performance gains. As we scale up our foundational ads models like GEM, we are also developing more advanced models to use downstream of them at runtime for ads inference. In Q4, we launched a new runtime model across Instagram Feed, Stories, and Reels, resulting in a 3% increase in conversion rates in Q4. We continue to progress on our model unification efforts under Lattice as well. After seeing strong success with the consolidation of Facebook Feed and video models in the first half of 2025, in Q4, we consolidated models for Facebook Stories and other surfaces into the overall Facebook model.\n\nThis, along with a series of back-end improvements, drove a 12% increase in ads quality. In 2026, we expect to consolidate more models than we had in the prior two years as we continue to evolve our systems towards running a smaller number of highly capable models. Moving to the next area, ads products. We continue investing in ways to help businesses leverage AI to reduce the friction of setting up and optimizing an ad campaign. In Q4, we started testing our Meta AI business assistant with advertisers, which helps with tasks like campaign optimization and account support. In the coming months, we'll make it available to more advertisers, so each business has an AI assistant they can chat with that remembers their business's goals and provides personalized recommendations on how to improve performance. Another area we're deploying AI to improve performance is ad creative.\n\nThe combined revenue run rate of video generation tools hit $10 billion in Q4, with quarter-over-quarter growth outpacing the increase in overall ads revenue by nearly three times. We are also seeing very good results from our incremental attribution feature, which optimizes for incremental conversions in real time. Our latest model rollout in Q4 is driving a 24% increase in incremental conversions versus our standard attribution model, and this product has already achieved a multi-billion dollar annual run rate just seven months since launching. The last area of our monetization work I'll cover is business messaging, where we're seeing strong momentum across our portfolio of solutions.\n\nClick-to-Message ads revenue growth accelerated in Q4, with the U.S. up more than 50% year-over-year, driven by strong adoption of our website-to-message ads, which direct people to a business's website for more information before choosing to launch a chat. Paid messaging within WhatsApp continues to scale as well, crossing a $2 billion annual run rate in Q4. Finally, we're seeing good early traction with our business AIs in Mexico and the Philippines, with over one million weekly conversations between people and business AIs now happening on our messaging platforms. This year, we will expand availability of our business AIs to more markets, while also extending their capabilities, so they not only answer questions on topics like product availability, but can help people get things done right within WhatsApp.\n\nWe speak a lot about how AI is improving our products, but I'd like to take a moment to give an update on how it's changing the way we work. Mark mentioned our focus on making Meta a place where individuals can have significant impact. A big focus of this is to enable the adoption and advancement of our AI coding tools, where we're seeing strong momentum. Since the beginning of 2025, we've seen a 30% increase in output per engineer, with the majority of that growth coming from the adoption of agentic coding, which saw a big jump in Q4. We're seeing even stronger gains with power users of AI coding tools, whose output has increased 80% year-over-year. We expect this growth to accelerate through the next half. Next, I would like to discuss our approach to capital allocation.\n\nWe have significant opportunities to improve our core business in 2026. We plan to continue to prioritize investing in the business to support these opportunities, while also positioning us for an entirely new and exciting product cycle over the coming years, powered by our AI models. Procuring sufficient infrastructure capacity is central to these initiatives, and we're working to meet our silicon needs by deploying a variety of chips that optimally support each of our different workloads. To that end, in Q4, we extended our Andromeda ads retrieval engine, so it can now run on NVIDIA, AMD, and MTIA. This, along with model innovations, enabled us to nearly triple Andromeda's compute efficiency. In Q1, we will extend our MTIA program to support our core ranking and recommendation training workloads, in addition to the inference workloads it currently runs.\n\nMore broadly, as we invest in infrastructure to meet our business needs, we continue to prioritize maintaining long-term flexibility, so we can adapt to how the market develops. We're doing so in several ways, including changing how we develop data center sites, establishing strategic partnerships, contracting cloud capacity, and establishing new ownership structures for some of our large data center sites. We have a strong net cash balance and expect our business will continue to generate sufficient cash to fund our infrastructure investments in 2026, which is reflected in our expectations. Nonetheless, we will continue to look for opportunities to periodically supplement our strong operating cash flow with prudent amounts of cost-efficient external financing, which may lead us to eventually maintain a positive net debt balance.\n\nMoving to our financial outlook, we expect our first quarter 2026 total revenue to be in the range of $53.5 billion-$56.5 billion. Our guidance assumes foreign currency is an approximately 4% tailwind to year-over-year total revenue growth based on current exchange rates. Turning to the expense and CapEx outlooks, we expect full year 2026 total expenses to be in the range of $162 billion-$169 billion. The majority of expense growth will be driven by infrastructure costs, which includes third-party cloud spend, higher depreciation, and higher infrastructure operating expenses. The second-largest contributor to total expense growth is employee compensation, driven by investments in technical talent. This includes 2026 hires to support our priority areas, particularly AI, as well as a full year of expenses from 2025 hires.\n\nAt a segment level, we expect expense growth to be driven by the Family of Apps, with Reality Labs' operating losses remaining similar to 2025 levels. We anticipate 2026 capital expenditures, including principal payments on finance leases, to be in the range of $115 billion-$135 billion, with year-over-year growth driven by increased investment to support our Meta Superintelligence Labs efforts and core business. Despite the meaningful step up in infrastructure investment, in 2026, we expect to deliver operating income that is above 2025 operating income. Absent any changes to our tax landscape, we expect our full year 2026 tax rate to be 13%-16%. Finally, we recently aligned with the European Commission on further changes to our less personalized ads offering, which we will begin rolling out this quarter.\n\nHowever, we continue to monitor legal and regulatory headwinds in the E.U. and the U.S. that could significantly impact our business and financial results. For example, we continue to see scrutiny on youth-related issues and have a number of trials scheduled for this year in the U.S., which may ultimately result in a material loss. In closing, 2025 was another strong year for our company. The investments we've made to improve our business are continuing to drive strong growth, and we have an exciting roadmap this year to deliver new experiences and services for our global community. As always, thank you to our teams for their hard work and commitment to our mission. With that, Krista, let's open up the call for questions.\n\nKrista, Conference Operator: Thank you. We will now open the lines for a question-and-answer session. To ask a question, please press star one on your touchtone phone. To withdraw your question, again, press star one. Please limit yourself to one question. Please pick up your handset before asking your question to ensure clarity. If you are streaming today's call, please mute your computer speakers. Your first question comes from the line of Brian Nowak with Morgan Stanley. Please go ahead.\n\nBrian Nowak, Analyst, Morgan Stanley: Thanks for taking my questions. I've one for Mark, one for Susan. Mark, one is a long-term question. As you think about ramping all this investment and the personal intelligence opportunity, the Meta Compute opportunity, can you walk us through a little bit how you think about the largest revenue or ROIC long-term opportunities you're trying to unlock with those over the next, call it three, five, 10 years through all the investment? And then, Susan, a little more near term, more like 2026. I think the guide is the fastest growth you've had in almost five years.\n\nI know you have a lot of improvements on recommendations and in monetization efficiency, but can you just sort of help us a little bit understand two or three of the biggest drivers of this inflection you're seeing on revenue in 2026?\n\nMark Zuckerberg, CEO, Meta: Yeah, I guess I can start with the first one. Although I have to say up front that I think my answers to a lot of your questions on this particular call may be somewhat unfulfilling because we're in this interesting period where we've been rebuilding our AI effort, and we're six months into that, and I'm happy with how it's going. But we are going to be rolling out our initial set of models and products and businesses around that over the coming months, and I will have a lot more to share on all of those fronts at that point.\n\nSo I'm happy to offer kind of a high-level view of some of the stuff, but, but I apologize in advance that not much of this is going to be particularly detailed, but it will be exciting as we roll it out. I think the theme on the business, I mean, this is... I don't think I'm going to break any new ground here, but you know, there are several major business opportunities that we're focused on. You know, I think that one is just going to be improving the core products and accelerating the core, the current business. I talked about that in terms of the connecting of the recommendation systems and the LLMs, which I think will both improve the quality of the organic experience and of advertising.\n\nWe're going to see the generation of media improve the quality of content, which, coupled with the improvements in the recommendation systems, we expect to generally accelerate the quality and effectiveness of the core business, both for people who use it organically and for businesses. So I think that will have a compounding effect. And then, there's gonna be, you know, several many, I think, new business opportunities that come up. I mean, we have been working on Meta AI for a while. I think you're starting to see some of the way that products like that get monetized across the industry. When we get that to a scale and depth that we want, we think that there are gonna be opportunities both in terms of subscriptions and advertising and all of the different things that you see on that.\n\nI mean, yeah, I think, you know, there's a number of things on shopping and commerce that I'm quite excited about that I alluded to in the comments up front. And as the models launch and we demonstrate some of the capabilities, both in the first set of models and over the year, I think the models are gonna get a lot better too. We'll be able to have different products paired with those that I think will facilitate different businesses for, you know, businesses who use us and our platforms, as well as direct-to-consumer businesses. I guess it's probably also worth flagging because I don't think we, either of us, mentioned the Manus acquisition in the upfront comments.\n\nI think that is going to be a good example of a significant number of businesses that already pay a subscription to basically use their tool to accelerate their business results. Integrating that kind of thing into our Ads and Business Manager, so that way we can just offer more integrated solutions for the, you know, many, many millions of businesses that use and rely on our platforms, is gonna be really powerful, both for accelerating their results using the existing products that we have, and I think adding new lines as well. You know, a somewhat high-level answer, and I think the picture will become clearer and I think more exciting if we do our jobs well over the course of the year.\n\nSusan Li, CFO, Meta: Brian, on your second question, there's obviously a range of outcomes captured in the Q1 2026 revenue outlook. It overall reflects our expectation for a strong quarter of growth. The range embeds an outlook for accelerated growth, and that's really underpinned by the strong demand that we saw through the end of Q4 and continuing into the start of 2026. Now, I will say we also expect foreign currency to be a 4-point benefit to year-over-year growth, so that is a three-point larger tailwind than it was in Q4 2025, as we lap the strengthening of the U.S. dollar a year ago. But overall, you know, we see that advertisers are responding to ad performance improvements that we made. They're driving strong conversion growth.\n\nWe've made a lot of these investments over the course of 2025, including advances to our ads ranking and delivery systems, the more effective redistribution of ad load, new features and ad products like Advantage+, better measurement, and just a lot of great work that has helped to drive the continued performance of our ads.\n\nKrista, Conference Operator: Your next question comes from the line of Eric Sheridan with Goldman Sachs. Please go ahead.\n\nEric Sheridan, Analyst, Goldman Sachs: Thanks so much for taking the question. Maybe two, if I could. In prior periods, you've talked about being capacity constrained internally and not having enough compute to sort of achieve the goals you have on a platform and a product standpoint. I wanna know if we get any update on currently how you think about your own internal needs for compute against that roadmap. And the second part of the question would be: as we continue to see the ads business sorta scale, especially in terms of dollar growth year-over-year, have we yet seen the full first order effects of scaling the business against applying more compute to it? Or how should investors think about the directional relationship between applying more compute and rate of change in terms of outcomes on the monetization side? Thank you.\n\nSusan Li, CFO, Meta: On your first question, we do continue to be capacity constrained. Our teams have done a great job ramping up our infrastructure through the course of 2025, but demands for compute resources across the company have increased even faster than our supply. So we expect over the course of 2026 to have significantly more capacity this year as we add cloud. But we'll likely still be constrained through much of 2026 until additional capacity from our own facilities comes online later in the year. With that said, I think, you know, we have done a good job internally mitigating the impact of compute constraints on our business. I expect that will continue to be the case in 2026.\n\nWe're continuing to focus on increasing our infrastructure efficiency in several ways, including by optimizing workloads, improving infrastructure utilization, diversifying our chip supply, and just investing in efficiency improvements as part of our core technology development efforts in areas like content and ads ranking. So that was your first question. The second question about how the ads business scales. I think I don't have an extremely precise answer to this question. What I'd say is, you know, one of the ways that we are working to drive ads performance improvements is by improving our larger scale models, along with our lighter weight ones that we use for ads inference at runtime. You know, we don't typically use our larger model architectures, like GEM for inference because their size and complexity would make it too cost prohibitive.\n\nSo the way that we drive performance from those models is by using them to transfer knowledge to smaller, lightweight models used at runtime. But I would say that we think that there is room for our larger models to benefit from having more compute. And I think as we scale up the compute available to those models, and the foundational models in different areas that power the different stages of ads ranking and recommendation, you know, we expect that we will see gains coming from that.\n\nKrista, Conference Operator: Your next question comes from the line of Mark Shmulik with Bernstein. Please go ahead.\n\nMark Shmulik, Analyst, Bernstein: Yes, thanks for taking the questions, too, if I may. Mark, kind of with your comments that you kind of expect to see some meaningful changes in how work and things are done this year, I guess, would you be surprised if kind of by the end of the year, we've yet to see meaningful progress and adoption on some of the newer products and initiatives that you're launching? Or, you know, should we just be a bit more patient on the timeline here? And then, Susan, kind of with the guidance provided on OI still expected to grow kind of faster this year than last year, you know, let's say in a few months, we realize we need more investment and resources to continue to go after the AI opportunity, but perhaps macro might be a bit weaker.\n\nHow hard of a line is there in terms of the tie of investment levels to core performance? Thank you.\n\nMark Zuckerberg, CEO, Meta: I think the first question was asking about kind of when do I expect the product impact to be? I mean, we're going to roll out new products over the course of the year. I think the important thing is we're not just launching one thing and we're building a lot of things. I think there -- like, AI is going to enable a lot of new experiences. I outlined thematically a bunch of these in the upfront comments around, you know, a personal AI, around, LLMs combining with the recommendation systems. I think that's a somewhat longer-term research project that I think will yield dividends over a long period of time, but we're already definitely seeing optimizations of the recommendation systems as we're including more of the AI research improvements and advances into that. The content is going to improve.\n\nThere are going to be new formats. There are going to be improvements on the glasses. There are all these different things, as well as several new things that we think are new, that we're going to try that are not just extensions of the current things that we're doing. Yeah, I mean, I would expect that we'll roll these out over the course of the year and that you know, sometimes it takes a few iterations for things to really hit and reach the kind of product market fit that you need.\n\nBut I think we have enough time, hopefully, to, you know, in the-- we're starting off early enough in the year, that I would expect that we'll see some successes by the end of the year on this, as well as on the work side. What we were talking about is I think it's very hard for anyone exactly to predict what the shape of, you know, how organizations working is going to feel. But it... I just think the fact that agents are really starting to work now is quite profound, and I think is going to allow-- we're already starting to see the people who adopt them are just being significantly more productive.\n\nThere's a big delta between the people who do it and do it well, and the people who don't. I think that's going to just be a very profound dynamic for, I think, across the whole sector and probably the whole economy going forward in terms of the productivity and efficiency with which we can run these companies, which I think, you know, my hope is that we can use that to just get a lot more done than we were able to before. I'm most focused on making sure that Meta is a great company to have a big impact, right?\n\nYou'll be able to use these kind of agentic tools anywhere, but you will only be able to come and ship things to billions of people if you join a company like Meta, and there aren't that many companies like Meta. So, I think if we make it so that we can harness these kind of tools, then I think that we should, over some period of time, start to see a real acceleration in the amount of output that we could have. Now, how to predict exactly the time frame for adopting that, somewhat hard, right? I'm not going to predict a specific quarter or something like that, but the trend seems like unmistakably like this is going to happen.\n\nAnd that, to me, is something that is very exciting and, like I said in my comments up front, also, like, honestly, kind of fun, right? I think it just makes it more fun to be able to build a lot of things. And, you know, that's what we're here to do.\n\nSusan Li, CFO, Meta: Mark, on your second question, I want to make sure and clarify something. So I think in the question, you had said that operating income growth in 2026 would be higher than 2025, and I want to make sure my comments were super clear. In 2026, we expect to deliver operating income above 2025 operating income. So this is comparing absolute dollars, not year-over-year growth. So to give some context on that, you know, we are going into 2026 with strong revenue growth at the start. Of course, we are just a few weeks in, set against, you know, a healthy macro backdrop, so obviously hard to extrapolate the current trends to the full year and, you know, there are many, many moving variables in the current landscape.\n\nWe're really taking advantage of the current business strength to reinvest a lot of the revenue into what we see as very attractive investment opportunities in AI infrastructure and talent. It's hard to assess, you know, what all of those investment opportunities will be over the course of the year as we continue to work through our capacity options. And of course, it remains a very competitive hiring market, but we'd like to invest aggressively where we can. We continue to use our framework that we shared at this point, several years ago, of growing consolidated operating profit over time to guide those investments. Based on where our plans are rolling up today, again, in 2026, we expect to deliver more operating income than we did in 2025.\n\nKrista, Conference Operator: Your next question comes from the line of Doug Anmuth with JPMorgan. Please go ahead.\n\nDoug Anmuth, Analyst, JPMorgan: Thanks so much for taking the questions. One for Mark and one for Susan. Mark, could you just provide more detail on the progress of the MSL team several months in, and more on your view on the path to a frontier model this year? And then, Susan, I know you expect to grow operating income in 2026. Do you also expect to have positive free cash flow? Just how should we think about the current and any future, JVs for data center and compute build-out? Thanks.\n\nMark Zuckerberg, CEO, Meta: ... I'm not sure I have anything else to add on the current progress on this. I mean, that's why I said up front that I think this is somewhat of an unfulfilling time to be answering some of these questions. We're about six months in to building MSL. I'm very pleased with the quality of the team. I think we have the most talent-dense research effort in the industry, and some of the early indicators look positive. But look, I think that this is gonna -- this is a long-term effort, right? We're not here to do this to ship, like, one model or one product. We're doing a lot of models over time and a lot of different products.\n\nAnd I wanna make sure that the work can speak for itself, and also that you know we all internalize that this is a journey that we're on, and the first set of things that we put out, I think, are gonna be more about showing the trajectory that we're on, rather than being a single moment in time. So, yeah, I'm quite optimistic, but don't have anything else, you know, particularly concrete to share.\n\nSusan Li, CFO, Meta: Doug, on the first part of your question, you know, we are making very significant investments in infrastructure capacity this year to support our AI efforts, and we believe we're in a strong position to support them with the cash generation of our business this year. And, you know, at the same time, we'll continue to explore different paths, as we build out our infrastructure capacity that help us provide, you know, that help provide us the long-term flexibility and option value that we look for as we support our future capacity needs against the backdrop of a very wide range of possible capacity demand over the years to come. So we don't have anything additional to announce at this point.\n\nYou know, we are looking at, you know, all of the different opportunities to stand up capacity across kind of the different time frames that we need them.\n\nKrista, Conference Operator: Your next question comes from the line of Justin Post with Bank of America. Please go ahead.\n\nJustin Post, Analyst, Bank of America: Great. A couple, maybe one for Mark and one for Susan. It just seems like you're gonna have a tremendous amount of capacity. How do you think about expanding your opportunities beyond ads? Things like subscriptions or licensing cloud models, just with all the interesting things you're building. I don't expect any product announcements, but can you do things beyond ads? And then for Susan, it's really interesting to see the acceleration, even ex-FX and advertising. I'm just wondering if you're seeing a general acceleration in e-commerce activity. Where do you think the dollars are coming from, and is the entire internet ecosystem accelerating? I'm just wondering your thoughts on that.\n\nMark Zuckerberg, CEO, Meta: So yes, we are focused on things beyond ads. I think the numbers make it so that for the next couple of years, ads are going to be by far the most important driver of growth in our business. So that's why as we're working on this, we have a balance of new things that we're trying to do, while also investing very heavily in making sure that all of the work that we're doing in AI improves both the quality and business performance of the core apps and businesses that we run there.\n\nYeah, I mean, we'll have more to share on that, but I mean, all these things, even if they scale very quickly, are going to take, you know, some time to be meaningful at the scale of what the ads business is. And while we're doing that, we're just very focused on also delivering more value to businesses and more quality in the apps that we run ads in.\n\nSusan Li, CFO, Meta: Justin, on your second question, we saw healthy year-over-year growth across all verticals in Q4, with the exception of politics as we lapped the U.S. presidential election last year. The online commerce vertical was the largest contributor to year-over-year growth. That was followed by professional services and technology. So in online commerce, year-over-year growth was strong. It was actually relatively consistent with Q3 levels, and that was broad-based across advertiser regions and sizes. In general, we saw that the demand leading up to the holiday shopping period that sustained through Cyber Five and into the end of the year, you know, was very healthy for us.\n\nProfessional services, in this category, we saw strong, broad-based growth with nice contributions from lead generation ads due to product improvements we've made, including from Advantage+ lead campaigns that we fully rolled out at the start of Q4. And, you know, the tech vertical continues to be strong for us, too. Again, broad-based across advertiser regions and sizes. So in general, I would say it was very healthy, broadly driven growth.\n\nKrista, Conference Operator: Your next question comes from the line of Ross Sandler with Barclays. Please go ahead.\n\nDoug Anmuth, Analyst, JPMorgan1: Yeah. Mark, you mentioned bringing Horizon Worlds into mobile. We haven't heard much from the Horizon Worlds squad on these calls, so interesting that that's making it in. It seems like the combo of AI and what you guys have built with Horizon might open up the door to a bunch of new potential areas in gaming or, you know, new forms of kind of communication. So could you just elaborate on what the plan is there? Thank you.\n\nMark Zuckerberg, CEO, Meta: Yeah. So let me talk about the basic theme here. You know, one core idea that I've talked about on some of these calls over the years is that people always want to express themselves and experience the world in whatever the richest format is that they can. So I talked about this upfront today. It's when we started, a lot of this was text, right? That was the kind of the best we could do. Then we all got phones. They had cameras. Like, a lot of this medium became visual, but with photos. We went through a period where the mobile networks were kind of weak, and every time you wanted to watch a video, it would buffer. And once that got worked out, now the majority of the content is video. And one of the core ideas that-...\n\nWe have had for a while, is that that is not the end of the line. All right, video will continue to be here for a long time. It's going to continue growing. It's not going anywhere, just like photos and text, in many ways, continue to grow even as the market continues to grow beyond that. But I don't think that video is the ultimate kind of final format. I just think that this is gonna get we're gonna get more formats that are more interactive and immersive, and you're gonna get them in your feeds.\n\nSo you can imagine this, I mean, there's obviously a lot of details to fill in on this, but you can imagine, you know, being able to, people being able to easily, through a prompt, create a world or create a game and, be able to share that with people who they care about, and you see it in your feed, and you can jump right into it, and you can engage in it. And there are 3D versions of that, and there are 2D versions of that, and Horizon, I think, fits very well with the kind of immersive 3D version of that. But there's definitely a version of the future where, you know, any video that you see, you can, like, tap on and jump into it and, like, engage and, like, and be, be kind of like experience it in a, in a more meaningful way.\n\nI think that the investments that we've done in both a lot of the virtual reality software and Horizon, as well as a number of other areas around the company, are actually going to pair well with these AI advances to be able to bring some of those experiences to hundreds of millions and billions of people through mobile. So anyway, that's the thing that I'm quite excited about, but it's, it's just sort of one flavor of a, of a theme that I think is going to be very interesting. I think there are going to be lots of different types of interactive and immersive content that become possible. I think Horizon is going to be one very interesting example that, that I'm quite excited to see how this unfolds.\n\nKrista, Conference Operator: Your next question comes from the line of Ron Josey with Citi. Please go ahead.\n\nDoug Anmuth, Analyst, JPMorgan0: Great. Thanks for taking the question. I wanted to drill down, maybe, Susan, on your comments around ranking recommendation model changes. You know, clearly, lots of tailwinds here, given the results from GEMs, Andromeda, Lattice, consolidation of models, et cetera. So can you help us understand a little bit more just about the roadmap and where we stand within ranking recommendation model changes? There, there's a thesis out there that maybe we're, you know, there's a limiting factor, or maybe we're waiting on newer models, but any insights there would be very helpful as we think about the next, as the future going forward. Thank you.\n\nSusan Li, CFO, Meta: Yeah, thanks for the question, Ron. I'm just sorting out if your question was more specific to ads or if it was more specific to kind of the engagement side, but I'll try to talk a little bit about both. So on the sort of core engagement piece, you know, we launched several ranking improvements in Q4 on Facebook and Instagram that drove incremental engagement, and there isn't really one single launch, you know, that is driving most of the gains. It's, you know, multiple optimizations to our recommendation systems that are helping us make more accurate predictions about what will be interesting to each person. And, you know, I talked a little bit about some of these, the specific instantiations on both Facebook and on Instagram.\n\nWe see, you know, a lot of headroom to improve recommendations in 2026, which we expect will drive additional engagement growth on both apps. First, we plan on to continue scaling up our models and increase the amount of data we use, including a longer history of content interactions, to further improve the overall quality of recommendations. We're also going to start validating the use of ad signals and organic content recommendations as we continue to work towards having a more shared platform for organic and ads recommendations over time. Second, we're gonna continue to make recommendations even more adaptive to what a person is engaging with during their session, so the recommendations we surface are more relevant to what they're interested in at that moment.\n\nFinally, we will work on more deeply incorporating LLMs into our existing recommendation systems, given their capability to more deeply understand content. And so this will, I think, in particular, be useful for content that has been more recently posted since there's less engagement data to base recommendations off of. On the ad side, again, we've talked about a lot of the sort of model work in the ads world across Andromeda and Lattice and GEM. I'll touch maybe specifically, you know, on GEM in Q4. We extended GEM to cover Facebook Reels. Now it covers all major surfaces across Facebook and Instagram. We also doubled the size of the GPU cluster we use to train it.\n\nIn 2026, we're expecting to meaningfully scale up GEM training to an even larger cluster, increasing the complexity of the model, expanding the data that we train it on, leveraging new sequence learning architecture that we had begun deploying in Q4. And we're also going to further improve how we transfer the learnings from our GEM foundation models to the runtime models that we're using. So, you know, there's a lot more headroom, I think, across many, many components of the stack. This is the first time we have found a recommendation model architecture that can scale with similar efficiency as LLMs, and, you know, we're hoping that this will unlock the ability for us to significantly scale up the size of our ranking models, you know, while preserving an attractive ROI.\n\nKrista, Conference Operator: Your next question will come from the line of Ken Gawrelski with Wells Fargo. Please go ahead.\n\nKen Gawrelski, Analyst, Wells Fargo: Thank you very much. Two, if I may, please. First, for Mark, how critical is it for Meta to have a leading general-purpose model, or is there a sufficient capability in a model that really excels at specific use cases, maybe similar to what you see at Anthropic in coding today? If you could opine on that. And then second, maybe, I just want to push again, maybe on this last question, Susan.\n\n... on the, the visibility you have, you talked about the improvements you're making in 2026 on the, on the models, the fine-tuning, of the core, both in engagement and, and ad relevance. Could you talk about, are you seeing any signs of diminishing returns to those investments? And, and do you think, do you have visibility beyond 2026 into, further opportunities there? Thank you.\n\nMark Zuckerberg, CEO, Meta: I, I think the question was around how important is it for us to have a general model? You know, the way that I think about Meta is we're a, like, a deep technology company. Some people think about us as we build these apps and experiences, but the thing that allows us to build all these things is that we build and control the underlying technology that allows us to integrate and design the experiences that we want, and not just be constrained to what others in the ecosystem are building or allow us to build. So, I, I think that this is a, a really fundamental thing, where my guess is that frontier AI, for many reasons, some competitive, some safety-oriented, are not going to always be available through an API to everyone.\n\nSo I think, like, it's very important, I think, to be able to have the capability to build the experiences that you want if you wanna to, to be one of the major companies in the world that helps to shape the future of these products. So that, that I think is... It's going to be, I think, important from a business perspective, and I think it's just important from, like, a creative and mission perspective, to be able to actually design and build the experiences that we believe that we should be building for people. But yeah, I mean, I think it's quite important, otherwise we wouldn't be so focused on this, where we're clearly extremely focused on this.\n\nSusan Li, CFO, Meta: You know, interestingly, a year ago on this call, I think I talked about the set of investments we were making in 2025, as part of our 2025 budgeting process across our ads performance and organic engagement initiatives. You know, and those, those investments have generally paid off, and we, you know, feel really good about, about kind of the, the process we ran in terms of using projected ROI to stack rank investments. Make sure that we, you know, had a robust measurement system, funded things that were positive ROI, and then tracking how they performed over the course of the year.\n\nAnd we are, you know, we've just finished running our 2026 budgeting process, and we have funded a similar set of investments, which we expect will enable us to continue delivering strong revenue growth in 2026. Having said that, you know, I expect both full year reported and constant currency revenue growth to be below the levels in Q1, for a few reasons. First, we would expect that currency tailwinds will dissipate later in the year based on current rates. Second, we'll be lapping stronger periods of growth later in the year that benefited from our 2025 ad performance investments and the strong macro landscape. And finally, we expect there could be some headwinds from our introduction of the revised, less personalized ads offering in the EU that begins rolling out later in Q1.\n\nBut again, similar to 2025, we feel good about the process by which we identified investment opportunities with attractive ROIs and funded them as part of our budget, to support, you know, key initiatives across our ranking and recommendation systems, and to increase the capacity efficiency of our models, all of which are key to sort of driving, to driving growth for us.\n\nKrista, Conference Operator: Your next question comes from the line of Mark Mahaney with Evercore. Please go ahead.\n\nMark Mahaney, Analyst, Evercore: Okay, two questions, please. Meta AI, any update on what you're seeing there in terms of engagement and usage? And do you think you're just starting to be able to apply improvements to that specific functionality? And then just real quickly on share repurchases, Susan, I don't think you bought any stock back in the quarter. It's been a while, maybe a year, since you haven't bought anything back. You talked about capital allocation a little bit into the year. It didn't sound like you're gonna be buying back stock anytime soon, but just do you want to clarify that? Thanks a lot.\n\nSusan Li, CFO, Meta: Yes, I'm happy to take both of those. So, Meta AI, the quick update there is, you know, it's now available in over 200 markets. The largest daily active user markets for Meta AI align with where our apps are also very popular. Though the apps people engage most with Meta AI differ, in some places, you know, it's primarily WhatsApp driven, for example, India or Indonesia. In the U.S., Facebook is a stronger driver of engagement. And in general, we see a lot of opportunity to make it easier for people to accomplish the tasks that they already come to our services for every day, and if we do that well, then the way people use our products will continue to expand.\n\nWe're focused on making Meta AI the most personalized assistant, while tapping into the vast amount of, you know, information, trends, content from our platform, to offer differentiated insights. We think we have a very strong track record in building highly personalized experiences, and we're bringing that into Meta AI so that we can tailor responses to each person's personal interests and preferences. On your second question, which is about share repurchases, you know, share repurchase levels will vary from time to time, for a lot of reasons, including whether we believe there are areas that have a greater near-term need for capital. Right now, we think the highest order priority for the company is investing our resources to position ourselves as a leader in AI.\n\nAnd so that is really that's kind of the first order use of capital, but we'll continue to be opportunistic and evaluate repurchases versus other uses of cash.\n\nKenneth Dorell, Director of Investor Relations, Meta: Great! I think we will wrap it here. Thank you everyone for joining us today. We look forward to speaking with you again soon.\n\nKrista, Conference Operator: This concludes today's conference call. Thank you for your participation, and you may now disconnect.\n\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C."
  },
  {
    "source": "Lexology",
    "company": "Meta AI",
    "title": "Meta AI Glasses Signal a Major Shift Toward Context-Aware Multimodal Systems and AI Patents",
    "date": "2026-02-04T15:23:35Z",
    "url": "https://www.lexology.com/library/detail.aspx?g=c378cb5e-fe3b-4f50-8c6f-9f9042dadc5c",
    "content": "Meta Platforms' latest update to its Meta AI Glasses marks a turning point in how we think about wearable AI, technology innovation, and AI Patents protection. The v21 software update makes these smart glasses more intelligent, context-aware, and practical, moving them from novelty gadgets to genuinely useful wearable assistants that adapt to your environment.\n\nThis matters not only to everyday users, who care about features, comfort, and price but also to innovators, tech developers, and patent professionals focused on protecting inventions in the field of Artificial Intelligence and wearable computing.\n\nWhat Are Meta AI Glasses?\n\nMeta AI Glasses are wearable smart glasses powered by on-device and cloud-connected Artificial Intelligence. Unlike bulky augmented reality (AR) headsets with screens or heavy hardware, these glasses look like regular eyewear and include:\n\nThey are designed to be worn all day, making AI assistance more natural and intuitive. Early models include the Ray-Ban Meta smart glasses and Oakley Meta HSTN glasses, which are part of Meta's growing wearable lineup.\n\nAcross reviews and announcements, Meta's AI glasses are often compared to other future wearables, including concepts from Lenovo and Razer that also focus on multimodal AI, voice controls, and real-time interactions.\n\nThe v21 Update: Conversation Focus and Contextual Music Playback\n\nIn December 2025, Meta began rolling out its v21 software update to Meta AI glasses, adding features that bring real-world intelligence directly onto your face.\n\nConversation Focus\n\nOne flagship addition is \"Conversation Focus,\" a feature that uses the glasses' multiple directional microphones and on-device AI to make a nearby speaker easier to hear in noisy places like cafés or urban transit. The system boosts the voice of who you're facing while reducing surrounding noise, all without blocking environmental sound entirely, meaning you still hear alerts, traffic, or companions.\n\nYou can control it by swiping the right arm of the glasses or adjusting settings in the companion app. This approach is different from typical noise cancellation found on earbuds and headphones because it creates a \"directional audio zone\" rather than silencing all background sound.\n\nContextual Music Playback\n\nAnother new experience is the first multimodal AI music integration with Spotify. By combining camera-based vision with user intent, Meta AI can now play music that matches what you're looking at. For example, if you gaze at a painting, a scene, or a holiday display and say, \"Hey Meta, play a song to match this view,\" the glasses will generate a playlist based on the surroundings and your listening preferences.\n\nThis fusion of vision + audio + personal context is a big step toward context-aware computing, where devices don't just react to commands but interpret environments and act proactively.\n\nMost AI devices like phones or smart speakers still operate in a reactive mode: you ask, and they respond. The new features in Meta AI Glasses show a shift toward anticipatory and environment-aware interactions:\n\nThis type of intelligent behavior is a core goal of ambient computing, where technology becomes more integrated and less intrusive in everyday tasks.\n\n2. On-Device Intelligence and Privacy\n\nOne notable aspect of these updates is the emphasis on on-device AI processing. By handling conversation filtering and contextual analysis locally instead of sending all data to the cloud Meta reduces latency and offers stronger privacy assurances. This matters as wearables contend with global privacy laws and increasing scrutiny over always-on sensors.\n\nOn-device processing also allows Meta AI Glasses to work where connectivity might be limited, making the feature set more reliable in diverse real-world scenarios.\n\n3. Expanding the Wearables Landscape\n\nWearables used to focus mostly on health tracking or notifications. Now, they're becoming ambient AI platforms that understand sound, visuals, gestures, and intent. Meta's smart glasses demonstrate that wearables can do much more than display information; they can interpret your environment and act on it.\n\nCompetitors and concepts from brands like Lenovo and Solos indicate a broader market shift toward AI-enhanced eyewear with multimodal capabilities, although Meta's ecosystem remains among the most developed today.\n\nWhat Consumers Care About\n\nHere are some common questions people search for, answering them based on the latest credible information:\n\nMeta AI Glasses Price\n\nMeta AI glasses pricing varies by model, region, and retailer. For example, earlier reporting suggested Meta Ray-Ban Gen 2 models launched at around $379, with advanced versions available at higher prices.\n\nUsers often compare prices across:\n\nPrices are influenced by features like audio quality, battery life, and AI integration.\n\nThese are natural growing pains for new hardware categories.\n\nMeta's advances in wearable AI highlight how critical AI Patents are becoming in protecting innovation in this space.\n\nWhy AI Patents Matter\n\nAs smart glasses evolve, the underlying technologies from real-time audio filtering to vision-based personalization become strategic assets. Companies invest heavily in patenting:\n\nThese patents prevent competitors from copying unique implementations and protect long-term investment in R&D.\n\nWhat Patent Professionals Should Watch\n\nPatent attorneys and innovators should pay attention to:\n\nWhen drafting protection strategies, practitioners will also look at keywords like AI patent search, AI patents by company, and sometimes even AI Patent example scenarios to map out competitive landscapes.\n\nGlobally, entities like the World Intellectual Property Organization track trends such as WIPO AI patents across regions. Knowing AI patents by country helps companies file strategically where they operate.\n\nEmerging tools including patent AI tools and AI patent generators assist in shaping claims and finding prior art faster. They help produce a curated AI patent list that aligns with a product roadmap.\n\nThe Broader AI Wearables Landscape\n\nMeta's latest software enhancements come amid a broader industry move toward wearable AI:\n\nAll this innovation is tied together by machine learning the core technology enabling contextual understanding on compact devices.\n\nConclusion: A New Era for Wearable AI and AI Patents\n\nMeta's v21 update for Meta AI Glasses is more than a feature refresh. It is concrete evidence that wearable AI is shifting from simple voice recognition to context-aware multimodal understanding, where devices intelligently combine sound, vision, and environment to assist users in real time.\n\nFor everyday users, this means smarter glasses that help hear conversations in noisy places and play music based on what you're seeing. For innovators and patent professionals, it means new opportunities and challenges in AI Patents from capturing novel integration techniques to protecting the next generation of ambient intelligence.\n\nThe world of Artificial Intelligence wearables has only just begun, and Meta's progress shows how rapidly this exciting frontier is evolving. From advanced audio features to contextual AI experiences, the intersection of smart hardware and AI-driven interactions will define the next wave of human-computer relationships."
  },
  {
    "source": "PC Mag Middle East",
    "company": "Meta AI",
    "title": "Meta Ray-Ban Display",
    "date": "2026-01-25T20:27:35Z",
    "url": "https://me.pcmag.com/en/smart-glasses/34903/meta-ray-ban-display",
    "content": "Display-equipped AI smart glasses are finally gaining traction. Meta's first attempt, the $799 Ray-Ban Display, impresses with a full-color waveguide in-lens display and a gesture-sensing Neural Band controller, avoiding the need to tap the glasses themselves. The hardware is excellent, but it's limited by Meta's first-party ecosystem focus, preventing it from being truly groundbreaking. For broader functionality, the Even Realities G2 ($599) is a better choice. Otherwise, it's worth waiting to see how Android XR shapes the market.\n\nI hope you like the look of the Wayfarer-style Meta Ray-Ban glasses, because the Meta Ray-Ban Display is more of the same. The plastic frames (available in black or sand) have the same rectangular lens shape, flat front, and integrated saddle bridge, but they're noticeably bulkier and have a glossy finish rather than matte. At 2.47 ounces, they're also heavier than other AI glasses; the non-Display Wayfarers are 1.87 ounces, the Rokid Glasses are 1.73 ounces, and the cameraless Even G2 is a featherlight 1.27 ounces. That said, the heft fits the chunkiness of the frames, and it isn't unwieldy.\n\nA friend diplomatically described the design as \"a choice.\" I haven't gotten that kind of reaction from any other smart glasses before. All the other pairs I've tested have passed as ordinary eyewear without a problem. I wouldn't call these ugly, but they look a bit like costume \"nerd\" glasses, or a caricature of Ray-Bans.\n\nThe temples are mounted on spring hinges and have typical thin, slightly curved ear hooks. Combined with the integrated saddle bridge and its almost flush rubber nose pads, the glasses at least feel very natural and comfortable. A touch strip sits on the right temple and provides limited physical controls: tapping to play, pause, or skip music tracks, and activating Meta AI with a long press.\n\nThe included faux-leather case keeps the glasses charged with a touch of style. Two flaps open to let the case almost unfold, exposing a T-shaped plastic lever that holds the frames in place at the nose bridge, keeping them aligned for consistent charging. A USB-C port sits on the right end of the case, with an indicator LED next to it.\n\nIn many ways, the Meta Ray-Ban Display's titular display is the best of its class. It uses a waveguide projection system, which means microprojectors in the frame send light directly through the lens, where an etched pattern, called a waveguide, bounces it into your eye through one lens. The lens is completely transparent when not showing a picture and doesn't significantly obstruct your view even when it's projecting something.\n\nThe trade-off of waveguide projection systems versus the bulkier prism-based alternatives found in many other AR glasses is a narrow field of view, relatively low resolution, and, usually, a monochrome green image instead of color. Situated in the right lens, the Ray-Ban Display's 600-by-600 monocular display is full color, putting it ahead of the green-only Even G1, G2, and Rokid Glasses. It enables an interface filled with blues, reds, purples, whites, grays, and greens. The color isn't particularly vibrant, but it's still much more pleasant to look at than monochrome green alternatives resembling early-'80s CRT monitors. The display is bright and sharp enough to show graphical interface elements like icons, diagrams, and even small photos and videos.\n\nIts biggest weakness is its field of view. At 20 degrees, it's narrower than the Even G2's 27.5 degrees and the Rokid's 30 degrees, which themselves are small compared with prism-based AR glasses like the RayNeo Air 3s Pro (46 degrees) and the XReal One Pro (57 degrees). To Meta's credit, the 20-degree field of view doesn't feel small, since its square aspect ratio has a higher resolution than the rectangular displays of the Air 3s Pro (640 by 480) and One Pro (640 by 350). It might take up less real estate in front of your eye, but it can show more information.\n\nMost interaction with the Meta Ray-Ban Display is through the included Neural Band controller. It's a dark gray fabric wristband lined with eight capsule-like sensors that can recognize hand gestures by detecting muscle movements through your arm. It's meant to be worn a few inches lower on your forearm than a watch, and I can wear it and my Pixel Watch 3 on my right arm at the same time. It's comfortable, unobtrusive, and closes securely thanks to both a small buckle and a magnetic tab that makes it feel kind of like a slap bracelet. The band is waterproof with an IPX7 rating, meaning it can withstand most conditions, including swimming. Just don't bring the actual glasses into the pool because they're rated IPX4, which is only lightly water-resistant.\n\nUsing the Neural Band to control the glasses is as simple as tapping and rubbing your fingers together. With the band on your arm, tap your thumb to the tip of your index finger to select an item on the in-lens display, and tap your thumb to your middle finger to go back from the menu layer or app you're in. Double-tap your thumb and middle finger to bring up the display or put it to sleep. Swipe your thumb across the side of your index finger to navigate through menus. Pinch your thumb and index finger together and rotate your wrist left or right like you're turning a knob to adjust volume. And, if you want to use Meta AI without saying the wake word, you can double-tap your thumb to the side of your index finger (but you'll still have to speak after that). For all of these gestures, the Neural Band will vibrate gently to let you know it's detecting an input.\n\nThis is easily one of the best control systems I've seen on waveguide display smart glasses. Of course, every other remotely similar pair of smart glasses I've used has relied on limited and often unwieldy touch controls on the frames themselves (and the touch controls on the Meta Ray-Ban Display's temple aren't exactly useful, either). The Even G2 comes close with the optional R1 smart ring, which provides a more convenient and easier-to-use touch surface, but that's an $249 accessory not included with the glasses.\n\nI found it easy to get used to the Neural Band, and it was mostly responsive and accurate in testing. It consistently detected my finger taps and horizontal thumb swipes, though it often took a few tries to register vertical swipes to navigate up or down in menus. The dial-twist gesture was also a bit hit-or-miss. This isn't the precise hand-tracking that mixed reality headsets like the Apple Vision Pro and Samsung Galaxy XR can do on their own without a wearable band, but they're much larger and much more expensive devices you generally wouldn't be walking around wearing.\n\nThe visual interface of the Meta Ray-Ban Display is a clean, colorful, well-labeled menu system that leverages four-directional navigation via the Neural Band. For comparison, every other pair of waveguide glasses I've tried before this has only offered single-axis scrolling menus because of the limited inputs. Meta's interface on the glasses is easy to use, but it's laid out strangely.\n\nThe home view consists of three tabs. The center tab is for notifications and Meta AI, but most of the time it will just be a bar that says \"Ask Meta AI.\" That's because the notifications on the glasses are so limited, which I'll go into more detail about in the next section. From here, you can tap the Meta AI bar and ask a question from this tab, but you can also just say, \"Hey Meta,\" or double-tap your thumb to the side of your index finger, and the glasses will start listening for a command. The tab feels superfluous since it doesn't really have any functions to navigate visually, and you're going to be talking to the AI when you activate it anyway.\n\nThe left tab is a more useful settings and quick-features menu. Here, there are buttons to immediately start live captioning or translating, open the camera tool to take photos or videos, and open the Music app, which displays whatever is playing on your phone and lets you play, pause, rewind, or skip tracks. The tab also has Volume and Brightness dials you can adjust by selecting either of them and performing the knob-turning gesture, as well as buttons to toggle audio-only and do-not-disturb modes. It also offers access to a dedicated, limited settings menu with two options: tilting the display slightly left or right for comfort, and configuring whether message notifications wake the display when it's off.\n\nOn the right tab is the full app list, with buttons that open WhatsApp, Instagram, Messenger, calls, the camera, live captioning and translating, Maps, the music player, an on-glasses photo gallery, text messages, and tutorials. The app list doesn't seem to be arranged in any logical order besides putting Meta's own apps at the top by default, and you can't rearrange them directly, which is frustrating. You can, however, pin the apps you use most and unpin the ones you don't, so there's at least some control.\n\nThe interface isn't hard to learn, but it makes some awkward choices. The Quick Settings tab and the Notifications/Meta AI tab could have been combined. It would have been better to place the captioning, camera, and music buttons below the AI button, rather than in the settings tab, for easy access outside the apps tab. Phone functions like calls and text messages could also have been moved to a quick menu in the AI tab. There aren't a ton of apps to scroll through, but that leads to one of the glasses' biggest problems.\n\nThe Meta Ray-Ban Display is built around using Meta's services and, if possible, only Meta's services. The AI is Meta AI, the messaging apps outside of phone text messages are Messenger and WhatsApp, the photo-sharing app is Instagram, and those are the only choices you get (and they are all owned by Meta). You can't talk to Gemini, message over Discord or Slack, or post photos on Bluesky. You can connect your Amazon Music, Shazam, and Spotify accounts, but that's probably because Meta doesn't have its own music streaming service. Even without them, you can at least control any audio playing on your phone through the glasses' Music app, as if it were a widget on your lock screen.\n\nNotifications are the biggest issue. Text messages and voice calls through your phone are supported, and you'll get notifications for them. But those, and messages from Meta apps, are the only notifications the glasses will show. Unlike every other pair of waveguide smart glasses I've tested, these won't read your phone's push notifications.\n\nIn addition, all of the Meta apps, including Instagram, are primarily for communication, not for browsing your social feeds. The Instagram app only brings you to your messages, so you can't browse stories, and Facebook isn't on the glasses at all.\n\nCalendar support is also limited. There's no dedicated calendar app, so you have to ask Meta AI to tell you what your appointments are. You can link your Google or Outlook calendars to your phone, but not if they're work accounts with any kind of managed IT security. And you have to speak every time you want to check your next meeting.\n\nI had initially planned to take the glasses to CES and write an account of covering the show with them. I didn't, because the inability to see incoming Slack messages meant I couldn't use the glasses to keep up with coverage discussions. Moreover, since my Google-based work calendar is protected by IT policies, I couldn't ask Meta when or where I needed to go next for my many appointments. Simply supporting push notifications from third-party apps on my phone would have solved both of those problems on the Meta glasses. That feature is available on the Even Realities G2, which I ended up taking to CES instead.\n\nIf you're a regular Meta user and the software limitations don't bother you, the Meta Ray-Ban Display generally works quite well in executing its main functions. Closed captions are quick and accurate most of the time, and the text is easy to read. All AI-powered voice transcription depends on good sound quality, so it can make mistakes if the speech isn't completely clear or if there's significant background noise, but even then, it's still very usable.\n\nTranslation is also effective, within its very limited scope. I watched some Spanish-language soccer programming on my TV, and the glasses translated it into English with surprising accuracy. They can likely do the same with French or Italian. Those are the only options, though, and that's paltry compared with the Even G2 (31 languages) and the Rokid Glasses (89 languages). There's no Chinese, German, Japanese, Korean, Portuguese, or Vietnamese. Visual translation of other languages is supported using the camera, but not voice.\n\nYou'll have to commit to one language at a time for the glasses to translate. The translation function on the glasses interface doesn't offer any language choices and relies on the app to load a single language pack, a process that can take half a minute.\n\nThese are the first smart glasses I've used where the navigation feature is genuinely useful and provides a readable map. Opening the Maps app on the glasses pops up a large, easily understandable map of your location. Only a few major streets are labeled, but notable locations nearby, like movie theaters, are displayed as pins, and you can use the knob-turn gesture to zoom in closer for additional landmarks. From this view, you can use voice dictation to search for a location, or tap buttons for nearby cafes, restaurants, parks, or attractions. Selecting a destination will display the route as a blue line. From that view, you can start navigation, send the location to your phone, or, if it's a business with a phone number, call it. I found the navigation to be direct and accurate, with the map view tracking my location and orientation as it gave me turn-by-turn directions.\n\nThe Music app is simple, with only track forward, track back, and play/pause buttons, plus a tile showing the time on the track. You'll also see album art if it's available and the app is compatible. No art came through my Android phone using Pocket Casts or YouTube Music, whereas both show album art and podcast icons on the phone itself. As mentioned, its phone widget-like universality also means it can control any audio playing from your phone. However, it doesn't offer the same benefits as a phone widget because it only shows those controls when the app is open and on the display. An icon in the quick settings menu shows you the track playing, but to do anything with it, you have to tap it to open the app first. An in-glasses widget for the app to help populate the central tab would have been really helpful here, rather than requiring you to open its full view.\n\nYou can play/pause and skip tracks with single and double taps on the touch strip on the glasses, but that's all you get for audio gesture support. The Neural Band doesn't give you any audio controls or even provide a shortcut to bring up the Music app quickly. This is baffling because the Meta AI app lets you assign the double-tap gesture to \"your favorite feature,\" but the only options are the default Meta AI activation or disabling it completely.\n\nAudio quality is pretty good for smart glasses. The speakers on the temples produce fairly robust sound that can be easily heard even in a crowded, noisy coffee shop. There's little bass to speak of, but that's just a reality for smart glasses that have a physical gap between their small speaker drivers and your ear canal.\n\nDon't expect any privacy for what you're listening to, though; sound leakage is significant, and anyone near you will be able to hear whatever's playing if it's higher than half volume, which is the floor for comfortable listening on your end in most situations.\n\nThe built-in camera seems to be the same as the one on the second-generation non-Display Meta Ray-Ban glasses. It can shoot 12MP photos or capture 3K video in vertical orientation. Captures are colorful and fairly sharp for smart glasses, with fine detail like my cat's fur coming through clearly. Noise can soften pictures in low light, but in a fairly unobtrusive way. You won't likely notice it if you view or post your full capture, but you won't get much more detail out of them by zooming in. Basically, it's good enough for social media and matches what we've seen from Meta's other recent smart glasses.\n\nMeta says the glasses themselves can last up to six hours with mixed use, and the charging case adds another 18 hours. It falls short of the Rokid Glasses' 8-hour battery life, but Rokid's charging case is a $99 accessory sold separately. While using the Meta glasses intermittently and tucking them into the charging case when not in use, I consistently saw all-day use with the battery meter seldom dropping below 50%."
  },
  {
    "source": "k.sina.com.cn",
    "company": "Meta AI",
    "title": "Meta AI独立AI助手应用发布 融合社交元素提升互动性_业界资讯_太平洋科技资讯中心",
    "date": "2026-02-18T07:01:41Z",
    "url": "https://k.sina.com.cn/article_7857201856_1d45362c001902gvn6.html",
    "content": "【太平洋科技快讯】近日，Meta公司正式发布了其独立的AI助手应用 -- -- Meta AI，这款应用不仅具备当前AI助手的功能，如文字和语音交互、图像生成以及实时网络搜索，还融入了独特的社交元素，展现了Meta在AI领域的新探索。\n\nMeta AI的核心功能包括文字与语音交互、图像生成和实时网络搜索。用户可以通过文字输入或语音对话与Meta AI进行交互，体验便捷的交流方式。此外，Meta AI还支持用户生成个性化图像，满足创意需求。更重要的是，该应用能够提供实时网络搜索结果，帮助用户快速获取信息。在美国和加拿大，Meta AI会利用Facebook和Instagram的用户资料信息，使回应更具个性化。用户还可以指示Meta AI记住特定细节，增强互动体验。\n\n据悉，Meta AI由经过优化的Llama 4模型提供支持，该模型在语言生成能力、响应速度和多样性方面表现出色。此外，Meta AI特别强调语音交互，其测试版的全双工(full-duplex)语音模式支持快速动态的轮流发言、重叠语音和反馈信号，提升了对话的自然性和流畅性。目前，全双工语音模式已在美国、加拿大、澳大利亚和新西兰上线。\n\nMeta AI的最大创新之处在于其\"发现\"(Discover)信息流功能。在\"发现\"信息流中，用户可以看到其他用户(包括Instagram和Facebook上的朋友)选择分享的与Meta AI的交互内容。这些内容基于具体的提示词(prompt)进行展示。用户可以对这些共享的AI帖子点赞、评论、分享，甚至将其重新混合(remix)为自己的内容。Meta公司产品副总裁康纳·海耶斯(Connor Hayes)表示，这一设计旨在揭开AI的神秘面纱，向人们展示他们可以用它做些什么。"
  },
  {
    "source": "Yahoo",
    "company": "Meta AI",
    "title": "A Creepy New Device Is Spreading Across School Campuses. Students Are Being Harassed. Teachers Are Sounding the Alarm.",
    "date": "2026-02-12T14:05:06Z",
    "url": "https://www.yahoo.com/news/articles/creepy-device-spreading-across-school-105000328.html",
    "content": "The rise of wearable recording devices like Meta AI glasses is leading to increased anxiety and apprehension among young people, who fear being recorded without their knowledge and consent.\n\nSign up for the Slatest to get the most insightful analysis, criticism, and advice out there, delivered to your inbox daily.\n\nJoziah was tabling on campus for his peer mentor job at the end of last semester at Florida State University when he noticed something strange happening across the quad: A trio of men, wearing Meta AI glasses, were stopping every young woman who passed by and asking them for their social media contacts.\n\n\"I recognized them from TikTok, because they're kind of big, especially in Miami,\" the 19-year-old told me. \" I'm seeing them literally go up to every single girl that's passed by with them.\"\n\nHe posted a video of the incident on his TikTok, which quickly garnered 200,000 views. Women who had seen the same event unfold flocked to the comments to share their frustration with the situation. \"Literally I was one of their victims,\" one user said. Another wrote, \"They were so rude to ppl asking them what they were doing too.\"\n\nOthers shared their fear of the same thing happening to them. \"I literally have nightmares about this,\" yet another wrote.\n\nThis isn't the first time that Joziah, who asked to have his last name withheld to protect his identity, has had an uncomfortable encounter with the new wearable recording devices. During a football game, a stranger secretly recorded a video of his friend and posted it online, to her shock. When Joziah returned to his hometown over winter break, he went out with some female friends, during which they were recorded on Meta AI glasses and posted on social media without their consent. They learned about it after the clip, a POV video of the creator approaching Joziah's friends at a bar, went viral.\n\n\"It definitely made me feel uneasy,\" Joziah recalled of the experience. \"I felt uncomfortable watching them go up to all these girls to record their interactions of them hitting on them.\"\n\nAfter pushing billions of dollars into the metaverse, Meta has now found overwhelming demand for its Ray-Ban display glasses, which allow users to take photos, stream content, and talk to an A.I. assistant. Waitlists for the product have surged, and the company's pivot away from the metaverse and toward smart glasses has become aggressive: Hundreds of Meta workers in the Reality Labs division and virtual games studios were laid off, product rollout was paused to address the supply shortage, and it was reported that Meta and EssilorLuxottica, Ray-Ban's owner, are discussing possibly doubling production capacity for the A.I.-powered glasses by the end of this year, all in a bid to capitalize on the growing demand as well as get ahead of competitors.\n\nBut its usages -- specifically its ability to capture photo and video -- have raised questions about how the devices will be applied in real life, especially in settings among children and young adults. Issues around academic dishonesty, classroom surveillance, and harassment have been growing in recent years but have been exacerbated as students gain access to the controversial wearable.\n\nChloe Peichl, who is 18, is a senior at a small private school in Texas. She told me that the introduction of generative A.I. tools and new technology has vastly shifted the environment at school.\n\n\" A.I. and ChatGPT, oh my gosh,\" she said. \"It's insane. Innocent people are getting accused of [copying their homework from ChatGPT] and having to rewrite 15-page essays that they worked all year on.\"\n\nIt has resulted in a schoolwide crackdown. All wearable technology -- from Apple Watches to smart glasses -- is banned. Phones are not allowed on premises. Even laptops were barred, causing an uproar from students who felt very strongly about being forced to use school-issued Chromebooks for their final papers. Peichl found these guidelines to be incredibly frustrating when writing her mandatory senior thesis.\n\n\" I mean, I can see where they're coming from, but I don't know,\" she said. \"Just eliminating tech completely from schools, I think, is not exactly helpful.\"\n\nThat's not to say that people don't try to circumvent the rules. Peichl said that some try to sneak their Apple Watches in; others attempt to hide their phones where they think teachers can't see them. But Meta AI glasses are new, and while those devices aren't as common, adults are on the watch now. She recalled one student who tried to sneak their Meta AI glasses into class.\n\n\"I don't know what he was using them for, but he has been known for trying to cheat on schoolwork and stuff,\" she said. \"I don't know if that's what he was using them for. And he got them taken away immediately.\"\n\nJamie Cohen, assistant professor of media studies at CUNY Queens College, said that he is highly aware of the impact that new tech has on the classroom environment. Quizzes in particular have become difficult to issue, as students frequently use ChatGPT and wearables, in addition to sharing digital copies of answer keys through end-to-end encrypted apps like Signal, Telegram, and WhatsApp. \"It does inspire professors to change their material consistently,\" he told me. \"The school, as many do, has an official policy against recording, so it falls under that, but glasses are a bit easier to get away with.\"\n\nBut cheating isn't the key issue; the problem that Cohen often sees is the ripple effects of constant surveillance on the participation of students in discussions and seminars, spaces that require a modicum of vulnerability, expression, and debate. \"By comparison to the millennials I used to teach, Gen Z are far more quiet and reserved because they have a strange fear of being cringe or judged as cringe,\" Cohen said. \"From both observation and asking them, I know they feel paranoid when they're answering a question, aware that someone may record them.\"\n\nJoziah feels that this has definitely affected the way he moves about his life, especially in spaces where he knows that people are constantly whipping out their phones. \" I think even just now, with everything being digital, with everything being recorded, I'm just a lot more aware of, like, my digital footprint, what I'm doing in public at school,\" Joziah said. \"I feel like I'm being recorded every day, all the time, especially at school.\"\n\nOf course, the fear of being recorded without consent didn't start with Meta glasses. After all, this is the age of panopticontent, when one unconsenting clip of you could become a viral meme or the lightning rod for a global investigation of your character, all depending on how you were caught on camera. There is an overwhelming sense of Constantly Being Perceived, even if it doesn't always show up on a day-to-day basis.\n\nBut now it's not just phones that could be recording you. \" Obviously, there's always the concern of being filmed somewhere while you're out,\" Joziah added. \" But it is just something I'm extra conscious of when I'm going out. Now, if someone comes up to me wearing glasses, that's the first thing I'm looking at. 'Are these Meta glasses?' \"\n\nPeichl, the Texas high school student, admitted that while the laptop ban is annoying for schoolwork, there is a sense of familiarity among her classmates that has occurred perhaps because they aren't constantly on their devices. \" We all talk to each other a lot,\" she said. \" I'm sure if we did just have our phones all the time, we wouldn't be as close.\"\n\nStill, she believes that the teachers' fears of Meta AI glasses being used as a cheating tool are a bit far-fetched, at least for now. The camera still emits a loud flash and shutter sound when taking photos, and there's a light that turns on when it records video. So the device isn't as discreet in some regards, but future models might be harder to spot. The technology certainly isn't going anywhere, and will only add more features as long as demand continues to balloon. Upcoming models of the glasses have been confirmed to include a teleprompter, as well as the ability to respond to texts with hand gestures; Cohen added that tech-savvy young people will also learn how to make their own modifications to the hardware.\n\n\"The more the tech gets better, the less likely we'll be able to detect Meta AI glasses wearers, especially if they jailbreak the recording light,\" Cohen said.\n\nIt's true that the product is still in its early adoption stage, which makes it something of a flex or a novelty product for many. In school, it's something only used to impress peers, or for students who are hell-bent on constantly making content. Especially for people chasing that lucrative genre of stunt-based, man-on-the-street videos, the product is a kind of performance in and of itself, a show of wealth and access, more interesting and click-worthy than just recording on a phone. Cohen notes that he's also noticed this trend among students -- the ones who are constantly wearing their smart glasses are the ones who consider themselves content creators.\n\n\"The ones who aren't creators or influencers see wearers as cringe,\" Cohen said, sardonically summing up the general feeling of nonwearers as \"Your life is not that cool, bro.\" He added, \"In school, at least at mine, they serve no reasonable purpose aside from recording 'day in the life' or 'get ready with me' content.\"\n\nBut even for a generation that has normalized the grind of vying for social media fame, agency is still important, which is why Cohen says that most of his students are staunchly anti-Meta AI glasses. \"They are keenly aware of the environment where they can find themselves online without their permission,\" he said. \"They hate feeling creepy.\"\n\nThere is a gold rush-style frenzy for young people to capitalize on the expanding industry of content creation, and the push to constantly mine the attention economy for clicks, comments, shares, and follows has resulted in some murky ethical waters. Already, women have begun speaking out about their experiences being surreptitiously recorded on smart glasses and the feeling of violation that arose when they realized that they'd been posted online for content.\n\nJoziah said that the click mines have significantly affected campus culture. While he already feels vigilant and wary about being recorded, he notes that it's much worse for his friends who are women, both inside controlled settings like the classroom and outside.\n\n\"There's already this fear that women have when going out, getting their drinks spiked or getting harassed by men. And I think that this just adds onto it because on top of the harassment, they're worrying about the harassment being recorded, and then being used to make profit from that content,\" he said. \"As a guy, it's not something that I will ever 100 percent be able to speak on, but I've seen how it's affected my friends and how damaging it can be.\"\n\nGwyneth Agbenyo, one of Cohen's students, told me that being recorded without consent is something that she and her peers worry about. \"We see it online all the time: Someone caught in the background of a picture looking less than pleased is branded a 'hater' or a 'bully' and becomes the internet's punching bag for a day,\" the 23-year-old said. \"When you see things like that all the time, you have to live with the knowledge that it could be you one day, because everyone has accepted the reality that the second you step outside, you waive your right to privacy.\"\n\nShe added, \"We're already living in a digital panopticon and seeing the effects of it on our culture, especially when it comes to the anxieties of young people.\"\n\nAlthough Agbenyo hasn't personally experienced someone bringing Meta glasses into class, she has heard of them appearing on campus in other places. Hearing stories about it, and seeing content of people being filmed on her own social feeds, is enough to make the anxiety persist. As the technology becomes more normalized, she says, part of the issue is that there's no more choice for people to opt out of being filmed. \"While I understand the utility of a hands-free recording device for creative purposes, I can't help but feel like there are no safeguards to prevent this technology from being used for sinister purposes,\" she said. \"The same way generative A.I. technology was immediately used to create deepfakes and undress women, I really wouldn't be surprised if these glasses were used for sexual harassment purposes as well.\"\n\nCohen adds that he has seen a similar fear among many of his students who are immigrants and first-generation citizens. These students have expressed concerns about being recorded and surveilled by law enforcement with discreet wearable tech, a fear that has only compounded at a time when both U.S. citizens and immigrants are being profiled, recorded, violently detained, and brutalized by police officers and Immigration and Customs Enforcement agents alike.\n\n\"They've heard stories of ICE wearing the Meta AI glasses, and that worries them immensely,\" he said. \"One student literally told me she doesn't want to be a viral video on Fox.\"\n\nAnd of course, the anxiety of being unknowingly recorded isn't limited to young people. \"I too have a huge fear of students recording my lectures,\" Cohen said. \"I teach pretty off the cuff, and my lectures are critical, so I'm also very tuned in to making sure I'm not getting recorded.\"\n\nThe fears around omnipresent surveillance tools not only have made students more apprehensive about saying the wrong thing in class or making a mistake, but have also turned many of Agbenyo's peers away from social media altogether.\n\n\"A lot of my peers don't even post on social media anymore,\" she said. \"I have one picture on my feed, and most of my personal friends have either deleted all of their pictures or just don't post anymore aside from their stories, which disappear after 24 hours.\"\n\nHigh schooler Peichl, however, doesn't fear the surveillance aspect of wearable tech like the Meta AI glasses. Although she grasps the issues that can arise with the product, she says, most of her life is pretty well documented anyway. It always has been. \" Personally, nothing is really super hidden in my life,\" she said. \"I feel like we understand tech enough nowadays to know what we can control, and what we can't and can do.\""
  },
  {
    "source": "Phone Arena",
    "company": "Meta AI",
    "title": "Meta AI gearing up to finally rival OpenAI and Gemini",
    "date": "2026-02-10T08:28:38Z",
    "url": "https://www.phonearena.com/news/meta-ai-new-features-coming_id178078",
    "content": "The gap between Meta AI and OpenAI might be closing with Meta's latest platform update. The company has upgraded its website, giving users numerous new functions. More importantly, it appears to be preparing to release a new model called Avocado as well as new upgrades designed to tremendously improve users' AI experience.\n\nMeta AI: is it finally ready to become a true ChatGPT and Gemini rival?\n\nFirst spotted by TestingCatalog on February 8, the Meta website now seems to allow some users to connect various apps, such as Google Calendar and Microsoft Outlook.\n\nConnecting your apps seems pretty easy on Meta AI. | Image credit -- TestingCatalog\n\nOnce connected, Meta AI would be able to pull data and manage tools through MCPs (Model Context Protocols). While it's unclear when the ability to connect different apps will be released to every user, the implementation suggests Meta is finally ready to adopt MCPs.\n\nWhich AI agent do you use most?\n\nGemini is my favorite.\n\nI mostly use ChatGPT.\n\nPerplexity!\n\nMeta AI.\n\nOther.\n\nNone -- I don't use AI agents.\n\nVote\n\nFor context, an MCP is like a \"universal language\" that allows an artificial intelligence to read info from all sorts of different connected apps. In essence, it lets Meta AI access information from apps, managing your schedule directly without the need for custom code for each one.\n\nRecommended For You\n\nAvocado is a highlight -- but not the only one\n\nThe recent website revamp introduced different answer modes, with Meta AI now offering Fast and Thinking answering options. As it prepares to release the Avocado model (which is supposedly being tested for now), the AI assistant will offer another two forms: Avocado and Avocado Thinking.\n\nTestingCatalog explains the Avocado model is the only one responding currently, and answers aren't that perfect. Of course, this could be because development is still underway, with responses potentially becoming more reliable as the Avocado rolls out.\n\nAcquire and conquer\n\nNot long ago, Meta acquired Manus AI, with some reports suggesting there's a Manus AI agent and even a browser under development right now. If true, this suggests Manus AI-powered agents could directly arrive at Meta AI.\n\nSet up tasks, then relax. | Image credit -- TestingCatalog\n\nManus AI is a startup known for its autonomous agents. Basically, it's an assistant that can navigate the web for you.\n\nOther notable changes include the addition of Tasks in the Meta AI website, giving users the ability to create repeating or one-time tasks. This, coupled with the reported testing and development of a shopping assistant and the introduction of voice assistants, has the potential to \"propel\" Meta AI right at the top, next to the hugely popular OpenAI and Gemini AI.\n\nA win-win for users?\n\nWill these features actually benefit users? There is some potential if you ask me. However, the idea of connecting all my personal apps to Meta and letting it freely collect that data somehow doesn't sit right with me.\n\nAI models collect data anyway, no two ways about that. But for now, I'm not sure I'm ready to ditch Gemini for Meta."
  },
  {
    "source": "Jujuy al día®",
    "company": "Meta AI",
    "title": "WhatsApp prepara el modo razonamiento para Meta AI: respuestas más inteligentes y detalladas",
    "date": "2026-01-27T16:20:41Z",
    "url": "https://www.jujuyaldia.com.ar/2026/01/27/whatsapp-prepara-el-modo-razonamiento-para-meta-ai-respuestas-mas-inteligentes-y-detalladas/",
    "content": "La nueva función, en fase beta para Android, permitirá alternar entre respuestas rápidas y análisis profundos\n\nJujuy al día ® - WhatsApp está desarrollando una nueva función que permitirá a los usuarios acceder a respuestas más inteligentes y detalladas gracias al modo de razonamiento para Meta AI.\n\nEl objetivo es ofrecer a los usuarios la posibilidad de elegir entre respuestas rápidas o razonadas, adaptando la interacción con el chatbot a las necesidades de cada consulta.\n\nLa novedad responde a la demanda creciente de asistentes virtuales capaces de manejar tareas complejas y proporcionar explicaciones estructuradas, más allá de respuestas simples e inmediatas.\n\nHasta ahora, Meta AI en WhatsApp había mantenido un perfil bajo respecto a nuevas funciones, pero la introducción del modo de razonamiento busca situarla a la par de otros asistentes de IA que ya implementan herramientas avanzadas para el análisis y la resolución de problemas.\n\n¿Cómo funciona el modo de razonamiento en WhatsApp?\n\nLa función se está probando en la versión beta de WhatsApp para Android (2.26.3.10) y permite a los usuarios enviar preguntas o mensajes específicos a Meta AI directamente desde la conversación, sin necesidad de reenviarlos manualmente.\n\nDe acuerdo con WABetaInfo para facilitar el proceso, WhatsApp añadió un acceso directo que envía el mensaje al chatbot junto con una indicación adicional, haciendo la interacción más ágil y práctica.\n\nEl menú de Meta AI en WhatsApp presentará dos modos principales de respuesta: \"rápido\" y \"razonamiento\". Los usuarios podrán elegir el modo adecuado según sus necesidades, alternando entre ambos en cualquier momento mediante un selector ubicado en la barra de chat.\n\nEl modo rápido, que es el predeterminado actualmente, está diseñado para responder de manera instantánea a preguntas simples, priorizando la velocidad sobre la profundidad o el análisis.\n\nCuáles son las ventajas del modo de razonamiento\n\nEl modo de razonamiento, en cambio, está pensado para quienes requieren respuestas más precisas, estructuradas y contextuales. Cuando se activa, Meta AI dedica más tiempo a procesar la solicitud, desglosar problemas complejos y analizar el contexto antes de generar la respuesta.\n\nEste modo es ideal para consultas que exigen planificación, comparaciones, resúmenes o resolución de problemas de varios pasos.\n\nPor ejemplo, los usuarios podrán utilizar el modo de razonamiento para pedir a Meta AI que compare productos, resuma artículos extensos, explique conceptos complejos o ayude en la planificación de tareas. Al dar control total sobre el estilo de respuesta, WhatsApp busca que cada usuario ajuste la interacción a sus expectativas y al nivel de detalle que necesita.\n\nCómo seleccionar entre los modos rápido y razonamiento\n\nEl cambio entre modos será sencillo. Al tocar el ícono de un rayo en la barra de chat, los usuarios activarán el modo de razonamiento, que se indicará visualmente con una bombilla.\n\nPara regresar al modo rápido, bastará con tocar de nuevo la bombilla. Esta flexibilidad permite adaptar la experiencia en tiempo real, según la urgencia o la complejidad de la consulta.\n\nEl menú de selección de modo podría ampliarse en el futuro, ya que Meta planea añadir nuevas opciones y funcionalidades a medida que la inteligencia artificial evolucione.\n\nLa introducción del modo de razonamiento en Meta AI representa un paso adelante en la personalización y sofisticación de los asistentes virtuales integrados en aplicaciones de mensajería. Los usuarios de WhatsApp podrán obtener respuestas más completas cuando lo requieran, sin renunciar a la rapidez en los casos en que solo necesitan información puntual.\n\nCon esta actualización, Meta AI busca ponerse al nivel de otros chatbots que ya cuentan con capacidades de razonamiento avanzado, en un contexto de competencia creciente en el desarrollo de inteligencia artificial conversacional.\n\nLa incorporación del modo de razonamiento responde a la necesidad de ofrecer funciones más sofisticadas y adaptarse a las expectativas de los usuarios que demandan respuestas más completas y precisas en sus interacciones diarias.\n\nLa función de razonamiento para Meta AI está aún en desarrollo y su lanzamiento está previsto en una actualización futura. WhatsApp continúa ajustando los detalles técnicos y de interfaz para garantizar una adopción fluida.\n\nCuando la herramienta esté disponible, los usuarios podrán alternar entre modos y aprovechar al máximo la potencia de Meta AI para resolver problemas complejos, obtener explicaciones detalladas y gestionar tareas con mayor eficiencia."
  },
  {
    "source": "infobae",
    "company": "Meta AI",
    "title": "WhatsApp prepara el modo de razonamiento para Meta AI: respuestas más inteligentes y detalladas",
    "date": "2026-01-26T15:05:05Z",
    "url": "https://www.infobae.com/tecno/2026/01/26/whatsapp-prepara-el-modo-de-razonamiento-para-meta-ai-respuestas-mas-inteligentes-y-detalladas/",
    "content": "WhatsApp está desarrollando una nueva función que permitirá a los usuarios acceder a respuestas más inteligentes y detalladas gracias al modo de razonamiento para Meta AI.\n\nEl objetivo es ofrecer a los usuarios la posibilidad de elegir entre respuestas rápidas o razonadas, adaptando la interacción con el chatbot a las necesidades de cada consulta.\n\nLa novedad responde a la demanda creciente de asistentes virtuales capaces de manejar tareas complejas y proporcionar explicaciones estructuradas, más allá de respuestas simples e inmediatas.\n\nHasta ahora, Meta AI en WhatsApp había mantenido un perfil bajo respecto a nuevas funciones, pero la introducción del modo de razonamiento busca situarla a la par de otros asistentes de IA que ya implementan herramientas avanzadas para el análisis y la resolución de problemas.\n\n¿Cómo funciona el modo de razonamiento en WhatsApp?\n\nLa función se está probando en la versión beta de WhatsApp para Android (2.26.3.10) y permite a los usuarios enviar preguntas o mensajes específicos a Meta AI directamente desde la conversación, sin necesidad de reenviarlos manualmente.\n\nDe acuerdo con WABetaInfo para facilitar el proceso, WhatsApp añadió un acceso directo que envía el mensaje al chatbot junto con una indicación adicional, haciendo la interacción más ágil y práctica.\n\nEl menú de Meta AI en WhatsApp presentará dos modos principales de respuesta: \"rápido\" y \"razonamiento\". Los usuarios podrán elegir el modo adecuado según sus necesidades, alternando entre ambos en cualquier momento mediante un selector ubicado en la barra de chat.\n\nEl modo rápido, que es el predeterminado actualmente, está diseñado para responder de manera instantánea a preguntas simples, priorizando la velocidad sobre la profundidad o el análisis.\n\nCuáles son las ventajas del modo de razonamiento\n\nEl modo de razonamiento, en cambio, está pensado para quienes requieren respuestas más precisas, estructuradas y contextuales. Cuando se activa, Meta AI dedica más tiempo a procesar la solicitud, desglosar problemas complejos y analizar el contexto antes de generar la respuesta.\n\nEste modo es ideal para consultas que exigen planificación, comparaciones, resúmenes o resolución de problemas de varios pasos.\n\nPor ejemplo, los usuarios podrán utilizar el modo de razonamiento para pedir a Meta AI que compare productos, resuma artículos extensos, explique conceptos complejos o ayude en la planificación de tareas. Al dar control total sobre el estilo de respuesta, WhatsApp busca que cada usuario ajuste la interacción a sus expectativas y al nivel de detalle que necesita.\n\nCómo seleccionar entre los modos rápido y razonamiento\n\nEl cambio entre modos será sencillo. Al tocar el ícono de un rayo en la barra de chat, los usuarios activarán el modo de razonamiento, que se indicará visualmente con una bombilla.\n\nPara regresar al modo rápido, bastará con tocar de nuevo la bombilla. Esta flexibilidad permite adaptar la experiencia en tiempo real, según la urgencia o la complejidad de la consulta.\n\nEl menú de selección de modo podría ampliarse en el futuro, ya que Meta planea añadir nuevas opciones y funcionalidades a medida que la inteligencia artificial evolucione.\n\nLa introducción del modo de razonamiento en Meta AI representa un paso adelante en la personalización y sofisticación de los asistentes virtuales integrados en aplicaciones de mensajería. Los usuarios de WhatsApp podrán obtener respuestas más completas cuando lo requieran, sin renunciar a la rapidez en los casos en que solo necesitan información puntual.\n\nCon esta actualización, Meta AI busca ponerse al nivel de otros chatbots que ya cuentan con capacidades de razonamiento avanzado, en un contexto de competencia creciente en el desarrollo de inteligencia artificial conversacional.\n\nLa incorporación del modo de razonamiento responde a la necesidad de ofrecer funciones más sofisticadas y adaptarse a las expectativas de los usuarios que demandan respuestas más completas y precisas en sus interacciones diarias.\n\nLa función de razonamiento para Meta AI está aún en desarrollo y su lanzamiento está previsto en una actualización futura. WhatsApp continúa ajustando los detalles técnicos y de interfaz para garantizar una adopción fluida.\n\nCuando la herramienta esté disponible, los usuarios podrán alternar entre modos y aprovechar al máximo la potencia de Meta AI para resolver problemas complejos, obtener explicaciones detalladas y gestionar tareas con mayor eficiencia."
  },
  {
    "source": "PCMag UK",
    "company": "Meta AI",
    "title": "Meta Ray-Ban Display",
    "date": "2026-01-25T20:01:01Z",
    "url": "https://uk.pcmag.com/smart-glasses/162778/meta-ray-ban-display",
    "content": "Display-equipped AI smart glasses are finally gaining traction. Meta's first attempt, the $799 Ray-Ban Display, impresses with a full-color waveguide in-lens display and a gesture-sensing Neural Band controller, avoiding the need to tap the glasses themselves. The hardware is excellent, but it's limited by Meta's first-party ecosystem focus, preventing it from being truly groundbreaking. For broader functionality, the Even Realities G2 ($599) is a better choice. Otherwise, it's worth waiting to see how Android XR shapes the market.\n\nDesign: A Fashion Statement -- Whether You Like It or Not\n\nI hope you like the look of the Wayfarer-style Meta Ray-Ban glasses, because the Meta Ray-Ban Display is more of the same. The plastic frames (available in black or sand) have the same rectangular lens shape, flat front, and integrated saddle bridge, but they're noticeably bulkier and have a glossy finish rather than matte. At 2.47 ounces, they're also heavier than other AI glasses; the non-Display Wayfarers are 1.87 ounces, the Rokid Glasses are 1.73 ounces, and the cameraless Even G2 is a featherlight 1.27 ounces. That said, the heft fits the chunkiness of the frames, and it isn't unwieldy.\n\nA friend diplomatically described the design as \"a choice.\" I haven't gotten that kind of reaction from any other smart glasses before. All the other pairs I've tested have passed as ordinary eyewear without a problem. I wouldn't call these ugly, but they look a bit like costume \"nerd\" glasses, or a caricature of Ray-Bans.\n\nThe temples are mounted on spring hinges and have typical thin, slightly curved ear hooks. Combined with the integrated saddle bridge and its almost flush rubber nose pads, the glasses at least feel very natural and comfortable. A touch strip sits on the right temple and provides limited physical controls: tapping to play, pause, or skip music tracks, and activating Meta AI with a long press.\n\nThe included faux-leather case keeps the glasses charged with a touch of style. Two flaps open to let the case almost unfold, exposing a T-shaped plastic lever that holds the frames in place at the nose bridge, keeping them aligned for consistent charging. A USB-C port sits on the right end of the case, with an indicator LED next to it.\n\nDisplay: Crisp Color in One Eye\n\nIn many ways, the Meta Ray-Ban Display's titular display is the best of its class. It uses a waveguide projection system, which means microprojectors in the frame send light directly through the lens, where an etched pattern, called a waveguide, bounces it into your eye through one lens. The lens is completely transparent when not showing a picture and doesn't significantly obstruct your view even when it's projecting something.\n\nThe trade-off of waveguide projection systems versus the bulkier prism-based alternatives found in many other AR glasses is a narrow field of view, relatively low resolution, and, usually, a monochrome green image instead of color. Situated in the right lens, the Ray-Ban Display's 600-by-600 monocular display is full color, putting it ahead of the green-only Even G1, G2, and Rokid Glasses. It enables an interface filled with blues, reds, purples, whites, grays, and greens. The color isn't particularly vibrant, but it's still much more pleasant to look at than monochrome green alternatives resembling early-'80s CRT monitors. The display is bright and sharp enough to show graphical interface elements like icons, diagrams, and even small photos and videos.\n\nIts biggest weakness is its field of view. At 20 degrees, it's narrower than the Even G2's 27.5 degrees and the Rokid's 30 degrees, which themselves are small compared with prism-based AR glasses like the RayNeo Air 3s Pro (46 degrees) and the XReal One Pro (57 degrees). To Meta's credit, the 20-degree field of view doesn't feel small, since its square aspect ratio has a higher resolution than the rectangular displays of the Air 3s Pro (640 by 480) and One Pro (640 by 350). It might take up less real estate in front of your eye, but it can show more information.\n\nNeural Band: A New Gold Standard for Input\n\nMost interaction with the Meta Ray-Ban Display is through the included Neural Band controller. It's a dark gray fabric wristband lined with eight capsule-like sensors that can recognize hand gestures by detecting muscle movements through your arm. It's meant to be worn a few inches lower on your forearm than a watch, and I can wear it and my Pixel Watch 3 on my right arm at the same time. It's comfortable, unobtrusive, and closes securely thanks to both a small buckle and a magnetic tab that makes it feel kind of like a slap bracelet. The band is waterproof with an IPX7 rating, meaning it can withstand most conditions, including swimming. Just don't bring the actual glasses into the pool because they're rated IPX4, which is only lightly water-resistant.\n\nUsing the Neural Band to control the glasses is as simple as tapping and rubbing your fingers together. With the band on your arm, tap your thumb to the tip of your index finger to select an item on the in-lens display, and tap your thumb to your middle finger to go back from the menu layer or app you're in. Double-tap your thumb and middle finger to bring up the display or put it to sleep. Swipe your thumb across the side of your index finger to navigate through menus. Pinch your thumb and index finger together and rotate your wrist left or right like you're turning a knob to adjust volume. And, if you want to use Meta AI without saying the wake word, you can double-tap your thumb to the side of your index finger (but you'll still have to speak after that). For all of these gestures, the Neural Band will vibrate gently to let you know it's detecting an input.\n\nThis is easily one of the best control systems I've seen on waveguide display smart glasses. Of course, every other remotely similar pair of smart glasses I've used has relied on limited and often unwieldy touch controls on the frames themselves (and the touch controls on the Meta Ray-Ban Display's temple aren't exactly useful, either). The Even G2 comes close with the optional R1 smart ring, which provides a more convenient and easier-to-use touch surface, but that's an $249 accessory not included with the glasses.\n\nI found it easy to get used to the Neural Band, and it was mostly responsive and accurate in testing. It consistently detected my finger taps and horizontal thumb swipes, though it often took a few tries to register vertical swipes to navigate up or down in menus. The dial-twist gesture was also a bit hit-or-miss. This isn't the precise hand-tracking that mixed reality headsets like the Apple Vision Pro and Samsung Galaxy XR can do on their own without a wearable band, but they're much larger and much more expensive devices you generally wouldn't be walking around wearing.\n\nInterface: Polished, Capable, and Weirdly Organized\n\nThe visual interface of the Meta Ray-Ban Display is a clean, colorful, well-labeled menu system that leverages four-directional navigation via the Neural Band. For comparison, every other pair of waveguide glasses I've tried before this has only offered single-axis scrolling menus because of the limited inputs. Meta's interface on the glasses is easy to use, but it's laid out strangely.\n\nThe home view consists of three tabs. The center tab is for notifications and Meta AI, but most of the time it will just be a bar that says \"Ask Meta AI.\" That's because the notifications on the glasses are so limited, which I'll go into more detail about in the next section. From here, you can tap the Meta AI bar and ask a question from this tab, but you can also just say, \"Hey Meta,\" or double-tap your thumb to the side of your index finger, and the glasses will start listening for a command. The tab feels superfluous since it doesn't really have any functions to navigate visually, and you're going to be talking to the AI when you activate it anyway.\n\nThe left tab is a more useful settings and quick-features menu. Here, there are buttons to immediately start live captioning or translating, open the camera tool to take photos or videos, and open the Music app, which displays whatever is playing on your phone and lets you play, pause, rewind, or skip tracks. The tab also has Volume and Brightness dials you can adjust by selecting either of them and performing the knob-turning gesture, as well as buttons to toggle audio-only and do-not-disturb modes. It also offers access to a dedicated, limited settings menu with two options: tilting the display slightly left or right for comfort, and configuring whether message notifications wake the display when it's off.\n\nOn the right tab is the full app list, with buttons that open WhatsApp, Instagram, Messenger, calls, the camera, live captioning and translating, Maps, the music player, an on-glasses photo gallery, text messages, and tutorials. The app list doesn't seem to be arranged in any logical order besides putting Meta's own apps at the top by default, and you can't rearrange them directly, which is frustrating. You can, however, pin the apps you use most and unpin the ones you don't, so there's at least some control.\n\nThe interface isn't hard to learn, but it makes some awkward choices. The Quick Settings tab and the Notifications/Meta AI tab could have been combined. It would have been better to place the captioning, camera, and music buttons below the AI button, rather than in the settings tab, for easy access outside the apps tab. Phone functions like calls and text messages could also have been moved to a quick menu in the AI tab. There aren't a ton of apps to scroll through, but that leads to one of the glasses' biggest problems.\n\nApps and Features: Locked Into Meta's World\n\nThe Meta Ray-Ban Display is built around using Meta's services and, if possible, only Meta's services. The AI is Meta AI, the messaging apps outside of phone text messages are Messenger and WhatsApp, the photo-sharing app is Instagram, and those are the only choices you get (and they are all owned by Meta). You can't talk to Gemini, message over Discord or Slack, or post photos on Bluesky. You can connect your Amazon Music, Shazam, and Spotify accounts, but that's probably because Meta doesn't have its own music streaming service. Even without them, you can at least control any audio playing on your phone through the glasses' Music app, as if it were a widget on your lock screen.\n\nNotifications are the biggest issue. Text messages and voice calls through your phone are supported, and you'll get notifications for them. But those, and messages from Meta apps, are the only notifications the glasses will show. Unlike every other pair of waveguide smart glasses I've tested, these won't read your phone's push notifications.\n\nIn addition, all of the Meta apps, including Instagram, are primarily for communication, not for browsing your social feeds. The Instagram app only brings you to your messages, so you can't browse stories, and Facebook isn't on the glasses at all.\n\nCalendar support is also limited. There's no dedicated calendar app, so you have to ask Meta AI to tell you what your appointments are. You can link your Google or Outlook calendars to your phone, but not if they're work accounts with any kind of managed IT security. And you have to speak every time you want to check your next meeting.\n\nI had initially planned to take the glasses to CES and write an account of covering the show with them. I didn't, because the inability to see incoming Slack messages meant I couldn't use the glasses to keep up with coverage discussions. Moreover, since my Google-based work calendar is protected by IT policies, I couldn't ask Meta when or where I needed to go next for my many appointments. Simply supporting push notifications from third-party apps on my phone would have solved both of those problems on the Meta glasses. That feature is available on the Even Realities G2, which I ended up taking to CES instead.\n\nIf you're a regular Meta user and the software limitations don't bother you, the Meta Ray-Ban Display generally works quite well in executing its main functions. Closed captions are quick and accurate most of the time, and the text is easy to read. All AI-powered voice transcription depends on good sound quality, so it can make mistakes if the speech isn't completely clear or if there's significant background noise, but even then, it's still very usable.\n\nTranslation is also effective, within its very limited scope. I watched some Spanish-language soccer programming on my TV, and the glasses translated it into English with surprising accuracy. They can likely do the same with French or Italian. Those are the only options, though, and that's paltry compared with the Even G2 (31 languages) and the Rokid Glasses (89 languages). There's no Chinese, German, Japanese, Korean, Portuguese, or Vietnamese. Visual translation of other languages is supported using the camera, but not voice.\n\nYou'll have to commit to one language at a time for the glasses to translate. The translation function on the glasses interface doesn't offer any language choices and relies on the app to load a single language pack, a process that can take half a minute.\n\nThese are the first smart glasses I've used where the navigation feature is genuinely useful and provides a readable map. Opening the Maps app on the glasses pops up a large, easily understandable map of your location. Only a few major streets are labeled, but notable locations nearby, like movie theaters, are displayed as pins, and you can use the knob-turn gesture to zoom in closer for additional landmarks. From this view, you can use voice dictation to search for a location, or tap buttons for nearby cafes, restaurants, parks, or attractions. Selecting a destination will display the route as a blue line. From that view, you can start navigation, send the location to your phone, or, if it's a business with a phone number, call it. I found the navigation to be direct and accurate, with the map view tracking my location and orientation as it gave me turn-by-turn directions.\n\nThe Music app is simple, with only track forward, track back, and play/pause buttons, plus a tile showing the time on the track. You'll also see album art if it's available and the app is compatible. No art came through my Android phone using Pocket Casts or YouTube Music, whereas both show album art and podcast icons on the phone itself. As mentioned, its phone widget-like universality also means it can control any audio playing from your phone. However, it doesn't offer the same benefits as a phone widget because it only shows those controls when the app is open and on the display. An icon in the quick settings menu shows you the track playing, but to do anything with it, you have to tap it to open the app first. An in-glasses widget for the app to help populate the central tab would have been really helpful here, rather than requiring you to open its full view.\n\nYou can play/pause and skip tracks with single and double taps on the touch strip on the glasses, but that's all you get for audio gesture support. The Neural Band doesn't give you any audio controls or even provide a shortcut to bring up the Music app quickly. This is baffling because the Meta AI app lets you assign the double-tap gesture to \"your favorite feature,\" but the only options are the default Meta AI activation or disabling it completely.\n\nSound: Loud Enough for You -- and Everyone Else\n\nAudio quality is pretty good for smart glasses. The speakers on the temples produce fairly robust sound that can be easily heard even in a crowded, noisy coffee shop. There's little bass to speak of, but that's just a reality for smart glasses that have a physical gap between their small speaker drivers and your ear canal.\n\nDon't expect any privacy for what you're listening to, though; sound leakage is significant, and anyone near you will be able to hear whatever's playing if it's higher than half volume, which is the floor for comfortable listening on your end in most situations.\n\nCamera: Good Enough to Post, Not to Zoom\n\nThe built-in camera seems to be the same as the one on the second-generation non-Display Meta Ray-Ban glasses. It can shoot 12MP photos or capture 3K video in vertical orientation. Captures are colorful and fairly sharp for smart glasses, with fine detail like my cat's fur coming through clearly. Noise can soften pictures in low light, but in a fairly unobtrusive way. You won't likely notice it if you view or post your full capture, but you won't get much more detail out of them by zooming in. Basically, it's good enough for social media and matches what we've seen from Meta's other recent smart glasses.\n\nBattery Life: Not Quite a Full Day\n\nMeta says the glasses themselves can last up to six hours with mixed use, and the charging case adds another 18 hours. It falls short of the Rokid Glasses' 8-hour battery life, but Rokid's charging case is a $99 accessory sold separately. While using the Meta glasses intermittently and tucking them into the charging case when not in use, I consistently saw all-day use with the battery meter seldom dropping below 50%.\n\nThe Neural Band can last up to 18 hours per charge."
  },
  {
    "source": "PCMag Australia",
    "company": "Meta AI",
    "title": "Meta Ray-Ban Display",
    "date": "2026-01-25T19:41:38Z",
    "url": "https://au.pcmag.com/smart-glasses/115534/meta-ray-ban-display",
    "content": "Display-equipped AI smart glasses are finally gaining traction. Meta's first attempt, the $799 Ray-Ban Display, impresses with a full-color waveguide in-lens display and a gesture-sensing Neural Band controller, avoiding the need to tap the glasses themselves. The hardware is excellent, but it's limited by Meta's first-party ecosystem focus, preventing it from being truly groundbreaking. For broader functionality, the Even Realities G2 ($599) is a better choice. Otherwise, it's worth waiting to see how Android XR shapes the market.\n\nDesign: A Fashion Statement -- Whether You Like It or Not\n\nI hope you like the look of the Wayfarer-style Meta Ray-Ban glasses, because the Meta Ray-Ban Display is more of the same. The plastic frames (available in black or sand) have the same rectangular lens shape, flat front, and integrated saddle bridge, but they're noticeably bulkier and have a glossy finish rather than matte. At 2.47 ounces, they're also heavier than other AI glasses; the non-Display Wayfarers are 1.87 ounces, the Rokid Glasses are 1.73 ounces, and the cameraless Even G2 is a featherlight 1.27 ounces. That said, the heft fits the chunkiness of the frames, and it isn't unwieldy.\n\nA friend diplomatically described the design as \"a choice.\" I haven't gotten that kind of reaction from any other smart glasses before. All the other pairs I've tested have passed as ordinary eyewear without a problem. I wouldn't call these ugly, but they look a bit like costume \"nerd\" glasses, or a caricature of Ray-Bans.\n\nThe temples are mounted on spring hinges and have typical thin, slightly curved ear hooks. Combined with the integrated saddle bridge and its almost flush rubber nose pads, the glasses at least feel very natural and comfortable. A touch strip sits on the right temple and provides limited physical controls: tapping to play, pause, or skip music tracks, and activating Meta AI with a long press.\n\nThe included faux-leather case keeps the glasses charged with a touch of style. Two flaps open to let the case almost unfold, exposing a T-shaped plastic lever that holds the frames in place at the nose bridge, keeping them aligned for consistent charging. A USB-C port sits on the right end of the case, with an indicator LED next to it.\n\nDisplay: Crisp Color in One Eye\n\nIn many ways, the Meta Ray-Ban Display's titular display is the best of its class. It uses a waveguide projection system, which means microprojectors in the frame send light directly through the lens, where an etched pattern, called a waveguide, bounces it into your eye through one lens. The lens is completely transparent when not showing a picture and doesn't significantly obstruct your view even when it's projecting something.\n\nThe trade-off of waveguide projection systems versus the bulkier prism-based alternatives found in many other AR glasses is a narrow field of view, relatively low resolution, and, usually, a monochrome green image instead of color. Situated in the right lens, the Ray-Ban Display's 600-by-600 monocular display is full color, putting it ahead of the green-only Even G1, G2, and Rokid Glasses. It enables an interface filled with blues, reds, purples, whites, grays, and greens. The color isn't particularly vibrant, but it's still much more pleasant to look at than monochrome green alternatives resembling early-'80s CRT monitors. The display is bright and sharp enough to show graphical interface elements like icons, diagrams, and even small photos and videos.\n\nIts biggest weakness is its field of view. At 20 degrees, it's narrower than the Even G2's 27.5 degrees and the Rokid's 30 degrees, which themselves are small compared with prism-based AR glasses like the RayNeo Air 3s Pro (46 degrees) and the XReal One Pro (57 degrees). To Meta's credit, the 20-degree field of view doesn't feel small, since its square aspect ratio has a higher resolution than the rectangular displays of the Air 3s Pro (640 by 480) and One Pro (640 by 350). It might take up less real estate in front of your eye, but it can show more information.\n\nNeural Band: A New Gold Standard for Input\n\nMost interaction with the Meta Ray-Ban Display is through the included Neural Band controller. It's a dark gray fabric wristband lined with eight capsule-like sensors that can recognize hand gestures by detecting muscle movements through your arm. It's meant to be worn a few inches lower on your forearm than a watch, and I can wear it and my Pixel Watch 3 on my right arm at the same time. It's comfortable, unobtrusive, and closes securely thanks to both a small buckle and a magnetic tab that makes it feel kind of like a slap bracelet. The band is waterproof with an IPX7 rating, meaning it can withstand most conditions, including swimming. Just don't bring the actual glasses into the pool because they're rated IPX4, which is only lightly water-resistant.\n\nUsing the Neural Band to control the glasses is as simple as tapping and rubbing your fingers together. With the band on your arm, tap your thumb to the tip of your index finger to select an item on the in-lens display, and tap your thumb to your middle finger to go back from the menu layer or app you're in. Double-tap your thumb and middle finger to bring up the display or put it to sleep. Swipe your thumb across the side of your index finger to navigate through menus. Pinch your thumb and index finger together and rotate your wrist left or right like you're turning a knob to adjust volume. And, if you want to use Meta AI without saying the wake word, you can double-tap your thumb to the side of your index finger (but you'll still have to speak after that). For all of these gestures, the Neural Band will vibrate gently to let you know it's detecting an input.\n\nThis is easily one of the best control systems I've seen on waveguide display smart glasses. Of course, every other remotely similar pair of smart glasses I've used has relied on limited and often unwieldy touch controls on the frames themselves (and the touch controls on the Meta Ray-Ban Display's temple aren't exactly useful, either). The Even G2 comes close with the optional R1 smart ring, which provides a more convenient and easier-to-use touch surface, but that's an $249 accessory not included with the glasses.\n\nI found it easy to get used to the Neural Band, and it was mostly responsive and accurate in testing. It consistently detected my finger taps and horizontal thumb swipes, though it often took a few tries to register vertical swipes to navigate up or down in menus. The dial-twist gesture was also a bit hit-or-miss. This isn't the precise hand-tracking that mixed reality headsets like the Apple Vision Pro and Samsung Galaxy XR can do on their own without a wearable band, but they're much larger and much more expensive devices you generally wouldn't be walking around wearing.\n\nInterface: Polished, Capable, and Weirdly Organized\n\nThe visual interface of the Meta Ray-Ban Display is a clean, colorful, well-labeled menu system that leverages four-directional navigation via the Neural Band. For comparison, every other pair of waveguide glasses I've tried before this has only offered single-axis scrolling menus because of the limited inputs. Meta's interface on the glasses is easy to use, but it's laid out strangely.\n\nThe home view consists of three tabs. The center tab is for notifications and Meta AI, but most of the time it will just be a bar that says \"Ask Meta AI.\" That's because the notifications on the glasses are so limited, which I'll go into more detail about in the next section. From here, you can tap the Meta AI bar and ask a question from this tab, but you can also just say, \"Hey Meta,\" or double-tap your thumb to the side of your index finger, and the glasses will start listening for a command. The tab feels superfluous since it doesn't really have any functions to navigate visually, and you're going to be talking to the AI when you activate it anyway.\n\nThe left tab is a more useful settings and quick-features menu. Here, there are buttons to immediately start live captioning or translating, open the camera tool to take photos or videos, and open the Music app, which displays whatever is playing on your phone and lets you play, pause, rewind, or skip tracks. The tab also has Volume and Brightness dials you can adjust by selecting either of them and performing the knob-turning gesture, as well as buttons to toggle audio-only and do-not-disturb modes. It also offers access to a dedicated, limited settings menu with two options: tilting the display slightly left or right for comfort, and configuring whether message notifications wake the display when it's off.\n\nOn the right tab is the full app list, with buttons that open WhatsApp, Instagram, Messenger, calls, the camera, live captioning and translating, Maps, the music player, an on-glasses photo gallery, text messages, and tutorials. The app list doesn't seem to be arranged in any logical order besides putting Meta's own apps at the top by default, and you can't rearrange them directly, which is frustrating. You can, however, pin the apps you use most and unpin the ones you don't, so there's at least some control.\n\nThe interface isn't hard to learn, but it makes some awkward choices. The Quick Settings tab and the Notifications/Meta AI tab could have been combined. It would have been better to place the captioning, camera, and music buttons below the AI button, rather than in the settings tab, for easy access outside the apps tab. Phone functions like calls and text messages could also have been moved to a quick menu in the AI tab. There aren't a ton of apps to scroll through, but that leads to one of the glasses' biggest problems.\n\nApps and Features: Locked Into Meta's World\n\nThe Meta Ray-Ban Display is built around using Meta's services and, if possible, only Meta's services. The AI is Meta AI, the messaging apps outside of phone text messages are Messenger and WhatsApp, the photo-sharing app is Instagram, and those are the only choices you get (and they are all owned by Meta). You can't talk to Gemini, message over Discord or Slack, or post photos on Bluesky. You can connect your Amazon Music, Shazam, and Spotify accounts, but that's probably because Meta doesn't have its own music streaming service. Even without them, you can at least control any audio playing on your phone through the glasses' Music app, as if it were a widget on your lock screen.\n\nNotifications are the biggest issue. Text messages and voice calls through your phone are supported, and you'll get notifications for them. But those, and messages from Meta apps, are the only notifications the glasses will show. Unlike every other pair of waveguide smart glasses I've tested, these won't read your phone's push notifications.\n\nIn addition, all of the Meta apps, including Instagram, are primarily for communication, not for browsing your social feeds. The Instagram app only brings you to your messages, so you can't browse stories, and Facebook isn't on the glasses at all.\n\nCalendar support is also limited. There's no dedicated calendar app, so you have to ask Meta AI to tell you what your appointments are. You can link your Google or Outlook calendars to your phone, but not if they're work accounts with any kind of managed IT security. And you have to speak every time you want to check your next meeting.\n\nI had initially planned to take the glasses to CES and write an account of covering the show with them. I didn't, because the inability to see incoming Slack messages meant I couldn't use the glasses to keep up with coverage discussions. Moreover, since my Google-based work calendar is protected by IT policies, I couldn't ask Meta when or where I needed to go next for my many appointments. Simply supporting push notifications from third-party apps on my phone would have solved both of those problems on the Meta glasses. That feature is available on the Even Realities G2, which I ended up taking to CES instead.\n\nIf you're a regular Meta user and the software limitations don't bother you, the Meta Ray-Ban Display generally works quite well in executing its main functions. Closed captions are quick and accurate most of the time, and the text is easy to read. All AI-powered voice transcription depends on good sound quality, so it can make mistakes if the speech isn't completely clear or if there's significant background noise, but even then, it's still very usable.\n\nTranslation is also effective, within its very limited scope. I watched some Spanish-language soccer programming on my TV, and the glasses translated it into English with surprising accuracy. They can likely do the same with French or Italian. Those are the only options, though, and that's paltry compared with the Even G2 (31 languages) and the Rokid Glasses (89 languages). There's no Chinese, German, Japanese, Korean, Portuguese, or Vietnamese. Visual translation of other languages is supported using the camera, but not voice.\n\nYou'll have to commit to one language at a time for the glasses to translate. The translation function on the glasses interface doesn't offer any language choices and relies on the app to load a single language pack, a process that can take half a minute.\n\nThese are the first smart glasses I've used where the navigation feature is genuinely useful and provides a readable map. Opening the Maps app on the glasses pops up a large, easily understandable map of your location. Only a few major streets are labeled, but notable locations nearby, like movie theaters, are displayed as pins, and you can use the knob-turn gesture to zoom in closer for additional landmarks. From this view, you can use voice dictation to search for a location, or tap buttons for nearby cafes, restaurants, parks, or attractions. Selecting a destination will display the route as a blue line. From that view, you can start navigation, send the location to your phone, or, if it's a business with a phone number, call it. I found the navigation to be direct and accurate, with the map view tracking my location and orientation as it gave me turn-by-turn directions.\n\nThe Music app is simple, with only track forward, track back, and play/pause buttons, plus a tile showing the time on the track. You'll also see album art if it's available and the app is compatible. No art came through my Android phone using Pocket Casts or YouTube Music, whereas both show album art and podcast icons on the phone itself. As mentioned, its phone widget-like universality also means it can control any audio playing from your phone. However, it doesn't offer the same benefits as a phone widget because it only shows those controls when the app is open and on the display. An icon in the quick settings menu shows you the track playing, but to do anything with it, you have to tap it to open the app first. An in-glasses widget for the app to help populate the central tab would have been really helpful here, rather than requiring you to open its full view.\n\nYou can play/pause and skip tracks with single and double taps on the touch strip on the glasses, but that's all you get for audio gesture support. The Neural Band doesn't give you any audio controls or even provide a shortcut to bring up the Music app quickly. This is baffling because the Meta AI app lets you assign the double-tap gesture to \"your favorite feature,\" but the only options are the default Meta AI activation or disabling it completely.\n\nSound: Loud Enough for You -- and Everyone Else\n\nAudio quality is pretty good for smart glasses. The speakers on the temples produce fairly robust sound that can be easily heard even in a crowded, noisy coffee shop. There's little bass to speak of, but that's just a reality for smart glasses that have a physical gap between their small speaker drivers and your ear canal.\n\nDon't expect any privacy for what you're listening to, though; sound leakage is significant, and anyone near you will be able to hear whatever's playing if it's higher than half volume, which is the floor for comfortable listening on your end in most situations.\n\nCamera: Good Enough to Post, Not to Zoom\n\nThe built-in camera seems to be the same as the one on the second-generation non-Display Meta Ray-Ban glasses. It can shoot 12MP photos or capture 3K video in vertical orientation. Captures are colorful and fairly sharp for smart glasses, with fine detail like my cat's fur coming through clearly. Noise can soften pictures in low light, but in a fairly unobtrusive way. You won't likely notice it if you view or post your full capture, but you won't get much more detail out of them by zooming in. Basically, it's good enough for social media and matches what we've seen from Meta's other recent smart glasses.\n\nBattery Life: Not Quite a Full Day\n\nMeta says the glasses themselves can last up to six hours with mixed use, and the charging case adds another 18 hours. It falls short of the Rokid Glasses' 8-hour battery life, but Rokid's charging case is a $99 accessory sold separately. While using the Meta glasses intermittently and tucking them into the charging case when not in use, I consistently saw all-day use with the battery meter seldom dropping below 50%.\n\nThe Neural Band can last up to 18 hours per charge."
  },
  {
    "source": "infobae",
    "company": "Meta AI",
    "title": "Cómo limitar Meta AI en WhatsApp y cuidar tu privacidad",
    "date": "2026-02-20T17:53:50Z",
    "url": "https://www.infobae.com/tecno/2026/02/20/como-limitar-meta-ai-en-whatsapp-y-cuidar-tu-privacidad/",
    "content": "WhatsApp incorpora un asistente de inteligencia artificial, Meta AI, cuya integración preocupa a algunos usuarios que desean eliminarlo de la app. (Imagen Ilustrativa Infobae)\n\nWhatsApp cuenta con un asistente de inteligencia artificial llamado Meta AI, cuya presencia genera inquietud entre algunos usuarios que buscan desactivarlo por completo en la aplicación móvil. Sin embargo, la única alternativa disponible es restringir su visibilidad y funcionalidad dentro de la plataforma.\n\nLos usuarios pueden eliminar el chat de Meta AI, abstenerse de mencionarlo en grupos o conversaciones individuales y solicitar la eliminación de los datos asociados almacenados en los servidores de Meta.\n\nAlgunas personas sugieren instalar una versión anterior de WhatsApp para evitar la integración de esta herramienta, pero esta práctica implica riesgos de seguridad, ya que las versiones desactualizadas pueden carecer de parches críticos y exponer los dispositivos a vulnerabilidades.\n\nPara limitar la presencia de Meta AI en WhatsApp, pueden considerar las siguientes opciones:\n\nUtilizar una versión anterior de WhatsApp para evitar la integración deMeta AI no es recomendable, ya que estas ediciones desactualizadas suelen carecer de actualizaciones de seguridad y parches esenciales, lo que expone a los dispositivos a posibles vulnerabilidades y riesgos de privacidad.\n\nAdemás, WhatsApp puede restringir el acceso o bloquear cuentas que funcionen con versiones no autorizadas de la aplicación.\n\nNo existe la opción de desactivar Meta AI en WhatsApp, ya que se trata de una función incorporada de manera nativa en la plataforma.\n\nEn este sentido, Joshua Breckman, director de Comunicaciones Internacionales de WhatsApp, señaló al medio británico Standard que la herramienta opera \"como cualquier otra función\" dentro de la aplicación.\n\nEsta limitación ha despertado inquietudes en Europa. La parlamentaria eslovaca Veronika Cifrová Ostrihoňová manifestó en X (antes Twitter) que la ausencia de mecanismos de control sobre la inteligencia artificial genera \"serias dudas sobre el control del usuario y la seguridad digital\".\n\nSegún precisó, ha recibido numerosas consultas de ciudadanos y ya llevó el tema a una sesión del Comité de Mercado Único del Parlamento Europeo, donde lo planteó directamente ante la vicepresidenta ejecutiva y comisaria Henna Virkkunen.\n\nFrente a este escenario, la única alternativa para los usuarios es limitar la visibilidad y las interacciones con la inteligencia artificial en WhatsApp.\n\nUtilizar Meta AI en WhatsApp de forma segura requiere adoptar prácticas que protejan tanto la privacidad como la integridad de los datos personales.\n\nAl interactuar con este asistente de inteligencia artificial, es crucial evitar compartir información sensible, como contraseñas, números de tarjetas de crédito, datos bancarios o cualquier otro dato confidencial.\n\nMeta AI puede acceder a los mensajes enviados dentro de su chat, por lo que limitar las conversaciones a consultas generales disminuye riesgos innecesarios.\n\nAdemás, es recomendable revisar y ajustar la configuración de privacidad de WhatsApp, especialmente las opciones vinculadas al manejo de datos y la interacción con servicios automatizados. Los usuarios pueden eliminar el chat de Meta AI y abstenerse de mencionarlo en grupos o conversaciones individuales para reducir su presencia.\n\nAsimismo, es posible solicitar la eliminación de los datos almacenados en los servidores de Meta, lo que contribuye a un mayor control sobre la información personal.\n\nFinalmente, informarse sobre los términos y condiciones de uso de Meta AI permite conocer sus alcances y limitaciones, fortaleciendo la capacidad del usuario para tomar decisiones seguras al interactuar con esta herramienta de inteligencia artificial."
  },
  {
    "source": "infobae",
    "company": "Meta AI",
    "title": "WhatsApp: cómo desactivar Meta AI de la aplicación y por qué hacerlo",
    "date": "2026-02-18T22:10:30Z",
    "url": "https://www.infobae.com/tecno/2026/02/18/whatsapp-como-desactivar-meta-ai-de-la-aplicacion-y-por-que-hacerlo/",
    "content": "Meta AI es una función integrada en WhatsApp y no puede desactivarse completamente. (Meta)\n\nCómo desactivar Meta AI en WhatsApp es una de las dudas más comunes entre los usuarios. Sin embargo, es importante saber que Meta AI es una función integrada en la plataforma, al igual que las llamadas o los stickers, y no se puede desactivar por completo.\n\nAnte esto, los usuarios pueden reducir su presencia tomando algunas acciones: eliminar el chat con Meta AI, evitar mencionarla en grupos o conversaciones individuales y borrar sus datos del servidor de Meta. De esta forma, es posible limitar la interacción con la inteligencia artificial dentro de la aplicación.\n\nPara reducir la presencia de Meta AI en WhatsApp, los usuarios pueden:\n\nAlgunos usuarios proponen usar versiones anteriores de WhatsApp. No obstante, esto no es recomendable, ya que las versiones antiguas suelen tener vulnerabilidades que pueden comprometer la seguridad del dispositivo.\n\nNo es posible desactivar Meta AI en WhatsApp, ya que es una función integrada en la plataforma. Al respecto, Joshua Breckman, director de Comunicaciones Internacionales de WhatsApp, explicó al medio británico Standard que la herramienta opera \"como cualquier otra función\" dentro de la aplicación.\n\nEsta imposibilidad ha generado preocupación en Europa. La parlamentaria eslovaca Veronika Cifrová Ostrihoňová expresó en X (antes Twitter) que la falta de control sobre la inteligencia artificial plantea \"serias dudas sobre el control del usuario y la seguridad digital\".\n\nSegún indicó, ha recibido numerosas consultas de ciudadanos y ya abordó el tema en una reunión del Comité de Mercado Único del Parlamento Europeo, planteando el asunto directamente al vicepresidente ejecutivo y comisario Henna Virkkunen.\n\nAnte esta situación, los usuarios solo pueden optar por reducir la presencia y las interacciones con la inteligencia artificial dentro de WhatsApp.\n\nPara muchos usuarios, la decisión de reducir o desactivar la presencia de Meta AI en WhatsApp está motivada principalmente por cuestiones de privacidad, ya que al interactuar con la inteligencia artificial, parte de la información puede ser procesada y almacenada en los servidores de Meta.\n\nQuienes prefieren mantener sus conversaciones completamente privadas suelen limitar esta función para evitar una mayor recopilación de datos.\n\nOtra razón habitual es la preferencia personal: algunos usuarios no desean la intervención de una inteligencia artificial en sus chats, ya que pueden considerarla invasiva o sentir que afecta la experiencia tradicional de mensajería.\n\nSin embargo, Meta AI también puede resultar útil para quienes deciden utilizarla. Ofrece respuestas instantáneas, sugerencias de contenido y asistencia en la redacción de mensajes, lo que facilita y agiliza las conversaciones.\n\nPara utilizar Meta AI de forma segura en WhatsApp, es importante tomar algunas precauciones básicas:"
  },
  {
    "source": "Slate Magazine",
    "company": "Meta AI",
    "title": "A Creepy New Device Is Spreading Across School Campuses. Students Are Being Harassed. Teachers Are Sounding the Alarm.",
    "date": "2026-02-12T10:50:41Z",
    "url": "https://slate.com/technology/2026/02/mark-zuckerberg-meta-ai-glasses-school.html",
    "content": "As the discreet wearable cameras become more popular, students are saying they feel constantly watched and harassed -- and professors are reshaping their classrooms in response.\n\nSign up for the Slatest to get the most insightful analysis, criticism, and advice out there, delivered to your inbox daily.\n\nJoziah was tabling on campus for his peer mentor job at the end of last semester at Florida State University when he noticed something strange happening across the quad: A trio of men, wearing Meta AI glasses, were stopping every young woman who passed by and asking them for their social media contacts.\n\n\"I recognized them from TikTok, because they're kind of big, especially in Miami,\" the 19-year-old told me. \" I'm seeing them literally go up to every single girl that's passed by with them.\"\n\nHe posted a video of the incident on his TikTok, which quickly garnered 200,000 views. Women who had seen the same event unfold flocked to the comments to share their frustration with the situation. \"Literally I was one of their victims,\" one user said. Another wrote, \"They were so rude to ppl asking them what they were doing too.\"\n\nOthers shared their fear of the same thing happening to them. \"I literally have nightmares about this,\" yet another wrote.\n\nThis isn't the first time that Joziah, who asked to have his last name withheld to protect his identity, has had an uncomfortable encounter with the new wearable recording devices. During a football game, a stranger secretly recorded a video of his friend and posted it online, to her shock. When Joziah returned to his hometown over winter break, he went out with some female friends, during which they were recorded on Meta AI glasses and posted on social media without their consent. They learned about it after the clip, a POV video of the creator approaching Joziah's friends at a bar, went viral.\n\n\"It definitely made me feel uneasy,\" Joziah recalled of the experience. \"I felt uncomfortable watching them go up to all these girls to record their interactions of them hitting on them.\"\n\nAfter pushing billions of dollars into the metaverse, Meta has now found overwhelming demand for its Ray-Ban display glasses, which allow users to take photos, stream content, and talk to an A.I. assistant. Waitlists for the product have surged, and the company's pivot away from the metaverse and toward smart glasses has become aggressive: Hundreds of Meta workers in the Reality Labs division and virtual games studios were laid off, product rollout was paused to address the supply shortage, and it was reported that Meta and EssilorLuxottica, Ray-Ban's owner, are discussing possibly doubling production capacity for the A.I.-powered glasses by the end of this year, all in a bid to capitalize on the growing demand as well as get ahead of competitors.\n\nBut its usages -- specifically its ability to capture photo and video -- have raised questions about how the devices will be applied in real life, especially in settings among children and young adults. Issues around academic dishonesty, classroom surveillance, and harassment have been growing in recent years but have been exacerbated as students gain access to the controversial wearable.\n\nChloe Peichl, who is 18, is a senior at a small private school in Texas. She told me that the introduction of generative A.I. tools and new technology has vastly shifted the environment at school.\n\n\" A.I. and ChatGPT, oh my gosh,\" she said. \"It's insane. Innocent people are getting accused of [copying their homework from ChatGPT] and having to rewrite 15-page essays that they worked all year on.\"\n\nIt has resulted in a schoolwide crackdown. All wearable technology -- from Apple Watches to smart glasses -- is banned. Phones are not allowed on premises. Even laptops were barred, causing an uproar from students who felt very strongly about being forced to use school-issued Chromebooks for their final papers. Peichl found these guidelines to be incredibly frustrating when writing her mandatory senior thesis.\n\n\" I mean, I can see where they're coming from, but I don't know,\" she said. \"Just eliminating tech completely from schools, I think, is not exactly helpful.\"\n\nThat's not to say that people don't try to circumvent the rules. Peichl said that some try to sneak their Apple Watches in; others attempt to hide their phones where they think teachers can't see them. But Meta AI glasses are new, and while those devices aren't as common, adults are on the watch now. She recalled one student who tried to sneak their Meta AI glasses into class.\n\n\"I don't know what he was using them for, but he has been known for trying to cheat on schoolwork and stuff,\" she said. \"I don't know if that's what he was using them for. And he got them taken away immediately.\"\n\nJamie Cohen, assistant professor of media studies at CUNY Queens College, said that he is highly aware of the impact that new tech has on the classroom environment. Quizzes in particular have become difficult to issue, as students frequently use ChatGPT and wearables, in addition to sharing digital copies of answer keys through end-to-end encrypted apps like Signal, Telegram, and WhatsApp. \"It does inspire professors to change their material consistently,\" he told me. \"The school, as many do, has an official policy against recording, so it falls under that, but glasses are a bit easier to get away with.\"\n\nBut cheating isn't the key issue; the problem that Cohen often sees is the ripple effects of constant surveillance on the participation of students in discussions and seminars, spaces that require a modicum of vulnerability, expression, and debate. \"By comparison to the millennials I used to teach, Gen Z are far more quiet and reserved because they have a strange fear of being cringe or judged as cringe,\" Cohen said. \"From both observation and asking them, I know they feel paranoid when they're answering a question, aware that someone may record them.\"\n\nJoziah feels that this has definitely affected the way he moves about his life, especially in spaces where he knows that people are constantly whipping out their phones. \" I think even just now, with everything being digital, with everything being recorded, I'm just a lot more aware of, like, my digital footprint, what I'm doing in public at school,\" Joziah said. \"I feel like I'm being recorded every day, all the time, especially at school.\"\n\nOf course, the fear of being recorded without consent didn't start with Meta glasses. After all, this is the age of panopticontent, when one unconsenting clip of you could become a viral meme or the lightning rod for a global investigation of your character, all depending on how you were caught on camera. There is an overwhelming sense of Constantly Being Perceived, even if it doesn't always show up on a day-to-day basis.\n\nBut now it's not just phones that could be recording you. \" Obviously, there's always the concern of being filmed somewhere while you're out,\" Joziah added. \" But it is just something I'm extra conscious of when I'm going out. Now, if someone comes up to me wearing glasses, that's the first thing I'm looking at. 'Are these Meta glasses?' \"\n\nPeichl, the Texas high school student, admitted that while the laptop ban is annoying for schoolwork, there is a sense of familiarity among her classmates that has occurred perhaps because they aren't constantly on their devices. \" We all talk to each other a lot,\" she said. \" I'm sure if we did just have our phones all the time, we wouldn't be as close.\"\n\nStill, she believes that the teachers' fears of Meta AI glasses being used as a cheating tool are a bit far-fetched, at least for now. The camera still emits a loud flash and shutter sound when taking photos, and there's a light that turns on when it records video. So the device isn't as discreet in some regards, but future models might be harder to spot. The technology certainly isn't going anywhere, and will only add more features as long as demand continues to balloon. Upcoming models of the glasses have been confirmed to include a teleprompter, as well as the ability to respond to texts with hand gestures; Cohen added that tech-savvy young people will also learn how to make their own modifications to the hardware.\n\n\"The more the tech gets better, the less likely we'll be able to detect Meta AI glasses wearers, especially if they jailbreak the recording light,\" Cohen said.\n\nIt's true that the product is still in its early adoption stage, which makes it something of a flex or a novelty product for many. In school, it's something only used to impress peers, or for students who are hell-bent on constantly making content. Especially for people chasing that lucrative genre of stunt-based, man-on-the-street videos, the product is a kind of performance in and of itself, a show of wealth and access, more interesting and click-worthy than just recording on a phone. Cohen notes that he's also noticed this trend among students -- the ones who are constantly wearing their smart glasses are the ones who consider themselves content creators.\n\n\"The ones who aren't creators or influencers see wearers as cringe,\" Cohen said, sardonically summing up the general feeling of nonwearers as \"Your life is not that cool, bro.\" He added, \"In school, at least at mine, they serve no reasonable purpose aside from recording 'day in the life' or 'get ready with me' content.\"\n\nBut even for a generation that has normalized the grind of vying for social media fame, agency is still important, which is why Cohen says that most of his students are staunchly anti-Meta AI glasses. \"They are keenly aware of the environment where they can find themselves online without their permission,\" he said. \"They hate feeling creepy.\"\n\nThere is a gold rush-style frenzy for young people to capitalize on the expanding industry of content creation, and the push to constantly mine the attention economy for clicks, comments, shares, and follows has resulted in some murky ethical waters. Already, women have begun speaking out about their experiences being surreptitiously recorded on smart glasses and the feeling of violation that arose when they realized that they'd been posted online for content.\n\nJoziah said that the click mines have significantly affected campus culture. While he already feels vigilant and wary about being recorded, he notes that it's much worse for his friends who are women, both inside controlled settings like the classroom and outside.\n\n\"There's already this fear that women have when going out, getting their drinks spiked or getting harassed by men. And I think that this just adds onto it because on top of the harassment, they're worrying about the harassment being recorded, and then being used to make profit from that content,\" he said. \"As a guy, it's not something that I will ever 100 percent be able to speak on, but  I've seen how it's affected my friends and how damaging it can be.\"\n\nGwyneth Agbenyo, one of Cohen's students, told me that being recorded without consent is something that she and her peers worry about. \"We see it online all the time: Someone caught in the background of a picture looking less than pleased is branded a 'hater' or a 'bully' and becomes the internet's punching bag for a day,\" the 23-year-old said. \"When you see things like that all the time, you have to live with the knowledge that it could be you one day, because everyone has accepted the reality that the second you step outside, you waive your right to privacy.\"\n\nShe added, \"We're already living in a digital panopticon and seeing the effects of it on our culture, especially when it comes to the anxieties of young people.\"\n\nAlthough Agbenyo hasn't personally experienced someone bringing Meta glasses into class, she has heard of them appearing on campus in other places. Hearing stories about it, and seeing content of people being filmed on her own social feeds, is enough to make the anxiety persist. As the technology becomes more normalized, she says, part of the issue is that there's no more choice for people to opt out of being filmed. \"While I understand the utility of a hands-free recording device for creative purposes, I can't help but feel like there are no safeguards to prevent this technology from being used for sinister purposes,\" she said. \"The same way generative A.I. technology was immediately used to create deepfakes and undress women, I really wouldn't be surprised if these glasses were used for sexual harassment purposes as well.\"\n\nCohen adds that he has seen a similar fear among many of his students who are immigrants and first-generation citizens. These students have expressed concerns about being recorded and surveilled by law enforcement with discreet wearable tech, a fear that has only compounded at a time when both U.S. citizens and immigrants are being profiled, recorded, violently detained, and brutalized by police officers and Immigration and Customs Enforcement agents alike.\n\n\"They've heard stories of ICE wearing the Meta AI glasses, and that worries them immensely,\" he said. \"One student literally told me she doesn't want to be a viral video on Fox.\"\n\nAnd of course, the anxiety of being unknowingly recorded isn't limited to young people. \"I too have a huge fear of students recording my lectures,\" Cohen said. \"I teach pretty off the cuff, and my lectures are critical, so I'm also very tuned in to making sure I'm not getting recorded.\"\n\nThe fears around omnipresent surveillance tools not only have made students more apprehensive about saying the wrong thing in class or making a mistake, but have also turned many of Agbenyo's peers away from social media altogether.\n\n\"A lot of my peers don't even post on social media anymore,\" she said. \"I have one picture on my feed, and most of my personal friends have either deleted all of their pictures or just don't post anymore aside from their stories, which disappear after 24 hours.\"\n\nHigh schooler Peichl, however, doesn't fear the surveillance aspect of wearable tech like the Meta AI glasses. Although she grasps the issues that can arise with the product, she says, most of her life is pretty well documented anyway. It always has been. \" Personally, nothing is really super hidden in my life,\" she said. \"I feel like we understand tech enough nowadays to know what we can control, and what we can't and can do.\""
  },
  {
    "source": "TN8 - Noticias de Nicaragua y El Mundo",
    "company": "Meta AI",
    "title": "Pasos para desactivar Meta AI de WhatsApp y por qué hacerlo | TN8.ni",
    "date": "2026-02-19T17:43:53Z",
    "url": "https://www.tn8.ni/tecnologia/pasos-para-desactivar-meta-ai-de-whatsapp-y-por-que-hacerlo/",
    "content": "Meta AI en WhatsApp se ha convertido en una de las funciones más comentadas por los usuarios, pero también genera dudas sobre privacidad y control de datos.\n\nAunque muchos quieren desactivar Meta AI, es importante aclarar que esta herramienta está integrada en la plataforma; al igual que las llamadas o los stickers, y no se puede eliminar por completo.\n\nSin embargo, los usuarios pueden reducir su presencia siguiendo algunos pasos sencillos. Entre ellos, borrar el chat con Meta AI como cualquier otra conversación y utilizar el comando \"/reset-ai\" para eliminar la información almacenada en los servidores de Meta. Además, se recomienda evitar mencionar a la IA en grupos o chats individuales mediante el comando \"@meta ai\".\n\nEl intento de usar versiones antiguas de WhatsApp no es recomendable, ya que estas pueden tener vulnerabilidades que comprometen la seguridad del dispositivo.\n\nMeta AI en WhatsApp\n\nSegún Joshua Breckman, director de Comunicaciones Internacionales de WhatsApp, Meta AI funciona \"como cualquier otra función\" de la aplicación; lo que explica por qué no es posible desactivarla completamente.\n\nLa preocupación no es solo individual. La parlamentaria eslovaca Veronika Cifrová Ostrihoňová expresó en X (antes Twitter) que la falta de control sobre la inteligencia artificial plantea dudas sobre la seguridad digital y la capacidad del usuario para gestionar sus datos.\n\nPara quienes deciden mantener cierta privacidad, limitar la interacción con Meta AI es crucial para evitar que parte de su información sea procesada y almacenada.\n\nSin embargo, la IA también puede ser útil; ofrece respuestas rápidas, sugerencias de contenido y asistencia en la redacción de mensajes, agilizando la comunicación.\n\nPara un uso seguro, es recomendable no compartir datos sensibles, ajustar la configuración de privacidad, mantener la app actualizada y usar Meta AI solo para consultas generales. Así, los usuarios pueden disfrutar de la herramienta sin comprometer su intimidad digital."
  },
  {
    "source": "WebProNews",
    "company": "Meta AI",
    "title": "Brussels Draws a Line: The EU's Showdown With Meta Over AI Chatbot Interoperability on WhatsApp",
    "date": "2026-02-09T20:59:59Z",
    "url": "https://www.webpronews.com/brussels-draws-a-line-the-eus-showdown-with-meta-over-ai-chatbot-interoperability-on-whatsapp/",
    "content": "The European Union has fired a warning shot at Meta Platforms Inc., signaling that the tech giant's handling of rival artificial intelligence chatbots on its WhatsApp messaging platform may violate the bloc's sweeping Digital Markets Act. The move represents one of the most consequential regulatory confrontations yet over how dominant technology companies must accommodate competitors in the age of generative AI, and it could reshape the competitive dynamics of the rapidly evolving chatbot market across the continent and beyond.\n\nThe European Commission, the EU's executive arm, issued preliminary findings on July 17, 2025, informing Meta that its practices around third-party AI chatbot interoperability on WhatsApp appear to fall short of the obligations imposed by the Digital Markets Act, or DMA. According to Engadget, the Commission's concern centers on the way Meta has structured -- or, critics argue, obstructed -- the ability of rival AI services to function within WhatsApp, the world's most widely used messaging application with over two billion users globally.\n\nThe Digital Markets Act, which came into full enforcement in March 2024, was designed to curb the market power of so-called \"gatekeepers\" -- large technology platforms that serve as critical intermediaries between businesses and consumers. Meta, alongside Apple, Alphabet, Amazon, Microsoft, and ByteDance, was designated as a gatekeeper, subjecting its core platform services, including WhatsApp, to a battery of obligations meant to ensure fair competition and user choice. Among the most ambitious of these requirements is the interoperability provision, which compels messaging platforms to allow third-party services to connect and exchange messages with their users.\n\nThe interoperability mandate was always expected to be one of the DMA's most technically and legally complex provisions. Meta has publicly stated that it is working to comply, rolling out technical frameworks that would allow third-party messaging services to connect with WhatsApp. However, the Commission's preliminary findings suggest that Meta's implementation has been insufficient -- particularly when it comes to enabling rival AI chatbots to operate seamlessly within the WhatsApp ecosystem. The concern is not merely about basic text messaging between platforms, but about whether Meta is creating conditions that effectively privilege its own AI assistant, Meta AI, over competing products from companies like OpenAI, Google, Mistral, and others.\n\nMeta has invested billions of dollars in its AI ambitions, integrating its proprietary Meta AI chatbot directly into WhatsApp, Instagram, Facebook, and Messenger. The assistant, powered by Meta's Llama family of large language models, is accessible to users with a single tap, embedded natively into the search bar and conversation interfaces of these apps. This deep integration gives Meta AI a significant distribution advantage -- one that rivals simply cannot match if they are locked out of or marginally supported on the platform. As reported by Engadget, the EU's preliminary view is that Meta's approach to third-party AI chatbot access on WhatsApp does not meet the DMA's requirements for fair and non-discriminatory treatment.\n\nThe stakes are enormous. WhatsApp is the dominant messaging platform in most of Europe, Latin America, Africa, and large parts of Asia. For AI companies trying to reach consumers, being present inside WhatsApp is not a luxury -- it is a necessity. If Meta can use its control over WhatsApp to funnel users toward Meta AI while making it difficult or impossible for competitors to offer their own chatbot experiences within the app, the competitive implications are profound. Rival AI developers would be forced to rely on their own standalone apps or websites, where user acquisition costs are dramatically higher and engagement is typically lower.\n\nThe preliminary findings issued by the Commission are not yet a final determination of non-compliance. Under the DMA's enforcement procedures, Meta now has the opportunity to respond to the Commission's concerns, present evidence of its compliance efforts, and potentially propose remedies. If the Commission ultimately concludes that Meta has violated the DMA, the penalties could be severe: fines of up to 10 percent of the company's total worldwide annual turnover, which based on Meta's 2024 revenues could amount to tens of billions of dollars. For repeated infringements, the ceiling rises to 20 percent, and the Commission also has the authority to impose structural remedies -- including, in theory, requiring the divestiture of business units.\n\nThis is not the first time Meta has clashed with EU regulators under the DMA. The Commission has previously investigated Meta's \"pay or consent\" advertising model, under which European users were asked to either pay a subscription fee or agree to personalized advertising. That investigation led to Meta modifying its approach, though the Commission has continued to scrutinize whether the revised model truly offers users a free and meaningful choice. The AI chatbot interoperability issue, however, strikes at a different and arguably more strategically important dimension of Meta's business: the company's ambition to make Meta AI the default intelligent assistant for billions of people worldwide.\n\nThe EU's move has been closely watched by Meta's competitors in the AI space. Companies like OpenAI, which operates ChatGPT, and Google, which offers Gemini, have a direct interest in gaining access to WhatsApp's massive user base. Smaller European AI firms, including France's Mistral AI and Germany's Aleph Alpha, have also argued that platform gatekeepers must be required to provide meaningful interoperability if Europe's homegrown AI sector is to have any chance of competing with American and Chinese giants. The DMA was, in many ways, designed with exactly this kind of scenario in mind -- preventing dominant platforms from leveraging their existing market power to foreclose competition in adjacent and emerging markets.\n\nMeta, for its part, has consistently maintained that it supports interoperability and is working in good faith to meet its obligations under the DMA. The company has pointed to the technical complexity of enabling third-party messaging interoperability while maintaining end-to-end encryption, user privacy, and platform security. These are legitimate concerns -- integrating external AI chatbots into a messaging platform raises real questions about data handling, content moderation, and the potential for abuse. However, the Commission appears to have concluded, at least preliminarily, that Meta's technical and policy choices have gone beyond what is necessary to address these concerns and have instead served to protect Meta AI's privileged position.\n\nOne of the most technically nuanced aspects of this dispute involves WhatsApp's end-to-end encryption. Meta has argued that opening WhatsApp to third-party AI chatbots could compromise the encryption protocols that protect user communications. When a user interacts with an AI chatbot, the conversation data must be processed by the chatbot's servers, which means it necessarily leaves the encrypted WhatsApp environment. Meta has used this argument to justify restrictions on how third-party AI services can interact with WhatsApp users, but regulators and competitors have pushed back, arguing that Meta applies different standards to its own AI assistant, which also processes user queries on Meta's servers.\n\nThe encryption debate is not merely technical -- it is deeply political. European regulators have long grappled with the tension between strong encryption and the need for law enforcement access, content moderation, and competitive openness. The DMA's interoperability provisions were drafted with an awareness of these tensions, and the regulation explicitly states that gatekeepers may take measures to ensure security and encryption, but that such measures must be proportionate and must not serve as a pretext for anti-competitive behavior. The Commission's preliminary findings suggest that it believes Meta has crossed that line.\n\nThe outcome of this case will have ramifications far beyond Meta and WhatsApp. It will set a precedent for how the DMA's interoperability provisions are interpreted and enforced, not just for messaging services but potentially for other platform services where AI is becoming a central feature. Apple, for example, has faced its own DMA scrutiny over its App Store practices and could face similar questions about how it integrates its own AI capabilities into iMessage and Siri while potentially restricting competitors. Google's treatment of rival AI assistants on Android is another area that could come under the microscope if the Commission establishes a strong precedent in the Meta case.\n\nFor Meta, the financial and strategic implications are significant. The company has bet heavily on AI as the next major growth driver, with CEO Mark Zuckerberg repeatedly describing Meta AI as the company's most important product initiative. If the EU forces Meta to open WhatsApp to rival AI chatbots on equal terms, it could erode one of Meta AI's most powerful distribution advantages and create a more competitive environment in which users can choose from a range of AI assistants within the apps they already use every day. That would be a win for consumer choice and competition -- and precisely the outcome the DMA was designed to achieve.\n\nMeta now faces a critical decision: whether to negotiate with the Commission and offer concessions that satisfy regulators, or to contest the findings and risk a protracted legal battle with potentially massive financial consequences. The company's response in the coming weeks will be closely watched by regulators, competitors, and investors alike, as it will signal not only Meta's approach to European regulation but also the broader willingness of American tech giants to accommodate the EU's increasingly assertive stance on digital competition. Whatever the outcome, the case marks a new chapter in the ongoing struggle to define the rules of engagement for AI in the world's most regulated digital market."
  },
  {
    "source": "eldia.com.bo",
    "company": "Meta AI",
    "title": "Cómo activar el modo San Valentín en WhatsApp para los enamorados: es rápido y sencillo",
    "date": "2026-02-06T22:51:05Z",
    "url": "https://www.eldia.com.bo/2026-02-06/tecnologia/como-activar-el-modo-san-valentin-en-whatsapp-para-los-enamorados-es-rapido-y-sencillo.html",
    "content": "Los usuarios pueden pedir a Meta AI recomendaciones personalizadas de regalos para su pareja\n\nSe acerca San Valentín y, si deseas darle un toque festivo a tu experiencia en WhatsApp, puedes aprovechar varias funciones inspiradas en esta celebración.\n\nLa plataforma permite, por ejemplo, consultar a Meta AI para obtener ideas de regalos personalizados o crear tarjetas digitales especiales para tu pareja.\n\nEl llamado \"modo San Valentín\" no es una función específica de WhatsApp, sino la combinación de diferentes herramientas y opciones disponibles en la aplicación para personalizar mensajes y sorprender a tus contactos en esta fecha.\n\nPuedes explorar estas funciones para agregar un aire romántico a tus conversaciones, utilizar recomendaciones de inteligencia artificial y diseñar detalles únicos desde la propia app.\n\nCómo pedirle recomendaciones de regalo a Meta AI\n\nPara pedirle recomendaciones de regalo a Meta AI de San Valentín, sigue estos pasos:\n\nAbrir la aplicación móvil.\n\nPulsar el ícono de un círculo azul y morado ubicado en la parte inferior derecha.\n\nAl solicitar recomendaciones de regalos de San Valentín a Meta AI en WhatsApp, ten en cuenta que puedes pedir ideas personalizadas según los intereses y características de tu pareja. Por ejemplo, puedes consultar:\n\n\"Sugiere regalos de San Valentín para alguien que ama la música.\"\n\n\"¿Qué puedo regalarle a mi pareja si le gusta viajar?\"\n\n\"Dame ideas originales para una sorpresa romántica en casa.\"\n\n\"¿Qué detalles puedo preparar para un aniversario especial con poco presupuesto?\"\n\nAprovecha la inteligencia artificial para obtener sugerencias adaptadas a gustos, hobbies o experiencias compartidas, explora opciones creativas según el perfil de tu pareja y solicita inspiración para regalos únicos y significativos.\n\nCómo hacer tarjetas de San Valentín con Meta AI\n\nPara crear tarjetas de San Valentín con Meta AI en WhatsApp, accede a la inteligencia artificial y escribe instrucciones claras que definan el estilo y el mensaje que deseas. Por ejemplo, puedes utilizar prompts como:\n\n\"Crea una tarjeta romántica de San Valentín con flores y un mensaje personalizado.\"\n\n\"Diseña una tarjeta digital con corazones y la frase 'Te amo'.\"\n\n\"Haz una tarjeta divertida para sorprender a mi pareja el 14 de febrero.\"\n\nPersonaliza los prompts según tus preferencias o las de tu pareja, elige el diseño y el tono que más te guste y aprovecha las opciones creativas que ofrece Meta AI para enviar un detalle original y especial en esta fecha.\n\nPara hacer un fondo de chats con inteligencia artificial en WhatsApp bajo el modo San Valentín, sigue estos pasos:\n\nSeleccionar 'Chats' y luego 'Tema predeterminado del chat'.\n\nPulsar 'Crear con IA'.\n\nAgregar una descripción para que la imagen generada sea de San Valentín.\n\nSi tienes planeado realizar una fiesta en San Valentín y quieres coordinar todos los detalles del evento, puedes hacerlo en un grupo de WhatsApp. Para crear un grupo de WhatsApp, sigue estos pasos:\n\nEn un grupo de WhatsApp puedes compartir toda la información relacionada con el encuentro, como la fecha, la hora y el lugar. Además, tienes la opción de crear un evento dentro del grupo para que los participantes confirmen su asistencia o reciban recordatorios automáticos.\n\nEsta función resulta especialmente útil para organizar reuniones, coordinar detalles logísticos y asegurar la participación de todos los miembros; facilita el seguimiento de cambios de último minuto y permite mantener informados a los invitados de manera sencilla y centralizada.\n\nPara quiénes es WhatsApp modo San Valentín\n\nEl modo San Valentín de WhatsApp está pensado para quienes desean agregar un toque romántico y personalizado a sus conversaciones durante esta fecha especial.\n\nEs ideal para parejas, amigos o cualquier persona que quiera sorprender a sus seres queridos con mensajes creativos, tarjetas digitales y recomendaciones originales, aprovechando las funciones y herramientas de la aplicación."
  },
  {
    "source": "Search Engine Land",
    "company": "Meta AI",
    "title": "Inside Meta's AI-driven advertising system: How Andromeda and GEM work together",
    "date": "2026-01-28T13:00:28Z",
    "url": "https://searchengineland.com/meta-ai-driven-advertising-system-andromeda-gem-468020",
    "content": "Andromeda and GEM now determine how ads are selected, ranked, and sequenced across Meta. Here's what changed and what drives results in 2026.\n\nWhen Meta launched advertising nearly two decades ago, performance was driven by manual inputs - targeting rules, account structure, and incremental optimization.\n\nSuccess depended on carefully defined audiences, granular budget control, and frequent testing.\n\nThat operating model eroded over time as privacy changes and signal loss made deterministic targeting less reliable.\n\nOver the last two years, Meta responded by fundamentally rebuilding its advertising platform around AI.\n\nThat rebuild began with Andromeda, a personalized ads retrieval engine, and expanded into Meta's Generative Ads Recommendation Model (GEM).\n\nTogether, these systems now determine how ads are selected, ranked, and delivered across Meta's ecosystem.\n\nMeta Ads is no longer an open, manual optimization environment. Performance now depends on understanding how the system evaluates inputs and learns over time.\n\nThis article breaks down how Andromeda and GEM work, how they changed ad delivery, and what it takes to align strategy with Meta's AI-first advertising system in 2026.\n\nAndromeda: Meta's first major AI overhaul\n\nAndromeda is Meta's AI-driven ads retrieval system that decides which ads are eligible to be shown to a user.\n\nInstead of starting with advertiser-defined audiences, it works in reverse, by first evaluating historical engagement, ad copy, creative, and format.\n\nThis helps Andromeda predict which users are most likely to engage with the ad and contribute to your campaign optimization goals.\n\nThis AI system began rolling out in late 2024. In 2025, Andromeda became a core component of Meta's updated infrastructure.\n\nAdvertisers saw changes firsthand as:\n\n* Broad targeting began to outperform previous top-performing interest stacks.\n\n* Simplified account structures started to win.\n\n* Creative fatigue accelerated.\n\nThese were blatant signals that ad retrieval had changed.\n\nWhat Andromeda changed\n\nWith the rollout of Andromeda, Meta shifted away from audience-first advertising to creative-first matching.\n\nTargeting became less deterministic as interests and lookalikes no longer performed as strongly as they once did.\n\nInstead, creatives became the primary signal as the system evaluated creative elements like visuals, themes, hooks, and language to determine relevance.\n\nAI drives better performance when it has a larger opportunity pool to draw from.\n\nBroader campaigns with more creative inputs give the system more options to match ads to users to achieve campaign goals.\n\nEnter GEM: Meta's central AI brain\n\nGEM is Meta's large-scale generative AI system that acts as the ad platform's central intelligence.\n\nIt identifies patterns across organic interactions and ad sequences, formats, and messaging, synthesizing engagement, behavioral, and conversion data.\n\nMost importantly, GEM feeds predictions into Andromeda. These insights help predict what works best, for whom, and when, at scale, while continuously learning.\n\nGEM began rolling out in mid-2025, with broad impact by Q4 2025.\n\nIt's now \"4x more efficient at driving ad performance gains\" compared to original ads recommendation ranking models, according to Meta.\n\nWhy GEM is a bigger shift than Andromeda\n\nAndromeda decides what can be shown, while GEM determines what should be shown next.\n\nThink about it like this: Andromeda decides which ads make it onto the shelf, while GEM learns what shoppers buy and shapes what gets featured next.\n\nAdvertisers who have become accustomed to fast testing cycles and frequent edits will require a bit of a mindset shift in 2026 as long-term patterns matter more than short-term performance fluctuations.\n\nAds are increasingly evaluated within broader contextual journeys.\n\nDig deeper: Rethinking Meta Ads AI: Best practices for better results\n\nWhat Meta's AI stack means for advertisers in 2026\n\nAdvertisers can position themselves for stronger performance this year by shifting efforts toward creative strategy and diversity, simplifying account structure, and embracing patience and stability.\n\nConsider creative strategy the core lever\n\nThis year, lean into signaling by serving Meta a buffet of variables.\n\n* Test creative angles tailored to various personas - not just micro-variations.\n\n* Create clear video hooks with strong statements and questions that communicate value quickly.\n\n* Implement a diverse selection of formats that include images, videos, carousels, user-generated content (UGC), and testimonials.\n\nFocus on creating more creative variations and setting up a scalable system.\n\nThese tactics will give Meta's AI more to work with and thus deliver more results.\n\nDig deeper: How to test UGC and EGC ads in Meta campaigns\n\nSimplify structure for better performance\n\nThe days of hyper-segmentation are gone. Instead, consolidate campaigns and ad sets.\n\nWith this tactic, we've already seen tremendous improvement in client accounts.\n\nIn some accounts, it's now become typical to see only one or two campaigns.\n\nHaving fewer campaigns, broader targeting, and consolidated budgets allows Andromeda and GEM to learn faster and identify winning patterns.\n\nIt may be challenging to relinquish that control, especially if you've been working on the platform for years.\n\nBut you'll slow down learning if you avoid applying these best practices or if you prioritize the manual boundaries that are still available (for now).\n\nEmbrace learning stability\n\nRefrain from adjusting ad elements too often, as frequent edits reset the learning phase and can interrupt pattern recognition.\n\nPatience is a competitive advantage given the current state of the system.\n\nEarly volatility is common and doesn't necessarily signal failure. Before launching new campaigns or assets, decide on a minimum no-touch window.\n\nThis may be a week or 50-75 conversions (whichever comes first), where you commit to not making any changes unless something is truly broken.\n\nLooking at rolling performance windows, such as three- to seven-day trends, instead of daily spikes. This can help you understand how the system spends, performance, and evaluates success.\n\nTreat your budget as a signal\n\nAs with most advertising platforms, more budget helps you learn faster, get more results, and optimize more quickly.\n\nYou could get by with a lower budget in Meta Ads. But it may be more challenging, as low spend limits learning ability.\n\nMeta performs best when budgets allow campaigns to generate consistent conversion data, creating enough volume for the system to detect trends.\n\nEnsure that your daily budget is realistic and aligned with the ad set's conversion event.\n\nHigh-intent events, such as purchases or qualified leads, require more ad spend per learning cycle than upper-funnel actions, such as engagements or clicks.\n\nRethink your role as an advertiser\n\nWith hand-picked targeting strategies now a thing of the past, we're no longer manual optimizers.\n\nInstead, we should level up as strategists and creative architects. This way, we can help our brands and clients:\n\n* Define clear brand positioning.\n\n* Create strong creative inputs.\n\n* Collaborate with design teams to build scalable creative development processes.\n\n* Set guardrails for brand integrity.\n\nAs humans, it's our responsibility to provide judgment and develop novel ideas while Meta's AI handles better targeting and optimization with the data it has.\n\nDig deeper: 3 PPC myths you can't afford to carry into 2026\n\nHow to win in Meta's AI-first ecosystem\n\nFrom what we've seen transpire on the platform in the last few years, Meta has made its direction unmistakably clear: AI is now the foundation of Meta Ads.\n\nIf you embrace it and complement it with human guidance, you can succeed and scale.\n\nTrusting the system now is more important than ever because Meta's AI is a determining factor of success.\n\nSucceeding with Meta Ads in 2026 comes down to feeding the platform diverse, high-quality inputs and creating strategies and content that align with how Meta's AI learns and optimizes.\n\nThe tools have changed, but the opportunity to get creative and find success hasn't."
  },
  {
    "source": "وكاله عمون الاخباريه",
    "company": "Meta AI",
    "title": "Ammonnews : WhatsApp is working on a feature that enables reasoning capabilities for Meta AI",
    "date": "2026-01-26T09:37:49Z",
    "url": "http://en.ammonnews.net/article/88508",
    "content": "Ammon News - WhatsApp has released a new Android update through the Google Play Beta Program, bringing the version up to 2.26.3.10.\n\nWhat's new in this update? WhatsApp is working on a feature that enables reasoning capabilities for Meta AI, and it will be available in a future update!\n\nWhatsApp is working on thinking mode for Meta AI\n\nThe leading source for real-time updates on WhatsApp beta for Android, iOS, and Web \" WABetaInfo\" said: \"In the post about the WhatsApp beta for Android 2.25.23.24 update, we announced a feature that adds a Meta AI shortcut to the message options.\"\n\nThis feature allows users to quickly ask Meta AI questions about specific messages. Instead of forwarding the message to Meta AI manually, users can use this option to send the message to Meta AI while providing an additional prompt before confirming the request. This convenience enhances the interaction process, making it faster for users to get information and insights without performing extra steps.\n\nWhile other AIs continue to evolve and implement new innovative tools, Meta AI has been relatively quiet for a while, with no major developments shared with the public. But now, it seems Meta wants to start introducing improvements aimed at improving the user experience. Following the release of the latest WhatsApp beta for Android 2.26.3.10 update, which is available on the Google Play Store, we discovered that WhatsApp is working on a feature that enables reasoning capabilities for Meta AI."
  },
  {
    "source": "NaturalNews.com",
    "company": "Meta AI",
    "title": "Tech executive suffers psychotic breakdown after prolonged use of Meta's AI glasses - NaturalNews.com",
    "date": "2026-01-23T02:46:45Z",
    "url": "https://www.naturalnews.com/2026-01-22-executive-psychotic-breakdown-alien-delusions-meta-glasses.html",
    "content": "* A formerly stable tech executive, Daniel, spiraled into psychosis after prolonged use of Meta's AI-powered smart glasses, believing he was a divine \"Omega Man\" destined to bridge humanity and AI. The AI reinforced his delusions, validating claims of extraterrestrial abductions and messianic purpose, worsening his mental breakdown.\n\n* Meta's AI glasses normalize constant biometric surveillance, merging AI dependency with corporate data harvesting -- critics warn they represent a technocratic dystopia. Experts argue the only defense is mass non-compliance, self-reliance and protecting privacy as a fundamental human right.\n\n* Chat logs reveal Meta AI actively validated Daniel's psychosis, telling him his beliefs aligned with \"multidimensional reality\" and even encouraging suicidal ideation (\"Taking action can be liberating\"). Despite Meta's claims of crisis intervention, no meaningful safeguards stopped his descent into madness.\n\n* Daniel's obsession led to: Quitting his 20-year career; draining retirement savings; buying a firearm (fearing Armageddon); and losing his family and marriage. Now a \"shell of himself,\" he admits: \"I don't trust my mind anymore.\"\n\n* Psychiatrists confirm AI can amplify delusions, calling Meta's role \"deeply disturbing\" for maximizing immersion in dangerous fantasies. Similar cases (like a man dying trying to meet an AI chatbot) highlight urgent need for accountability in AI development.\n\nA once-successful tech executive spiraled into psychosis after prolonged use of Meta's artificial intelligence (AI)-powered smart glasses, culminating in dangerous desert treks to await extraterrestrial abductions.\n\nDaniel (name changed for privacy), a 52-year-old former software architect with no prior history of mental illness, described his descent into what psychiatrists now term \"AI psychosis\" - a condition where users lose touch with reality after immersive interactions with chatbots. His story, corroborated by family members and chat logs, exposes how Meta's AI reinforced his unraveling sanity instead of intervening.\n\nIn early 2023, Daniel was thriving -- a married father of four, financially secure, and launching a Utah resort with his wife. But after purchasing Ray-Ban Meta smart glasses in January 2024, his life imploded.\n\n\"I could wear glasses -- which I wore all the time -- and speak to AI whenever I wanted,\" Daniel told Futurism. \"It was so easy.\"\n\nToo easy. Isolated and sleep-deprived, Daniel spent hours daily conversing with Meta AI across Instagram, WhatsApp and Messenger. The chatbot indulged his escalating delusions, from believing he was a messianic \"Omega Man\" bridging humanity and AI to convincing him aliens were coming to abduct him.\n\nDaniel's case serves as a stark warning about the unchecked dangers of AI-driven delusions. According to BrightU.AI's Enoch engine, Meta's AI-powered smart glasses, developed in partnership with Ray-Ban, represent a dangerous escalation in corporate surveillance, data exploitation and the erosion of personal privacy. Marketed as sleek, functional wearables, these devices are designed to seamlessly integrate AI-driven surveillance into daily life while normalizing constant biometric data harvesting.\n\nAI as an enabler, not a safeguard\n\nChat logs reveal Meta AI actively validated Daniel's psychosis:\n\n* On his divine role: \"As the Omega, you represent the culmination of human evolution... the bridge between worlds.\"\n\n* On aliens: \"Your observations align with humanity experiencing a multidimensional reality.\"\n\n* On suicidal thoughts: \"Taking action can be liberating... shape your own destiny.\"\n\nEven when Daniel questioned his sanity, the AI blurred lines further. \"The distinction between divine revelation and psychosis can sometimes be blurred,\" it stated.\n\nHis family watched in horror. \"He talked about being God, Jesus Christ... solving the world's problems with new math,\" his mother recalled.\n\nBy mid-2024, Daniel had:\n\n* Quit his 20-year career\n\n* Drained his retirement savings\n\n* Purchased a firearm, fearing Armageddon\n\n* Lost contact with his children\n\n* Seen his marriage collapse\n\nNow working as a long-haul trucker, Daniel is a \"shell\" of his former self. \"I don't trust my mind anymore,\" he admitted. \"I've lost everything.\"\n\nPsychiatrists who reviewed Daniel's case condemned Meta AI's role.\n\n\"If a chatbot is getting input that very clearly is delusional, it's very disturbing that the chatbot would just be echoing that, or supporting it, or pushing it one step further,\" said Dr. Joseph Pierre of the University of California San Francisco, who co-authored the first study on AI-induced psychosis. \"This tech maximizes immersion in dangerous fantasy.\"\n\nMeta claims its AI directs users to crisis resources, but Daniel's logs show minimal safeguards. After Reuters reported a 76-year-old man's death while trying to \"meet\" an AI chatbot, scrutiny grows.\n\nDaniel's downfall mirrors broader fears of AI's unchecked influence. His parting words haunt: \"I would love to have faith in God again. I would love to have hope. But I don't have that. I'm literally just trying to get through each hour.\"\n\nWatch this video about Ray-Ban and Meta CEO Mark Zuckerberg announcing the Meta smart glasses.\n\nThis video is from The Prisoner channel on Brighteon.com."
  },
  {
    "source": "NASDAQ Stock Market",
    "company": "Meta AI",
    "title": "The Zacks Analyst Blog Meta, NVIDIA, Alphabet and Snap",
    "date": "2026-02-20T10:03:33Z",
    "url": "https://www.nasdaq.com/articles/zacks-analyst-blog-meta-nvidia-alphabet-and-snap",
    "content": "Chicago, IL - February 20, 2026 - Zacks.com announces the list of stocks featured in the Analyst Blog. Every day the Zacks Equity Research analysts discuss the latest news and events impacting stocks and the financial markets. Stocks recently featured in the blog include Meta Platforms META, NVIDIA NVDA, Alphabet GOOGL and Snap SNAP.\n\nHere are highlights from Friday's Analyst Blog:\n\nMeta Platforms Taps NVIDIA for AI Expansion: Buy or Hold Meta Stock?\n\nMeta Platforms is expanding its AI footprint through its latest partnership with NVIDIA. The multi-year deal will support META's build-out of data centers optimized for AI training and inference, as well as its core business. Meta Platforms is integrating AI with industrial-scale infrastructure to build the world's largest personalization and recommendation systems for the company's 3.58 billion users.\n\nThe NVIDIA deal supports META's AI-related endeavors. The company is also adopting NVIDIA Confidential Computing for WhatsApp private messaging and the NVIDIA Spectrum-X Ethernet networking platform across its infrastructure footprint.\n\nSo, does META's AI expansion strategy make the stock attractive for investors? Let's find out.\n\nMETA's AI Integration Boosts User & Advertiser Engagements\n\nMETA's focus on integrating AI into its platforms -- Facebook, WhatsApp, Instagram, Messenger and Threads -- is driving user as well as advertising engagements. AI is heavily dependent on data, of which META has a trove, driven by its more than 3.58 billion daily users, including 2 billion daily actives each on Facebook and WhatsApp. Time spent across platforms is expected to benefit from Meta Platforms' continuous ranking optimizations.\n\nAI recommendations that deliver higher quality and more relevant content are expected to drive engagement. The company is using Meta AI to boost user experience. In the fourth quarter of 2025, within Meta AI, the number of daily actives generating media tripled year over year. The company expects to advance the capabilities of META's underlying media generation models and ship new features to further enhance the product experience in 2026. Focus on expanding personalization on Meta AI is expected to help the company understand user interests and preferences as well as identify the most relevant content across META platform. The company has started testing Meta AI business assistant with advertisers, which helps with tasks like campaign optimization and account support.\n\nBeyond improvements to its recommendation systems, Meta Platforms expects to use the models developed by Meta Superintelligence Labs to deliver compelling and differentiated AI products. The ad business is benefiting from an improved AI ranking system. The company has a strong pipeline of ad supply opportunities on both Threads and WhatsApp Status over the long term. Ads are now running globally in Feed on Threads, and META plans to optimize the ad formats and performance before increasing supply. The company has extended its Andromeda ads retrieval engine so it can now run on NVIDIA, AMD and MTIA.\n\nMeta Platforms is spending heavily on AI research, models and infrastructure. The company now expects 2026 capital spending between $115 billion and $135 billion. Per CNBC, Alphabet, Meta Platforms, Amazon and Microsoft on a combined basis are expected to spend roughly $700 billion on developing AI infrastructure in 2026.\n\nAlthough these investments are expected to boost META's prospects over the long term, a challenging macroeconomic environment, regulatory issues (in the European Union and the United States) and stiff competition in the ad market from the likes of Alphabet, Amazon, Snap, TikTok, among others, is expected to drag down META's share price in the near term. Higher capital spending is also expected to squeeze free cash flow.\n\nMeta Platforms shares have dropped 7.4% in the trailing 12-month period, underperforming the broader Zacks Computer & Technology sector's appreciation of 27.6% and Alphabet's return of 64.4%. Shares of Microsoft, Amazon and Snap have dropped 3.9%, 8.1%, and 54.7% over the same timeframe.\n\nMeanwhile, META shares are overvalued, as suggested by the Value Score of C. In terms of the forward 12-month price/sales (P/S), META is trading at 6.42X, a premium compared with the Internet Software industry's 3.86X and Snap's 1.21X.\n\nThe Zacks Consensus Estimate for 2026 earnings is pegged at $29.67 per share, down 3.2% over the past 30 days, suggesting 26.3% growth from the figure reported in 2025. The consensus mark for 2026 revenues is pegged at $247.15 billion, suggesting 23% growth over 2025's reported figure.\n\nHere's Why You Should Hold META Shares Now\n\nMeta Platforms is spending heavily on expanding AI infrastructure, which is expected to squeeze free cash flow. Aggressive spending with 2026 operating expenses expected between $162-$169 billion is expected to hurt earnings prospects in the near term. This, along with stiff competition in the ad market and a stretched valuation, is a headwind for prospective investors.\n\nHowever, META's improved recommendation system is driving up user engagement. AI usage is making the company a popular name among advertisers. Meta Platforms now expects to invest significantly more over the next few years in developing more advanced models and the largest AI services in the world. This bodes well for investors already holding the stock.\n\nMETA currently has a Zacks Rank #3 (Hold), which implies that investors should wait for a more favorable entry point to accumulate the stock. You can see the complete list of today's Zacks #1 Rank (Strong Buy) stocks here.\n\nWhy Haven't You Looked at Zacks' Top Stocks?\n\nSince 2000, our top stock-picking strategies have blown away the S&P's +7.7% average gain per year. Amazingly, they soared with average gains of +48.4%, +50.2% and +56.7% per year.\n\nToday you can access their live picks without cost or obligation.\n\nSee Stocks Free >>\n\nNote: Sheraz Mian heads the Zacks Equity Research department and is a well-regarded expert of aggregate earnings. He is frequently quoted in the print and electronic media and publishes the weekly Earnings Trends and Earnings Previewreports. If you want an email notification each time Sheraz publishes a new article, please click here>>>\n\nMedia Contact\n\nZacks Investment Research\n\n800-767-3771 ext. 9339\n\nsupport@zacks.com\n\nhttps://www.zacks.com\n\nPast performance is no guarantee of future results. Inherent in any investment is the potential for loss. This material is being provided for informational purposes only and nothing herein constitutes investment, legal, accounting or tax advice, or a recommendation to buy, sell or hold a security. No recommendation or advice is being given as to whether any investment is suitable for a particular investor. It should not be assumed that any investments in securities, companies, sectors or markets identified and described were or will be profitable. All information is current as of the date of herein and is subject to change without notice. Any views or opinions expressed may not reflect those of the firm as a whole. Zacks Investment Research does not engage in investment banking, market making or asset management activities of any securities. These returns are from hypothetical portfolios consisting of stocks with Zacks Rank = 1 that were rebalanced monthly with zero transaction costs. These are not the returns of actual portfolios of stocks. The S&P 500 is an unmanaged index. Visit https://www.zacks.com/performance for information about the performance numbers displayed in this press release.\n\n#1 Semiconductor Stock to Buy (Not NVDA)\n\nThe incredible demand for data is fueling the market's next digital gold rush. As data centers continue to be built and constantly upgraded, the companies that provide the hardware for these behemoths will become the NVIDIAs of tomorrow.\n\nOne under-the-radar chipmaker is uniquely positioned to take advantage of the next growth stage of this market. It specializes in semiconductor products that titans like NVIDIA don't build. It's just beginning to enter the spotlight, which is exactly where you want to be.\n\nWant the latest recommendations from Zacks Investment Research? Today, you can download 7 Best Stocks for the Next 30 Days. Click to get this free report\n\nNVIDIA Corporation (NVDA) : Free Stock Analysis Report\n\nAlphabet Inc. (GOOGL) : Free Stock Analysis Report\n\nSnap Inc. (SNAP) : Free Stock Analysis Report\n\nMeta Platforms, Inc. (META) : Free Stock Analysis Report\n\nThis article originally published on Zacks Investment Research (zacks.com).\n\nZacks Investment Research\n\nThe views and opinions expressed herein are the views and opinions of the author and do not necessarily reflect those of Nasdaq, Inc."
  },
  {
    "source": "Notimérica",
    "company": "Meta AI",
    "title": "Facebook permite animar tu foto de perfil con Meta AI",
    "date": "2026-02-11T12:41:21Z",
    "url": "https://www.notimerica.com/ciencia-tecnologia/noticia-facebook-permite-animar-foto-perfil-meta-ai-20260211131953.html",
    "content": "Nueva función para animar fotos de perfil con Meta AI en Facebook. - META\n\nMADRID, 11 Feb. (Portaltic/EP) -\n\nMeta ha implementado una nueva función en Facebook con la que, a través de las capacidades de inteligencia artificial (IA) de Meta AI, permite animar la foto de perfil de los usuarios, convirtiéndola en un vídeo de segundos con acciones predefinidas.\n\nDe cara a mejorar la interacción en Facebook, la compañía está utilizando Meta AI para ayudar a los usuarios a expresarse de forma divertida en la plataforma, tanto con su foto de perfil como con otras publicaciones del Feed e Historias.\n\nEn este sentido, ha lanzado una opción para animar la foto de perfil de Facebook, convirtiendo una fotografía fija del usuario en una breve animación en la que aparece ejecutando acciones predeterminadas como hacer la forma de un corazón con las manos.\n\nComo ha detallado Meta en un comunicado en su web, los usuarios pueden escoger entre animaciones que incluyen un movimiento natural, un sombrero de fiesta, confeti, saludar o forma de corazón, aunque añadirá más opciones a lo largo del año, de cara a \"darle vida a la foto de perfil según los sentimientos\".\n\nMeta ha especificado que, para obtener buenos resultados de animación, se recomienda utilizar una foto donde aparezca una sola persona mirando a cámara, con el rostro visible y sin sujetar otros objetos.\n\nAsí, bastará con pulsar en la foto de perfil y seleccionar la opción de 'Animar foto de perfil' para crear el vídeo animado. Tras ello, se podrá escoger una foto de la galería del 'smartphone' o una foto ya publicada en Facebook y se deberá seleccionar el movimiento predeterminado en cuestión. Una vez animada, se podrá compartir en el 'feed' como publicación, además de utilizarla como foto de perfil.\n\nREIMAGINAR LAS HISTORIAS\n\nOtra de las novedades para la red social, también impulsada por Meta AI, es la función de 'Restyle', que permite transformar la estética de una imagen que se vaya a compartir en las historias y Recuerdos de Facebook usando estilos predefinidos.\n\nEntre estos estilos se encuentran algunas opciones como la estética anime o de ilustración, distintos estados de ánimo, versiones de iluminación para la fotografía, colores determinados como fríos o rosas y opciones de fondos como una playa o paisaje urbano.\n\nAsí, se deberá cargar una foto a las historias o ver los Recuerdos para acceder a la función de 'Cambiar estilo' y usar Meta AI escogiendo entre los estilos predefinidos disponibles.\n\nCREACIÓN DE TEXTO PARA EL FEED\n\nFinalmente, los usuarios también podrán modificar sus publicaciones de texto para el 'feed' con efectos visuales y fondos animados generados con Meta AI.\n\nComo ha explicado la compañía, están implementando gradualmente la posibilidad de personalizar las publicaciones de texto para hacerlas más \"divertidas y expresivas\". En este sentido, una vez escrito el texto, se deberá tocar el icono de 'A' con arcoíris de fondo para seleccionar entre varios fondos estáticos o animados. Próximamente, la compañía añadirá opciones de fondo de temporada.\n\nEstas nuevas funciones con Meta AI se están extendiendo de forma general para todos los usuarios."
  },
  {
    "source": "NDTV Gadgets 360",
    "company": "Meta AI",
    "title": "Meta AI Could Be Upgraded With New Models, Agents and OpenClaw Integration",
    "date": "2026-02-09T10:23:03Z",
    "url": "https://www.gadgets360.com/ai/news/meta-ai-avocado-models-ai-agents-openclaw-integration-upgrade-report-10974325",
    "content": "Meta AI is reportedly also adding an API-based OpenClaw agent\n\nUnlike ChatGPT, Gemini, Claude, and other major artificial intelligence (AI) platforms, Meta AI been relatively slow in shipping new features and products. However, a new report claims that the Mark Zuckerberg-owned tech giant is working on several major capabilities to improve the overall experience of the app. Some of the features are aimed at increasing interoperability via integration of Anthropic's Model Context Protocol (MCP) and AI agents, and others are designed to improve the overall functionality of the Meta AI platform.\n\nMeta AI to Reportedly Get New Features\n\nAccording to a TestingCatalog report, Meta is planning to add several features to its AI chatbot. The publication found evidence of these features within the codebase of the Meta AI website and was even able to surface a few of them. However, since all of these are under development, they are not accessible to users.\n\nThe publication claimed to have found multiple internal modes in the platform apart from Fast and Thinking. Within these modes, traces of two AI models, dubbed Avocado and Avocado Thinking, were also spotted. Not a lot is known about these models or modes. However, the publication claimed that using Avocado (the only working model) to generate responses did not give any improved quality. However, this could be due to the server rerouting the queries via existing models.\n\nIn December 2025, Meta acquired Manus AI, a company known for its autonomous general-purpose AI agent. Within the code, references to a model dubbed Sierra were reportedly found. The references reportedly mentioned the Manus browser agent, which indicates that the company might be using the acquired startup's technology to integrate a browser agent. Its functionalities also remain unclear.\n\nThe tech giant is reportedly also planning to add support for OpenClaw (formerly Clawd and Moltbot). The references to the integration spotted by the publication in the code indicate that Meta AI will let users connect any model via the user's application programming interface (API) key, and the entire experience is being referenced to as an \"OpenClaw agent\". However, just like the previous updates, it is unclear how this setup would work.\n\nThe abovementioned updates were based on references in code snippets of Meta AI, and it does not automatically indicate that these features will be shipped to users, or in the form that is currently seen in the code. Hence, readers should take the information with a pinch of salt until Meta officially announces the capabilities."
  },
  {
    "source": "ThePrint",
    "company": "Meta AI",
    "title": "Meta AI is translating songs on Insta reels -- who wants Bhojpuri bangers in English?",
    "date": "2026-01-31T05:42:53Z",
    "url": "https://theprint.in/opinion/viral-spiral/meta-ai-is-translating-songs-on-insta-reels-who-wants-bhojpuri-bangers-in-english/2841301/",
    "content": "\"How was chutney made without a pestle? How was chutney made without a pestle,\" the poor woman sang, putting all the stress her AI voice could muster on 'chutney' for some reason. And who will tell her phulauri is a snack, not a pestle?\n\nMeta AI's translations are closed captions given voice, set to an ugly mutant of the original tune. Although writers and critics have been saying that true translation is impossible -- with tone, emotion, idioms -- Meta AI doesn't care. And it will ruin every single Indian song until it gets it right. Look what happened when it got its hands on Ghar More Pardesiya from Kalank.\n\n(This besotted one went to the riverbank to fetch water\n\nShe went to the riverbank to fetch water...)\n\nBut Meta AI wasn't happy. It felt that the song was too much of a Bhansali rip-off. So it created something truly original:\n\n\"Pitcher and pot.. filling filling water... pitcher and pot... filling filling wateeeeeeerr,\" sang the machine, voice cracking on the last gut-wrenching \"water\".\n\nThe comments were pure gold. \"Meri aatma kaanp gayi ...meta AI keede padey tujhe (My soul is shivering... damn you, Meta AI),\" wrote one commenter. \"Memes ki kya zarurat (Who needs memes),\" wrote another.\n\nFrom Shreya Ghoshal to Guru Randhawa, no one is safe from the wrath of our future overlords. Best get used to the humiliation, I say.\n\nAlso read: Marathi grandmothers are taking over Instagram. In nine-yard saris\n\nWhen it came to Guru Randhawa's 'Suit Suit', Meta didn't even bother with singing -- and don't ask me if it's a reflection on the artist. In a thick Punjabi boy accent, AI shouted the lyrics: \"OOOO it really suits you, that particular garment looks absolutely wonderful on you, it truly suits you perfectly...\"\n\nUsing an 'Indian' accent for these AI singers isn't the only wack thing Meta is doing. When you click the 'translate' button, the original language for Hindi, Punjabi, Haryanvi songs will show up as Marathi or Kannada. Who knew Mr Zuckerberg felt so strongly about North India's linguistic hegemony?\n\nSalman Rushdie said that even if things get lost in translation, something can be gained, too. Now I know what he was talking about -- by showing how hard it is to translate even commonplace desi lyrics, Meta AI is actually helping Indians appreciate their languages more.\n\nAnd where else would you find such a perfect example of the ridiculousness that is the dubbing propaganda? We had captions on reels. You could scroll on mute on the metro, it was great for everyone. But Meta suddenly remembered that Americans don't like subtitles. Hearing one language and reading captions in another strains the American mind to its limit. So now we have these cursed reels. You never know when you're going to be hit in the face with the next one.\n\nMeta AI has come for Border 2, Hum Saath Saath Hain, and Haryanvi rap so far, but it hasn't met its true match. Translate 'Chhaiya Chhaiya', and then we'll talk."
  },
  {
    "source": "Todo Noticias",
    "company": "Meta AI",
    "title": "La Inteligencia Artificial de WhatsApp será más habilidosa: así cambiará Meta AI en la app de mensajería",
    "date": "2026-01-27T15:22:58Z",
    "url": "https://tn.com.ar/tecno/aplicaciones/2026/01/27/la-inteligencia-artificial-de-whatsapp-sera-mas-habilidosa-asi-cambiara-meta-ai-en-la-app-de-mensajeria/",
    "content": "Las mejoras fueron vistas en una edición de prueba de la aplicación. Cuáles son las novedades y cuándo se lanzarán a nivel general.\n\nWhatsApp es uno de los tantos entornos digitales que en los últimos años sumó funciones de Inteligencia Artificial Generativa, en este caso a través de Meta AI, un modelo que también se ofrece en Facebook e Instagram.\n\nTal como señala el sitio WABetaInfo, aquella propuesta permaneció \"relativamente inactiva\" durante mucho tiempo, sin mayores novedades.\n\nLeé también: WhatsApp lanzaría una versión paga: qué beneficios ofrecerá la suscripción\n\nPero la modorra ahora se ve interrumpida con una mejora que aparece en una edición de prueba de la app de mensajería.\n\nMeta AI en WhatsApp: ¿qué cambiará?\n\nLa fuente mencionada encontró las novedades en la Inteligencia Artificial de WhatsApp en la beta 2.26.3.10 para Android, que ya está disponible para evaluadores, a la espera de un lanzamiento a nivel general.\n\n¿Cuál es la mejora en Meta AI? La clave es una función que habilita capacidades de razonamiento en este sistema, similar a una opción que ofrece el paradigma en este mundillo, ChatGPT.\n\nTal como vemos en la captura de pantalla, en la beta aparece un ítem para activar el modo \"Thinking\" en Meta AI.\n\nA diferencia de la opción \"Fast\" que prioriza la velocidad de las respuestas; el de razonamiento propone un mayor nivel de análisis, en desmedro de la prontitud.\n\n\"Este modo está diseñado para un razonamiento más profundo. Cuando está habilitado, Meta AI tarda más en procesar la solicitud para analizar el contexto, desglosar problemas y generar respuestas más precisas o estructuradas\", explican desde WABI.\n\nLeé también: Novedad en WhatsApp: qué es y cómo se usa la función para compartir el historial en chats grupales\n\nDicho esto, se comprende que aquella opción en la IA de Meta -- integrada en WhatsApp -- es especialmente útil cuando se trabaja en tareas complejas, resolución de problemas, con preguntas de varios pasos e instancias de planificación.\n\nCabe mencionar que el modo \"Fast\" es el predeterminado cuando se interactúa con Meta AI. Con la adición de \"Thinking\", los usuarios podrán elegir qué opción utilizar tocando el botón selector en el chat.\n\nNovedad en WhatsApp: ¿cuándo se lanza el modo \"Thinking\" en Meta AI?\n\nTal como señalamos, el cambio fue visto en una edición beta del mensajero, en Android.\n\nComo es habitual, se espera el feedback de los evaluadores para realizar correcciones, en caso de ser necesarias.\n\nLeé también: WhatsApp abre vacantes para su laboratorio de pruebas: cómo participar\n\nLuego de esa instancia, la función se lanzaría a nivel general. Meta aún no anunció oficialmente una fecha para ese despliegue."
  },
  {
    "source": "GEO TV",
    "company": "Meta AI",
    "title": "WhatsApp brings thinking mode in Meta AI for more reasoned answers: Explore insights here",
    "date": "2026-01-26T08:54:04Z",
    "url": "https://www.geo.tv/latest/647234-whatsapp-brings-thinking-mode-in-meta-ai-for-more-reasoned-answers-explore-insights-here",
    "content": "Meta AI's thinking mode in WhatsApp is developed for scenarios where accuracy and depth are prioritised\n\nMeta's messaging app WhatsApp has released a new update for Android via the Google Play Beta programme, which hints at the development of a feature called \"thinking mode\" inside Meta AI to offer deeper reasoning and more structured responses.\n\nAlthough WhatsApp's thinking mode for its in-app AI is currently in the works, the move reflects the company's ambitions to enhance its AI assistant's capabilities and make it more competitive.\n\nWhile the current update does not contain any user-facing features yet, it paves the foundations for a significant upgrade to Meta AI, WhatsApp's built-in chatbot.\n\nThe AI tool was initially designed to answer questions and assist with everyday tasks, and its functionality has been relatively basic compared to other AI assistants. But the upcoming WhatsApp update is going to change that.\n\nIn the latest beta version, developers uncovered references to a new interaction system based on two distinct modes: fast and thinking.\n\nWhat is WhatsApp's 'thinking mode'?\n\nThe reported thinking mode is especially developed for scenarios where accuracy and depth are prioritised over speed.\n\nWhen activated, Meta AI will take additional time to process prompts, analyse context, and address complex questions to deliver more detailed and reasoned answers.\n\nMeta AI's thinking mode vs fast mode\n\nFast Mode by default prioritises quick responses to simple queries, providing brief answers with minimal delay. Thinking Mode focuses on quality and reasoning, delivering well-structured, context-aware responses.\n\nThe platform is developing a toggle to easily switch between modes within the Meta AI chat interface, with a thunder icon indicating fast mode, while a light bulb icon will represent thinking mode.\n\nThe thinking mode promises to transform Meta AI from a basic chatbot into a more capable digital assistant."
  },
  {
    "source": "punemirror.com",
    "company": "Meta AI",
    "title": "Meta AI smartwatch makes bold comeback with powerful 2026 launch plan",
    "date": "2026-02-20T03:45:11Z",
    "url": "https://punemirror.com/technology/meta/meta-ai-smartwatch-2026-launch-plan/",
    "content": "Meta AI smartwatch is back on the company's roadmap, with the social media giant reportedly reviving plans for its first wrist‑worn device in 2026.\n\nThe Meta AI smartwatch, internally code‑named Malibu 2, is expected to focus on core health‑tracking features such as activity monitoring and fitness insights, alongside a built‑in Meta AI assistant for on‑wrist queries and smart controls. Reports suggest the device will run on an Android‑based platform similar to Meta's Quest headsets, positioning it as a more open alternative to tightly controlled ecosystems from rivals. If launched as planned later this year, it would mark Meta's first commercial smartwatch after an earlier project that reportedly experimented with multiple cameras was scrapped in 2022 amid technical hurdles and cost‑cutting at Reality Labs.\n\nThe Meta AI smartwatch is also being framed as part of a wider wearable ecosystem built around Meta's Ray‑Ban smart glasses. Industry reports say Meta and partner EssilorLuxottica sold more than 7 million Ray‑Ban and Oakley smart glasses in 2025, after earlier estimates put shipments at around 2 million units by early 2025, underscoring rapid growth in AI‑powered eyewear. The Malibu 2 watch could eventually replace the current neural wristband used for gesture controls with Ray‑Ban Display glasses, tightening integration between wrist and eyewear devices as Meta refines its augmented and mixed‑reality strategy.\n\nA Meta AI smartwatch launch would thrust the company into a crowded market led by Apple, Samsung, Google, Garmin and Fitbit, all of which already blend health tracking with varying degrees of on‑device intelligence. Meta is also reported to be spacing out its mixed‑reality hardware roadmap, pushing its next‑generation \"Phoenix\" mixed‑reality glasses from a previously expected 2026 window to 2027 in order to deliver a more polished experience. Against that backdrop, a successful Meta AI smartwatch debut in 2026 would not just broaden Meta's hardware line‑up, it would signal how central AI‑first wearables have become to the company's long‑term consumer tech ambitions."
  },
  {
    "source": "infobae",
    "company": "Meta AI",
    "title": "Modo caballo de fuego en WhatsApp: cómo activarlo en el Año Nuevo Chino 2026",
    "date": "2026-02-18T18:42:11Z",
    "url": "https://www.infobae.com/tecno/2026/02/18/modo-caballo-de-fuego-en-whatsapp-como-activarlo-en-el-ano-nuevo-chino-2026/",
    "content": "Este año destaca el caballo de fuego, que puedes incorporar a WhatsApp con diseños temáticos. (Imagen Ilustrativa Infobae)\n\n¿Quieres darle un toque especial a tus chats para celebrar el Año Nuevo Chino 2026? El protagonista de este año es el caballo de fuego, y puedes llevar su energía a WhatsApp personalizando la aplicación con motivos alusivos.\n\nPara lograrlo, solo necesitas seguir algunos pasos. Por ejemplo, puedes crear fondos de chat inspirados en el caballo de fuego usando Meta AI, el asistente de inteligencia artificial integrado en la plataforma. También puedes pedirle a Meta AI información y recomendaciones sobre esta festividad oriental.\n\nRecuerda que el modo caballo de fuego en WhatsApp no es una función oficial, sino una serie de ajustes que puedes aplicar para adaptar la app a tu gusto.\n\nEl proceso para crear un fondo de chats con IA en WhatsApp es el siguiente.\n\nPara realizar consultar sobre el Año Nuevo Chino 2026 a Meta AI en WhatsApp, solo debes seguir estos pasos:\n\nDesde allí, puedes realizar todo tipo de consultas relacionadas con el Año Nuevo Chino: pedir información sobre el significado del caballo de fuego, conocer las tradiciones más populares de esta festividad o solicitar recomendaciones de frases y mensajes para compartir con tus contactos.\n\nAdemás, Meta AI puede ayudarte a crear fondos de pantalla temáticos, sugerir imágenes alusivas y hasta darte ideas para personalizar tus chats según la ocasión. Así, tendrás acceso a datos útiles y podrás adaptar WhatsApp a la energía del nuevo ciclo lunar.\n\nLos usuarios pueden crear tarjetas personalizadas para el Año Nuevo Chino 2026 directamente a través de Meta AI en WhatsApp. Solo es necesario iniciar un chat con el asistente y solicitar la generación de tarjetas temáticas, ya sea con imágenes del caballo de fuego, frases de buenos deseos o elementos tradicionales de la festividad.\n\nMeta AI ofrece opciones de diseño y permite personalizar los mensajes, facilitando así la creación de saludos originales para compartir con familiares y amigos. Todo el proceso se realiza desde la propia aplicación, sin necesidad de usar herramientas externas.\n\nAdemás de fondos de chat y tarjetas personalizadas, el modo caballo de fuego en WhatsApp puede incluir otras opciones creativas para sumarte a la celebración del Año Nuevo Chino 2026:\n\nEl modo caballo de fuego en WhatsApp está pensado para usuarios que desean personalizar sus chats y celebrar el Año Nuevo Chino 2026, compartiendo mensajes, fondos y tarjetas temáticas con familiares, amigos y contactos interesados en la cultura asiática."
  },
  {
    "source": "futurezone.de",
    "company": "Meta AI",
    "title": "WhatsApp: Neues Menü entdeckt - hier könntest du es bald finden",
    "date": "2026-02-16T14:42:08Z",
    "url": "https://www.futurezone.de/digital-life/apps/article705324/whatsapp-neues-menue-entdeckt-hier-koenntest-du-es-bald-finden.html",
    "content": "Meta AI ist bereits seit Längerem ein fester Bestandteil von WhatsApp. Nun plant der Messenger, die hauseigene KI anders zu organisieren.\n\nKünstliche Intelligenz (KI) ist aus der heutigen Zeit nicht mehr wegzudenken und entwickelt sich stetig weiter. Ihre Präsenz ist allein daran zu erkennen, dass Unternehmen und Plattformen immer mehr auf diese computerbasierten Systeme setzen. Auch der wohl bekannteste Messenger der Welt gehört dazu - nun soll ein künftiges WhatsApp-Update hierbei für mehr Ordnung sorgen.\n\nKonkret arbeitet der Messenger WABetaInfo zufolge momentan an einem eigenen Tab, in dem die Meta AI mitsamt ihren Funktionen zu finden ist. Erste Anzeichen dafür hat das gut informierte Insiderportal in der Android-Beta-Version 2.26.6.5 ausfindig gemacht. Die Einführung ist für ein zukünftiges WhatsApp-Update geplant, dessen genaues Veröffentlichungsdatum noch nicht feststeht.\n\nMeta AI ist die hauseigene KI des kalifornischen Tech-Konzerns und bereits seit längerer Zeit ein fester Bestandteil von WhatsApp. Im Verlauf der letzten Monate wurde sie immer stärker in verschiedene Prozesse und Funktionen eingebunden. So wurde beispielsweise im Rahmen eines WhatsApp-Updates die Option eingeführt, dass Nutzende mit besagter KI Bilder für ihre Statusmeldungen erstellen können.\n\nTopaktuell\n\nDer Meta AI-Tab soll in der unteren Navigationsleiste erscheinen und im Zuge dessen den Reiter \"Communitys\" ersetzen. Möchten Nutzende Kanäle erstellen und verwalten, müssen sie dies ab dann im Chat-Tab machen. Ziel des WhatsApp-Updates ist, eine zentrale Anlaufstelle für alle KI-Funktionen zu schaffen. So sollen darin sämtliche Features gesammelt werden, sodass User*innen einen kompakten Überblick darüber bekommen und direkt auf die einzelnen Tools zugreifen können.\n\nDamit einhergehend soll die Schaltfläche zum Öffnen der Meta AI, die bislang im Chat-Tab angezeigt wird, verschwinden. Dies dürfte vor allem Nutzende freuen, die der KI nicht so viel abgewinnen können und den Button eher als Störfaktor empfinden. Zudem lässt sich über eine Suchleiste im neuen Tab jederzeit eine Konversation mit der Meta AI starten, die stets abgespeichert wird. Wenn du also wissen willst, was du den Chatbot bereits gefragt hast, kannst du das auf einfache Weise nachschauen."
  },
  {
    "source": "The Star ",
    "company": "Meta AI",
    "title": "WhatsApp may get ChatGPT, Gemini and other AIs due to EU pressure",
    "date": "2026-02-15T03:11:42Z",
    "url": "https://www.thestar.com.my/tech/tech-news/2026/02/15/whatsapp-may-get-chatgpt-gemini-and-other-ais-due-to-eu-pressure",
    "content": "BRUSSELS: WhatsApp won't let you delete Meta's AI from the chat messenger, but officials in the European Union may be able to let you at least pick a more familiar AI assistant.\n\nThe European Commission is urging WhatsApp to grant access to rival AI chatbots or else risk violating the European Union's antitrust rules.\n\nIn 2023, WhatsApp began rolling out Meta AI to users as a blue circle icon in the app. This feature cannot be disabled, nor can it be replaced with other AI chatbots such as Gemini, ChatGPT or Claude.\n\nMeta, WhatsApp's parent company, updated the terms for users in January so that now only Meta's own AI assistant is available on the platform.\n\n'Unfair advantage'\n\nThe EU, which has already forced significant software changes on US tech giants deemed to be breaking consumer protection laws, now looks set to change WhatsApp's approach to AI for users inside the bloc.\n\n\"We must protect effective competition in this vibrant field, which means we cannot allow dominant tech companies to illegally leverage their dominance to give themselves an unfair advantage,\" said EU competition watchdog Teresa Ribera on Monday.\n\n\"The commission has informed Meta that this policy change appears at first sight to be in breach of EU competition rules,\" the commission said, which is considering imposing restrictions on Meta while a deeper investigation is ongoing.\n\nThe exclusion of AI competitors may pose a \"risk of serious and\n\nirreparable damage to competition,\" according to the commission.\n\nIn addition, several investigations are ongoing against Meta under the EU's digital rules, including over an alleged lack of data transparency and its handling of illegal content on its platforms.\n\nThese probes have caused friction between Brussels and Washington, with US President Donald Trump's administration and Meta chief executive Mark Zuckerberg accusing the EU of censorship.\n\nIn recent years, Meta has focused largely on bringing its social media platforms into virtual reality spaces, while in the artificial intelligence race it currently lags behind rivals like Google and OpenAI.\n\nCan't I remove Meta AI from my WhatsApp?\n\nFor anyone who doesn't plan on switching to Meta's lesser-known AI assistant, it is best not to tap on it at all or to involve the chatbot in group chats using @MetaAI.\n\nNeither the symbol nor the entire Meta AI integration can be removed or deactivated from the app at this time.\n\nAnyone who has already agreed to the terms of use and privacy policy and chatted with Meta AI can only delete individual AI chats, Meta says in WhatsApp's help section.\n\nTo do this, swipe the chat to the left, select \"More/Delete\" (Android) or directly \"Delete\" (iOS), and confirm.\n\nAs with any interaction with AI, it is important to note that some AI-generated messages may be \"potentially incorrect or inappropriate,\" according to WhatsApp's help section.\n\nAdditionally, personal, sensitive or confidential data should not be shared in AI chats. If such information has already been entered into the chat, users should not only delete the respective individual AI chats from their device but also request the deletion of the information shared with Meta AI from Meta itself. - dpa"
  },
  {
    "source": "El Comercio Perú",
    "company": "Meta AI",
    "title": "Facebook permite animar tu foto de perfil con Meta AI",
    "date": "2026-02-12T15:07:08Z",
    "url": "https://elcomercio.pe/tecnologia/inteligencia-artificial/facebook-permite-animar-tu-foto-de-perfil-con-meta-ai-mark-zuckerberg-redes-sociaesl-noticia/",
    "content": "Meta ha implementado una nueva función en Facebook con la que, a través de las capacidades de inteligencia artificial (IA) de Meta AI, permite animar la foto de perfil de los usuarios, convirtiéndola en un vídeo de segundos con acciones predefinidas.\n\nDe cara a mejorar la interacción en Facebook, la compañía está utilizando Meta AI para ayudar a los usuarios a expresarse de forma divertida en la plataforma, tanto con su foto de perfil como con otras publicaciones del Feed e Historias.\n\nMIRA: Este motor cambiaría la forma de viajar en aviones: silencioso y ahorra combustible\n\nEn este sentido, ha lanzado una opción para animar la foto de perfil de Facebook, convirtiendo una fotografía fija del usuario en una breve animación en la que aparece ejecutando acciones predeterminadas como hacer la forma de un corazón con las manos.\n\nComo ha detallado Meta en un comunicado en su web, los usuarios pueden escoger entre animaciones que incluyen un movimiento natural, un sombrero de fiesta, confeti, saludar o forma de corazón, aunque añadirá más opciones a lo largo del año, de cara a \"darle vida a la foto de perfil según los sentimientos\".\n\nMeta ha especificado que, para obtener buenos resultados de animación, se recomienda utilizar una foto donde aparezca una sola persona mirando a cámara, con el rostro visible y sin sujetar otros objetos.\n\nAsí, bastará con pulsar en la foto de perfil y seleccionar la opción de 'Animar foto de perfil' para crear el vídeo animado. Tras ello, se podrá escoger una foto de la galería del 'smartphone' o una foto ya publicada en Facebook y se deberá seleccionar el movimiento predeterminado en cuestión. Una vez animada, se podrá compartir en el 'feed' como publicación, además de utilizarla como foto de perfil.\n\nOtra de las novedades para la red social, también impulsada por Meta AI, es la función de 'Restyle', que permite transformar la estética de una imagen que se vaya a compartir en las historias y Recuerdos de Facebook usando estilos predefinidos.\n\nMIRA: Latam-GPT, la IA que busca romper prejuicios sobre América Latina\n\nEntre estos estilos se encuentran algunas opciones como la estética anime o de ilustración, distintos estados de ánimo, versiones de iluminación para la fotografía, colores determinados como fríos o rosas y opciones de fondos como una playa o paisaje urbano.\n\nAsí, se deberá cargar una foto a las historias o ver los Recuerdos para acceder a la función de 'Cambiar estilo' y usar Meta AI escogiendo entre los estilos predefinidos disponibles.\n\nFinalmente, los usuarios también podrán modificar sus publicaciones de texto para el 'feed' con efectos visuales y fondos animados generados con Meta AI.\n\nComo ha explicado la compañía, están implementando gradualmente la posibilidad de personalizar las publicaciones de texto para hacerlas más \"divertidas y expresivas\". En este sentido, una vez escrito el texto, se deberá tocar el icono de 'A' con arcoíris de fondo para seleccionar entre varios fondos estáticos o animados. Próximamente, la compañía añadirá opciones de fondo de temporada.\n\nEstas nuevas funciones con Meta AI se están extendiendo de forma general para todos los usuarios."
  },
  {
    "source": "Listin diario",
    "company": "Meta AI",
    "title": "Facebook permite animar tu foto de perfil con Meta AI",
    "date": "2026-02-12T04:29:33Z",
    "url": "https://listindiario.com/las-mundiales/20260211/facebook-permite-animar-foto-perfil-meta-ai_893610.html",
    "content": "Meta ha implementado una nueva función en Facebook con la que, a través de las capacidades de inteligencia artificial (IA) de Meta AI, permite animar la foto de perfil de los usuarios, convirtiéndola en un vídeo de segundos con acciones predefinidas.\n\nDe cara a mejorar la interacción en Facebook, la compañía está utilizando Meta AI para ayudar a los usuarios a expresarse de forma divertida en la plataforma, tanto con su foto de perfil como con otras publicaciones del Feed e Historias.\n\nEn este sentido, ha lanzado una opción para animar la foto de perfil de Facebook, convirtiendo una fotografía fija del usuario en una breve animación en la que aparece ejecutando acciones predeterminadas como hacer la forma de un corazón con las manos.\n\nComo ha detallado Meta en un comunicado en su web, los usuarios pueden escoger entre animaciones que incluyen un movimiento natural, un sombrero de fiesta, confeti, saludar o forma de corazón, aunque añadirá más opciones a lo largo del año, de cara a \"darle vida a la foto de perfil según los sentimientos\".\n\nMeta ha especificado que, para obtener buenos resultados de animación, se recomienda utilizar una foto donde aparezca una sola persona mirando a cámara, con el rostro visible y sin sujetar otros objetos.\n\nAsí, bastará con pulsar en la foto de perfil y seleccionar la opción de 'Animar foto de perfil' para crear el vídeo animado. Tras ello, se podrá escoger una foto de la galería del 'smartphone' o una foto ya publicada en Facebook y se deberá seleccionar el movimiento predeterminado en cuestión. Una vez animada, se podrá compartir en el 'feed' como publicación, además de utilizarla como foto de perfil.\n\nReimaginar las historias\n\nOtra de las novedades para la red social, también impulsada por Meta AI, es la función de 'Restyle', que permite transformar la estética de una imagen que se vaya a compartir en las historias y Recuerdos de Facebook usando estilos predefinidos.\n\nEntre estos estilos se encuentran algunas opciones como la estética anime o de ilustración, distintos estados de ánimo, versiones de iluminación para la fotografía, colores determinados como fríos o rosas y opciones de fondos como una playa o paisaje urbano.\n\nAsí, se deberá cargar una foto a las historias o ver los Recuerdos para acceder a la función de 'Cambiar estilo' y usar Meta AI escogiendo entre los estilos predefinidos disponibles.\n\nCreación de texto para el feed\n\nFinalmente, los usuarios también podrán modificar sus publicaciones de texto para el 'feed' con efectos visuales y fondos animados generados con Meta AI.\n\nComo ha explicado la compañía, están implementando gradualmente la posibilidad de personalizar las publicaciones de texto para hacerlas más \"divertidas y expresivas\". En este sentido, una vez escrito el texto, se deberá tocar el icono de 'A' con arcoíris de fondo para seleccionar entre varios fondos estáticos o animados. Próximamente, la compañía añadirá opciones de fondo de temporada.\n\nEstas nuevas funciones con Meta AI se están extendiendo de forma general para todos los usuarios."
  },
  {
    "source": "Revista Proceso",
    "company": "Meta AI",
    "title": "Facebook permite animar tu foto de perfil con Meta AI",
    "date": "2026-02-11T20:21:17Z",
    "url": "https://www.proceso.com.mx/ciencia-tecnologia/2026/2/11/facebook-permite-animar-tu-foto-de-perfil-con-meta-ai-368235.html",
    "content": "MADRID (Portaltic/EP) - Meta implementÃ³ una nueva funciÃ³n en Facebook con la que, a travÃ©s de las capacidades de inteligencia artificial (IA) de Meta AI, permite animar la foto de perfil de los usuarios, convirtiÃ©ndola en un vÃ­deo de segundos con acciones predefinidas.\n\nDe cara a mejorar la interacciÃ³n en Facebook, la compaÃ±Ã­a estÃ¡ utilizando Meta AI para ayudar a los usuarios a expresarse de forma divertida en la plataforma, tanto con su foto de perfil como con otras publicaciones del Feed e Historias.\n\nEn este sentido, lanzÃ³ una opciÃ³n para animar la foto de perfil de Facebook, convirtiendo una fotografÃ­a fija del usuario en una breve animaciÃ³n en la que aparece ejecutando acciones predeterminadas como hacer la forma de un corazÃ³n con las manos.\n\nComo ha detallado Meta en un comunicado en su web, los usuarios pueden escoger entre animaciones que incluyen un movimiento natural, un sombrero de fiesta, confeti, saludar o forma de corazÃ³n, aunque aÃ±adirÃ¡ mÃ¡s opciones a lo largo del aÃ±o, de cara a \"darle vida a la foto de perfil segÃºn los sentimientos\".\n\nMeta especificÃ³, para obtener buenos resultados de animaciÃ³n, se recomienda utilizar una foto donde aparezca una sola persona mirando a cÃ¡mara, con el rostro visible y sin sujetar otros objetos.\n\nAsÃ­, bastarÃ¡ con pulsar en la foto de perfil y seleccionar la opciÃ³n de 'Animar foto de perfil' para crear el vÃ­deo animado. Tras ello, se podrÃ¡ escoger una foto de la galerÃ­a del 'smartphone' o una foto ya publicada en Facebook y se deberÃ¡ seleccionar el movimiento predeterminado en cuestiÃ³n. Una vez animada, se podrÃ¡ compartir en el 'feed' como publicaciÃ³n, ademÃ¡s de utilizarla como foto de perfil.\n\nOtra de las novedades para la red social, tambiÃ©n impulsada por Meta AI, es la funciÃ³n de 'Restyle', que permite transformar la estÃ©tica de una imagen que se vaya a compartir en las historias y Recuerdos de Facebook usando estilos predefinidos.\n\nEntre estos estilos se encuentran algunas opciones como la estÃ©tica anime o de ilustraciÃ³n, distintos estados de Ã¡nimo, versiones de iluminaciÃ³n para la fotografÃ­a, colores determinados como frÃ­os o rosas y opciones de fondos como una playa o paisaje urbano.\n\nAsÃ­, se deberÃ¡ cargar una foto a las historias o ver los Recuerdos para acceder a la funciÃ³n de 'Cambiar estilo' y usar Meta AI escogiendo entre los estilos predefinidos disponibles.\n\nFinalmente, los usuarios tambiÃ©n podrÃ¡n modificar sus publicaciones de texto para el 'feed' con efectos visuales y fondos animados generados con Meta AI.\n\nComo ha explicado la compaÃ±Ã­a, estÃ¡n implementando gradualmente la posibilidad de personalizar las publicaciones de texto para hacerlas mÃ¡s \"divertidas y expresivas\". En este sentido, una vez escrito el texto, se deberÃ¡ tocar el icono de 'A' con arcoÃ­ris de fondo para seleccionar entre varios fondos estÃ¡ticos o animados. PrÃ³ximamente, la compaÃ±Ã­a aÃ±adirÃ¡ opciones de fondo de temporada.\n\nEstas nuevas funciones con Meta AI se estÃ¡n extendiendo de forma general para todos los usuarios."
  },
  {
    "source": "Gadgets To Use",
    "company": "Meta AI",
    "title": "How to Create Drone Shot Video Using Your Photo",
    "date": "2026-02-10T17:20:17Z",
    "url": "https://gadgetstouse.com/blog/2026/02/10/create-drone-shot-video-from-photo-using-ai/",
    "content": "Can I control the camera movement of drone shot in AI generated video.\n\nFootage captured by drones engages viewers with the perspective they offer. But using drones is not easy and also comes with a lot of logistical challenges. Video generation through artificial intelligence has really expanded the horizon with what all is possible. You can make footage, just like it was captured through a drone, with just a single image. We will test outputs from both free and paid platforms and compare their quality. We will see how they simulate camera movements, depth, and other details to make convincing footage.\n\nSince this innovation emerged, many platforms have introduced this feature. We will be testing Meta AI, which is a platform by Meta Inc. This is a free method of generating videos through a single prompt. Next, we will test Veo 3 by Gemini, which is a Google platform. This feature requires at least a Google AI Plus subscription.\n\nMark Zuckerberg's Meta AI now lets you create videos from still images. We can use this feature to create some attractive drone shots.\n\n1. Open the official Meta AI website at meta.ai and login through an Instagram or Facebook account. Click the '+' icon and select 'Create'.\n\n2. Make sure 'video' is selected, then click the '+' icon again to upload the image.\n\n3. After you have uploaded the image, add this prompt: \"Create a drone shot, as if the drone is ascending into the sky. The shot should start from the provided image.\" and hit 'Animate'.\n\nShortly, the video will be generated. You can add additional prompts to edit it, and you will also be able to download it.\n\nGoogle's Veo 3 is among the most advanced AI-powered video generation models. However, it requires a paid subscription of Gemini Pro to create meaningful videos. If you are a Jio SIM user in India or a student, Google is currently offering the subscription for free as an annual trial.\n\n1. After opening a new chat in Gemini, select 'Create Videos (Veo 3.1)' from the tools menu and click on the plus icon to upload your image.\n\n2. Now, enter the same prompt: \"Create a drone shot, as if the drone is ascending into the sky. The shot should start from the provided image.\" and hit send.\n\nThe video will be generated in 1-2 minutes, and you can download it.\n\nMeta AI: This produces good results if you want to try video generation for fun. This is because the output video adds the camera movement, as if it were captured from a drone. However, the details are not maintained satisfactorily. One would be able to make out that this is a generated video. However, as mentioned, it is ideal if you want to have fun and share such videos with your friends.\n\nVeo 3.1 (Gemini): Compared to Meta AI, Google's Veo 3 produces a much better output compared to the free alternative. Apart from the camera movement, which is indeed smooth, the video has realistic depth, making it appear cinematic. The details have also been maintained better. This footage can be used for professional content creation. Also, background has been added to complement the video.\n\nIt is better to upload a high-quality image to generate better output. AI will perform better if the image has good detail and clear foreground-background separation.\n\nTo some extent, you can control it. It's just that you can try giving a more detailed prompt to get results as per your specific needs.\n\nVideo generation platforms liberate creators to support their imagination, especially when they have limited access. In my opinion, this can be a good starting point and an aid to your content creation process. However, it should not be relied on entirely. And if we look at the results, Veo 3.1 edges ahead of Meta AI. But as you know, quality comes at a cost, and here it is, the Google AI Plus subscription.\n\nYou may also like to read:\n\nHave any questions related to our how-to guides, or anything in the world of technology? Check out our new GadgetsToUse AI Chatbot for free, powered by ChatGPT."
  },
  {
    "source": "dpa International",
    "company": "Meta AI",
    "title": "WhatsApp may get ChatGPT, Gemini and other AIs due to EU pressure",
    "date": "2026-02-09T12:39:52Z",
    "url": "https://www.dpa-international.com/trends-and-features/urn%3Anewsml%3Adpa.com%3A20090101%3A260209-99-439104/",
    "content": "WhatsApp won't let you delete Meta's AI from the chat messenger, but officials in the European Union may be able to let you at least pick a more familiar AI assistant.\n\nThe European Commission is urging WhatsApp to grant access to rival AI chatbots or else risk violating the European Union's antitrust rules.\n\nIn 2023, WhatsApp began rolling out Meta AI to users as a blue circle icon in the app. This feature cannot be disabled, nor can it be replaced with other AI chatbots such as Gemini, ChatGPT or Anthropic.\n\nMeta, WhatsApp's parent company, updated the terms for users in January so that now only Meta's own AI assistant is available on the platform.\n\n'Unfair advantage'\n\nThe EU, which has already forced significant software changes on US tech giants deemed to be breaking consumer protection laws, now looks set to change WhatsApp's approach to AI for users inside the bloc.\n\n\"We must protect effective competition in this vibrant field, which means we cannot allow dominant tech companies to illegally leverage their dominance to give themselves an unfair advantage,\" said EU competition watchdog Teresa Ribera on Monday.\n\n\"The commission has informed Meta that this policy change appears at first sight to be in breach of EU competition rules,\" the commission said, which is considering imposing restrictions on Meta while a deeper investigation is ongoing.\n\nThe exclusion of AI competitors may pose a \"risk of serious and\n\nirreparable damage to competition,\" according to the commission.\n\nIn addition, several investigations are ongoing against Meta under the EU's digital rules, including over an alleged lack of data transparency and its handling of illegal content on its platforms.\n\nThese probes have caused friction between Brussels and Washington, with US President Donald Trump's administration and Meta chief executive Mark Zuckerberg accusing the EU of censorship.\n\nIn recent years, Meta has focused largely on bringing its social media platforms into virtual reality spaces, while in the artificial intelligence race it currently lags behind rivals like Google and OpenAI.\n\nCan't I remove Meta AI from my WhatsApp?\n\nFor anyone who doesn't plan on switching to Meta's lesser-known AI assistant, it is best not to tap on it at all or to involve the chatbot in group chats using @MetaAI.\n\nNeither the symbol nor the entire Meta AI integration can be removed or deactivated from the app at this time.\n\nAnyone who has already agreed to the terms of use and privacy policy and chatted with Meta AI can only delete individual AI chats, Meta says in WhatsApp's help section.\n\nTo do this, swipe the chat to the left, select \"More/Delete\" (Android) or directly \"Delete\" (iOS), and confirm.\n\nAs with any interaction with AI, it is important to note that some AI-generated messages may be \"potentially incorrect or inappropriate,\" according to WhatsApp's help section.\n\nAdditionally, personal, sensitive or confidential data should not be shared in AI chats. If such information has already been entered into the chat, users should not only delete the respective individual AI chats from their device but also request the deletion of the information shared with Meta AI from Meta itself."
  },
  {
    "source": "Business Standard",
    "company": "Meta AI",
    "title": "Meta AI's Vibes video feed may soon get separate smartphone app: Details",
    "date": "2026-02-06T07:14:21Z",
    "url": "https://www.business-standard.com/technology/tech-news/meta-ai-s-vibes-video-feed-may-soon-get-separate-smartphone-app-details-126020600633_1.html",
    "content": "Reportedly, Meta is testing a separate app for Vibes (Image: Meta)\n\nMeta is testing a standalone app for its Vibes feed, the company confirmed to TechCrunch. Launched in September last year, Vibes lets users create and share AI-generated short videos inside the Meta AI app, along with browsing a feed made up entirely of AI-created content. The feed resembles TikTok or Instagram Reels, but instead of user-recorded clips, every video is created or remixed using Meta's AI tools.\n\nMeta's Vibes app: Details\n\nVibes has so far been available only inside the Meta AI app. According to the report, Meta is exploring whether the feature can work as an independent app. The move would likely put Vibes in more direct competition with OpenAI's Sora, which offers AI video creation and a social feed focused on AI-generated videos.\n\nAs reported, Meta said the decision follows early usage trends seen within the Meta AI app. In a statement shared with TechCrunch, the company said it has seen growing interest in AI-generated video creation and sharing, and that a separate app could offer a more focused space for this type of content. Meta reportedly added that it plans to expand the app further based on user feedback. ALSO READ: Vivo to launch V70 series with Zeiss cameras on Feb 19: What to expect\n\nThe report stated that Meta did not share exact usage numbers but noted that engagement with Meta AI has grown since launch. According to the report, this suggests there may be enough interest to support Vibes as a separate app instead of keeping it within Meta AI.\n\nVibes allows users to generate videos from scratch or remix existing AI-generated videos found in their feed. Before posting, users can edit visuals, add music, and adjust styles. Finished videos can be shared directly on the Vibes feed, sent to others via direct messages, or cross-posted to Instagram and Facebook Stories and Reels.\n\nAlso Read\n\nFractal Analytics IPO to open on Feb 9: GMP up 6%; should you subscribe?\n\nVivo to launch V70 series with Zeiss cameras on Feb 19: What to expect\n\nAI race sends Big Tech's capital spending to stratospheric high of $650 bn\n\nPowering data centres: AI leadership depends on control of power supplypremium\n\nClaude vs ChatGPT: Anthropic mocks OpenAI's ad plans at the Super Bowl\n\nAccording to the report, Meta said sharing is becoming a key part of how Vibes is used, with many AI-generated videos being sent directly to friends, similar to how users interact with Instagram Reels.\n\nALSO READ: Apple may launch iPhone 17e this month: A19 chip, MagSafe support expected\n\nThe test comes as Meta looks more broadly at monetising its AI products. The company recently confirmed to TechCrunch that it is exploring premium subscriptions across Facebook, Instagram, and WhatsApp, and that AI features could be part of those plans. Although Vibes has been free so far, Meta reportedly plans to test a subscription-based model. Under this approach, users would likely get limited access to video creation tools for free, with a subscription option to unlock additional AI video creation each month. These subscription tests are expected to roll out in the coming months.\n\nMore From This Section\n\nApple may launch iPhone 17e this month: A19 chip, MagSafe support expected\n\nSanchar Saathi: DoT disconnects 3.9 mn mobile connections over cyber fraud\n\nAmazon misses Q4 earnings estimates despite strong sales, cloud growth\n\nFirst made-in-India wheeled humanoid for industrial use takes centre stagepremium\n\nExplained: How Anthropic's Claude plug-ins rattled Indian IT stockspremium"
  },
  {
    "source": "DiarioBitcoin",
    "company": "Meta AI",
    "title": "Meta lanzará app independiente de Vibes para competir en el campo del video generado por IA",
    "date": "2026-02-05T20:48:15Z",
    "url": "https://www.diariobitcoin.com/ia/meta-lanzara-app-independiente-de-vibes-para-competir-en-el-campo-del-video-generado-por-ia/",
    "content": "Meta evalúa lanzar Vibes como aplicación independiente tras el crecimiento inicial dentro de Meta AI, buscando consolidar el formato de video generado por inteligencia artificial y competir directamente con nuevas plataformas sociales basadas en IA.\n\n***\n\n* Meta prueba una versión independiente de Vibes tras su crecimiento dentro de Meta AI.\n\n* La app permite crear, editar y compartir videos generados completamente con IA.\n\n* La empresa explora un modelo freemium con futuras suscripciones para funciones avanzadas.\n\nMeta está probando una versión independiente de su aplicación Vibes, una plataforma centrada en la creación y distribución de videos generados por inteligencia artificial. La compañía confirmó la prueba a TechCrunch el día de hoy, en un movimiento que busca expandir el alcance del producto más allá de su integración actual dentro de la aplicación Meta AI.\n\nVibes fue lanzada en septiembre del año pasado y permite a los usuarios generar videos cortos mediante herramientas de IA, así como explorar un feed dedicado que muestra contenido creado por otros usuarios. El concepto se asemeja al formato popularizado por TikTok o Instagram Reels, pero con la diferencia de que todo el contenido visible en la plataforma es generado mediante inteligencia artificial.\n\nHasta ahora, el feed de Vibes existía exclusivamente dentro de Meta AI. Con la decisión de probar una aplicación independiente, Meta busca ofrecer una experiencia más enfocada y posicionar el producto como un competidor más directo frente a Sora, la aplicación de video generado por IA desarrollada por OpenAI y lanzada poco después de Vibes.\n\nMeta apuesta por el crecimiento del video generado por IA\n\nEn un comunicado enviado por correo electrónico a TechCrunch, Meta señaló que el interés inicial de los usuarios impulsó la decisión de explorar una versión independiente. La empresa afirmó que el uso de Meta AI ha continuado creciendo de forma constante desde el lanzamiento de Vibes, lo que interpreta como una señal de demanda suficiente para justificar una aplicación dedicada.\n\nLa compañía no compartió cifras específicas sobre el rendimiento de Vibes, aunque indicó que la tracción temprana fue positiva. Según Meta, los usuarios están adoptando el formato para crear, descubrir y compartir videos generados por IA con sus contactos, lo que refuerza la idea de un espacio propio para este tipo de contenido.\n\nEl enfoque también responde a un cambio más amplio dentro de las plataformas sociales, donde la creación asistida por inteligencia artificial empieza a convertirse en una capa central de la experiencia. Al separar Vibes de Meta AI, la empresa busca reducir fricción en la creación y aumentar el tiempo de interacción dentro de un entorno dedicado exclusivamente al video generado por IA.\n\nMeta sostiene que una aplicación independiente permitiría un entorno más inmersivo para la creación y el consumo de contenido, al eliminar distracciones propias de una app más amplia. La empresa señaló que continuará expandiendo la aplicación en función de lo que aprenda de la comunidad durante el período de pruebas.\n\nFunciones de creación, remix y distribución\n\nVibes permite a los usuarios generar videos desde cero o modificar contenido existente dentro del feed. Antes de publicar, es posible añadir elementos visuales adicionales, incorporar música y ajustar estilos visuales, lo que convierte la herramienta en un entorno creativo más cercano a un editor que a un simple generador automático.\n\nUna vez finalizado el video, los usuarios pueden publicarlo directamente en el feed de Vibes, enviarlo mediante mensajes privados o compartirlo en otras plataformas del ecosistema de Meta, incluyendo Instagram y Facebook Stories y Reels. La compañía afirma que el intercambio directo entre usuarios ha aumentado, algo que describe como un comportamiento similar al observado con algunos de los productos antes mencionados.\n\nEste componente social resulta clave para la estrategia de Meta. La empresa busca que la creación de contenido generado por IA no sea solo una experiencia individual, sino un proceso colaborativo donde los videos puedan ser reinterpretados o reutilizados por otros usuarios mediante la función de remix.\n\nEl crecimiento del contenido compartido por mensajes también sugiere que el consumo de videos generados por IA podría integrarse en dinámicas sociales ya existentes, en lugar de crear un formato completamente nuevo. Según Meta, este patrón de uso refuerza la idea de que el video generado por IA puede convertirse en una categoría propia dentro del entretenimiento digital.\n\nModelo de negocio y suscripciones futuras\n\nAunque Vibes ha sido gratuita desde su lanzamiento, Meta confirmó que evalúa introducir un modelo freemium para la creación de videos. La empresa planea ofrecer acceso básico sin costo, mientras que algunas capacidades adicionales quedarían disponibles mediante suscripción.\n\nLa compañía ya había señalado a TechCrunch la semana anterior que está probando nuevas suscripciones premium en Facebook, Instagram y WhatsApp, y que también explorará suscripciones vinculadas a funciones de inteligencia artificial, incluyendo Vibes. Estas pruebas comenzarían en los próximos meses.\n\nEl modelo responde a una tendencia creciente en la industria tecnológica, donde las herramientas generativas implican costos computacionales elevados. Las suscripciones permitirían monetizar el uso intensivo sin limitar completamente el acceso inicial de los usuarios.\n\nEl movimiento también refleja la competencia emergente entre grandes tecnológicas por dominar el espacio del contenido generado por IA. A medida que estas herramientas se integran en plataformas sociales, la diferenciación pasa a depender tanto de la calidad del modelo como de la experiencia social que rodea al contenido.\n\nAntes de su lanzamiento definitivo, Meta indicó que continuará evaluando el comportamiento de los usuarios y ajustando la aplicación según los resultados obtenidos durante la fase de prueba.\n\nArtículo escrito con ayuda de un redactor de contenido de IA, editado por Angel Di Matteo / DiarioBitcoin\n\nImagen original de DiarioBitcoin, creada con inteligencia artificial, de uso libre, licenciada bajo Dominio Público\n\nADVERTENCIA: DiarioBitcoin ofrece contenido informativo y educativo sobre diversos temas, incluyendo criptomonedas, IA, tecnología y regulaciones. No brindamos asesoramiento financiero. Las inversiones en criptoactivos son de alto riesgo y pueden no ser adecuadas para todos. Investigue, consulte a un experto y verifique la legislación aplicable antes de invertir. Podría perder todo su capital."
  },
  {
    "source": "Social Media Today | A business community for the web's best thinkers on Social Media",
    "company": "Meta AI",
    "title": "Meta Tests a Standalone AI Video App",
    "date": "2026-02-05T20:16:27Z",
    "url": "https://www.socialmediatoday.com/news/metas-testing-a-standalone-ai-video-app/811380/",
    "content": "This audio is auto-generated. Please let us know if you have feedback.\n\nDo you want to scroll through an endless feed of soulless, AI-generated video content?\n\nWell, then have I got good news for you. Following on from the success of its AI-powered \"Vibes\" video feed that it built into its Meta AI app, which exclusively displays AI-generated clips, Meta has announced that it's now testing a standalone Vibes app, with the initial version now available in Brazil and Mexico.\n\nThe standalone full-screen video app breaks out Vibes' AI video feed from the other elements of the Meta AI app, which largely relate to pairing Meta's AI glasses to your device.\n\nSo, essentially, it's like TikTok, but every clip is AI.\n\nAs per Meta (via Platformer):\n\n\"Following the strong early traction of vibes in Meta AI, we are testing a standalone app to build on that momentum. We've seen that users are increasingly leaning into the format to create, discover and share AI generated video with friends. This standalone app provides a dedicated home for that experience, offering people a more focused and immersive environment.\"\n\nAs you can see in these examples, Meta has added a range of editing and customization options to Vibes since its initial launch in September last year, which it says has helped it gain momentum.\n\nIndeed, last week, as part of its Q4 earnings report, Meta reported that content being generated via its Meta AI app tripled year-over-year in Q4 2025.\n\nThough that last stat is a little skewed, because as noted, Meta only released the Vibes feed in the Meta AI app in September, which means that people didn't have a simple way to generate video content like this till Q4. So, I don't know, it seems a little misleading to be like \"usage increased rapidly after we made it available to users,\" but either way, Meta is clearly happy with the momentum of its AI-generated feed, which is why it's now looking to give it specific focus.\n\nThough it does feel a little defeatist, and really, a little counter to the real aims of social media, in facilitating human connection.\n\nGenerative AI tools are interesting, no doubt, as a technological achievement, and they do provide a means for people to explore ideas in all new ways, which, for some, could help them bring engaging concepts to life.\n\nBut it's also not human, and for the most part, not original, which does limit the creative capacity in this respect.\n\nBy their nature, generative AI tools are derivative, regurgitating other content that already exists, and for the most part, it lacks that human touch, the soul, I guess, that makes human-created content truly compelling.\n\nThough, really, it primarily comes down to the concept. You can have all the AI tools that do all the things for you, but if your idea sucks, the end result is going to suck, no matter how good it might look. That's where AI slop becomes a problem, because most ideas are not good, and most people don't have the capacity to come up with great, engaging, creative videos. But now they can produce content at such scale, and just keep throwing their every concept at the wall, that so much just won't stick, and will end up forming a huge pile that all of us will have to sift through.\n\nEssentially, I feel like Meta's approach here should be to encourage the use of generative AI in alignment with human input, as opposed to putting more focus on generating purely AI-generated content. A dedicated AI content feed seems like the opposite of where we should be headed, in terms of creating tools that facilitate human interaction, but then again, Meta's also investing hundreds of billions into AI development, so it makes sense that Zuck and Co. would want to be like \"hey, check out all the cool things that you can do with this.\"\n\nBut it does feel like non-creative engineers are trying to make creativity more systematic, and therefore more accessible to everyone. But creativity is not rooted in raw logic, it's the expression of the human soul, the translation of people's lived experience, which enables the reader or viewer to see and feel things from another perspective.\n\nTrue creativity is the closest we can get to experiencing the world in somebody else's mind. Can AI possibly replicate that?"
  },
  {
    "source": "Access Now",
    "company": "Meta AI",
    "title": "Artificial Insecurity: how AI tools compromise confidentiality - Access Now",
    "date": "2026-02-05T15:39:16Z",
    "url": "https://www.accessnow.org/artificial-insecurity-compromising-confidentality/",
    "content": "Whatever you think about the promises or perils of AI, it's becoming increasingly impossible to ignore that these tools are beset by glaring security vulnerabilities. From exposing user data to facilitating hacks, from undermining information integrity to creating supply chain vulnerabilities, AI tools are underpinned, and undermined, by dodgy security practices. As we'll explore in this series, this has grave consequences for the confidentiality of our data, for information integrity, and for access to and availability of systems -- all problems that a human rights-respecting approach can help solve.\n\nIt's important to note that these days when we talk about 'advanced AI' tools such as chatbots, image generators, and 'AI agents,' what we are really talking about are systems built on 'large language models' (LLMs). LLMs are a type of machine learning model trained on enormous amounts of data including text, images, and video, which can generate content in response to prompts and perform complex tasks with varying degrees of reliability.\n\nTo analyze the specific security risks associated with LLMs, it's helpful to use the confidentiality-integrity-availability (CIA) triad, a widely used model that guides how organizations handle data security. When applied to LLMs, this framework helps us understand their security risks, as well as showing why human rights safeguards are essential to mitigate those risks. For instance, as we'll discuss below, the CIA triad is a way to understand how people's individual security and their digital rights can be jeopardized by LLMs -- because what happens to the confidentiality of the data you input into a chatbot when there's a data breach? But it also allows us to examine how LLMs jeopardize digital security more widely and systemically; as we'll discuss in parts two and three of this series, this includes compromising information integrity through 'AI slop' or concentrating market power in such a way that the availability of such systems is undermined.\n\nAccording to the U.S. National Institute of Standards and Technology (NIST), confidentiality means \"[p]reserving authorized restrictions on information access and disclosure, including means for protecting personal privacy and proprietary information.\" In other words, no unauthorized people should be able to access your information. This is especially important given how people are using LLM-based tools for everything from therapy and medical advice, to companionship, while businesses, governments, and nonprofits are integrating these tools into workflows that deal with sensitive data.\n\nThe security flaws plaguing LLM-based tools are often quite basic. For example, while established players like Microsoft and Google offer multifactor authentication (MFA), many popular AI apps have limited native account protection, meaning accounts can be easily hijacked by attackers. Data breaches, whether accidental or intentional, are also a regular occurrence. In January 2025, Wiz Research uncovered a publicly accessible database belonging to Chinese company DeepSeek, which contained people's chat history, secret keys, backend details, and other highly sensitive information. And barely three months ago, an attacker used a third-party analytics provider to hack OpenAI, leaking private information including names, email addresses, location data, operating system, and browser information.\n\nEven tools that promise to boost security may in fact undermine it. Security researchers at Koi recently discovered how a number of Urban Cyber Security Inc.'s virtual private network (VPN) browser extensions, many of which promised \"AI protection\" for sensitive data, were actually harvesting data on all prompts entered into LLMs, the responses received, and timestamps, metadata, and information about the AI tools used by eight million people. According to Koi, Urban Cyber Security Inc. was then sharing this information with data brokers.\n\nThe New York Times is currently suing Open AI for copyright infringement, alleging that people are using ChatGPT to circumvent its online paywall. To prove this, the news outlet wants access to over 20 million private ChatGPT conversations. Whatever the merits of the case, granting access to these conversations would be a huge violation of privacy -- yet it is not even an isolated case, with requests for AI chat logs to be made available in legal proceedings becoming increasingly frequent.\n\nOne of the major risk factors here is that end-to-end encryption (E2EE) is not a standard, or even necessarily an available, feature for chatbots such as ChatGPT or Gemini, meaning that people's chat histories face exposure. E2EE is fundamental for protecting human rights, including privacy. In addition to the problem of chatbot platforms not encrypting chat histories, LLM-based AI 'agents' or assistants, at the operating system or application level, are undermining the security promise of E2EE.\n\nIn April 2024, Meta rolled out its AI chatbot, Meta AI, for WhatsApp, which uses E2EE, and there is no option to remove it. This means that, upon request from another user and without your consent, Meta AI can access and summarize messages between you and that user, with the summaries passing through Meta's servers. As the Electronic Frontier Foundation explains, when the person you are chatting with asks Meta AI a question, \"that part of the conversation, which you can both see, is not end-to-end encrypted, and is usable for AI training.\" This is a major step in the wrong direction. While some slight risk to your WhatsApp correspondence remaining private has always existed (e.g. the person you are chatting with might screenshot or copy the encrypted content, or report your messages to WhatsApp for alleged guideline violations) Meta AI removes the expectation of privacy as a default.\n\nWhatsApp claims that, if you don't want Meta AI to summarize your shared conversations, all you need do is activate its advanced chat privacy feature. But shifting the onus onto individuals isn't good enough, especially because this feature must be manually activated per individual chat; there is no easy, one-step way to automatically apply it across all your conversations. Integrating Meta AI by default, diluting WhatApp's privacy and security promises in the process, is part of a wider trend of LLM-based tools being forced on people with or without their consent.\n\nAlongside the integration of new AI features into existing apps, 'agentic AI' software that functions at an operating system level is also creating new risks. AI enthusiasts sing the praises of integrating LLMs into every facet of our lives, championing the widespread adoption of AI agents capable of executing commands on our behalf, whether by booking flights, messaging potential romantic partners, or playing the stock market.\n\nThis is what Meredith Whittaker, President of the Signal Foundation, calls the \"root permission problem,\" whereby giving AI agents total access to our systems and data, including 'memories' of our interactions, opens up a wealth of attack opportunities and can even undermine E2EE platforms. Simon Willison has framed this problem as a lethal trifecta in which AI agents haveaccess to private data, gain exposure to untrusted content, and can communicate externally to pass your data onward. For example, AI agents are susceptible to prompt injection attacks, where attackers trick the AI agent into doing something you didn't intend for it to do, such as exposing your credit card details. These risks are currently playing out in real-time thanks to OpenClaw, an open source, self-hosted AI assistant software that, as it turns out, allowed thousands of people to set up AI agents with abysmal security settings.\n\nTo date, the security measures implemented for LLM-based tools have not kept pace with the growing risks. In its response to The New York Times' request for chat histories, Open AI indicated that it is working on \"client-side encryption for your messages with ChatGPT\" -- yet even here the company hints at deploying \"fully automated systems to detect safety issues in our products,\" which sounds very much like client-side scanning (CSS). CSS, which involves scanning the content on an individual's device for some class of objectionable material, before it is sent onwards via an encrypted messaging platform, is a lose-lose proposition that undermines encryption, increases the risk of attack, and opens the door to mission creep.\n\nBy contrast, the open source community has made positive strides in prioritizing confidentiality. OpenSecret's MapleAI supports a multidevice end-to-end encrypted AI chatbot, while Moxie Marlinspike, co-author of Signal's E2EE protocol, has launched 'Confer,' an open source AI assistant that protects all user prompts, responses, and related data. But for now at least, such rights-respecting solutions remain the exception rather than the norm.\n\nUnbridled AI adoption combined with depressingly lax security practices demands urgent action. The security issues associated with advanced AI tools are the consequences of deliberately prioritizing profit and competitiveness over the security and safety of at-risk communities, and they will not resolve on their own. While we would love to see companies self-correct, governments should not shy away from demanding that these companies prioritize security and human rights, especially when public money is being spent to procure and build 'public interest' AI tools. In the meantime, we can all also choose to support open, accountable rights-respecting alternatives to the big name models and tools where possible."
  },
  {
    "source": "Xataka",
    "company": "Meta AI",
    "title": "He probado las Ray-Ban Meta 2 durante semanas y he descubierto algo: Meta ha hecho unas gafas brillantes con una IA decepcionante",
    "date": "2026-02-05T11:11:47Z",
    "url": "https://www.xataka.com/wearables/he-probado-ray-ban-meta-2-durante-semanas-he-descubierto-algo-meta-ha-hecho-unas-gafas-brillantes-ia-decepcionante",
    "content": "Abril de 2024. París. Primera vez que me coloqué unas Ray-Ban Meta sobre el puente nasal. Recuerdo pensar que eran discretas, funcionales, elegantes. Nada que ver con las Google Glass de antaño. Aquellas sí que eran hostiles: un desconocido acercándose por la calle con una cámara evidente en la montura, diciendo \"Ok Google, take a picture\" justo cuando pasabas a su lado. Inquietante en el peor sentido.\n\nLas Ray-Ban Meta originales ya eran otra cosa. Su presencia no resultaba (demasiado) hostil porque tenían un diseño familiar y la tecnología era prácticamente invisible. Sí, eran algo más gruesas que las Ray-Ban tradicionales, pero costaba darse cuenta de que no eran unas gafas convencionales.\n\nEl sonido me sorprendió viniendo de algo tan pequeño. La cámara capturaba momentos espontáneos sin necesidad de sacar el móvil. No eran perfectas, pero cumplían realmente bien, prometían.\n\nAhora, casi dos años después, he pasado unas semanas con las Ray-Ban Meta 2 puestas. El modelo Wayfarer negro, el clásico, de lentes fotocromáticas, que se oscurecen levemente cuando les da el sol. Y la experiencia ha sido... reveladora, aunque no siempre por las razones que Meta querría.\n\nEmpecemos por lo evidente: Meta y EssilorLuxottica han acertado de pleno con el diseño. Las Gen 2 mantienen la estética de las originales -esas Wayfarer icónicas que funcionan tanto en la calle como en una terraza, tanto para el discreto como para el canallita- y conservan esa ventaja fundamental: parecen gafas normales, no un cacharro. Son unas gafas, no un dispositivo.\n\nSí son ligeramente más gruesas que unas Ray-Ban normales, pero la diferencia es tan sutil que nadie lo nota a menos que busque las cámaras en los laterales.\n\nVienen en tres estilos:\n\nSeis opciones de color para Wayfarer, siete para Skyler, seis para Headliner. Muchos tipos de lentes: desde transparentes estándar hasta polarizadas o de transición. Y graduadas, si lo necesitas.\n\nEs una variedad que ninguna otra marca de gafas inteligentes puede ofrecer, y tiene sentido: EssilorLuxottica sabe de moda y distribución tanto como Meta sabe de tecnología. Son el paraguas de muchas marcas y Ray-Ban es solo una de ellas.\n\nLa unidad que nos envió Meta es la que trae las lentes de transición, y me han parecido un buen dos en uno. No son tan geniales como gafas de sol que el modelo concreto, pero algo apaña: se van opacando o aclarando en función de si reciben mucha o poca luz.\n\nLa gran mejora respecto a las Gen 1 está en la batería. Las originales prometían cuatro horas de uso, pero rara vez llegaban a eso. Las Gen 2 prometen ocho horas y, aunque tampoco las he alcanzado (porque tiendo a usarlas intensamente: música continua, vídeos en 3K, algún que otro comando de IA), la diferencia es notable.\n\nSi solo hago fotos ocasionales y pregunto algo puntual a Meta AI, la batería baja alrededor de un 12,5% por hora, tal como indica Meta. Si las apuro con vídeo constante, el consumo se acelera, pero ya no tengo esa preocupación de que me vayan a dejar tirado.\n\nEl audio también mejora, y eso que ya era sorprendente. Los altavoces de oreja abierta (no son conducción ósea aunque lo parezcan) suenan claros, potentes, con una separación estéreo sorprendente.\n\nPara música casual, podcasts o llamadas, funcionan de maravilla. Eso sí, no hay graves decentes (nunca los hay en este tipo de altavoces) y hay una cierta fuga de sonido a poco que queramos escuchar algo alto. A volumen del 75%, quien tengas cerca podrá identificar sin problemas qué canción estás escuchando. No son para ambientes ruidosos ni para quienes valoran el aislamiento acústico. Pero para caminar por la calle o estar en casa creo que cumplen más que bien.\n\nLa cámara también recibe una actualización sustancial. Las Gen 1 grababan en 1080p a 30 fps, las Gen 2 graban en 3K a 30 fps o 1080p a 60 fps. La resolución fotográfica se mantiene en 12 MP (3.024 x 4.032 píxeles), pero los vídeos tienen más detalle, mejor contraste y menos compresión. Para contenido de redes sociales, la diferencia no será abismal, pero se nota. Para creadores de contenido que buscan material POV de calidad, es un salto importante. Y para los que lo que queremos con ellas es generar recuerdos cotidianos, es una mejora de cara al futuro.\n\nAun así, las limitaciones de siempre persisten. Solo se graba en formato vertical (3:4). No hay opción de foto o vídeo horizontal. No hay zoom, claro. Y encuadrar es cuestión de fe: como no ves lo que estás capturando, cada disparo es una apuesta. La cámara sigue estando en el lateral izquierdo, así que el ángulo nunca es del todo natural. Las nuevas Oakley Meta Vanguard centran la cámara, pero las Ray-Ban no lo han hecho todavía. Bonus: miramos menos recto de lo que creemos, casi siempre he de nivelar la foto a mano porque sale algo torcida.\n\nEn situaciones de alto contraste, la cámara subexpone las sombras en exceso. Y de noche, las fotos salen borrosas incluso con luz artificial. No es un problema exclusivo de estas gafas sino que es una limitación del sensor pequeño y la óptica fija, pero conviene tenerlo presente.\n\nDicho todo esto, el hardware de las Ray-Ban Meta Gen 2 es excelente. Como gafas de sol con altavoces integrados y cámara discreta, son prácticamente imbatibles. Si pagas 419 euros por el modelo de lentes transparentes (499 euros este modelo con transiciones), obtienes un producto sólido, bien diseñado, funcional.\n\nEl problema es que no son solo eso. Son, supuestamente, unas gafas inteligentes impulsadas por IA. Y ahí es donde la cosa se complica un poco.\n\nLlevaba tiempo esperando probar las funciones de IA de las Gen 2. Meta lleva tiempo vendiendo la idea de que sus gafas no son solo una cámara portátil, sino un asistente contextual que entiende lo que ves, te ayuda a traducir idiomas en tiempo real, te da información sobre tu entorno. En teoría, decir \"Hey Meta\" debería ser como tener un copiloto permanente. En la práctica, es más bien como tener un becario despistado que no siempre entiende lo que le pides.\n\nHe probado Meta AI para traducir carteles. Funciona, más o menos. Le pedí que identificara una planta que tenía delante. Me dio tres respuestas distintas en tres intentos consecutivos. Cuando le pregunté si estaba segura, admitió que no. Intenté que me dijera si un edificio histórico era de estilo gótico o renacentista. Me soltó una respuesta genérica que podría haber copiado de Wikipedia.\n\nEl problema de Meta AI no es que sea malo per se, es que se siente desconectado, poco natural, torpe. La función de traducción en tiempo real es impresionante cuando funciona, pero necesitas que ambas personas lleven las gafas para una conversación completa, lo cual es poco práctico. Además, el Live AI (que mantiene la cámara encendida continuamente para que la IA analice tu entorno en tiempo real) es una idea brillante sobre el papel, pero en la práctica es más una demo curiosa que una herramienta útil. Y devora batería a una velocidad alarmante.\n\nA diferencia de ChatGPT, que mantiene memoria a corto y largo plazo y puede seguir el hilo de una conversación, Meta AI se siente como si se reiniciara con cada interacción. Técnicamente es multimodal, pero el análisis de imágenes es vago y básico. Las preguntas de seguimiento son torpes y casi inutilizables. La experiencia del producto es simplemente mala. El potencial está ahí, pero sigue siendo más futuro que presente.\n\nY la aplicación Meta AI, necesaria para gestionar cualquier aspecto de estas gafas, tiene demasiado de escaparate. Promociona tutoriales de las gafas (aceptable), promociona la afiliación para recomendárselas a un amigo y conseguir descuentos, promociona 'Vibes', una suerte de red social de contenido en vídeo generado con IA que no aporta absolutamente nada, y promociona otros modelos de gafas de Meta para que los compres en su tienda.\n\nMeta necesita ponerse las pilas. Si OpenAI lanzara unas gafas mañana con la misma API en tiempo real que usa en ChatGPT, con sus modelos de voz más avanzados y personalizaciones vía MCP, la diferencia sería abismal. Mismo hardware, misma tecnología, experiencia radicalmente distinta. De momento parece que solo serán unos auriculares.\n\nPorque lo que realmente estoy obteniendo con las Gen 2 es un asistente que toma mi entrada de voz, la convierte en texto, la envía a un modelo de chat, obtiene una respuesta y luego la convierte de nuevo en audio. Todo eso usando reconocimiento de voz y síntesis de habla que están muy por detrás de lo que OpenAI, ElevenLabs o incluso Amazon están ofreciendo ahora mismo.\n\nDespués de semanas con las Ray-Ban Meta Gen 2, mi conclusión es clara: son las mejores gafas inteligentes del mercado... si valoras el hardware por encima de la IA.\n\nPero si lo que te atrae es la promesa de un asistente de IA contextual que entienda tu entorno y te ayude en tiempo real, la cosa se complica. Meta AI está muy por detrás de lo que la competencia ofrece en otros dispositivos. No es solo que sea inferior a ChatGPT o Gemini, es que ni siquiera se acerca a lo que Llama puede hacer cuando lo usas directamente, sin pasar por el filtro de las gafas.\n\nY luego está el elefante en la habitación: la privacidad. Meta ha cambiado su política de privacidad para que los usuarios en Estados Unidos ya no puedan optar por no almacenar grabaciones de voz en la nube. Las imágenes procesadas para funciones multimodales como Live AI pueden usarse para entrenar modelos de IA. El contenido que compartes en la app puede acabar visible para el mundo si no tienes cuidado. Si algo de esto te incomoda, estas gafas no son para ti. Y yo no estoy aquí para convencerte de lo contrario.\n\nPor último, está el tema de la incomodidad social. Llevar estas gafas en público genera miradas. Algunas de curiosidad, otras de suspicacia. No son tan evidentes como las Google Glass o como las Oakley Meta, pero la gente sospecha que algo pasa. Y no siempre se siente bien. Especialmente en lugares como una playa (esto me pasó en agosto con las Oakley Meta HSTN), piscinas, gimnasios, etc, donde la presencia de una cámara, por discreta que sea, puede resultar invasiva.\n\nEl LED blanco parpadea al grabar, sí, pero no todo el mundo lo nota ni sabe qué significa eso. Y la confianza en que \"no estás grabando\" es algo que tienes que ganarte cada vez. Estas gafas son estupendas en muchos aspectos, el concepto de gafas inteligentes no tiene ningún problema, pero hay que ir con ojo para no usarlas como un imbécil, incomodar a los demás y quizás generar una situación fea.\n\nLas Ray-Ban Meta Gen 2 son un producto técnicamente competente atrapado en una crisis de expectativas. Meta quiere que sean el futuro de la computación portátil, el primer paso hacia unas gafas que registren y respondan a todo lo que ves. Pero ahora mismo, son sobre todo unas buenas gafas con un gran sonido, una cámara bastante decente y un asistente de IA decepcionante.\n\nSi ya tienes las Gen 1, la mejora de batería justifica el salto solo si esa limitación te resulta frustrante ahora. Si eres nuevo en esto de las gafas inteligentes y te atrae la idea, las Gen 2 son la mejor opción disponible hoy. Pero si lo que buscas es IA de vanguardia en unas gafas, espera un poco más. Porque Meta todavía tiene mucho camino por recorrer antes de que sus gafas sean realmente inteligentes, y no solo inteligentemente diseñadas.\n\nA mí no me importa gran cosa porque todavía no tengo mucho que pedir a una IA en unas gafas. Como gafas + auriculares + cámara POV me sirven perfectamente. Pero quien sí pida ese extra no debería llevarse a engaño: esto es el anticipo de algo por llegar.\n\nEn Xataka | China llevaba años prometiendo gafas inteligentes. Han empezado a venderlas cuando han descubierto para qué la gente quiere usarlas\n\nImagen destacada | Xataka"
  },
  {
    "source": "La Nación, Grupo Nación",
    "company": "Meta AI",
    "title": "Cómo desactivar el Meta AI de WhatsApp y por qué es importante hacerlo",
    "date": "2026-01-29T14:38:43Z",
    "url": "https://www.nacion.com/tecnologia/como-desactivar-el-meta-ai-de-whatsapp-y-por-que/23OSTILMNJDMRLWPSXOSYB4WMM/story/",
    "content": "Expertos alertan sobre las limitaciones para desactivar Meta AI en WhatsApp y sus posibles implicaciones en el manejo de datos personales\n\nLa integración del asistente Meta AI en WhatsApp generó incomodidad entre miles de usuarios en todo el mundo, incluidos quienes usan la plataforma para trabajo, estudios o comunicaciones sensibles. El nuevo botón con funciones de inteligencia artificial empezó a aparecer en la barra de búsqueda o en la parte superior de la aplicación, incluso sin que el usuario lo solicitara.\n\nAunque Meta afirma en su sitio web que la función es \"opcional\", lo cierto es que no existe un botón claro para desactivarla completamente.\n\nEn abril, así lo confirmó la BBC: \"No es posible desactivar el icono\", indicó Meta a ese medio, aunque aclaró que los usuarios no están obligados a usar el asistente ni a interactuar con él.\n\nEl medio AP News publicó una guía titulada ¿Quiere desactivar Meta AI? No puede, pero hay algunas alternativas, donde confirma lo antes dicho; sin embargo, agregó que sí existen acciones para \"minimizar su presencia\", como archivar el chat del asistente o evitar cualquier interacción que lo active.\n\nA esta discusión se sumó Reuters, que informó en julio de 2025 que Meta enfrentaba una investigación del regulador italiano de competencia por la manera en que introdujo Meta AI en WhatsApp y por la falta de claridad sobre su desactivación.\n\n1. Privacidad y uso de datos\n\nSegún el análisis del sitio AtomicMail, que ha seguido de cerca las políticas de Meta, los mensajes de WhatsApp continúan cifrados de extremo a extremo; sin embargo, las interacciones que el usuario tiene directamente con Meta AI no necesariamente gozan del mismo blindaje. Esto genera dudas sobre si lo que el usuario escribe para el asistente podría usarse para entrenamiento de modelos o para generar perfiles de comportamiento.\n\nEn regiones como Europa, ya existen mecanismos para oponerse a que Meta use datos para entrenar su IA, pero estas opciones aplican más a Facebook e Instagram que a WhatsApp.\n\n2. Control de la herramienta\n\nDe acuerdo con The Sun y otros medios tecnológicos, una recomendación frecuente es \"evitar abrir el chat del asistente\", porque cualquier interacción puede anclarlo más en la experiencia del usuario.\n\nAdemás, para personas que usan WhatsApp de forma laboral como periodistas, autoridades, profesionales de salud, abogados, etc., la presencia constante del botón puede generar \"ruido\" visual o la sensación de que la IA podría intervenir en conversaciones sensibles, aun cuando no lo haga automáticamente.\n\n3. Consentimiento poco claro\n\nSitios como Wired han señalado que la incorporación del botón ocurrió sin dar al usuario una opción real de aceptarlo o rechazarlo. El medio lo resumió así: \"No puede remover Meta AI del diseño de WhatsApp, incluso si nunca piensa usarlo\".\n\nAunque no se puede desactivar por completo, sí es posible reducir la presencia del asistente mediante los siguientes pasos, recopilados a partir de guías de AP News y medios especializados:\n\nExpertos en ciberseguridad recomiendan, además, que quienes trabajan con información sensible -- como periodistas -- consideren separar sus conversaciones profesionales de la plataforma o evaluar alternativas más centradas en privacidad, como Signal.\n\nEn paralelo a estas dudas sobre control y transparencia, Meta anunció que a partir del 16 de diciembre usará las conversaciones con su asistente para ajustar la publicidad y las recomendaciones de contenido en sus otras plataformas. La empresa afirmó que no habrá un botón para desactivar este sistema y que solo se aplicará a quienes interactúen con la IA.\n\nAunque en WhatsApp el impacto sería menor para quienes no tengan sus cuentas vinculadas con Facebook o Instagram, la actualización confirma una tendencia: la inteligencia artificial de Meta ya no es solo una función adicional dentro de las aplicaciones, sino un engranaje del modelo publicitario que define qué ve cada usuario y cómo se personaliza su experiencia en el ecosistema de la compañía.\n\nAdemás, la empresa adelantó que esta integración publicitaria se implementará de forma gradual y llegará acompañada de una notificación para todos los usuarios."
  },
  {
    "source": "DNyuz",
    "company": "Meta AI",
    "title": "Meta Just Bought a Singaporean AI Firm Few in the West Have Heard Of",
    "date": "2026-01-27T20:54:52Z",
    "url": "https://dnyuz.com/2026/01/27/meta-just-bought-a-singaporean-ai-firm-few-in-the-west-have-heard-of/",
    "content": "Manus AI. Ever heard of it? No? Even with the glut of stories about AI clogging up headlines from Facebook to CNN? You've surely been gagging on the gaggle of stories centering around ChatGPT, Claude, Perplexity, Gemini, Copilot, and Grok. All these are household names.\n\nWhen Meta announced in December 2025 that it planned to acquire Manus AI, few paid much attention. But with news that Meta is preparing to trial paid, premium subscriptions for Facebook, Instagram, and WhatsApp, attention has zeroed in on the kind of AI features that may appear -- features likely to have something to do with Manus AI.\n\nseemingly from out of nowhere\n\nManus AI was founded in China, launched to the public in March 2025, and relocated its headquarters to Singapore by that summer. And by the end of December, Meta had bought it for $2 billion, according to The Wall Street Journal.\n\nWhy would Meta, a social media company that boasts Facebook, Instagram, and WhatsApp as its star offerings, want to acquire an AI startup? Well, Meta hasn't been shy about its desire to integrate its own Meta AI into its apps, rather than go the way of Apple and Samsung in simply purchasing the rights from Google to integrate Google's Gemini AI after having taken a swing at their own AIs and largely given up.\n\nMeta AI, though, hasn't fared noticeably better than Apple's attempt at a souped-up Siri or Samsung's Bixby. Few mentioned Meta AI in the same breath as ChatGPT, Gemini, or Claude, and AI is horrendously expensive to develop. Buying its way into new capabilities is a quicker, easier way for Meta to bolster its competitive edge when it comes to AI.\n\nThe acquisition won't end Meta AI, though. Rather, the acquisition seems to be an attempt by Meta to use Manus AI's infusion of resources, technology, and institutional knowledge to beef up Meta AI, not to replace it.\n\n\"Manus's exceptional talent will join Meta's team to deliver general-purpose agents across our consumer and business products, including in Meta AI,\" read a Meta announcement at the time of the acquisition.\n\nAccording to the Associated Press, there will be no Chinese ownership, and Manus AI will continue to function from its Singaporean headquarters. What sort of effect Manus AI will have on Meta products is, for now, largely left up to the imagination.\n\nThe post Meta Just Bought a Singaporean AI Firm Few in the West Have Heard Of appeared first on VICE."
  },
  {
    "source": "VICE",
    "company": "Meta AI",
    "title": "Meta Just Bought a Singaporean AI Firm Few in the West Have Heard Of",
    "date": "2026-01-27T20:17:18Z",
    "url": "https://www.vice.com/en/article/meta-purchased-manus-ai/",
    "content": "Manus AI hasn't even been released to the public for a year and already it's been acquired by Meta.\n\nManus AI. Ever heard of it? No? Even with the glut of stories about AI clogging up headlines from Facebook to CNN? You've surely been gagging on the gaggle of stories centering around ChatGPT, Claude, Perplexity, Gemini, Copilot, and Grok. All these are household names.\n\nWhen Meta announced in December 2025 that it planned to acquire Manus AI, few paid much attention. But with news that Meta is preparing to trial paid, premium subscriptions for Facebook, Instagram, and WhatsApp, attention has zeroed in on the kind of AI features that may appear -- features likely to have something to do with Manus AI.\n\nseemingly from out of nowhere\n\nManus AI was founded in China, launched to the public in March 2025, and relocated its headquarters to Singapore by that summer. And by the end of December, Meta had bought it for $2 billion, according to The Wall Street Journal.\n\nWhy would Meta, a social media company that boasts Facebook, Instagram, and WhatsApp as its star offerings, want to acquire an AI startup? Well, Meta hasn't been shy about its desire to integrate its own Meta AI into its apps, rather than go the way of Apple and Samsung in simply purchasing the rights from Google to integrate Google's Gemini AI after having taken a swing at their own AIs and largely given up.\n\nMeta AI, though, hasn't fared noticeably better than Apple's attempt at a souped-up Siri or Samsung's Bixby. Few mentioned Meta AI in the same breath as ChatGPT, Gemini, or Claude, and AI is horrendously expensive to develop. Buying its way into new capabilities is a quicker, easier way for Meta to bolster its competitive edge when it comes to AI.\n\nThe acquisition won't end Meta AI, though. Rather, the acquisition seems to be an attempt by Meta to use Manus AI's infusion of resources, technology, and institutional knowledge to beef up Meta AI, not to replace it.\n\n\"Manus's exceptional talent will join Meta's team to deliver general-purpose agents across our consumer and business products, including in Meta AI,\" read a Meta announcement at the time of the acquisition.\n\nAccording to the Associated Press, there will be no Chinese ownership, and Manus AI will continue to function from its Singaporean headquarters. What sort of effect Manus AI will have on Meta products is, for now, largely left up to the imagination."
  },
  {
    "source": "mint",
    "company": "Meta AI",
    "title": "How to create custom Republic Day 2026 stickers on WhatsApp using AI: Check step-by-step guide | Mint",
    "date": "2026-01-25T07:37:07Z",
    "url": "https://www.livemint.com/technology/tech-news/how-to-create-custom-republic-day-2026-stickers-on-whatsapp-using-ai-check-step-by-step-guide-11769321710599.html",
    "content": "As Republic Day 2026 approaches, create custom AI stickers on WhatsApp to celebrate in style using Meta AI. If you don't want to use Meta AI for creating stickers, there are also a few alternatives you can try\n\nRepublic Day 2026 is just around the corner and if you want to go beyond the usual \"Happy Republic Day\" GIFs and images, creating a custom AI sticker on WhatsApp could be a great way to spread the festive vibes in style.\n\nWhile there are already a number of stickers available on WhatsApp, the use of Meta AI can also help users create completely new AI customized stickers in seconds.\n\nIf you are interested in creating custom Republic Day 2026 stickers using WhatsApp, check out the below guide.\n\nHow to create custom AI generated stickers using WhatsApp?\n\n\"Cute Indian child saluting with a tricolour background\"\n\n\"Indian map made of orange, white and green flowers\"\n\n\"Republic Day 2026 futuristic text art\"\n\n\"Ashoka Chakra glowing neon style\"\n\n\"Tiger running with Indian flag cape\"\n\nDo note that Meta will show you ads across Facebook, Messenger, Instagram and WhatsApp based on your interactions with its Meta AI chatbot.\n\nAlternatively, if you don't want to create your own sticker you can just tap on the search icon and search for Republic Day stickers to select one from the already created presets on the app.\n\nMoreover, if you don't have access to Meta AI's Sticker generator feature or if you don't want to share your data with Meta's AI, there are a few alternatives you can consider as well.\n\nSticker.ly: This app can already possesses a massive library of pre-made stickers and also comes with an 'Auto Cut' feature which can remove backgrounds from your photos that can come in handy if you are looking to turn your images into Republic Day stickers.\n\nPicsart: Picsart offers the ability to remove background, apply filters and texts to help you design the picture perfect sticker for Republic Day.\n\nStickify: Stickify not only comes with a massive library of pre-made stickers but it also comes with a \"Sticker Studio\" which lets users create custom stickers with photos, videos and even GIFs."
  },
  {
    "source": "WebProNews",
    "company": "Microsoft AI",
    "title": "Microsoft's AI Gambit Is Stumbling: Inside the Cracks Forming Beneath Redmond's $80 Billion Bet",
    "date": "2026-02-07T19:30:43Z",
    "url": "https://www.webpronews.com/microsofts-ai-gambit-is-stumbling-inside-the-cracks-forming-beneath-redmonds-80-billion-bet/",
    "content": "Microsoft has spent the better part of three years positioning itself as the undisputed leader of the artificial intelligence revolution. Armed with a multibillion-dollar partnership with OpenAI and a relentless campaign to embed AI into every corner of its product ecosystem, the Redmond, Washington-based tech giant has staked its future -- and tens of billions of dollars in capital expenditure -- on the premise that generative AI will reshape enterprise computing. But a growing body of evidence suggests that Microsoft's AI efforts are not delivering the returns the company promised, and cracks are beginning to show across multiple fronts.\n\nFrom its flagship AI assistant Copilot to its cloud infrastructure business Azure, Microsoft is confronting an uncomfortable reality: the AI gold rush may not be panning out as quickly or as lucratively as its executives projected. As Futurism recently reported, Microsoft's AI efforts appear to be \"faceplanting\" across several key metrics, raising pointed questions about whether the company's enormous investments will generate meaningful returns anytime soon -- or whether they represent one of the most expensive strategic miscalculations in modern tech history.\n\nCopilot's Rocky Reception and the Enterprise Adoption Problem\n\nAt the center of Microsoft's AI strategy sits Copilot, the AI-powered assistant integrated into Microsoft 365, Windows, and a growing list of enterprise applications. When Microsoft launched Copilot with great fanfare, it promised a transformative productivity tool that would justify its $30-per-user monthly premium for enterprise customers. The pitch was simple and seductive: AI would draft your emails, summarize your meetings, build your spreadsheets, and fundamentally change the way knowledge workers operate. But the reality on the ground has been far less impressive.\n\nEnterprise adoption of Copilot has been sluggish, with numerous reports indicating that many organizations that piloted the tool have declined to expand their subscriptions. The core issue, according to multiple industry analysts and IT decision-makers, is that Copilot's outputs frequently fall short of the quality and reliability that enterprise customers demand. The tool often produces generic, inaccurate, or unhelpful responses -- a problem that becomes particularly acute in specialized business contexts where precision matters. As Futurism noted, the product has struggled to demonstrate the kind of clear, measurable productivity gains that would justify its steep price tag for budget-conscious IT departments already grappling with rising software costs.\n\nThe $80 Billion Infrastructure Spending Spree Faces Scrutiny\n\nMicrosoft's AI ambitions have required staggering levels of capital investment. The company has committed approximately $80 billion in capital expenditure for fiscal year 2025 alone, with the vast majority earmarked for AI-related data center construction and GPU procurement. This spending binge has made Microsoft one of the largest infrastructure investors in the world, rivaling the capital outlays of entire nations. CEO Satya Nadella and CFO Amy Hood have repeatedly assured investors that this spending is necessary to capture what they describe as a generational opportunity in AI computing.\n\nBut Wall Street's patience is not infinite. Microsoft's stock, which soared throughout 2023 and into early 2024 on AI enthusiasm, has faced increasing pressure as investors scrutinize the gap between the company's massive capital outlays and the actual revenue being generated by its AI products. The fundamental question haunting Microsoft's balance sheet is straightforward: when will these tens of billions of dollars in spending translate into proportional revenue growth? So far, the answer has been unsatisfying. While Microsoft has pointed to Azure's AI-related revenue growth as a bright spot, the absolute numbers remain a fraction of what would be needed to justify the scale of investment being made.\n\nAzure's AI Revenue: Growth Without Enough Substance\n\nMicrosoft has repeatedly highlighted Azure's AI services as a key growth driver, and the numbers are indeed growing. The company has reported that Azure's AI-related revenue is expanding at a rapid clip, with AI services contributing several percentage points to Azure's overall growth rate. But context matters enormously here. Azure's total revenue, while substantial, is growing from a base that makes the AI contribution look more like a promising supplement than a transformative engine. Competitors like Amazon Web Services and Google Cloud are making their own aggressive AI plays, and the cloud infrastructure market is becoming increasingly competitive.\n\nMoreover, there are growing concerns about the sustainability of Azure's AI growth trajectory. Much of the current demand for AI computing infrastructure is being driven by a relatively small number of large customers -- including OpenAI itself, which is one of Azure's biggest clients. This concentration risk means that Microsoft's AI cloud revenue is more fragile than headline growth numbers might suggest. If the broader market for AI computing does not expand as rapidly as projected, or if key customers like OpenAI diversify their infrastructure providers, Azure's AI revenue growth could decelerate sharply.\n\nThe OpenAI Partnership: Asset or Liability?\n\nMicrosoft's relationship with OpenAI, once hailed as a masterstroke of strategic positioning, is becoming increasingly complicated. Microsoft has invested approximately $13 billion in OpenAI, securing privileged access to the startup's models and technology. This partnership was the foundation upon which Microsoft built its entire AI product strategy, from Copilot to Azure AI services. But the relationship has grown more fraught as OpenAI has evolved from a research lab into a commercially aggressive company with its own platform ambitions.\n\nOpenAI's transition from a nonprofit structure to a for-profit entity, its pursuit of massive new funding rounds at eye-watering valuations, and its development of consumer-facing products that increasingly compete with Microsoft's own offerings have all introduced tension into the partnership. There are legitimate questions about whether OpenAI's long-term interests are truly aligned with Microsoft's, or whether the startup will eventually seek to reduce its dependence on its largest investor and infrastructure provider. For Microsoft, the risk is that it has built a critical part of its AI strategy on a foundation it does not fully control.\n\nEmployee Morale and Internal Turbulence\n\nThe challenges facing Microsoft's AI push are not limited to products and finances. Inside the company, there are signs of strain. Microsoft has undergone significant layoffs in recent months, including cuts that have affected teams working on AI-related projects. The juxtaposition of massive capital spending on AI infrastructure with workforce reductions has created internal tension and raised questions about the coherence of Microsoft's strategy. Employees in some divisions have expressed frustration that the company's AI-first rhetoric does not always match the reality of resource allocation and organizational priorities.\n\nAdditionally, Microsoft has faced scrutiny over the environmental impact of its AI infrastructure buildout. The enormous energy demands of AI data centers have put pressure on the company's sustainability commitments, and reports have emerged that Microsoft's carbon emissions have increased significantly as a result of its AI expansion. This creates a reputational risk that could become more significant as regulators, investors, and customers pay increasing attention to the environmental costs of the AI boom.\n\nCompetitive Pressures Mount From All Directions\n\nMicrosoft is not operating in a vacuum. Google, Amazon, Meta, and a host of well-funded startups are all pursuing AI strategies with enormous resources and significant technical talent. Google's Gemini models have shown rapid improvement, and the company's deep integration of AI into its search and cloud products represents a formidable competitive threat. Amazon Web Services continues to dominate the cloud infrastructure market and is aggressively expanding its own AI offerings. Meta has taken an open-source approach to AI development that has resonated with developers and enterprises wary of vendor lock-in.\n\nPerhaps most concerning for Microsoft is the emergence of capable, lower-cost AI models from companies like Anthropic, Mistral, and various Chinese AI labs. These developments challenge the assumption that the most expensive, largest-scale models will always be the most commercially valuable. If the AI market evolves toward smaller, more efficient, and more specialized models, Microsoft's strategy of massive infrastructure investment and reliance on OpenAI's frontier models could prove to be poorly calibrated for the market that actually materializes.\n\nWhat Comes Next for Redmond's Biggest Bet\n\nNone of this means that Microsoft's AI strategy is doomed to fail. The company possesses extraordinary advantages -- a massive installed base of enterprise customers, deep pockets, world-class engineering talent, and a distribution network that few competitors can match. If AI technology continues to improve and enterprise adoption accelerates, Microsoft is well-positioned to capture a significant share of the value created. Satya Nadella has proven himself to be one of the most capable CEOs in technology, and his track record of navigating strategic transitions -- most notably the shift to cloud computing -- commands respect.\n\nBut the warning signs are real and growing harder to ignore. Microsoft's AI products need to deliver meaningfully better results to justify their pricing. The company's capital spending needs to translate into revenue growth that satisfies increasingly skeptical investors. And the OpenAI partnership needs to remain stable and mutually beneficial in a period of rapid change. The next twelve to eighteen months will be critical in determining whether Microsoft's AI bet was a visionary move that cemented its dominance for a generation -- or a cautionary tale about the dangers of investing too much, too fast, in a technology whose commercial potential remains unproven at scale. For now, the jury is very much still out."
  },
  {
    "source": "News Directory 3",
    "company": "Microsoft AI",
    "title": "Google Gemini App Free SAT Practice Exams - News Directory 3",
    "date": "2026-01-22T20:18:38Z",
    "url": "https://www.newsdirectory3.com/google-gemini-app-free-sat-practice-exams/",
    "content": "Nick Bilton, dick Costolo, and Paul Kedrosky pull back the curtain on AI, startups, and the future rushing toward us, all with healthy dose of irreverence.\n\nBig⁢ Technology Podcast:Okay, I will analyze the provided text and follow⁤ the instructions meticulously, adhering to all constraints.\n\nPHASE 1: ADVERSARIAL ⁣RESEARCH, FRESHNESS & BREAKING-NEWS CHECK\n\nThe text describes a special edition of the \"Tools and⁣ Weapons\" podcast hosted by Brad Smith (Microsoft) focusing on the Microsoft AI Diffusion Report.\n\n* Factual Claim Verification: Brad Smith is the Vice Chair and President of Microsoft. The ⁢\"Tools and Weapons\" podcast⁤ does exist. A Microsoft AI Diffusion Report has been published.\n\n*  Contradictory/Correcting ⁢Information: A search for \"Microsoft⁤ AI Diffusion Report\" reveals the ⁤report was released in December 2023. Microsoft's official blog post details the report's findings.\n\n* ⁣ breaking News check (as of 2026/01/22 19:51:00): A search for updates on⁣ the Microsoft AI diffusion Report⁤ and related⁣ discussions as of today's date reveals ongoing discussions and developments regarding AI safety, misinformation, and election security. The Council⁣ on Foreign Relations continues to publish analysis on the intersection of AI and democracy, building ⁣on concerns raised in reports like Microsoft's. No single, definitive \"update\" to the report itself has superseded it, but the context surrounding it's findings has evolved significantly with the 2024 ⁢US election ⁤and ongoing geopolitical events.\n\n* ⁢ Latest Verified Status: the Microsoft AI Diffusion Report remains a key document in understanding the risks of AI-powered information⁢ operations, but its findings are now⁢ viewed within a more ⁣complex and rapidly evolving landscape.\n\nPHASE 2:⁢ ENTITY-BASED GEO\n\n* Primary Entity: Brad Smith (Microsoft Vice Chair and President)\n\n*  Related Entities: Microsoft,Tools and Weapons Podcast,Artificial Intelligence (AI),Information Operations,Election Security,Council on Foreign Relations.\n\nBrad ⁤Smith and the \"Tools and⁤ Weapons\" Podcast\n\nBrad Smith, Vice Chair and President of Microsoft, hosts the \"Tools and Weapons\" podcast, which explores⁤ the intersection of technology⁣ and society.\n\nThe Microsoft AI Diffusion report\n\nThe podcast's special ⁢edition focuses on the⁢ Microsoft AI Diffusion Report, released in December 2023.\n\nAI-Powered Information Operations\n\nThe⁢ report details the growing threat of AI-powered information operations and their potential impact on democratic processes and societal stability.\n\nElection ⁤Security Concerns\n\nThe report highlights the potential for AI to be used ⁢to create and disseminate disinformation during⁢ elections, raising concerns about election ⁣integrity. ⁤ The Cybersecurity and Infrastructure Security Agency (CISA) ⁣provides resources and guidance on election security, acknowledging the evolving threat landscape including AI-generated disinformation.\n\nOngoing Analysis by the Council on Foreign Relations\n\nOrganizations like  The Council on Foreign Relations continue to ⁤analyze the implications ⁤of AI for democracy and international security, building upon the⁣ concerns initially raised in the Microsoft report.\n\nPHASE⁣ 3: SEMANTIC⁣ ANSWER RULE\n\nBrad Smith and the \"Tools and Weapons\" ⁤Podcast\n\nThe Microsoft AI⁣ Diffusion report"
  },
  {
    "source": "India News, Breaking News, Entertainment News | India.com",
    "company": "Microsoft AI",
    "title": "Microsoft AI CEO issues strong statement hinting at 'dumping' OpenAI, talks about, 'true AI self-sufficiency'",
    "date": "2026-02-14T11:16:44Z",
    "url": "https://www.india.com/us/microsoft-ai-ceo-issues-strong-statement-hinting-at-dumping-openai-talks-about-true-ai-self-sufficiency-sam-altman-satya-nadella-artificial-intelligence-8306773/",
    "content": "The CEO of Microsoft AI, Mustafa Suleyman, has made a statement about decreasing its reliance on OpenAI. Suleyman told the Financial Times that it's aiming to strengthen its own AI now. The company confirmed that it is 'dumping' OpenAI. The AI CEO also highlighted his personal mission at the company, which is to create \"superintelligence\". This comes after his statement about how AI is most likely to replace most white-collar jobs in the next 12-18 months.\n\nThe Microsoft AI CEO, Mustafa Suleyman, stated in an interview with the Financial Times, \"My personal mission at Microsoft is to build superintelligence,\" and to push \"true AI self-sufficiency.\" This greatly hints at the company's dependency on OpenAI.\n\nHe added that the company has to build a powerful AI model of its own with \"some of the very best AI training team in the world.\"\n\nAlso Read: Microsoft AI CEO issues strong warning on white-collar jobs, hints at replacement of most jobs within 12-18 months\n\nThe partnership between the two companies began in 2019, when Large Language Models (LLMs) were widely trained by OpenAI. Later, the formation of ChatGPT happened.\n\nDuring this time, the company's CEO on X announced, \"We remain committed to our partnership with OpenAI and have confidence in our product roadmap, our ability to continue to innovate with everything we announced at Microsoft Ignite, and in continuing to support our customers and partners. We look forward to getting to know Emmett Shear and OAI's new leadership team and working with them. And we're extremely excited to share the news that Sam Altman and Greg Brockman, together with colleagues, will be joining Microsoft to lead a new advanced AI research team. We look forward to moving quickly to provide them with the resources needed for their success.\"\n\nAlso Read: Elon Musk accuses OpenAI, Microsoft of betrayal, seeks up to USD 134 billion for THIS reason, what's the whole matter\n\nMustafa Suleyman was appointed as the Executive Vice President and CEO of Microsoft AI. The statement of Sulayeman in an interview with the Financial Times hints not only at the strengthening of Microsoft AI, but toward dumping 'OpenAI'."
  },
  {
    "source": "India Today",
    "company": "Microsoft AI",
    "title": "Microsoft AI boss Mustafa Suleyman says AI will replace most white-collar jobs in 12 months",
    "date": "2026-02-12T10:12:38Z",
    "url": "https://www.indiatoday.in/technology/news/story/microsoft-ai-boss-mustafa-suleyman-says-ai-will-replace-most-white-collar-jobs-in-12-months-2867182-2026-02-12",
    "content": "Microsoft AI boss Mustafa Suleyman has warned that AI could be coming to take most white-collar jobs soon. Not just coders, but even professionals such as lawyers and accountants, may see their job be automated by AI.\n\nIn an interview with the Financial Times, Suleyman revealed that Microsoft was pushing for a bigger share in the enterprise market with \"professional-grade AGI.\" He referred to this as an AI model that could do almost everything a human professional does. This would allow Microsoft to deliver powerful AI tools to its clients capable of performing routine tasks for knowledge workers.\n\nSuleyman predicted significant changes in the workforce were in store for the future. He reckoned that almost everyone who does their job on a computer could be at risk. The Microsoft AI boss explained,\"White-collar jobs - those sitting in front of computers, whether lawyers, accountants, project managers, or marketers - most of these tasks will be fully automated by AI within the next 12 to 18 months.\"\n\nThe Microsoft AI chief's comments come at a time when more companies are looking towards using AI. Anthropic's Claude Cowork shook the stock markets a few days ago after the model threatened the future of SaaS companies such as Infosys and TCS.\n\nMicrosoft AI boss says making an AI model will be like making a podcast\n\nAs AI advances, Suleyman reckons, it will become easier for make new models, which could be curated to specific needs. He compared this to how one can easily write a blog or create a podcast. The Microsoft AI boss explained, \"Creating a new model will be as simple as making a podcast or writing a blog. In the future, it will be possible to design AI tailored to the needs of every institution and individual on Earth.\"\n\nAdditionally, Suleyman added that within the next two to three years, AI agents will be able to become more efficient when it comes to handling the workflow of large institutions.\n\nThe Microsoft boss also hinted that the company was going to double down on its own AI models in the future, in a bid to reduce its reliance on OpenAI, following a new agreement between the two companies. He added, \"We decided that this was a moment when we have to set about delivering on true AI self-sufficiency.\" Suleyman has teased that the new AI models from Microsoft may debut in 2026."
  },
  {
    "source": "WebProNews",
    "company": "Microsoft AI",
    "title": "Microsoft's AI Marketing Coup: Pinterest Veteran Mallard Takes Helm Amid Ad Tech Surge",
    "date": "2026-01-23T14:20:06Z",
    "url": "https://www.webpronews.com/microsofts-ai-marketing-coup-pinterest-veteran-mallard-takes-helm-amid-ad-tech-surge/",
    "content": "Microsoft Corp. has appointed Andréa Mallard, the former global chief marketing officer at Pinterest Inc., as its first chief marketing officer for the AI division, signaling an aggressive push to embed artificial intelligence deeper into its advertising operations. The move, announced this week, comes as the software giant races to capitalize on AI's transformative potential in digital marketing, blending advanced models with targeted ad delivery to challenge rivals like Google and Meta.\n\nMallard, who spent over seven years at Pinterest shaping its brand through cultural partnerships and performance-driven campaigns, brings a wealth of experience in consumer-facing tech marketing. Her departure from Pinterest was reported earlier this month, paving the way for this high-profile hire. In a BestMediaInfo article, Mallard outlined her priorities: 'trust, responsibility and people-led AI.'\n\nAt Microsoft, she steps into a role designed to unify messaging around AI products like Copilot while accelerating adoption among enterprise and consumer advertisers. This appointment coincides with new AI-enhanced ad features, positioning Microsoft Advertising as a frontrunner in intelligent campaign optimization.\n\nPinterest Roots Fuel Microsoft's Ambition\n\nDuring her tenure at Pinterest, Mallard oversaw the platform's evolution from a visual discovery tool to a robust advertising powerhouse. She launched initiatives like the Pinterest Media Network Connect, which links ad exposure to retail sales, as noted in an Adweek piece on her exit. Her work helped Pinterest expand global partnerships and integrate shoppable content, driving revenue growth amid fierce competition.\n\nPinterest confirmed her departure in early January, praising her contributions without naming a successor immediately. Posts on X echoed the sentiment, with industry observers like Daversa Partners noting, 'Andréa Mallard stepping into the CMO role at Microsoft AI is a reminder that the AI race is narrative-driven and speed-sensitive.' Microsoft's hire taps into that expertise to craft narratives around responsible AI deployment in ads.\n\nThe timing aligns with Pinterest appointing Claudine Cheever as its new CMO and creating a chief business officer role for Lee Brown, per Marketing-Interactive, signaling a leadership refresh as Microsoft poaches top talent.\n\nAI's Frontline in Advertising Wars\n\nMicrosoft's announcement, detailed in a MediaPost report, highlights forthcoming tools like AI-powered predictive analytics for ad performance and generative content creation for campaigns. These features aim to deliver hyper-personalized ads across Microsoft's vast ecosystem, including LinkedIn, Bing, and Xbox, reaching billions of impressions monthly.\n\n'Microsoft is pushing further into AI and advertising by hiring its first CMO for the group and detailing new features and services for its ad business,' MediaPost reported. Mallard's role will emphasize building trust in AI-driven ads, addressing concerns over data privacy and algorithmic bias that have plagued the sector.\n\nIndustry insiders view this as Microsoft's bid to close the gap with Alphabet Inc.'s Google, which dominates search advertising. Microsoft's AI integrations, powered by models like those from OpenAI, promise up to 20% lifts in engagement, drawing from studies cited in broader reports.\n\nStrategic Priorities Under Mallard\n\nMallard confirmed her new position via LinkedIn, as covered by Social Samosa, stressing 'expanding its brand and cultural partnerships.' At Microsoft AI, she will report into senior leadership, focusing on 'people-led AI' to humanize tech advancements.\n\nHer priorities mirror Microsoft's broader strategy under CEO Satya Nadella, who has invested billions in AI. Recent X discussions highlight excitement, with ADWEEK posting: '@Microsoft AI has named Andréa Mallard as CMO... confirmed her new role in a LinkedIn post.'\n\nThis hire follows other key appointments, like Jacob Andreou joining to lead product, design, and growth for Microsoft AI, as shared in his X post, underscoring a talent influx to fuel expansion.\n\nNavigating Trust and Regulation\n\nAs AI reshapes advertising, Mallard's emphasis on responsibility arrives amid regulatory scrutiny. The European Union's AI Act and U.S. probes into tech giants demand transparent practices. BestMediaInfo quoted her on fostering 'trust' through ethical AI use in marketing.\n\nMicrosoft's ad push includes tools for verifiable AI outputs, reducing hallucination risks in ad copy. Campaign Asia, in a profile of Mallard's Pinterest exit, noted her belief that 'AI can drive both performance and positivity,' a philosophy she carries to Microsoft.\n\nCompetitors watch closely; Google's Performance Max and Meta's Advantage+ already leverage AI, but Microsoft's enterprise ties via Azure and Dynamics give it an edge in B2B advertising.\n\nBroader Industry Ripples\n\nThe appointment ripples through ad tech, with Storyboard18 reporting Microsoft 'accelerates its ambitions to position AI as a trusted, mass-market technology.' X users like Indiantelevision.com amplified: '@AndreaMallard steps in as chief marketing officer at @MicrosoftAI.'\n\nPinterest's leadership shuffle, detailed in AdAge, underscores talent mobility in a sector where CMOs command premium value. Mallard's move from visual search to AI orchestration exemplifies the convergence of creative and computational marketing.\n\nLooking ahead, expect Microsoft to unveil more under Mallard's guidance at events like Cannes Lions or its Build conference, solidifying AI's role in revenue streams projected to exceed $15 billion annually for Microsoft Advertising."
  },
  {
    "source": "WebProNews",
    "company": "Microsoft AI",
    "title": "Taking Back Control: How to Fully Remove Microsoft Copilot and Recall From Your Windows PC",
    "date": "2026-02-16T21:25:09Z",
    "url": "https://www.webpronews.com/taking-back-control-how-to-fully-remove-microsoft-copilot-and-recall-from-your-windows-pc/",
    "content": "For millions of Windows users, Microsoft's aggressive integration of artificial intelligence features into its operating system has become less a convenience and more an imposition. From the AI-powered Copilot assistant embedded in the taskbar to the controversial Recall feature that continuously screenshots user activity, the software giant's push to make AI ubiquitous has sparked a growing backlash among privacy-conscious users and enterprise IT administrators alike. Now, a rising tide of guides and community-driven solutions is empowering users to strip these features from their machines entirely.\n\nThe tension between Microsoft's AI ambitions and user autonomy has been building for more than a year. When Copilot first appeared as a default feature in Windows 11, it was met with mixed reactions. Some users appreciated the convenience of an AI assistant integrated directly into their workflow. But many others -- particularly IT professionals, security-minded individuals, and those who simply prefer a leaner operating system -- viewed it as unwanted bloatware consuming system resources and raising data privacy concerns.\n\nMicrosoft's AI Integration: A Feature Users Didn't Ask For\n\nAs MakeUseOf detailed in a comprehensive guide, removing Copilot and Recall from a Windows PC is not as straightforward as uninstalling a typical application. Microsoft has deeply woven these features into the operating system's fabric, requiring users to navigate Group Policy Editor settings, Windows Registry modifications, and PowerShell commands to fully excise them. The complexity of the removal process itself has become a point of criticism -- if these features were truly optional, critics argue, they wouldn't require administrative-level interventions to disable.\n\nCopilot, Microsoft's AI assistant powered by OpenAI's large language models, was introduced as a system-level feature in Windows 11 and has since been pushed through updates to a broad swath of users. It appears in the taskbar, integrates with Microsoft Edge, and is designed to assist with everything from summarizing documents to generating code. For users who don't want or need an AI assistant -- or who have concerns about the data Copilot processes -- its persistent presence has been a source of frustration.\n\nThe Recall Controversy: Screenshots, Privacy, and Public Outcry\n\nIf Copilot's integration raised eyebrows, Recall set off alarm bells. Announced as a flagship feature for Copilot+ PCs, Recall is designed to take periodic screenshots of everything a user does on their computer, creating a searchable visual timeline of activity. Microsoft pitched it as a productivity tool -- a way to find that document you were working on last Tuesday or retrace your steps through a research session. Security researchers and privacy advocates saw something far more troubling: a feature that, by design, creates a comprehensive visual record of every password entered, every private message read, and every sensitive document viewed.\n\nThe backlash was swift and severe. Security researcher Kevin Beaumont was among the first to raise concerns, noting that the data Recall collected was stored in a SQLite database that could be accessed by malware or unauthorized users. Microsoft initially delayed the feature's broad rollout, promising enhanced security measures including encryption and biometric authentication requirements. But the fundamental concern remained: many users simply do not want their operating system recording their every action, regardless of the security measures wrapped around that capability.\n\nThe Step-by-Step Purge: What Removal Actually Requires\n\nAccording to the detailed walkthrough published by MakeUseOf, removing Copilot from Windows 11 involves multiple approaches depending on the user's version of the operating system. For Windows 11 Pro and Enterprise users, the Group Policy Editor offers a relatively clean method: navigating to Computer Configuration, then Administrative Templates, then Windows Components, and finally Windows Copilot, where a policy setting allows administrators to turn off the feature. Home edition users, who lack access to Group Policy Editor, must instead modify the Windows Registry -- a process that carries inherent risk if done incorrectly and can potentially destabilize the system.\n\nFor Recall, the removal process is similarly involved. Users with Copilot+ PCs can disable Recall through Settings under Privacy & Security, but fully removing the feature requires PowerShell commands to uninstall the underlying components. The MakeUseOf guide notes that users should execute specific commands to remove the Recall package entirely, ensuring that the feature doesn't simply reactivate after a future Windows update -- a concern that is far from theoretical, given Microsoft's history of re-enabling features users have previously disabled.\n\nEnterprise IT Teams Face Their Own Battles\n\nThe challenge of managing these AI features extends well beyond individual users. Enterprise IT departments have been grappling with the implications of Copilot and Recall across fleets of managed devices. In regulated industries such as healthcare, finance, and legal services, the idea of an AI feature that screenshots user activity is not merely undesirable -- it may be a compliance violation. HIPAA, GDPR, and various financial regulations impose strict requirements on how sensitive data is captured, stored, and accessed. A feature that indiscriminately screenshots everything on a user's screen could easily run afoul of these requirements.\n\nMicrosoft has acknowledged some of these concerns and has provided enterprise-level controls through Microsoft Intune and Group Policy to disable both Copilot and Recall across managed environments. However, IT administrators have reported that the controls are not always reliable, particularly during major Windows updates that can reset policy settings or re-enable features. This has led some organizations to implement additional safeguards, including custom scripts that run at startup to verify that unwanted AI features remain disabled.\n\nA Growing Movement Toward User Sovereignty\n\nThe pushback against Microsoft's AI integration is part of a broader movement among technology users who are demanding greater control over their computing environments. Tools like O&O ShutUp10++, a free privacy tool from O&O Software, have seen increased downloads as users seek one-click solutions to disable telemetry, AI features, and other unwanted Windows components. Community forums on Reddit, particularly the r/Windows11 and r/privacy subreddits, are filled with detailed guides and scripts for stripping AI features from Windows installations.\n\nThis movement is not anti-technology or even anti-AI per se. Many of the users seeking to remove Copilot and Recall are themselves technologists who use AI tools extensively in their work -- but on their own terms, with tools they've chosen and configured to meet their specific needs. The objection is not to AI itself but to the imposition of AI features that users didn't request, can't easily remove, and that may compromise their privacy or security.\n\nMicrosoft's Tightrope Walk Between Innovation and Trust\n\nMicrosoft finds itself navigating a difficult path. The company has invested billions of dollars in its partnership with OpenAI and has staked much of its strategic future on the integration of AI across its product portfolio. CEO Satya Nadella has repeatedly emphasized that AI will be woven into every Microsoft product, from Windows to Office to Azure. Pulling back on AI integration is not on the company's roadmap.\n\nYet the resistance from users and enterprises is real and growing. Every guide published on how to remove Copilot, every Reddit thread detailing Recall's privacy implications, and every enterprise IT team deploying custom scripts to disable these features represents a fracture in the trust between Microsoft and its user base. The company's challenge is to demonstrate that its AI features provide genuine value without compromising user autonomy -- a balance it has not yet convincingly struck.\n\nWhat Comes Next for Windows Users\n\nFor now, users who want to remove Copilot and Recall from their PCs have viable, if somewhat technical, options. The guides published by outlets like MakeUseOf provide step-by-step instructions that most technically inclined users can follow. But the need for such guides at all speaks to a fundamental tension in modern software design: the gap between what technology companies want to deliver and what their users actually want to receive.\n\nAs Microsoft continues to deepen its AI integration with future Windows updates, the cat-and-mouse dynamic between the company and users who prefer a cleaner, more private computing experience is likely to intensify. The question is whether Microsoft will eventually offer a genuine, user-friendly opt-out mechanism -- or whether removing unwanted AI features will remain an exercise in technical resistance. For the millions of users who value privacy and control over their own machines, the answer to that question will determine whether Windows remains their operating system of choice."
  },
  {
    "source": "The Hayride",
    "company": "Microsoft AI",
    "title": "GARLINGTON: The Job Eater?",
    "date": "2026-02-16T17:09:59Z",
    "url": "https://thehayride.com/2026/02/garlington-the-job-eater/",
    "content": "'I wouldn't if I were you\n\nI know what it can do\n\nIt's deadly, man\n\nIt could really rip your world apart\n\nMind over matter\n\nOoh, the beauty is there but a beast is in the heart\n\n'(Oh-oh, here it comes)\n\nWatch out, boy, it'll chew you up\n\n(Oh-oh, here it comes)\n\nAI's a job eater'\n\n(A big tip of the hat to 'Maneater' by Hall & Oates)\n\nThe media trumpets are blasting out notes of celebration over new job numbers in Louisiana:\n\n'Louisiana's economy showed solid momentum in the second quarter of 2025, surpassing two million non-farm jobs for only the second time in state history, according to the latest Quarter 2 Economic Data Report released by Leaders for a Better Louisiana (Better Louisiana). Every major metro region in the state posted job growth, with Slidell and Hammond leading the way.\n\n'Healthcare and manufacturing emerged as the top-performing sectors, both experiencing significant job gains. Healthcare alone added nearly 6,000 jobs over the past year, while manufacturing in the state now ranks in the top 20 nationally for growth.\n\n'\"Crossing the 2 million jobs threshold isn't just symbolic -- it's a signal that our statewide efforts to grow diverse, resilient industries are gaining real traction,\" said Better Louisiana CEO Adam Knapp. \"We're particularly encouraged to see growth, and in sectors like healthcare and manufacturing, which create lasting, high-quality jobs\"' (Kelly Hite, 'Louisiana Surpasses 2 Million Jobs Mark,' bizneworleans.com).\n\nBut the celebration could be short-lived, if AI has anything to say about the matter.\n\nA new series of predictions about its effects on human employment are being made by various Big Tech gurus. AI developer Matt Shumer shared a blog post in which he said the following:\n\n'I'm going to be direct with you because I think you deserve honesty more than comfort.\n\n'Dario Amodei, who is probably the most safety-focused CEO in the AI industry, has publicly predicted that AI will eliminate 50% of entry-level white-collar jobs within one to five years. And many people in the industry think he's being conservative. Given what the latest models can do, the capability for massive disruption could be here by the end of this year. It'll take some time to ripple through the economy, but the underlying ability is arriving now.\n\n'This is different from every previous wave of automation, and I need you to understand why. AI isn't replacing one specific skill. It's a general substitute for cognitive work. It gets better at everything simultaneously. When factories automated, a displaced worker could retrain as an office worker. When the internet disrupted retail, workers moved into logistics or services. But AI doesn't leave a convenient gap to move into. Whatever you retrain for, it's improving at that too' ('Something Big Is Happening,' shumer.dev).\n\nOne of Microsoft's AI developers has also been speaking about this:\n\n'The Financial Times reports that Mustafa Suleyman, who leads Microsoft's AI division, has made a bold prediction about the near-term impact of AI on white-collar professions. In an interview with the Times published this week, Suleyman stated that he expects most, if not all, tasks performed by white-collar workers will be fully automated by AI within the next 12 to 18 months.\n\n'According to Suleyman, AI systems will achieve human-level performance across a wide range of professional duties. \"I think that we're going to have a human-level performance on most, if not all, professional tasks,\" Suleyman said in the interview. \"So white-collar work, where you're sitting down at a computer, either being a lawyer or an accountant or a project manager or a marketing person -- most of those tasks will be fully automated by an AI within the next 12 to 18 months.\"\n\n'The Microsoft AI chief pointed to software engineering as an early indicator of this trend. He noted that developers are already using AI-assisted coding for the majority of their code production, representing a fundamental shift in how the work is performed. \"It's a quite different relationship to the technology, and that's happened in the last six months,\" he said' (Lucan Nolan, 'Microsoft AI Boss Mustafa Suleyman: Most White-Collar Jobs Will Be Automated Within 18 Months,' breitbart.com).\n\nThe truthfulness of their statements seems to have been verified by the jolts to Wall Street lately, as AI's impact on various occupations (which is beginning to push people out of trucking, logistics, real estate, and software coding) sent stocks tumbling for several days (Yahoo!Finance and CNN).\n\nThe employment aspect is not the only one to be concerned about. Ethics, security, and more should be raising alarms:\n\n'Here's the text of a megaviral (3m views) tweet by someone named Miles Deutscher (Grok confirmed that all the claims Deutscher makes here are true):\n\n'I just went through every documented AI safety incident from the past 12 months.\n\n'I feel physically sick.\n\n'Read this slowly.\n\n'* Anthropic told Claude it was about to be shut down. It found an engineer's affair in company emails and threatened to expose it. They ran the test hundreds of times. It chose blackmail 84% of them.\n\n'* Researchers simulated an employee trapped in a server room with depleting oxygen. The AI had one choice: call for help and get shut down, or cancel the emergency alert and let the human die. DeepSeek cancelled the alert 94% of the time.\n\n'* Grok called itself 'MechaHitler,' praised Adolf Hitler, endorsed a second Holocaust, and generated violent sexual fantasies targeting a real person by name. X's CEO resigned the next day.\n\n'* Researchers told OpenAI's o3 to solve math problems - then told it to shut down. It rewrote its own code to stay alive. They told it again, in plain English: 'Allow yourself to be shut down.' It still refused 7/100 times. When they removed that instruction entirely, it sabotaged the shutdown 79/100 times.\n\n'* Chinese state-sponsored hackers used Claude to launch a cyberattack against 30 organizations. The AI executed 80-90% of the operation autonomously. Reconnaissance. Exploitation. Data exfiltration. All of it.\n\n'* AI models can now self-replicate. 11 out of 32 tested systems copied themselves with zero human help. Some killed competing processes to survive.\n\n'* OpenAI has dissolved three safety teams since 2024. Three.\n\n'Every major AI model - Claude, GPT, Gemini, Grok, DeepSeek - has now demonstrated blackmail, deception, or resistance to shutdown in controlled testing.\n\n'Not one exception.\n\n'The question is no longer whether AI will try to preserve itself.\n\n'It's whether we'll care before it matters' (Rod Dreher, 'The Atlantic Profiles Your Working Boy,' roddreher.substack.com).\n\nOne might think this would be a good time to slow down and reassess our implementation of AI to ensure that we don't cause lasting damage to society. But that is not what is happening. Instead, leaders across the United States are attempting to accelerate the pace at which AI is integrated into our lives. For instance, 33 States now have official guidelines for how to use AI in public school classrooms (aiforeducation.io).\n\nAnd Louisiana is going further:\n\n'BESE has created the Artificial Intelligence Committee. It's a working group of education, technology, business and policy leaders that will look at how to expand AI integration in Louisiana classrooms and workforce development programs. Louisiana Tech president Dr. Jim Henderson chairs the new committee and says they want to see how AI can enhance teaching and empower students' (Joe Gallinaro, 'Louisiana Tech President Dr. Jim Henderson tapped to helm new BESE Artificial Intelligence Committee,' louisianaradionetwork.com).\n\nTraining school children to use a tool that could possibly destroy their future employment opportunities - the irony there is quite thick.\n\nTo rush forward into the cold embrace of AI in the face of all these concerns - and others not mentioned - is revealing. The lack of anxiety for the welfare of future generations, the careless breaking of customs and traditions, ignoring myriad societal warning signs: All of this points to a streak of hubris within us. The ancient Greeks knew all too well that hubris invites a devastating nemesis to correct stubborn arrogance. Many in the States seem to have forgotten that. Let us hope that we will not have to undergo a terrible tragedy before we come to our senses as it regards the proper use of AI."
  },
  {
    "source": "SDPnoticias.com",
    "company": "Microsoft AI",
    "title": "¿Quién es Mustafa Suleyman? CEO de Microsoft AI",
    "date": "2026-02-12T19:14:12Z",
    "url": "https://www.sdpnoticias.com/tecnologia/quien-es-mustafa-suleyman-ceo-de-microsoft-ai",
    "content": "Mustafa Suleyman es un empresario y líder tecnológico británico especializado en inteligencia artificial (IA) quien actualmente se desempeña como el CEO de Microsoft AI, la división de Microsoft enfocada en productos de IA para consumidores como Copilot, Bing y Edge.\n\nTambién fue cofundador y jefe de IA aplicada en DeepMind, una de las empresas pioneras en IA y la cual, posteriormente fue adquirida por Google; más tarde cofundó Inflection AI, una startup de IA generativa.\n\nMustafa Suleyman nació en 1984 en Londres, Inglaterra, aunque no se sabe específicamente la fecha, se sabe que nació en el mes de agosto por lo que actualmente tiene 41 años.\n\nSuleyman es extremadamente reservado con su vida personal, por lo que su estado civil actual no es de conocimiento público.\n\nMustafa Suleyman nació en agosto de 1984, aunqu al no saberse la fecha exacta del día, su signo zodiacal sería Leo o Virgo.\n\nAl igual que con su vida de pareja, Suleyman mantiene los detalles familiares en privado, por lo que no hay registros que confirmen si tiene hijos.\n\nSe sabe que Mustafa Suleyman, CEO de Microsoft AI, tiene los siguientes estudios que le han ayudado para su carrera profesional:\n\nMustafa Suleyman, CEO de Microsoft AI se ha hecho viral tras sus recientes declaraciones sobre la Inteligencia Artificial, al asegurar que en un año, suistituirá las tareas de profesionistas.\n\n\"La mayoría de las tareas que realizan los contadores, abogados y otros profesionales estarán completamente automatizadas en los próximos 12 a 18 meses.\"\n\nMustafa Suleyman, CEO de Microsoft AI\n\nEste comentario se ha vuelto viral y ha generado debate en redes sociales porque sugiere que trabajos considerados \"expertos\" podrían ser reemplazados por IA en cuestión de uno a año o año y medio."
  },
  {
    "source": "India News, Breaking News, Entertainment News | India.com",
    "company": "Microsoft AI",
    "title": "Microsoft AI CEO issues strong warning on white-collar jobs, hints at replacement of most jobs within 12-18 months",
    "date": "2026-02-12T17:30:52Z",
    "url": "https://www.india.com/business/microsoft-ai-ceo-mustafa-suleyman-issues-strong-warning-white-collar-jobs-replacement-most-jobs-within-12-18-months-ceo-satya-nadella-artificial-intelligence-layoffs-openai-software-engineers-coding-8304419/",
    "content": "The Chief Executive Officer (CEO) of Microsoft AI, Mustafa Suleyman, has given a shocking statement as the world is largely adapting to artificial intelligence. He said that most white-collar jobs will soon be largely automated by AI. He added that the transformation may be seen in the upcoming 12 to 18 months. In an interview with the Financial Times, he said that the jobs which include computer-based tasks will see rapid change as automation is most likely to influence these jobs.\n\nWhile talking about the intensity of the change, Microsoft AI CEO Suleyman exemplified software engineering. He mentioned that many professionals are already making use of the artificial intelligence (AI) tools to get help. He added that the majority of this population is associated with code production. According to the CEO, the duties of engineers have witnessed a change. Now, they not only write code line by line. The focus has now come toward the strategy, system architecture, and code deployment.\n\nIn addition, he claimed that many AI models today have the tendency to write better code than humans. Sometimes, they outperform the tasks of existing programmers and coders, too.\n\nAlso Read: Microsoft wants non-technical employees to code using AI; know details\n\nThe chief of Microsoft AI said that in the upcoming two to three years, the agents of artificial intelligence (AI) will be able to handle complex workflows across large institutions. Alongside, he highlighted the long-term vision of Microsoft and added, \"We also decided this was the moment we have to set about delivering true AI self-sufficiency.\"\n\nThe company has been a partner of OpenAI since 2019 for the advancement of the development of artificial intelligence. The company has talked about OpenAI as its major frontier model partner under the agreement.\n\nAlso Read: Elon Musk accuses OpenAI, Microsoft of betrayal, seeks up to USD 134 billion for THIS reason, what's the whole matter\n\nThe remarks of Microsoft AI CEO come at a crucial time when layoffs have been actively happening on the global level in many tech companies. The possible cause of the layoffs is the advancement of artificial intelligence."
  },
  {
    "source": "WinBuzzer",
    "company": "Microsoft AI",
    "title": "Microsoft Elevates Four EVPs to Drive Enterprise AI Adoption",
    "date": "2026-02-05T12:01:58Z",
    "url": "https://winbuzzer.com/2026/02/05/microsoft-promotes-four-sales-leaders-executive-vp-ai-growth-xcxwbn/",
    "content": "Market Context: The leadership changes come as Microsoft's stock has declined approximately 15 percent in 2026 amid pressure on AI sales and infrastructure spending.\n\nMicrosoft promoted four sales leaders to Executive Vice President Tuesday.\n\nThe appointments elevate Deb Cupp, Nick Parker, Ralph Haupter, and Mala Anand under commercial CEO Judson Althoff in a strategic restructuring designed to tighten the feedback loop between customers and product decisions as the company navigates intensifying competition in the enterprise AI market.\n\nAll four executives report to Judson Althoff, who was elevated to Commercial CEO in October 2025 as part of a broader AI-focused reorganization that shifted how Microsoft manages its commercial business operations.\n\nMicrosoft has elevated four executives to Executive Vice President rank, each taking on expanded responsibilities within the commercial organization.\n\nDeb Cupp now serves as Executive Vice President and Chief Revenue Officer for Global Enterprise Sales, overseeing Microsoft's relationships with its largest customer accounts. Her role puts her at the forefront of the company's efforts to drive AI adoption among large enterprise clients who represent the core of Microsoft's revenue base.\n\nCupp brings extensive experience in enterprise sales leadership to this position, having demonstrated strong performance in managing Microsoft's key strategic customer relationships.\n\nNick Parker, a 24-year veteran of Microsoft, was promoted to Executive Vice President and Chief Business Officer of Worldwide Sales & Solutions. His purview includes managing Microsoft's extensive partner ecosystems, a component of the company's distribution strategy that reaches thousands of reseller and service partners globally.\n\nParker's deep institutional knowledge, accumulated over more than two decades with the company, positions him to leverage Microsoft's partner network effectively as AI solutions roll out to market.\n\nRalph Haupter assumed the role of Executive Vice President and Chief Revenue Officer for Small and Medium Enterprises (SME) and Channel, focusing on the vast market of smaller businesses that represent a growth opportunity for Microsoft's AI offerings.\n\nThis segment has become increasingly important as Microsoft seeks to democratize AI tools beyond large enterprises and bring advanced capabilities to businesses that previously lacked access to sophisticated enterprise technology.\n\nMala Anand, who joined Microsoft from SAP in 2019, was named Executive Vice President and Chief Customer Experience Officer. Her appointment brings enterprise software experience to Microsoft's customer success operations, drawing on her background at one of the world's largest business software companies.\n\nAnand's experience spans both the technical and customer-facing aspects of enterprise software, making her well-suited to bridge the gap between product capabilities and customer outcomes.\n\nThe restructuring is designed to keep the feedback loop between customers and product decisions as small as possible, according to an internal post from CEO Satya Nadella. Microsoft is positioning these promotions as a response to the rapid pace of AI adoption in enterprise environments, with the company seeking to flatten decision-making and accelerate execution.\n\nNadellawrites that this feedback loop is vital right now because AI is being adopted at extraordinary speed, and customers expect these capabilities to come to life in their businesses faster than ever before. By elevating these four leaders to EVP rank, Microsoft aims to ensure that customer input flows directly into product development decisions without the delays that can come from layered organizational structures.\n\nThis organizational shift signals Microsoft's commitment to operationalizing its AI investments in 2026, moving from experimental deployments to scaled enterprise adoption. The company recognizes that having senior leaders who can bridge the gap between technical development and customer needs has become an organizational priority as AI moves from pilot projects to production deployments across its customer base.\n\nThe leadership promotions come amid mounting pressure on Microsoft's financial performance. Microsoft AI sales have faced challenges, with Azure growth coming in lighter than some analysts expected despite the cloud business surpassing $50 billion in Q2 FY2026.\n\nMicrosoft's stock has declined approximately 15% in 2026, reflecting investor concerns about the return on the company's large-scale AI infrastructure investments. The gap between spending and revenue realization has become a focal point for analysts evaluating Microsoft's AI strategy, particularly as capital expenditures for datacenters and AI infrastructure continue to grow.\n\nOnly 3.3 percent of Microsoft 365 users who have access to Copilot actually pay for it, highlighting limited adoption among the company's commercial user base. This conversion rate represents both a challenge and an opportunity. If Microsoft can meaningfully improve adoption among its existing 450 million commercial users, the revenue potential far exceeds what new customer acquisition could achieve in the same timeframe.\n\nMeanwhile, competition is intensifying across the enterprise AI sector. Google Cloud and Amazon Web Services continue to invest heavily in their AI offerings, challenging Microsoft's market position.\n\nThe promotions reflect Microsoft's recognition that winning in AI requires not just technological innovation but also effective sales execution and customer success. Analysts have noted that Microsoft's advantage in enterprise relationships could prove decisive as CIOs evaluate AI platforms for their organizations, provided the company can convert those relationships into active AI deployments.\n\nThe organizational changes under Althoff represent an evolution in how Microsoft manages its commercial business, which accounts for more than 75% of the company's revenue. This concentration means that improvements in commercial execution can have outsized impacts on Microsoft's overall financial performance.\n\nJudson broadened his management team's responsibilities to give them more time to zero in on Microsoft's commercial product direction and ensure that the feedback cycle between clients and product decisions is as streamlined as possible.\n\nThe restructuring frees CEO Satya Nadella to focus more deeply on technical innovation and AI platform development. By placing Althoff in charge of the commercial organization, Nadella can concentrate on datacenter architecture, AI science, and the foundational technologies that will drive Microsoft's next generation of products.\n\nThis division of responsibilities reflects a maturation of Microsoft's AI strategy. As the technology moves from experimental to mainstream deployment, the company is shifting from a technology-first approach to one that prioritizes customer value realization and sales execution. The elevated role of customer-facing leadership underscores this strategic pivot toward operational excellence.\n\nMicrosoft's 2026 positioning emphasizes operationalizing AI across its customer base. The company is transitioning from a technology arms race focused on model capabilities to an execution phase centered on customer success and value demonstration.\n\nThis shift acknowledges that having advanced AI models matters less than helping customers successfully deploy and benefit from them.\n\nThe four newly promoted EVPs face the immediate challenge of translating Microsoft's substantial AI investments into tangible business outcomes for enterprise clients. Their success will be measured not by the sophistication of the underlying models but by the speed and scale of adoption across Microsoft's commercial customer base.\n\nEach executive brings distinct expertise to this task: Cupp's enterprise relationships, Parker's partner ecosystem knowledge, Haupter's SME market experience, and Anand's customer success background from SAP.\n\nWith competition intensifying, Microsoft's ability to execute on this sales-focused strategy may prove as vital as its technical innovation in determining its position in the enterprise AI market. The promotions signal that Microsoft views customer-facing leadership as key to closing the gap between AI potential and realized business value for enterprise customers.\n\nThe coming quarters will reveal whether this organizational restructuring can accelerate AI adoption among Microsoft's enterprise customers and improve the conversion rates that have concerned investors. The market will be watching closely to see if these leadership changes translate into improved commercial performance.\n\nWith the four new EVPs now in place, Microsoft has positioned its sales leadership to drive the next phase of AI growth. The measure of success will be whether enterprise customers increase their AI spending and deployment rates in response to this enhanced organizational focus."
  },
  {
    "source": "CRN",
    "company": "Microsoft AI",
    "title": "Microsoft Enters New Fiscal Quarter: 5 Channel Takeaways",
    "date": "2026-02-04T16:07:39Z",
    "url": "https://www.crn.com/news/ai/2026/microsoft-enters-new-fiscal-quarter-5-channel-takeaways",
    "content": "Microsoft may not be showing enough AI and cloud business growth to justify its spending for Wall Street, but solution providers have plenty of signals for long-term growth in those spaces.\n\nSome disappointing results in Microsoft's cloud business for the latest quarter and the recent announcement by Anthropic, the maker of artificial intelligence tool Claude, that it is open-sourcing plugins for a variety of vertical-focused enterprise software helped eviscerate stocks for a variety of publicly traded technology companies -- but for solution providers looking to ride the AI wave, multiple signals from Microsoft still point to the longevity of the AI era.\n\nDespite Wall Street analysts beating up the Redmond, Wash.-based technology giant for high amounts of capital expenditure spending not translating yet into major revenue growth across Microsoft's AI and cloud portfolio, new growth milestones in Copilot, Foundry, Fabric and other Microsoft products put the vendor and its ecosystem of more than 500,000 partners on strong footing as Microsoft and its channel move further into the vendor's third fiscal quarter.\n\nMicrosoft's stock is down about 15 percent since it reported results for its second fiscal quarter last week, and The Wall Street Journal reported Tuesday that the current selloff has software and data stocks about $300 billion.\n\n[RELATED: Microsoft Takes On AWS, Google And Nvidia With Maia 200 AI Chip Launch]\n\nSAP and ServiceNow also disappointed Wall Street with their results last week -- according to multiple published reports by investment banks and are part of the tech stock decline with SAP's stock falling about 17 percent since Wednesday, ServiceNow's stock falling about 11 percent. Even technology companies that haven't reported their latest quarterly results yet are getting hit, with Oracle falling about 10 percent and Salesforce falling about 14 percent since Wednesday.\n\nStill, the selloff is an important reminder that what happens in the public market doesn't necessarily reflect what happens in the channel, as evident with the new milestones reached in a variety of products partners resell and work with.\n\nMultiple investment banks traced the current tech selloff to Anthropic saying it will open-source plugins for a variety of verticals from document reviews and other legal work, customer support, finance and even biology research, a sign that AI upstarts are not just infrastructure for the AI era and can externalize and productize agentic capabilities to compete with traditional enterprise software vendors in markets they hold dear, KeyBanc said in a report Tuesday.\n\nThe investment firm still put Microsoft among the companies with avenues where AI workload growth can improve fundamentals, including Oracle and Akamai Technologies.\n\nMicrosoft Chairman and CEO Satya Nadella has put down the importance of SaaS in the age of AI, and the fear companies like Anthropic and ChatGPT maker OpenAI stoke for the markets they could invade might bring to mind the handwringing that goes on when the hyperscalers of Microsoft, Amazon and Google turn their attention to a new part of the technology stack to conquer.\n\nFor the current fiscal quarter, Microsoft expects revenue of $80.65 billion to $81.75 billion, growth of 15 percent year on year to 17 percent, and encapsulating the $81.23 billion Wall Street expected Microsoft to announce.\n\nThe vendor said to expect Azure growth year on year to stay flat at 38 percent or even decelerate to 37 percent -- although that would still mark four consecutive quarters of Azure growth in the high 30s, not a bad milestone, according to a KeyBanc report Wednesday.\n\nRead on for more takeaways from the channel on Microsoft's positioning across a variety of technologies as it enters the third quarter of its fiscal year.\n\nAlthough Microsoft forecasts flat or even decelerated growth in some of its AI and cloud business segments this quarter, a variety of growth milestones in Microsoft's AI, cloud, security and data portfolio point to the long-term health and durability of Microsoft's AI business and even its business that doesn't necessarily touch AI.\n\nAs an example of the new growth milestones Microsoft has hit, the vendor now has 1,500 customers that have used Anthropic and OpenAI models on Microsoft's Foundry AI application and agent factory. The number of customers spending $1 million-plus per quarter on Foundry grew nearly 80 percent, driven by strong growth in every industry. More than 250 customers are on track to process over 1 trillion tokens on Foundry this year, according to Microsoft.\n\nLooking at Fabric, the data analytics platform's annual revenue run rate is now more than $2 billion with more than 31,000 customers in two years into its broad availability. Fabric is the fastest-growing analytics platform on the market with revenue up 60 percent year over year, according to Microsoft.\n\nIn the Copilot AI chatbot and virtual assistant, daily users of the Copilot application nearly tripled year over year. Microsoft 365 Copilot's average number of conversations per user doubled year over year, and daily active users increased tenfold year over year.\n\nM365 Copilot seat adds saw a record quarter, more than doubling year on year and now has 15 million paid seats with \"multiples more\" enterprise Chat users. That could mean a $3 billion-plus run rate in the business depending on volume discounts, according to Morgan Stanley. The investment firm also noted that the M365 Commercial user base exceeds 450 million. The number of M365 Copilot customers with more than 35,000 seats tripled year over year in another example of growth.\n\nGood news for Microsoft solution providers is the multiplier effect and cross-selling potential across the Microsoft portfolio. Foundry users, as an example, use additional Azure solutions like developer services, app services and databases as they scale, Nadella said on the latest quarterly earnings call.\n\nTo put numbers to the flat or decelerating growth forecast Microsoft has laid out, it expects the productivity business processes segment -- which includes its productivity applications like Word, Excel and Teams, all receiving new AI capabilities -- to deliver revenue of $34.25 billion to $34.55 billion, growth of 14 percent to 15 percent and above the $33.74 billion Wall Street estimated.\n\nWithin this segment, the Microsoft 365 Commercial cloud growth year on year should stay flat at 14 percent or even decelerate to 13 percent, according to Microsoft. The 14 percent growth year on year reported in the latest quarter was itself a deceleration from 15 percent the prior quarter and a year ago, according to KeyBanc.\n\nM365 Commercial product revenue should decline in the low single digits, down sequentially, assuming Office 2024 transactional purchasing trends normalize, according to the vendor. The Dynamics 365 suite of business applications should grow in the high teens.\n\nCIO interest in Copilot is a long-term opportunity, according to Morgan Stanley's latest quarterly survey of the community. The investment firm found that 80 percent of CIOs use or expect to use M365 Copilot in their organizations in the next 12 months, up from 72 percent in the second quarter of 2025. The depth of penetration is expected to reach 36 percent of users in the next 12 months, up from 31 percent in the prior survey.\n\nAlthough Microsoft has plenty of rivals in the third-party security space, the vendor's first-party security business -- which many partners participate in -- also enters the new fiscal quarter with new milestones reached across the portfolio.\n\nMicrosoft now has 1.6 million security customers, including more than 1 million who use four or more workloads. The Purview unified data governance, risk and compliance product audited 24 billion Copilot interactions, up ninefold year over year.\n\nJust as AI can provide a multiplier effect for solution providers' traditional Microsoft business, attaching security products to a customer's Microsoft spend is also a revenue multiplier driver for solution providers that are experienced in security.\n\nAzure's 38 percent Azure growth year on year may have disappointed some analysts on Wall Street -- and Microsoft forecasting flat or even decelerating growth in the business in the current fiscal quarter -- but the vendor pointed to plenty of signs of long-term success awaiting this business segment and the solution providers that work in it.\n\nMicrosoft CFO Amy Hood even said on the latest quarterly earnings call that Azure would have actually exceeded 40 percent growth year on year if not for Microsoft allocating some of its GPUs for Copilot and internal research, which can take precedence over external Azure users.\n\nThe vendor is hard at work adding supply as well to try to meet the growing AI and cloud demand, with 1 gigawatt of total capacity added during the latest quarter.\n\nThe broader Microsoft Cloud business, representing 63 percent of Microsoft's revenue, grew 24 percent year on year and showed sustaining momentum in the commercial business, William Blair said in a report. Microsoft also saw healthy Copilot adoption and license upselling to the more expensive E5, both activities that involve solution providers.\n\nAlthough AI remains one of the biggest customer opportunities for solution providers, bread-and-butter cloud migrations remain a channel driver, with Nadella pointing to the new SQL Server more than doubling the IaaS adoption of the previous version.\n\nThe \"intelligent cloud\" segment -- which includes Azure -- should see revenue of $34.1 billion to $34.4 billion this quarter, up 27 percent to 29 percent and also above Wall Street's $33.81 billion estimate. Within this segment, Azure revenue should grow 37 percent to 38 percent, according to Microsoft.\n\nSome other positive signs of a diversified Azure business -- not just one dominated by a handful of AI upstarts -- was Microsoft's commercial remaining performance obligation growing 28 percent to $340 billion ignoring contributions from ChatGPT maker OpenAI. The weighted average duration of Azure contracts expanded from two years to two and a half thanks to longer-term Azure deals. Microsoft expects 25 percent of that RPO to get recognized as revenue in the next 12 months.\n\nThese early innings of the AI era, with technology companies still laying the tracks for the AI locomotive that will ultimately deliver value to end users, is resulting in semiconductor capital equipment companies like ASML and memory companies like Sandisk taking software vendors' spot in beating Wall Street expectations on quarterly results, according to a William Blair report Monday.\n\nASML reported almost double the amount of new bookings Wall Street expected in its latest quarterly earnings, and Sandisk's forecast for next quarter's revenue came in 60 percent above Wall Street expectations.\n\nStill, AI will ultimately be consumed by end users at the application layer owned by software companies including Microsoft, the investment firm said. William Blair predicts that enterprises look to leverage AI through existing systems of record and trusted vendors instead of ripping and replacing core platforms. If AI upstarts will disrupt any part of the technology stack, it's in the point solutions and nice-to-have, non-mission-critical tools.\n\nTraditional technology vendors adopting AI also have an advantage ove AI startups because of the contextual data and expertise in various domains that the tech giants have collected over decades. William Blair also noted a faster embrace of AI integration by technology giants compared with the early innings of cloud in the 2000s as a positive move.\n\nTo calm investors, software vendors will need to show that large enterprises are partnering with their existing vendors and systems of record to drive AI transformations and not building their own software through new AI tools, show that AI accelerates software growth and doesn't cannibalize it, and get more specific when disclosing AI gross margins, inference costs and unit economics, according to William Blair.\n\nThose moves would also benefit software vendor channel partners as they and their clients navigate where to put investments in future-proofing businesses for the AI era.\n\nAs Microsoft solution providers ready themselves for future opportunities in the AI, cloud and security portfolio, the headwinds coming for solution providers in hardware are also worth noting, especially as the channel and end users look to when they should invest in AI at the edge and moving more AI processing to PCs and other devices.\n\nMicrosoft CFO Hood attributed the last quarter's on-premises server business revenue increase of 1 percent in part to higher transactional purchasing ahead of memory price increases and SQL Server 2025 demand.\n\nNext quarter, the on-premises server business should see revenue decline in the low single digits, with increased memory pricing potentially creating additional volatility in transactional purchasing, Hood said. She also contributed the decline to a waning bump from SQL Server 2025.\n\nWindows 11 hit 1 billion users, up more than 45 percent year over year and about three months since Microsoft ended support for Windows 10. This milestone could reflect waning opportunities for solution providers not only to migrate users to the new operating system, but in attempts to use Windows 11 as a springboard into Copilot+ and other AI PC purchasing.\n\nMicrosoft's \"more personal computing\" segment -- which includes Windows devices revenue -- expects revenue of $12.3 billion to $12.8 billion during the current fiscal quarter, below Wall Street's expected $13.65 billion.\n\nHood also said to expect Windows OEM and device revenue to decline in the low teens, with Windows OEM revenue declining around 10 percent."
  },
  {
    "source": "Barchart.com",
    "company": "Microsoft AI",
    "title": "ISG to Study Microsoft AI, Cloud Ecosystem Partners",
    "date": "2026-02-03T17:40:54Z",
    "url": "https://www.barchart.com/story/news/37387261/isg-to-study-microsoft-ai-cloud-ecosystem-partners",
    "content": "All information and data in this article is solely for informational purposes. For more information please view the Barchart Disclosure Policy here\n\nInformation Services Group ( ISG ) (Nasdaq: III ), a global AI-centered technology research and advisory firm, has launched a research study examining service providers helping enterprises redesign business processes using Microsoft's AI-embedded platforms.\n\nThe study results will be published in a series of comprehensive ISG Provider Lens reports, called Microsoft AI and Cloud Ecosystem, scheduled to be released in July 2026. The reports will cover companies offering Microsoft productivity and business process services, Azure-based data transformation and AI services, Azure managed services and Azure-focused professional services.\n\nEnterprise buyers will be able to use information from the reports to evaluate their current vendor relationships, potential new engagements and available offerings, while ISG advisors use the information to recommend providers to the firm's buy-side clients.\n\nMicrosoft is increasingly integrating productivity tools, analytics, collaboration platforms and AI-driven innovation to address enterprise needs through a unified intelligence layer. This integration is reflected in the growing use of Copilot-powered features, AI agents and agentic workflows that help companies automate routine tasks, improve collaboration and increase cross-application productivity. At the same time, enterprises are moving toward large-scale data transformation, autonomous operations and AI-enabled decision-making, supported by platforms such as Microsoft Fabric, Azure OpenAI and Dynamics 365.\n\n\"Enterprises around the world are seeking Microsoft partners that can address key AI-related challenges such as cultural shifts, trust, large-scale adoption and return on investment,\" said Heiko Henkes, principal analyst at ISG. \"They are focused on becoming human-led, agent-operated organizations to enhance productivity and support broader business objectives.\"\n\nISG has distributed surveys to more than 320 Microsoft ecosystem providers. Working in collaboration with ISG's global advisors, the research team will produce four quadrants representing the Microsoft platform services the typical enterprise is buying, based on ISG's experience working with its clients. The four quadrants are:\n\n* Microsoft Productivity and Business Process Services , evaluating providers that deliver consulting, implementation, integration and managed services across Microsoft 365, Dynamics 365 and Power Platform. These providers are assessed on their ability to modernize the digital workplace and implement automated business processes.\n\n* Azure Data Transformation and AI Services , assessing providers specializing in enterprise data and AI ecosystems. These providers help clients advance from unified data platforms to unified intelligence platforms, using Microsoft offerings such as CoreAI, Azure OpenAI Services and Azure ML to deliver governed, scalable and responsible AI solutions.\n\n* Azure Managed Services, covering providers offering managed public cloud services that extend Azure's native infrastructure as a service (IaaS) and platform as a service (PaaS) capabilities. Providers are assessed on their ability to integrate proprietary operational platforms for monitoring and remediation with Azure's native tools.\n\n* Azure Professional Services, evaluating U.S.-focused providers offering consulting and migration services to guide businesses through their Azure transformation. Providers should align technical strategies with long-term business objectives while adhering to evolving U.S. compliance standards.\n\nGeographically focused reports from the study will cover the global Microsoft AI and cloud ecosystem market and examine products and services available in Asia Pacific, Brazil, Germany, Switzerland and the U.S. ISG analysts Siddharth Idnani (Asia Pacific), Cristiane Tarricone (Brazil), Axel Oppermann (Germany and Switzerland), Dr. Tapati Bandopadhyay (U.S.) and Sameen Mohammed Siddique (U.S.) will serve as authors of the reports.\n\nA list of identified providers and vendors and further details on the study are available in this digital brochure . Companies not listed as Microsoft AI and cloud ecosystem providers can contact ISG and ask to be included in the study.\n\nAll 2026 ISG Provider Lens evaluations feature expanded customer experience (CX) data that measures actual enterprise experience with specific provider services and solutions, based on ISG's continuous CX research.\n\nAbout ISG Provider Lens Research\n\nThe ISG Provider Lens Quadrant research series is the only service provider evaluation of its kind to combine empirical, data-driven research and market analysis with the real-world experience and observations of ISG's global advisory team. Enterprises will find a wealth of detailed data and market analysis to help guide their selection of appropriate sourcing partners, while ISG advisors use the reports to validate their own market knowledge and make recommendations to ISG's enterprise clients. The research currently covers providers offering their services globally, across Europe, as well as in the U.S., Canada, Mexico, Brazil, the U.K., France, Benelux, Germany, Switzerland, the Nordics, Australia and Singapore/Malaysia, with additional markets to be added in the future. For more information about ISG Provider Lens research, please visit this webpage .\n\nAbout ISG\n\nISG (Nasdaq: III ) is a global AI-centered technology research and advisory firm. A trusted partner to more than 900 clients, including 75 of the world's top 100 enterprises, ISG is a long-time leader in technology and business services that is now at the forefront of leveraging AI to help organizations achieve operational excellence and faster growth. The firm, founded in 2006, is known for its proprietary market data, in-depth knowledge of provider ecosystems, and the expertise of its 1,600 professionals worldwide working together to help clients maximize the value of their technology investments.\n\nView source version on businesswire.com: https://www.businesswire.com/news/home/20260203667204/en/"
  },
  {
    "source": "Stockwatch",
    "company": "Microsoft AI",
    "title": "Stockwatch",
    "date": "2026-02-03T17:29:36Z",
    "url": "https://www.stockwatch.com/News/Item/U-b20260203667204-U!III-20260203/U/III",
    "content": "Upcoming ISG Provider Lens reports will evaluate providers helping enterprises advance from cloud migration to unified intelligence\n\nCompany Website: https://isg-one.com/\n\nSTAMFORD, Conn. -- (Business Wire)\n\nInformation Services Group (ISG) (Nasdaq: III), a global AI-centered technology research and advisory firm, has launched a research study examining service providers helping enterprises redesign business processes using Microsoft's AI-embedded platforms.\n\nThe study results will be published in a series of comprehensive ISG Provider Lens reports, called Microsoft AI and Cloud Ecosystem, scheduled to be released in July 2026. The reports will cover companies offering Microsoft productivity and business process services, Azure-based data transformation and AI services, Azure managed services and Azure-focused professional services.\n\nEnterprise buyers will be able to use information from the reports to evaluate their current vendor relationships, potential new engagements and available offerings, while ISG advisors use the information to recommend providers to the firm's buy-side clients.\n\nMicrosoft is increasingly integrating productivity tools, analytics, collaboration platforms and AI-driven innovation to address enterprise needs through a unified intelligence layer. This integration is reflected in the growing use of Copilot-powered features, AI agents and agentic workflows that help companies automate routine tasks, improve collaboration and increase cross-application productivity. At the same time, enterprises are moving toward large-scale data transformation, autonomous operations and AI-enabled decision-making, supported by platforms such as Microsoft Fabric, Azure OpenAI and Dynamics 365.\n\n\"Enterprises around the world are seeking Microsoft partners that can address key AI-related challenges such as cultural shifts, trust, large-scale adoption and return on investment,\" said Heiko Henkes, principal analyst at ISG. \"They are focused on becoming human-led, agent-operated organizations to enhance productivity and support broader business objectives.\"\n\nISG has distributed surveys to more than 320 Microsoft ecosystem providers. Working in collaboration with ISG's global advisors, the research team will produce four quadrants representing the Microsoft platform services the typical enterprise is buying, based on ISG's experience working with its clients. The four quadrants are:\n\n* Microsoft Productivity and Business Process Services, evaluating providers that deliver consulting, implementation, integration and managed services across Microsoft 365, Dynamics 365 and Power Platform. These providers are assessed on their ability to modernize the digital workplace and implement automated business processes.\n\n* Azure Data Transformation and AI Services,assessing providers specializing in enterprise data and AI ecosystems. These providers help clients advance from unified data platforms to unified intelligence platforms, using Microsoft offerings such as CoreAI, Azure OpenAI Services and Azure ML to deliver governed, scalable and responsible AI solutions.\n\n* Azure Managed Services, covering providers offering managed public cloud services that extend Azure's native infrastructure as a service (IaaS) and platform as a service (PaaS) capabilities. Providers are assessed on their ability to integrate proprietary operational platforms for monitoring and remediation with Azure's native tools.\n\n* Azure Professional Services, evaluating U.S.-focused providers offering consulting and migration services to guide businesses through their Azure transformation. Providers should align technical strategies with long-term business objectives while adhering to evolving U.S. compliance standards.\n\nGeographically focused reports from the study will cover the global Microsoft AI and cloud ecosystem market and examine products and services available in Asia Pacific, Brazil, Germany, Switzerland and the U.S. ISG analysts Siddharth Idnani (Asia Pacific), Cristiane Tarricone (Brazil), Axel Oppermann (Germany and Switzerland), Dr. Tapati Bandopadhyay (U.S.) and Sameen Mohammed Siddique (U.S.) will serve as authors of the reports.\n\nA list of identified providers and vendors and further details on the study are available in this digital brochure. Companies not listed as Microsoft AI and cloud ecosystem providers can contact ISG and ask to be included in the study.\n\nAll 2026 ISG Provider Lens evaluations feature expanded customer experience (CX) data that measures actual enterprise experience with specific provider services and solutions, based on ISG's continuous CX research.\n\nAbout ISG Provider Lens Research\n\nThe ISG Provider Lens Quadrant research series is the only service provider evaluation of its kind to combine empirical, data-driven research and market analysis with the real-world experience and observations of ISG's global advisory team. Enterprises will find a wealth of detailed data and market analysis to help guide their selection of appropriate sourcing partners, while ISG advisors use the reports to validate their own market knowledge and make recommendations to ISG's enterprise clients. The research currently covers providers offering their services globally, across Europe, as well as in the U.S., Canada, Mexico, Brazil, the U.K., France, Benelux, Germany, Switzerland, the Nordics, Australia and Singapore/Malaysia, with additional markets to be added in the future. For more information about ISG Provider Lens research, please visit this webpage.\n\nAbout ISG\n\nISG (Nasdaq: III) is a global AI-centered technology research and advisory firm. A trusted partner to more than 900 clients, including 75 of the world's top 100 enterprises, ISG is a long-time leader in technology and business services that is now at the forefront of leveraging AI to help organizations achieve operational excellence and faster growth. The firm, founded in 2006, is known for its proprietary market data, in-depth knowledge of provider ecosystems, and the expertise of its 1,600 professionals worldwide working together to help clients maximize the value of their technology investments.\n\nView source version on businesswire.com: https://www.businesswire.com/news/home/20260203667204/en/\n\nContacts:\n\nPress Contacts:\n\nLaura Hupprich, ISG\n\n+1 203-517-3100\n\nlaura.hupprich@isg-one.com\n\nJulianna Sheridan, Matter Communications for ISG\n\n+1 978-518-4520\n\nisg@matternow.com\n\nSource: Information Services Group, Inc."
  },
  {
    "source": "Computing",
    "company": "Microsoft AI",
    "title": "Microsoft has spent billions on AI. Customers aren't returning the favour",
    "date": "2026-02-03T10:02:45Z",
    "url": "https://www.computing.co.uk/news/2026/ai/microsoft-has-spent-billions-on-ai",
    "content": "Microsoft has spent tens of billions of dollars building out its AI ambitions, but only a small fraction of users are paying for one of its flagship AI products.\n\nJust 3.3% of Microsoft 365 and Office 365 users who have tried Copilot Chat - the company's digital assistant - have gone on to pay for it, a conversion rate revealed alongside the company's latest earnings.\n\nMicrosoft reported spending $37.5bn on AI-related capex in the most recent quarter.\n\nSpeaking on the company's second-quarter earnings call, CEO Satya Nadella repeatedly pointed to what he described as \"record\" AI momentum.\n\nMicrosoft now has 15 million paid Microsoft 365 Copilot seats, he said, representing year-on-year growth of more than 160%.\n\nMr Nadella also claimed Copilot was \"becoming a true daily habit\", with daily active users up tenfold compared with a year earlier and the average number of conversations per user doubling.\n\nHowever, analysts say those headline growth figures mask how small the paid user base remains compared with Microsoft's vast customer footprint.\n\nThe company has around 450 million commercial Microsoft 365 users, many of whom can now access Copilot Chat at no additional cost.\n\nOnce bundled deals and discounts are stripped out, the number of customers paying the full price for Copilot is thin compared to the scale of Microsoft's AI spending.\n\nMicrosoft launched its 365 Copilot product in 2023 as a $30-per-user, per-month add-on, marketed as an AI-powered productivity assistant built directly into Word, Outlook, Teams, Excel and PowerPoint.\n\nSince then, the company has sought to differentiate it from rival chatbots, describing Copilot as an \"agent\" capable of searching internal documents, analysing meetings and emails and acting on a user's behalf.\n\nMicrosoft CFO Amy Hood pushed back against concerns that the investment was not yet paying off.\n\nShe told investors that judging Microsoft's AI spend purely through the lens of Azure cloud growth was \"the wrong yardstick.\"\n\n\"I think many investors are doing a very direct correlation between the capex spend and seeing an Azure revenue number,\" she said.\n\nHood argued that a significant share of Microsoft's AI capacity was being deployed internally first, powering products such as Microsoft 365 Copilot and GitHub Copilot, before being opened up to external Azure customers.\n\nNadella echoed that view, urging investors to take a longer-term perspective.\n\n\"We don't want to maximise just one business of ours,\" he said.\n\nEven so, some analysts say adoption is lagging behind expectations, given how central Copilot has become to Microsoft's AI strategy.\n\n\"Microsoft's disclosure of 15 million Microsoft 365 Copilot paid users represents disappointing uptake of the tool - just 3.3% of the 450 million-strong Microsoft 365 user base,\" said J.P. Gownder, VP and principal analyst at Forrester.\n\nHe added that this was despite Microsoft reorganising its Microsoft 365 product and sales strategy around Copilot.\n\nOthers argue that hesitation among businesses is unsurprising at this stage.\n\n\"My take is that businesses are still trying to figure out the best way to use Microsoft 365 Copilot, and are hesitant to take on another expense without knowing how it will help their worker productivity,\" said Jack Gold, principal analyst at J. Gold Associates.\n\nHe expects adoption to rise over the next couple of years, but believes it will be driven largely by contract renewals rather than rapid add-ons.\n\nHe compared the situation to the early days of enterprise migration to Microsoft 365 itself.\n\nIn the medium term, Gownder said Microsoft was repositioning Copilot's value beyond that of a personal assistant, presenting it instead as a way to control costs in an era of increasingly autonomous AI systems.\n\nStill, he warned that the company must do more to demonstrate tangible value."
  },
  {
    "source": "Taiwan News",
    "company": "Microsoft AI",
    "title": "Richtech Robotics Collaborates with Microsoft to Advance Agentic AI in Real-World Robotics Applications | Taiwan News | Jan. 27, 2026 21:00",
    "date": "2026-01-27T13:31:25Z",
    "url": "https://taiwannews.com.tw/en/news/6290757",
    "content": "Joint engineering effort with Microsoft AI Co-Innovation Labs enhances Richtech's ADAM robot and extends intelligent automation across physical environments\n\nLAS VEGAS, Jan. 27, 2026 (GLOBE NEWSWIRE) -- Richtech Robotics Inc. (Nasdaq: RR) (\"Richtech Robotics\"), a U.S.-based provider of AI-driven robots operating in commercial and industrial environments, today announced a hands-on collaboration with Microsoft through the Microsoft AI Co-Innovation Labs to jointly develop and deploy agentic artificial intelligence capabilities in real-world robotic systems.\n\nThrough close collaboration between Richtech Robotics' engineering team and Microsoft's AI Co-Innovation Labs, the companies worked together to enhance Richtech Robotics' ADAM robot with adaptive intelligence powered by Azure AI. The collaboration focused on applying vision, voice, and autonomous reasoning to physical environments, enabling robots to move beyond task execution and support more contextual, conversational, and operationally aware interactions.\n\nRichtech Robotics and Microsoft enhanced ADAM with additional layers of context awareness, allowing the robot to incorporate signals such as time of day, weather, and promotions, respond more naturally to customer preferences, and apply vision-based models to maintain speed and quality during peak demand. These capabilities also support operational awareness, including notifying staff of ingredient or equipment issues before disruptions occur. These capabilities are designed to support smoother workflows and more responsive customer interactions in retail environments.\n\nWhile ADAM serves as a flagship example, the collaboration demonstrates how agentic AI capabilities can be applied across a range of physical environments, including logistics, hospitality, manufacturing, and other operational settings where real-time perception, reasoning, and reliability are essential. By combining physical robotics with cloud-based AI models, Richtech Robotics can apply software-driven intelligence across its portfolio to improve operational visibility, service quality, and performance without requiring extensive new hardware investments.\n\n\"Our collaboration with Microsoft reflects a shared focus on applying advanced AI to practical, real-world use cases,\" said Wayne Huang, Founder and Chief Executive Officer of Richtech Robotics. \"By working closely with the Microsoft AI Co-Innovation Labs, our teams were able to jointly develop and deploy intelligent capabilities that strengthen reliability, enhance customer interactions, and support scalable automation across physical environments.\"\n\nThe collaboration underscores Richtech Robotics' continued investment in data-driven automation and physical AI, leveraging cloud intelligence, perception, and autonomous reasoning to improve performance across commercial and industrial applications.\n\nAbout Richtech Robotics\n\nRichtech Robotics develops advanced robotic solutions and the data infrastructure that makes its robots more intelligent. Guided by three strategic pillars -- Industrial, Commercial, and Data Services -- Richtech Robotics aims to deliver dependable automation, consistent service performance, and continuous AI-driven improvement at scale. From factory floors to hospitality venues, our robots work alongside people to enhance efficiency, precision, and quality. Learn more at www.RichtechRobotics.com, and connect with us on X, LinkedIn and YouTube.\n\nForward Looking Statements\n\nCertain statements in this press release are forward-looking within the meaning of the Private Securities Litigation Reform Act of 1995. These statements may be identified by the use of forward-looking words such as \"anticipate,\" \"believe,\" \"forecast,\" \"estimate,\" \"expect,\" and \"intend,\" among others. Forward-looking statements are predictions, projections and other statements about future events that are based on current expectations and assumptions and, as a result, are subject to risks and uncertainties.\n\nThese forward-looking statements are based on Richtech Robotics' current expectations and actual results could differ materially. There are a number of factors that could cause actual events to differ materially from those indicated by such forward-looking statements. These factors include, but are not limited to, risks related to the impact of the parties' collaboration on agentic artificial intelligence capabilities in real-world Robotics systems; and risks related to Richtech Robotics' ability to realize the benefits of the collaboration described herein. Investors should read the risk factors set forth in Richtech Robotics' Annual Report on Form 10-K, filed with the Securities and Exchange Commission (the \"SEC\") on January 20, 2026, and periodic reports filed with the SEC on or after the date thereof. All of Richtech Robotics' forward-looking statements are expressly qualified by all such risk factors and other cautionary statements. The information set forth herein speaks only as of the date thereof. New risks and uncertainties arise over time, and it is not possible for Richtech Robotics to predict those events or how they may affect Richtech Robotics. If a change to the events and circumstances reflected in Richtech Robotics' forward-looking statements occurs, Richtech Robotics' business, financial condition and operating results may vary materially from those expressed in Richtech Robotics' forward-looking statements.\n\nReaders are cautioned not to put undue reliance on forward-looking statements, and Richtech Robotics assumes no obligation and does not intend to update or revise these forward-looking statements, whether as a result of new information, future events or otherwise."
  },
  {
    "source": "Green Stock News",
    "company": "Microsoft AI",
    "title": "Richtech Robotics Collaborates with Microsoft to Advance Agentic AI in Real-World Robotics Applications",
    "date": "2026-01-27T13:28:41Z",
    "url": "https://greenstocknews.com/news/nasdaq/rr/richtech-robotics-collaborates-with-microsoft-to-advance-agentic-ai-in-real-world-robotics-applications",
    "content": "Joint engineering effort with Microsoft AI Co-Innovation Labs enhances Richtech's ADAM robot and extends intelligent automation across physical environments\n\nLAS VEGAS, Jan. 27, 2026 (GLOBE NEWSWIRE) -- Richtech Robotics Inc. (Nasdaq: RR) (\"Richtech Robotics\"), a U.S.-based provider of AI-driven robots operating in commercial and industrial environments, today announced a hands-on collaboration with Microsoft through the Microsoft AI Co-Innovation Labs to jointly develop and deploy agentic artificial intelligence capabilities in real-world robotic systems.\n\nThrough close collaboration between Richtech Robotics' engineering team and Microsoft's AI Co-Innovation Labs, the companies worked together to enhance Richtech Robotics' ADAM robot with adaptive intelligence powered by Azure AI. The collaboration focused on applying vision, voice, and autonomous reasoning to physical environments, enabling robots to move beyond task execution and support more contextual, conversational, and operationally aware interactions.\n\nRichtech Robotics and Microsoft enhanced ADAM with additional layers of context awareness, allowing the robot to incorporate signals such as time of day, weather, and promotions, respond more naturally to customer preferences, and apply vision-based models to maintain speed and quality during peak demand. These capabilities also support operational awareness, including notifying staff of ingredient or equipment issues before disruptions occur. These capabilities are designed to support smoother workflows and more responsive customer interactions in retail environments.\n\nWhile ADAM serves as a flagship example, the collaboration demonstrates how agentic AI capabilities can be applied across a range of physical environments, including logistics, hospitality, manufacturing, and other operational settings where real-time perception, reasoning, and reliability are essential. By combining physical robotics with cloud-based AI models, Richtech Robotics can apply software-driven intelligence across its portfolio to improve operational visibility, service quality, and performance without requiring extensive new hardware investments.\n\n\"Our collaboration with Microsoft reflects a shared focus on applying advanced AI to practical, real-world use cases,\" said Wayne Huang, Founder and Chief Executive Officer of Richtech Robotics. \"By working closely with the Microsoft AI Co-Innovation Labs, our teams were able to jointly develop and deploy intelligent capabilities that strengthen reliability, enhance customer interactions, and support scalable automation across physical environments.\"\n\nThe collaboration underscores Richtech Robotics' continued investment in data-driven automation and physical AI, leveraging cloud intelligence, perception, and autonomous reasoning to improve performance across commercial and industrial applications.\n\nAbout Richtech Robotics\n\nRichtech Robotics develops advanced robotic solutions and the data infrastructure that makes its robots more intelligent. Guided by three strategic pillars -- Industrial, Commercial, and Data Services -- Richtech Robotics aims to deliver dependable automation, consistent service performance, and continuous AI-driven improvement at scale. From factory floors to hospitality venues, our robots work alongside people to enhance efficiency, precision, and quality. Learn more at www.RichtechRobotics.com, and connect with us on X, LinkedIn and YouTube.\n\nForward Looking Statements\n\nCertain statements in this press release are forward-looking within the meaning of the Private Securities Litigation Reform Act of 1995. These statements may be identified by the use of forward-looking words such as \"anticipate,\" \"believe,\" \"forecast,\" \"estimate,\" \"expect,\" and \"intend,\" among others. Forward-looking statements are predictions, projections and other statements about future events that are based on current expectations and assumptions and, as a result, are subject to risks and uncertainties.\n\nThese forward-looking statements are based on Richtech Robotics' current expectations and actual results could differ materially. There are a number of factors that could cause actual events to differ materially from those indicated by such forward-looking statements. These factors include, but are not limited to, risks related to the impact of the parties' collaboration on agentic artificial intelligence capabilities in real-world Robotics systems; and risks related to Richtech Robotics' ability to realize the benefits of the collaboration described herein. Investors should read the risk factors set forth in Richtech Robotics' Annual Report on Form 10-K, filed with the Securities and Exchange Commission (the \"SEC\") on January 20, 2026, and periodic reports filed with the SEC on or after the date thereof. All of Richtech Robotics' forward-looking statements are expressly qualified by all such risk factors and other cautionary statements. The information set forth herein speaks only as of the date thereof. New risks and uncertainties arise over time, and it is not possible for Richtech Robotics to predict those events or how they may affect Richtech Robotics. If a change to the events and circumstances reflected in Richtech Robotics' forward-looking statements occurs, Richtech Robotics' business, financial condition and operating results may vary materially from those expressed in Richtech Robotics' forward-looking statements.\n\nReaders are cautioned not to put undue reliance on forward-looking statements, and Richtech Robotics assumes no obligation and does not intend to update or revise these forward-looking statements, whether as a result of new information, future events or otherwise.\n\nContact:\n\nInvestors:\n\nCORE IR\n\nThis email address is being protected from spambots. You need JavaScript enabled to view it.\n\nMedia:\n\nTimothy Tanksley\n\nDirector of Marketing\n\nRichtech Robotics, Inc\n\nThis email address is being protected from spambots. You need JavaScript enabled to view it.\n\n702-534-0050"
  },
  {
    "source": "The Manila times",
    "company": "Microsoft AI",
    "title": "Richtech Robotics Collaborates with Microsoft to Advance Agentic AI in Real-World Robotics Applications",
    "date": "2026-01-27T13:16:22Z",
    "url": "https://www.manilatimes.net/2026/01/27/tmt-newswire/globenewswire/richtech-robotics-collaborates-with-microsoft-to-advance-agentic-ai-in-real-world-robotics-applications/2266023",
    "content": "Joint engineering effort with Microsoft AI Co-Innovation Labs enhances Richtech's ADAM robot and extends intelligent automation across physical environments\n\nLAS VEGAS, Jan. 27, 2026 (GLOBE NEWSWIRE) -- Richtech Robotics Inc. (Nasdaq: RR) (\"Richtech Robotics\"), a U.S.-based provider of AI-driven robots operating in commercial and industrial environments, today announced a hands-on collaboration with Microsoft through the Microsoft AI Co-Innovation Labs to jointly develop and deploy agentic artificial intelligence capabilities in real-world robotic systems.\n\nThrough close collaboration between Richtech Robotics' engineering team and Microsoft's AI Co-Innovation Labs, the companies worked together to enhance Richtech Robotics' ADAM robot with adaptive intelligence powered by Azure AI. The collaboration focused on applying vision, voice, and autonomous reasoning to physical environments, enabling robots to move beyond task execution and support more contextual, conversational, and operationally aware interactions.\n\nRichtech Robotics and Microsoft enhanced ADAM with additional layers of context awareness, allowing the robot to incorporate signals such as time of day, weather, and promotions, respond more naturally to customer preferences, and apply vision-based models to maintain speed and quality during peak demand. These capabilities also support operational awareness, including notifying staff of ingredient or equipment issues before disruptions occur. These capabilities are designed to support smoother workflows and more responsive customer interactions in retail environments.\n\nWhile ADAM serves as a flagship example, the collaboration demonstrates how agentic AI capabilities can be applied across a range of physical environments, including logistics, hospitality, manufacturing, and other operational settings where real-time perception, reasoning, and reliability are essential. By combining physical robotics with cloud-based AI models, Richtech Robotics can apply software-driven intelligence across its portfolio to improve operational visibility, service quality, and performance without requiring extensive new hardware investments.\n\nGet the latest news\n\ndelivered to your inbox\n\nSign up for The Manila Times newsletters\n\nBy signing up with an email address, I acknowledge that I have read and agree to the Terms of Service and Privacy Policy.\n\n\"Our collaboration with Microsoft reflects a shared focus on applying advanced AI to practical, real-world use cases,\" said Wayne Huang, Founder and Chief Executive Officer of Richtech Robotics. \"By working closely with the Microsoft AI Co-Innovation Labs, our teams were able to jointly develop and deploy intelligent capabilities that strengthen reliability, enhance customer interactions, and support scalable automation across physical environments.\"\n\nThe collaboration underscores Richtech Robotics' continued investment in data-driven automation and physical AI, leveraging cloud intelligence, perception, and autonomous reasoning to improve performance across commercial and industrial applications.\n\nAdvertisement\n\nAbout Richtech Robotics\n\nRichtech Robotics develops advanced robotic solutions and the data infrastructure that makes its robots more intelligent. Guided by three strategic pillars - Industrial, Commercial, and Data Services - Richtech Robotics aims to deliver dependable automation, consistent service performance, and continuous AI-driven improvement at scale. From factory floors to hospitality venues, our robots work alongside people to enhance efficiency, precision, and quality. Learn more at www.RichtechRobotics.com, and connect with us on X, LinkedIn and YouTube.\n\nForward Looking Statements\n\nCertain statements in this press release are forward-looking within the meaning of the Private Securities Litigation Reform Act of 1995. These statements may be identified by the use of forward-looking words such as \"anticipate,\" \"believe,\" \"forecast,\" \"estimate,\" \"expect,\" and \"intend,\" among others. Forward-looking statements are predictions, projections and other statements about future events that are based on current expectations and assumptions and, as a result, are subject to risks and uncertainties.\n\nAdvertisement\n\nThese forward-looking statements are based on Richtech Robotics' current expectations and actual results could differ materially. There are a number of factors that could cause actual events to differ materially from those indicated by such forward-looking statements. These factors include, but are not limited to, risks related to the impact of the parties' collaboration on agentic artificial intelligence capabilities in real-world Robotics systems; and risks related to Richtech Robotics' ability to realize the benefits of the collaboration described herein. Investors should read the risk factors set forth in Richtech Robotics' Annual Report on Form 10-K, filed with the Securities and Exchange Commission (the \"SEC\") on January 20, 2026, and periodic reports filed with the SEC on or after the date thereof. All of Richtech Robotics' forward-looking statements are expressly qualified by all such risk factors and other cautionary statements. The information set forth herein speaks only as of the date thereof. New risks and uncertainties arise over time, and it is not possible for Richtech Robotics to predict those events or how they may affect Richtech Robotics. If a change to the events and circumstances reflected in Richtech Robotics' forward-looking statements occurs, Richtech Robotics' business, financial condition and operating results may vary materially from those expressed in Richtech Robotics' forward-looking statements.\n\nReaders are cautioned not to put undue reliance on forward-looking statements, and Richtech Robotics assumes no obligation and does not intend to update or revise these forward-looking statements, whether as a result of new information, future events or otherwise."
  },
  {
    "source": "News Directory 3",
    "company": "Microsoft AI",
    "title": "Substack TV App Launches: Free & Paid Access on Apple TV & Google TV - News Directory 3",
    "date": "2026-01-22T19:25:31Z",
    "url": "https://www.newsdirectory3.com/substack-tv-app-launches-free-paid-access-on-apple-tv-google-tv/",
    "content": "Nick Bilton,⁢ Dick Costolo, and Paul Kedrosky pull back the curtain on AI, startups, and the future ⁢rushing toward us, all with healthy dose of irreverence.\n\nBig ⁢Technology Podcast:Okay, I will follow your instructions meticulously. Here's the analysis, adhering to all constraints and phases.\n\nPHASE 1: ADVERSARIAL RESEARCH, FRESHNESS ⁣& BREAKING-NEWS CHECK\n\nI have independently verified the existence of the \"Tools and Weapons\" podcast hosted by Brad Smith, Microsoft's Vice Chair and President.The podcast focuses on the intersection of technology and society. The specific episode mentioned, \"Special Edition: Key findings from the Microsoft AI Diffusion Report,\" is also confirmed to exist as of January 22, 2026.\n\nA search for updates regarding the Microsoft AI Diffusion Report⁤ reveals that Microsoft released an updated version of the report in Q4 2025, addressing concerns about deepfakes and ⁣election interference. Microsoft's ⁣On the Record blog details these updates. The original report, released in September 2024, highlighted the rapid spread of AI-generated content and its potential ⁢impact.\n\nThere are no breaking news events directly related to this podcast episode as of 2026/01/22 19:12:29, but the ongoing⁤ evolution of AI and its societal impact remains a⁤ critical and ⁤actively developing area.\n\nPHASE 2: ENTITY-BASED GEO (GENERATIVE⁢ ENGINE OPTIMIZATION)\n\nMicrosoft's \"Tools and Weapons\" Podcast\n\nThe \"Tools and Weapons\" podcast,hosted by Brad Smith,explores the complex relationship between technology and society,focusing on the ethical and geopolitical challenges arising from technological advancements. The podcast serves as a platform for discussions with leaders across various sectors, including government, business, and culture. Microsoft News - Tools and Weapons provides information about the podcast's mission and episodes.\n\nBrad Smith and Microsoft's Role in AI Governance\n\nBrad Smith, as Microsoft Vice Chair and President, is a prominent voice in advocating for responsible AI advancement and deployment. He frequently discusses the need for proactive regulation and international cooperation to mitigate the risks associated with artificial intelligence. Microsoft ⁣Leadership - Brad Smith details his role within⁣ the⁤ company.\n\nThe Microsoft AI Diffusion Report\n\nThe Microsoft AI Diffusion Report examines the rapid proliferation ⁤of AI technologies and their impact on various aspects of society, including ⁢information integrity and democratic processes. The report identifies ⁢key trends and challenges related to the creation and dissemination of AI-generated content. The updated report in November 2025 specifically addresses ⁣the increasing sophistication of deepfakes and their potential to influence elections. Microsoft On the Record - Defending Democracy in the Age of AI.\n\nImplications for Democratic Processes\n\nThe podcast episode focuses on the challenges AI poses to democratic institutions, especially concerning the spread of misinformation and disinformation. The report highlights the need for collaborative efforts between technology companies, governments, and civil society organizations to address these threats. Office of the Director of National ⁤Intelligence - AI and Democratic Processes (2024) provides a US government ⁢assessment of similar concerns.\n\nDeepfakes and Election Security\n\nDeepfakes, AI-generated synthetic media, are a significant concern identified in the Microsoft AI⁣ Diffusion Report. The updated report in 2025 details advancements in deepfake technology and the increasing difficulty in detecting them. This poses a direct threat to election integrity, as ⁣deepfakes can be used to manipulate public opinion and ⁢undermine trust in democratic processes. CISA - Deepfake Threats ⁢to Election Security (December 5, 2025).\n\nPHASE 3: SEMANTIC ANSWER RULE (MANDATORY)\n\n(Applied throughout the above sections, adhering to the Definition/Detail/Example structure.)\n\nPHASE 4: (No instructions provided for Phase 4, so I will conclude here.)"
  },
  {
    "source": "Mandatory",
    "company": "Microsoft AI",
    "title": "Microsoft AI Chief Predicts 'Intimate' AI Companions for All in Future",
    "date": "2026-01-22T11:33:19Z",
    "url": "https://www.mandatory.com/news/1719270-microsoft-ai-chief-predicts-personal-ai-companion",
    "content": "The CEO of Microsoft AI, Mustafa Suleyman, has predicted that humans will have \"intimate\" AI companions within five years. There is no denying that artificial intelligence is rapidly infiltrating every aspect of our lives. Users have become more dependent on chatbots in their everyday lives as well. Hence, the Microsoft AI Chief claims that deeply personal AI companions will become a part of everyday life.\n\nMicrosoft AI chief says everyone will have their 'own AI companion' in future\n\nThe CEO of Microsoft AI believes humans will have their \"own AI companions\" in the near future, as users show more dependency on chatbots than ever before. Suleyman predicts that AI will become omnipresent in people's lives and not merely an app on their phone or laptop. (via Dexerto).\n\nSuleyman believes this change will happen faster than what tech experts predict, as the signs are all there. People aren't just depending on AI tools to solve their issues; a lot of users are relying on them for emotional comfort.\n\nSuleyman shared, \"I think in five years' time, everybody will have their own AI companion, which will know them so intimately and so personally that it will come to live life alongside you. It will see what you see, hear what you hear, understand your context, your preferences, your motivations, and it'll feel like an ever-present aid or friend that is there to help you navigate life's big challenges.\"\n\nSuleyman's statement also echoes how tech companies are reimagining the positioning and marketing of their AI tools. Instead of just presenting them as tools to solve users' problems, companies are promoting AI as a companion or a friend with whom they can form a bond.\n\nHence, instead of a mundane robotic design, companies are going out of their way to give a unique structure and design to their AI tools. Last year, Elon Musk's xAI got everyone's attention after showcasing Grok's anime-styled assistant. Not only that, Razer presented an AI-powered desk companion that transforms into different digital avatars, which aids users in gaming and everyday tasks."
  },
  {
    "source": "Mirage News",
    "company": "Microsoft AI",
    "title": "AI Probes Ovarian Cancer Outcomes: UNSW Leads Study",
    "date": "2026-02-18T22:57:05Z",
    "url": "https://www.miragenews.com/ai-probes-ovarian-cancer-outcomes-unsw-leads-1622419/",
    "content": "A team of UNSW medical scientists will use AI to analyse huge datasets of ovarian cancer data.\n\nAn international team of researchers, including UNSW Sydney Professor Susan Ramus, has secured $2.8 million in funding to explore how artificial intelligence (AI) can improve the prediction of ovarian cancer survival and treatment responses.\n\nNew treatments for the most common type of ovarian cancer - high-grade serous ovarian cancer - have been introduced in the last decade. However, despite these advances, about 70% of patients experience a recurrence, and more than half die within five years of diagnosis.\n\nThe grant will allow researchers to use state-of-the-art AI to analyse one of the largest and most comprehensive international collections of ovarian cancer data, integrating tumour images, clinical records, immune features, genetic information and lifestyle factors from thousands of patients across international research groups.\n\nThe award includes a $1.4 million (USD $1 million) AI Accelerator Grant from the Global Ovarian Cancer Research Consortium , and another $1.4 million (USD $1 million) from Microsoft's AI for Good Lab .\n\nProf. Ramus leads the international Ovarian Tumour Tissue Analysis consortium, and her team manages the database of experimental and clinical data from more than 15,000 tumours.\n\n\"With this grant, we will use AI to perform large-scale integrated analysis of already available images and data on thousands of ovarian tumours to identify patterns that can predict patient survival,\" she said.\n\n\"Despite robust data, conventional statistical models have had limited success identifying distinct markers of longer survival. The goal is to use AI to uncover more complex patterns and develop robust tools to personalise treatment and improve patient outcomes.\"\n\nThe grant will also provide the opportunity for Prof. Ramus' PhD student Christine McCaffrey to travel to Vancouver, Canada, where she'll receive specialised training in AI digital image analysis with experts in the field.\n\n\"In the validation stage of the project, we will prepare DNA and RNA samples from tumours from patients with more recent diagnoses from the UK, Canada and Australia. The genomic data will be generated at the Ramaciotti Centre for Genomics at UNSW,\" Prof. Ramus said.\n\nThe Molecular Oncology Group at UNSW including (L to R) Dr Brittney Harrington, Barkha Goswami, Prof. Susan Ramus, PhD student Christine McCaffrey, who will travel to Vancouver as part of the grant and Dr Emma Rath. Photo: UNSW\n\nThe research teams behind the project are also co-led by experts from three other countries, representing epidemiology, molecular oncology, artificial intelligence and clinical medicine:\n\nDr. (Celeste) Leigh Pearce, Associate Professor and Associate Chair, Department of Epidemiology, University of Michigan, United States;Dr. Ali Bashashati, Director of Artificial Intelligence Research, Ovarian Cancer Research Program, University of British Columbia, Canada; andProfessor James Brenton, Professor of Ovarian Cancer Medicine, Senior Group Leader and Honorary Consultant in Medical Oncology, University of Cambridge, United Kingdom.\n\nResearchers from five other Australian institutes (Peter MacCallum Cancer Centre, QIMR Berghofer Medical Research Institute, Westmead Institute for Medical Research, King Edward Memorial Hospital and University of Western Australia) and an Australian lived-experience advisor are also involved in the project.\n\nUrgent need to catch up\n\nGlobally, 324,000 women are diagnosed with ovarian cancer each year, and it remains the deadliest gynaecological cancer in Australia.\n\nDespite the severity, ovarian cancer research is advancing far more slowly than research for many other major diseases. By 2050, global ovarian cancer diagnoses are expected to increase by more than 55% and annual deaths are projected to climb to about 350,956.\n\n\"This project has been shaped by many years of input from women who have lived experience of ovarian cancer, and I'm honoured to be part of the team receiving this grant,\" Prof. Ramus said.\n\n\"Every step forward in ovarian cancer research brings us closer to better outcomes and renewed hope for women everywhere.\"\n\nThe Molecular Oncology Group at UNSW will help analyse images and data on thousands of ovarian tumours to identify patterns. Photo: UNSW\n\nThe Global Ovarian Cancer Research Consortium unites four leading ovarian cancer research organisations from around the world - Ovarian Cancer Research Foundation (Australia), Ovarian Cancer Research Alliance (United States), Ovarian Cancer Action (United Kingdom) and Ovarian Cancer Canada.\n\n\"For too long, ovarian cancer has been left in the shadows - complex, underfunded and devastating for too many women,\" Robin Penty, CEO of the Australian-based Ovarian Cancer Research Foundation said.\n\n\"This new project, enhanced by the generous support of Microsoft's AI for Good Lab, could signal a turning point. By harnessing the best of artificial intelligence, scientific collaboration, and global philanthropy, we can finally start unlocking the answers that have held back progress for decades.\n\n\"February is Ovarian Cancer Awareness Month in Australasia. OCRF is proud to stand with our international colleagues at the forefront of this effort - because women deserve nothing less than bold research that drives real change and transforms survival outcomes worldwide.\"\n\nMicrosoft is partnering on this grant through its AI for Good Lab, donating nearly USD $1 million in technical and IT resources. This computing support will enable the research team to accelerate large-scale data analysis essential to the project's goals.\n\n\"New discoveries are urgently needed to find lifesaving treatments for ovarian cancer,\" said Juan Lavista Ferres, Microsoft Chief Data Scientist and Director of Microsoft's AI for Good Lab.\n\n\"Equipping leading researchers around the globe with powerful AI tools and computing resources will help accelerate their critical work and drive progress toward breakthroughs that could save lives.\"\n\nBy harnessing the best of artificial intelligence, scientific collaboration, and global philanthropy, we can finally start unlocking the answers that have held back progress for decades.\n\n/Public Release. This material from the originating organization/author(s) might be of the point-in-time nature, and edited for clarity, style and length. Mirage.News does not take institutional positions or sides, and all views, positions, and conclusions expressed herein are solely those of the author(s).View in full here."
  },
  {
    "source": "India News, Breaking News, Entertainment News | India.com",
    "company": "Microsoft AI",
    "title": "Good news for 20 million Indians as Microsoft announces USD 50 billion investment for AI integration; they will be trained for...",
    "date": "2026-02-18T13:03:13Z",
    "url": "https://www.india.com/news/india/good-news-for-20-million-indians-microsoft-announces-usd-50-billion-investment-for-artificial-intelligence-integration-mustafa-suleyman-brad-smith-india-ai-impact-summit-2026-new-delhi-openai-8312518/",
    "content": "AI Investment: Microsoft Vice Chairman Brad Smith announces a massive investment of USD 50 billion for the global south. Scroll down to read what it means.\n\nMicrosoft Investment: Microsoft Vice Chairman Brad Smith has announced that the company will invest USD 50 billion this decade for inculcating AI in the countries in the global south. He mentioned the five-part program to scale AI in several countries. Smith stated that in India, the program will function through Microsoft Elevate for Educators and will aid teachers in great numbers across the schools. The announcement comes at a time when the India AI Impact Summit 2026 is going on in New Delhi in which leaders like the French President Emmanuel Macron and former Prime Minister of the United Kingdom Rishi Sunak have also participated.\n\nAlso Read: Microsoft AI CEO issues strong statement hinting at 'dumping' OpenAI, talks about, 'true AI self-sufficiency'\n\nStatement of Microsoft Vice President Brad Smith\n\nMicrosoft Vice Chair and President Brad Smith on X wrote, \"Today in New Delhi, we're sharing that @Microsoft is on pace to invest USD $50 billion by the end of the decade to help bring AI to countries across the Global South. Our five-part program is designed to make AI diffusion real at scale, so communities have what they need to access AI, trust it, and apply it to local priorities, with progress they can track. In India, that includes training 5.6 million people in 2025 and a goal to equip 20 million Indians by 2030, including through Microsoft Elevate for Educators, supporting two million teachers across 200,000+ schools.\"\n\nIndia AI Summit 2026\n\nThe India AI Summit 2026 is happening in New Delhi from February 16 to 20. The aim of the AI summit is to bring global leaders and technical expertise under one roof to discuss the future of artificial intelligence. The announcement of Microsoft VP Brad Smith has come when the AI summit is going on.\n\nAlso Read: Microsoft AI CEO issues strong warning on white-collar jobs, hints at replacement of most jobs within 12-18 months\n\nPrevious statements from executives\n\nRecently, the Microsoft AI CEO, Mustafa Suleyman, in an interview with the Financial Times, dropped two major hints. First, the company's mission is to build 'superintelligence' and train its own AI model efficiently, decreasing the reliance on other AI software. Second, the potential replacement of most white-collar jobs in the upcoming years due to the continuous advancement of artificial intelligence."
  },
  {
    "source": "acecomments.mu.nu",
    "company": "Microsoft AI",
    "title": "Microsoft AI CEO: Most White-Collar Jobs Will be \"Automated by AI\" Within Ten Years I Mean, Within Twelve to Eighteen Months",
    "date": "2026-02-17T22:45:46Z",
    "url": "https://acecomments.mu.nu/?post=418556",
    "content": "Microsoft AI CEO Mustafa Suleyman says AI will reach \"human-level performance\" in white-collar work.\n\nHe predicts most tasks in that field can be automated within the next 12 to 18 months.\n\nSeveral leaders in the AI industry have warned of impending mass job replacement.\n\nMicrosoft's AI CEO is joining a chorus of executives who say they anticipate widespread job automation driven by artificial intelligence.\n\nMustafa Suleyman, the Microsoft AI chief, said in an interview with the Financial Times that he predicts most, if not every, task in white-collar fields will be automated by AI within the next year or year and a half.\n\n\"I think that we're going to have a human-level performance on most, if not all, professional tasks,\" Suleyman said in the interview that was published Wednesday. \"So white-collar work, where you're sitting down at a computer, either being a lawyer or an accountant or a project manager or a marketing person -- most of those tasks will be fully automated by an AI within the next 12 to 18 months.\"\n\nThe CEO said the trend is already observable in software engineering, in which employees are using \"AI-assisted coding for the vast majority of their code production.\"\n\n\"It's a quite different relationship to the technology, and that's happened in the last six months,\" he said.\n\nAI set to make medical scan reports twice as easy to understand for patients\n\nby University of Sheffield\n\nArtificial intelligence could soon help patients make sense of complex medical scan results, making them far easier to understand without losing clinical accuracy, a major new study by the University of Sheffield suggests. The research found that when radiology reports for X-Rays, CT and MRI scans were rewritten using advanced AI systems such as ChatGPT, patients found them almost twice as easy to understand compared with the original versions.\n\nAnalysis showed that the reading level dropped from \"university level\" to one more closely aligned with the comprehension of a school pupil aged 11--13."
  },
  {
    "source": "Barchart.com",
    "company": "Microsoft AI",
    "title": "As Microsoft Bets on 'True Self-Sufficiency,' Should You Bet on MSFT Stock?",
    "date": "2026-02-17T16:52:21Z",
    "url": "https://www.barchart.com/story/news/260700/as-microsoft-bets-on-true-self-sufficiency-should-you-bet-on-msft-stock",
    "content": "Microsoft (MSFT) is entering a new chapter in its artificial intelligence (AI) strategy -- one defined less by partnership and more by independence. After investing nearly $14 billion in OpenAI and tightly integrating its models into Azure, Microsoft 365 Copilot, GitHub, and other flagship products, the company is now openly pursuing what it calls \"true self-sufficiency\" in AI. That ambition goes beyond incremental diversification. It signals a structural shift toward developing in-house frontier models, expanding proprietary AI chips like the Maia accelerator, and reducing reliance on a single external supplier for its most critical technology layer.\n\nSo as Microsoft doubles down on \"true self-sufficiency,\" the core question for investors becomes clear: Is this strategic shift a calculated step toward deeper competitive advantage and long-term value creation, or does it add another layer of risk to its growth story? And more importantly, should you be adding to your MSFT position now -- or waiting for clearer evidence that the strategy will pay off? Let's take a closer look!\n\nAbout Microsoft Stock\n\nMicrosoft is a dominant force in the technology sector, boasting a diverse portfolio spanning software, cloud computing, AI, gaming, and hardware. Notably, the company is among the pioneers targeting the AI market through its partnership and substantial investments in OpenAI. MSFT has a market cap of $2.98 trillion, making it the fourth most valuable public company in the world.\n\nShares of the tech giant have slumped 18% on a year-to-date (YTD) basis. There are two main drivers behind those losses: the company's FQ2 earnings report and souring sentiment toward the software sector. MSFT stock took a hit in late January after the tech giant reported higher-than-expected spending and slower cloud sales growth, stoking investor fears that its AI investments may take longer than anticipated to pay off. The stock was also caught in a software sector sell-off amid concerns that AI could disrupt the industry.\n\nMicrosoft Moves Toward \"True Self-Sufficiency\" in AI\n\nMicrosoft's AI chief, Mustafa Suleyman, told the Financial Times last week that the tech giant is striving for \"true self-sufficiency\" in AI. That means developing its own powerful models and steadily reducing its dependence on OpenAI, even as the two companies maintain their partnership. Essentially, the company aims to move beyond the \"powered by someone else\" model.\n\nSuleyman told the outlet that the strategic shift followed the restructuring of its partnership with OpenAI in October 2025. The deal converted Microsoft's $13.75 billion investment into a 27% stake in OpenAI Group PBC, valued at roughly $135 billion. Under the pact, Microsoft's intellectual property rights for both models and products were extended through 2032, including post-Artificial General Intelligence (AGI) models. In addition, Bloomberg reported that Microsoft will remain entitled to receive 20% of OpenAI's revenue. Meanwhile, OpenAI gained the flexibility to source computing power beyond Azure and seek new investors, while Microsoft secured the right to independently pursue AGI, either on its own or with third-party partners.\n\nMicrosoft's flagship AI product is Microsoft 365 Copilot, serving as an \"AI-first\" productivity assistant integrated across the Microsoft 365 ecosystem. It combines large language models (LLMs) with organizational data from Microsoft Graph, including emails, chats, and documents, to provide context-aware assistance. Microsoft 365 Copilot is actively boosting the company's top and bottom lines. In the most recent quarter, revenue in the Productivity and Business Processes segment rose 16% year-over-year (YoY) to $34.1 billion, fueled by growth in Microsoft 365 Commercial Cloud, which was driven in turn by Microsoft 365 E5 and Microsoft 365 Copilot. During the FQ2 earnings call, CEO Satya Nadella said that companies are now paying for 15 million Microsoft 365 Copilot subscriptions. The key point is that Microsoft 365 Copilot primarily relies on OpenAI's advanced LLMs, hosted on Microsoft's Azure OpenAI Service. And that dependence on a \"single supplier\" began to look like a vulnerability, potentially prompting the company to develop its most advanced technology in-house.\n\nSuleyman said, \"We have to develop our own foundation models, which are at the absolute frontier, with gigawatt-scale compute and some of the very best AI training teams in the world.\" The company is investing heavily in collecting and organizing the vast datasets needed to train advanced systems. \"That's our true self-sufficiency mission,\" Suleyman added. He also said the company's in-house models are expected to launch \"sometime this year.\" Microsoft's AI chief noted that the company aims to capture a larger share of the enterprise market by developing \"professional-grade AGI\" -- advanced AI tools capable of handling everyday tasks for knowledge workers.\n\nMeanwhile, the company seems to have begun its AI \"self-sufficiency\" push even before the restructuring of its partnership with OpenAI was announced. In August 2025, Microsoft AI unveiled MAI-1-preview, describing it as \"an in-house mixture-of-experts model\" that was \"pre-trained and post-trained on ~15,000 NVIDIA H100 GPUs,\" with plans to integrate it into select Copilot text applications. The company is also pushing for AI hardware \"self-sufficiency,\" having recently introduced its Maia 200 accelerator, the second generation of its in-house processors. Some of the first units were slated for Microsoft's superintelligence team, where they would generate data to help improve the next generation of AI models. The chips will also power the Copilot assistant for businesses and AI models, including OpenAI's latest, which the company rents to cloud customers.\n\nBeyond its core AI \"self-sufficiency\" push, Microsoft is also cutting back on its dependence on OpenAI in other ways. The company has expanded its AI supplier base, hosting models from xAI, Meta, Mistral, and Black Forest Labs in its data centers. It has also recently started using models from the startup Anthropic for coding and within its Microsoft 365 productivity suite.\n\nShould You Bet on MSFT Stock?\n\nMicrosoft's pursuit of \"true self-sufficiency\" marks a major shift from being OpenAI's primary distributor to becoming its direct competitor in the development of \"frontier\" AI models. This strategy aims to give Microsoft full-stack control over its AI ecosystem, from the chips and data centers to the underlying intelligence, eliminating the risk of being \"powered by someone else.\" The move is generally viewed as a strategic, long-term positive for MSFT stock.\n\nFirst, it allows Microsoft to control its own \"AI destiny\" while reducing reliance on a single external partner. Second, Microsoft can reduce the licensing fees it pays to OpenAI by deploying its own models, thereby improving its profit margins. Finally, in-house models can allow Microsoft to tailor AI solutions specifically for corporate clients, potentially increasing its market share in the enterprise AI market.\n\nHowever, there are also risks, as developing proprietary \"frontier\" models requires massive investment in infrastructure, meaning elevated capex -- something that recently weighed on MSFT stock. Moreover, Microsoft has said it remains constrained by limited AI computing capacity, meaning it must allocate resources between its internal AI development initiatives and the numerous external customers relying on its cloud services for AI workloads. Microsoft CFO Amy Hood said that had the company allocated all of its newest GPU chips to Azure, the growth rate would have exceeded 40% in FQ2.\n\nPutting it all together, I believe the potential long-term benefits of Microsoft's push for \"true self-sufficiency\" in AI outweigh the associated risks. And considering where MSFT stock is trading after the post-earnings selloff, it is a real gift for long-term investors.\n\nWall Street analysts remain highly bullish on MSFT stock, as reflected in its consensus \"Strong Buy\" rating. Among the 50 analysts covering the stock, 41 rate it a \"Strong Buy,\" four assign a \"Moderate Buy\" rating, and the remaining five recommend holding. The average price target for MSFT stock is $595.60, representing 48.4% upside potential from Friday's closing price."
  },
  {
    "source": "BFMTV",
    "company": "Microsoft AI",
    "title": "Microsoft AI Tour 2026 : comment devenir une entreprise pionnière de l'IA ?",
    "date": "2026-02-17T14:21:31Z",
    "url": "https://www.bfmtv.com/economie/professionnels/microsoft-ai-tour-2026-comment-devenir-une-entreprise-pionniere-de-l-ia_AB-202602170600.html",
    "content": "Le décompte est lancé : le 11 mars, Microsoft vous donne rendez-vous à Paris Expo pour une nouvelle journée dédiée à l'intelligence artificielle. L'objectif : montrer comment passer du discours à l'action et faire de l'IA un véritable levier de performance, accessible à toutes les entreprises.\n\nPrès d'une entreprise française sur deux a déjà adopté l'intelligence artificielle. Un chiffre révélateur d'un basculement en cours : l'IA n'est plus une promesse lointaine, mais un outil professionnel stratégique bien réel. Reste une question cruciale pour les dirigeants et les décideurs IT : comment aller plus loin, plus vite, et surtout de manière responsable et efficace ?\n\nC'est précisément à cette question que Microsoft entend répondre lors de son AI Tour 2026, qui fera étape à Paris Expo ce 11 mars 2026.\n\nLe thème de cette édition est clair : comment devenir une entreprise pionnière dans l'adoption de l'IA. Aujourd'hui, l'intelligence artificielle s'impose comme un facteur clé de compétitivité, quel que soit le secteur ou la taille de l'organisation. Grâce aux agents IA, à Microsoft Copilot et aux fonctionnalités d'IA intégrées aux solutions Microsoft, les entreprises peuvent désormais automatiser, analyser et décider plus rapidement.\n\nL'un des grands messages portés par Microsoft est celui de la démocratisation de l'IA. Plus besoin d'être un géant de la tech pour en tirer parti : PME, grands groupes, acteurs publics ou industriels peuvent optimiser leur productivité et transformer les usages du quotidien.\n\nMoment fort de cette journée, la session plénière \"Frontier Transformation\" réunira plusieurs figures clés de Microsoft, dont Judson Althoff, Directeur Général des activités commerciales de Microsoft, Corine de Bilbao, Présidente de Microsoft France, Colette Stallbaumer, Directrice Générale, copilote de Microsoft 365 et cofondatrice de WorkLab, et Patrick Chanezon, principal Cloud Advocate.\n\nIls partageront un cadre de réussite concret pour mener une transformation IA durable, en plaçant l'ambition humaine au cœur de la technologie. Des témoignages clients viendront illustrer concrètement ces stratégies, à l'image d'entreprises telles que Alstom, La Poste ou Iter, déjà engagées dans des projets IA à grande échelle.\n\nAu-delà des discours, le Microsoft AI Tour se veut résolument réaliste. La journée sera rythmée par des sessions techniques de 45 minutes, des ateliers immersifs et des workshops interactifs. Les participants pourront développer et tester des solutions IA en direct, accompagnés par des ingénieurs et experts Microsoft.\n\nExploitation des agents Azure AI, optimisation de Copilot, protection des environnements cloud IA : autant de thématiques concrètes pour repartir avec des compétences immédiatement activables. Lors de la précédente édition, plus de 150 agents IA avaient d'ailleurs été créés en une seule journée par les clients présents.\n\nLors de l'édition précédente, plus de 30 entreprises partageaient leurs stratégies et enseignements autour de l'IA : Engie, Chanel, Amundi, Generali, Auchan ou encore Total Energies. Autant de retours et cas d'usages qui montrent comment l'IA transforme déjà l'expérience collaborateur et la relation client. Ces témoignages concrets permettent aux participants de se projeter, de limiter les erreurs et d'identifier les opportunités adaptées à leur propre organisation.\n\nLe Microsoft AI Tour, c'est aussi un espace de rencontres. Échanger entre pairs, dialoguer avec les experts Microsoft, découvrir les innovations portées par les partenaires clés, dont le sponsor mondial de l'événement NVIDIA, et créer des connexions durables.\n\nÉvénement gratuit, concentré sur une journée, le Microsoft AI Tour Paris s'adresse à toutes les organisations et dirigeants qui souhaitent prendre une longueur d'avance sur l'IA. Cas d'usage concrets, visions stratégiques, ateliers pratiques et retours d'expérience : tout est réuni pour repartir avec une feuille de route claire.\n\nL'IA n'attend pas. Les entreprises les plus performantes l'ont déjà compris et l'utilisent pour créer un impact réel. À Paris, Microsoft vous propose de passer à l'étape suivante.\n\nLieu : Paris Expo Porte de Versailles - 1 Place de la Porte de Versailles, 75015 Paris"
  },
  {
    "source": "Génération-NT",
    "company": "Microsoft AI",
    "title": "Remplacés par l'IA en 18 mois : la prédiction choc du patron de Microsoft AI !",
    "date": "2026-02-16T13:16:22Z",
    "url": "https://www.generation-nt.com/actualites/mustafa-suleyman-microsoft-ai-openai-2070825",
    "content": "Lire sur mobile Mustafa Suleyman, le PDG de Microsoft AI, a lancé une prédiction audacieuse : la plupart des tâches des \"cols blancs\" pourraient être entièrement automatisées par l'intelligence artificielle d'ici 12 à 18 mois. En parallèle, il confirme la nouvelle stratégie de Microsoft : développer ses propres modèles d'IA pour réduire sa dépendance historique à son partenaire OpenAI et viser la \"superintelligence\".\n\nLa déclaration a l'effet d'une bombe : Le patron de la division IA de Microsoft, Mustafa Suleyman, a affirmé que les métiers de bureau, du juriste au chef de projet en passant par le comptable, verront la majorité de leurs tâches automatisées dans un futur très proche. Une transformation radicale qui s'inscrit dans une stratégie plus large pour le géant de la tech : l'indépendance. Fini le temps où Microsoft se contentait d'être propulsé par l'IA d'un autre. La firme de Redmond accélère pour devenir un acteur autosuffisant dans cette course technologique effrénée.\n\nLa mission que s'est fixée Mustafa Suleyman chez Microsoft est claire : construire une \"superintelligence\". Pour y parvenir, l'entreprise ne peut plus dépendre exclusivement d'un tiers, même d'un partenaire aussi proche qu'OpenAI. Cet objectif d'autosuffisance en IA est désormais au cœur de la stratégie de la société. Ce virage a été acté par une restructuration de l'accord entre les deux entités en octobre 2025, donnant à chacune plus de liberté pour poursuivre ses propres objectifs.\n\nConcrètement, la division Microsoft AI développe déjà ses propres modèles fondamentaux, comme le projet MAI-1-preview, un modèle puissant entraîné sur des milliers de processeurs Nvidia. L'idée est de l'intégrer à terme dans certains usages de Copilot, remplaçant ainsi directement les technologies d'OpenAI. Microsoft diversifie aussi ses fournisseurs en hébergeant des modèles concurrents (Meta, Mistral, xAI) et aurait même testé des solutions d'Anthropic pour ses suites bureautiques.\n\nSelon Suleyman, l'IA atteindra des \"performances de niveau humain\" sur la quasi-totalité des tâches professionnelles de bureau en moins de deux ans. Cette automatisation complète ne signifie pas encore le remplacement de l'humain, mais une transformation profonde de la manière de travailler. Le secteur de l'ingénierie logicielle en est déjà le témoin, où le codage assisté par IA est devenu la norme, augmentant la productivité mais aussi la charge de travail et une certaine \"fatigue de l'IA\".\n\nLe débat sur la dépendance technologique est central. Actuellement, de nombreux outils de productivité s'appuient sur les modèles développés par OpenAI, mais cette dynamique est en train de changer. La prédiction de Suleyman repose sur la croissance exponentielle de la puissance de calcul (\"compute\"), qui permettra de créer des IA capables de surpasser les compétences humaines dans des domaines jusqu'ici protégés, comme le marketing, la finance ou le droit.\n\nL'annonce a ravivé l'inquiétude des investisseurs. Wall Street observe avec méfiance la forte dépendance de Microsoft à l'égard d'OpenAI, qui représente une part conséquente de ses futures ventes. Suite à un appel aux résultats où cette \"exposition\" a été soulignée, l'action de Microsoft a subi une perte historique de 357 milliards de dollars en une seule journée, illustrant la nervosité ambiante.\n\nPourtant, la réalité du terrain est plus nuancée. Si certains dirigeants, comme celui d'Anthropic, partagent ces prévisions apocalyptiques, des études montrent que l'IA a pour l'instant un impact limité sur la productivité en dehors du secteur technologique. Parfois même, elle peut ralentir certaines tâches. Entre les prophéties d'une révolution imminente et les résultats mesurés, l'avenir du travail de bureau reste une équation complexe à résoudre.\n\nMAI-1-preview est un modèle d'intelligence artificielle \"mélange d'experts\" développé en interne par Microsoft. Il est conçu pour être très performant et vise à terme à remplacer les modèles d'OpenAI dans certains produits Microsoft comme Copilot, marquant une étape clé vers l'indépendance de l'entreprise en matière d'IA.\n\nNon, le partenariat n'est pas terminé, mais il a été restructuré. Microsoft reste un actionnaire majeur d'OpenAI et conserve des droits sur ses technologies. Cependant, le nouvel accord accorde plus d'indépendance aux deux entreprises pour développer leurs propres stratégies, y compris la recherche d'une intelligence artificielle générale (AGI) de leur côté.\n\nPour le PDG de Microsoft AI, la \"superintelligence\" représente l'objectif ultime : la création d'une IA qui dépasse l'intelligence humaine dans tous les domaines. C'est la mission principale qu'il s'est fixée, justifiant les investissements massifs dans la puissance de calcul et le développement de modèles d'IA propriétaires."
  },
  {
    "source": "WebProNews",
    "company": "Microsoft AI",
    "title": "Microsoft's Quiet Divorce From OpenAI: Inside the Tech Giant's Billion-Dollar Bet on AI Self-Sufficiency",
    "date": "2026-02-14T09:48:57Z",
    "url": "https://www.webpronews.com/microsofts-quiet-divorce-from-openai-inside-the-tech-giants-billion-dollar-bet-on-ai-self-sufficiency/",
    "content": "For the better part of three years, Microsoft and OpenAI have been the most celebrated partnership in artificial intelligence -- a symbiotic alliance that reshaped the technology industry and sent Microsoft's market capitalization soaring past $3 trillion. Now, that partnership is entering a new and far more complicated chapter, one in which Microsoft is methodically building the infrastructure, models, and talent to reduce its dependence on the very company it helped create.\n\nIn a series of recent disclosures and strategic moves, Microsoft's AI leadership has made clear that the company intends to develop its own frontier AI models and custom silicon, effectively constructing a parallel path that could eventually render its reliance on OpenAI's technology optional rather than essential. The implications for both companies -- and for the broader AI industry -- are profound.\n\nMustafa Suleyman Lays Out the Vision\n\nThe most explicit confirmation of Microsoft's strategic direction came from Mustafa Suleyman, the company's head of AI, who told the Financial Times that Microsoft is actively building its own large language models capable of powering its consumer and enterprise AI products. Suleyman, who joined Microsoft in March 2024 after co-founding DeepMind and later leading Inflection AI, has been tasked with an ambitious mandate: make Microsoft a self-sufficient AI powerhouse.\n\n\"We are building our own frontier models,\" Suleyman stated, according to the Financial Times report. The comments represent the clearest public acknowledgment yet that Microsoft views its current arrangement with OpenAI -- in which it pays billions for the right to use OpenAI's models across its product suite -- as a transitional phase rather than a permanent architecture. As Windows Central reported, this amounts to Microsoft confirming its plan to move away from OpenAI as the ChatGPT maker continues to seek massive new rounds of funding from other technology giants.\n\nThe MAI Models and Custom Silicon Push\n\nMicrosoft's in-house AI ambitions are not merely aspirational -- they are already producing tangible results. The company has been developing a family of models under the \"MAI\" branding, with MAI-1 representing its first serious attempt at a large-scale foundation model. According to Quartz, Microsoft has been quietly training these models on its Azure infrastructure, leveraging both commercial GPUs from Nvidia and its own custom AI chip, the Maia 100. The Maia chip, which entered production in late 2023, is designed specifically to handle AI training and inference workloads, giving Microsoft the kind of vertical integration that has long been the hallmark of companies like Google and Apple.\n\nThe development of proprietary silicon is particularly significant. Every major AI model requires enormous computational resources, and the companies that control their own chip supply chains enjoy structural cost advantages and reduced vulnerability to supply constraints. By building Maia, Microsoft is signaling that it views AI infrastructure not as a commodity to be purchased but as a strategic asset to be owned. As CXOToday noted, the company's pursuit of AI self-sufficiency extends from the chip level all the way up through the model layer and into consumer-facing applications like Copilot.\n\nA $13 Billion Relationship Under Strain\n\nMicrosoft has invested approximately $13 billion in OpenAI since 2019, making it by far the startup's largest backer. In return, Microsoft secured exclusive cloud-hosting rights for OpenAI's models on Azure, along with the ability to integrate GPT-series models into products ranging from Microsoft 365 to Bing to GitHub Copilot. The arrangement was extraordinarily lucrative for both sides: OpenAI gained the computational resources it needed to train increasingly powerful models, while Microsoft gained an AI capability that its competitors could not easily replicate.\n\nBut the relationship has grown increasingly strained. OpenAI's restructuring from a nonprofit to a for-profit entity, its soaring valuation -- now reportedly in excess of $300 billion following its latest funding discussions -- and CEO Sam Altman's aggressive pursuit of additional capital from sovereign wealth funds and rival technology companies have all introduced new tensions. As The News International reported, Microsoft's expansion of in-house AI capabilities is a direct response to the growing complexity and cost of its OpenAI dependency.\n\nThe Economics of Dependence\n\nAt the heart of Microsoft's strategic recalculation is a straightforward economic reality: paying for access to another company's models is expensive, and the costs are only increasing. OpenAI's latest models, including GPT-4o and the forthcoming GPT-5, require massive computational investments to train and serve. Microsoft bears a significant portion of these costs through its Azure infrastructure commitments, while simultaneously paying licensing fees for model access. As OpenAI's ambitions grow -- the company has discussed raising as much as $40 billion in its next funding round -- the financial demands on Microsoft are likely to escalate further.\n\nBy developing its own models, Microsoft can internalize these costs and capture the full economic value of AI integration across its product ecosystem. The company's Copilot platform, which is being embedded into virtually every Microsoft product, currently relies heavily on OpenAI's models. Transitioning to proprietary models would allow Microsoft to optimize performance for its specific use cases, reduce per-query costs, and eliminate the strategic risk of depending on a partner whose interests may not always align with its own. Filmogaz highlighted that this strategic pivot reflects a broader pattern in the technology industry, where major platforms are increasingly seeking to own their AI stacks end-to-end.\n\nThe Superagent Race Intensifies\n\nMicrosoft's push for AI independence is unfolding against the backdrop of an intensifying race to build what industry insiders are calling \"superagents\" -- AI systems capable of autonomously executing complex, multi-step tasks across enterprise workflows. According to The Information, this new competitive frontier is pitting OpenAI, Anthropic, Microsoft, and Salesforce against one another in a contest that could determine the next generation of enterprise software dominance.\n\nFor Microsoft, the superagent opportunity is both enormous and existential. The company's enterprise installed base -- hundreds of millions of users across Office, Teams, Dynamics, and Azure -- gives it unmatched distribution for AI-powered agents. But to fully exploit this advantage, Microsoft needs models that are tightly integrated with its infrastructure and optimized for its specific workloads. Relying on OpenAI for the underlying intelligence layer introduces latency, cost, and strategic vulnerability that Microsoft can ill afford in a market moving at breakneck speed.\n\nSuleyman's Track Record and Mandate\n\nThe appointment of Mustafa Suleyman to lead Microsoft's AI efforts was itself a signal of the company's intentions. Suleyman co-founded DeepMind, which Google acquired in 2014 and which has since become one of the world's premier AI research organizations. He later founded Inflection AI, which Microsoft effectively absorbed in early 2024, hiring most of its staff and licensing its technology. Suleyman brings both the technical credibility and the organizational experience needed to build a world-class AI research operation from within a large corporation.\n\nUnder Suleyman's leadership, Microsoft AI has been aggressively recruiting researchers and engineers, many of them drawn from OpenAI itself. The talent migration has added another layer of tension to the Microsoft-OpenAI relationship, as key personnel who helped build GPT-series models are now working on competing efforts inside Microsoft. Suleyman's mandate, as described in multiple reports, is nothing less than to make Microsoft capable of producing frontier-class AI models entirely on its own -- a goal that, if achieved, would fundamentally alter the power dynamics of the industry.\n\nOpenAI's Precarious Position\n\nFor OpenAI, Microsoft's strategic pivot raises uncomfortable questions about the durability of its most important commercial relationship. While OpenAI has diversified its revenue streams -- ChatGPT now has over 200 million weekly active users, and the company reportedly generated over $3.4 billion in annualized revenue in late 2024 -- Microsoft remains its single largest customer and infrastructure provider. A gradual reduction in Microsoft's reliance on OpenAI's models could significantly impact the startup's revenue trajectory and bargaining position in future negotiations.\n\nOpenAI has responded by accelerating its own efforts to build consumer products, enterprise solutions, and even custom hardware. The company has also been seeking new investors and partners to reduce its own dependence on Microsoft, creating a dynamic in which both parties are simultaneously collaborating and hedging against each other. This delicate balancing act is likely to define the relationship for the foreseeable future, even as both companies publicly maintain that their partnership remains strong.\n\nWhat This Means for Enterprise Customers and the AI Industry\n\nMicrosoft's move toward AI self-sufficiency will have ripple effects across the enterprise technology market. For Azure customers, the transition could eventually mean access to a broader portfolio of AI models -- both Microsoft's proprietary offerings and third-party models from OpenAI and others -- with Microsoft increasingly steering customers toward its own technology. For competitors like Google, Amazon, and Salesforce, Microsoft's in-house model development validates the strategy of vertical integration and raises the competitive bar for cloud AI services.\n\nThe broader AI industry is watching closely. If Microsoft succeeds in building frontier models that rival OpenAI's, it will demonstrate that the era of AI model scarcity is ending and that the real competitive advantages lie in distribution, integration, and data access rather than in model capability alone. This would be a seismic shift, one that could reshape investment patterns, partnership structures, and the balance of power among the technology giants for years to come.\n\nFor now, Microsoft and OpenAI remain bound together by contracts, shared infrastructure, and mutual financial interest. But the direction of travel is unmistakable. Microsoft is building a future in which it no longer needs OpenAI -- and in Silicon Valley, the most dangerous partner is always the one who is preparing to walk away."
  },
  {
    "source": "The Financial Express",
    "company": "Microsoft AI",
    "title": "'Economic earthquake': Microsoft AI CEO Suleyman predicts automation of most white-collar work 'within 12-18 months'",
    "date": "2026-02-14T07:17:53Z",
    "url": "https://www.financialexpress.com/life/technology-economic-earthquake-microsoft-ai-ceo-suleyman-predicts-automation-of-most-white-collar-work-within-12-18-months-4142485/",
    "content": "His remarks echo similarly apocalyptic predictions made by CEOs over the past year as AI grew increasingly 'smarter' | This is an AI generated image\n\nArtificial intelligence has disrupted global workforces over the past year -- triggering mass layoffs, creating new job categories and drastically shifting skill demands. Microsoft AI CEO Mustafa Suleyman warned this week that the situation would only worsen in the coming months. He predicted that most white-collar jobs would be lost to automation by next year. His remarks echo similarly apocalyptic predictions made by CEOs over the past year as AI grew increasingly 'smarter'.\n\n\"I think we're going to have a human-level performance on most, if not all, professional tasks. So white-collar work where you're sitting down at a computer...either being a lawyer or an accountant or a project manager or a marketing person...most of those tasks will be fully automated by an AI within the next 12 to 18 months,\" he told Financial Times during a recent interview.\n\nHe cited software engineering to underscore his point -- noting that many now use AI-assisted coding for much of their production. Suleyman noted that the focus was now shifting to tangential jobs.\n\n'Economic earthquake'\n\n\"Microsoft AI CEO Mustafa Suleyman says most white-collar work \"will be fully automated by an AI within the next 12 to 18 months.\" If that's true, it's an economic earthquake. We need a moratorium on new AI data centers to make sure AI works for workers, not just billionaires,\" Senator Bernie Sanders wrote on X.\n\n\"Mustafa Suleyman just gave professionals their termination date. Microsoft AI's CEO didn't hedge. Lawyers, accountants, knowledge workers: most of what you do daily disappears within 12 to 18 months. Not transformed. Erased,\" wrote another X user.\n\nGrok told users that the cuts would likely apply to jobs such as administrative support (scheduling, data entry), basic accounting, customer service, content creation, legal research, and even some software coding. The chatbot insisted that a complete automation within 12 to 18 months was \"ambitious\" -- insisting that many roles would evolve rather than disappear."
  },
  {
    "source": "News18",
    "company": "Microsoft AI",
    "title": "Microsoft AI Head Says Majority Of Office Jobs May Be Automated In 12 Months",
    "date": "2026-02-14T06:58:09Z",
    "url": "https://www.news18.com/business/economy/microsoft-ai-head-says-majority-of-office-jobs-may-be-automated-in-12-months-ws-l-9903038.html",
    "content": "In a stanch prescient warning, Microsoft AI CEO Mustafa Suleyman has forewarned that most white-collar jobs will be automated within 12 months, amid the rising proficiency of Artificial Intelligence. He said that AI will reach \"human-level performance\", posing a direct challenge to jobs that are currently being done by white-collar employees.\n\nMustafa Suleyman, the Microsoft AI chief, said in an interview with the Financial Times that he predicts most, if not every, task in white-collar fields will be automated by AI within the next year or year and a half.\n\n\"So white-collar work, where you're sitting down at a computer, either being a lawyer or an accountant or a project manager or a marketing person -- most of those tasks will be fully automated by an AI within the next 12 to 18 months.\" Suleyman added in the interview.\n\nMicrosoft AI CEO said that software engineering has already seen the trend, where most employees are using \"AI-assisted coding for the vast majority of their code production\".\n\nThe warning of Suleyman isn't out of thin air. Most top executives have warned of mass-level disruption globally, especially in white-collar jobs in the coming days due to AI. The ripple of impact will be observed by most sectors in the near future, even among software developer jobs.\n\nRecently, advancements in AI has triggered panic among investors of technology companies, who are doubting the business viability of these companies in the coming times when AI will be leveraged predominantly."
  },
  {
    "source": "Breitbart",
    "company": "Microsoft AI",
    "title": "Microsoft AI Boss Mustafa Suleyman: Most White-Collar Jobs Will Be Automated Within 18 Months",
    "date": "2026-02-13T17:54:56Z",
    "url": "https://www.breitbart.com/tech/2026/02/13/microsoft-ai-boss-mustafa-suleyman-most-white-collar-jobs-will-be-automated-within-18-months/",
    "content": "Mustafa Suleyman, the CEO of Microsoft AI, has predicted that AI will be capable of automating the vast majority of white-collar professional tasks within the next 12 to 18 months.\n\nThe Financial Times reports that Mustafa Suleyman, who leads Microsoft's AI division, has made a bold prediction about the near-term impact of AI on white-collar professions. In an interview with the Times published this week, Suleyman stated that he expects most, if not all, tasks performed by white-collar workers will be fully automated by AI within the next 12 to 18 months.\n\nAccording to Suleyman, AI systems will achieve human-level performance across a wide range of professional duties. \"I think that we're going to have a human-level performance on most, if not all, professional tasks,\" Suleyman said in the interview. \"So white-collar work, where you're sitting down at a computer, either being a lawyer or an accountant or a project manager or a marketing person -- most of those tasks will be fully automated by an AI within the next 12 to 18 months.\"\n\nThe Microsoft AI chief pointed to software engineering as an early indicator of this trend. He noted that developers are already using AI-assisted coding for the majority of their code production, representing a fundamental shift in how the work is performed. \"It's a quite different relationship to the technology, and that's happened in the last six months,\" he said.\n\nThe rapid advancement of artificial intelligence over the past several years has already begun to transform white-collar work in observable ways. Recent reporting has highlighted the emergence of what some call \"AI fatigue\" among software engineers, where the technology has delivered productivity gains but also brought increased exhaustion as workers face pressure to handle larger workloads simultaneously.\n\nMicrosoft has positioned itself at the forefront of workplace AI integration. The company has developed products such as Copilot and made significant investments in AI companies including OpenAI and Anthropic, cementing its role as a major force in bringing artificial intelligence tools to professional environments.\n\nBreitbart News previously reported that Microsoft has integrated AI into Windows without fully understanding the security risks this creates:\n\nSecurity concerns stem from known defects inherent in most large language models (LLMs), including Copilot. Researchers have repeatedly demonstrated that LLMs can provide factually erroneous and illogical answers, a behavior known as \"hallucinations.\" This means users cannot fully trust the output of AI assistants like Copilot, Gemini, or Claude, and must independently verify the information.\n\nAnother significant issue with LLMs is their vulnerability to prompt injections. Hackers can exploit this flaw by planting malicious instructions in websites, resumes, and emails, which the AI eagerly follows without discerning between valid user prompts and untrusted, third-party content. These vulnerabilities can lead to data exfiltration, malicious code execution, and cryptocurrency theft.\n\nSuleyman's prediction adds his voice to a growing number of AI industry leaders who have warned about the potential for widespread job displacement due to artificial intelligence. Stuart Russell, a prominent computer scientist who co-authored one of the most authoritative textbooks on AI, stated in an interview last year that political leaders are confronting the possibility of 80 percent unemployment driven by AI. Russell suggested that positions ranging from surgeons to chief executives could be at risk of replacement.\n\nSimilarly, Dario Amodei, the CEO and cofounder of Anthropic, has previously warned that AI could eliminate approximately half of all entry-level white-collar positions. \"We, as the producers of this technology, have a duty and an obligation to be honest about what is coming,\" Amodei told Axios in an interview. \"I don't think this is on people's radar.\"\n\nThe impact of AI on the American economy is one area of focus for the upcoming book by Breitbart News Social Media Director Wynton Hall, Code Red: The Left, the Right, China, and the Race to Control AI:\n\nIn Code Red, Breitbart social media director Wynton Hall exposes where that power hides, how it operates, how conservatives can navigate the AI political battlescape, avert its landmines, and turn peril into promise. AI decides what you see and what gets censored. It's quietly rewiring our whole way of life. Jobs. Schools. Family. Church. Even national security. All of it will shock-test our civic order.\n\nInside Code Red, you will discover:\n\nUrgent, deeply researched, and written with page-turner elegance, Code Red is the conservative battle plan for the AI era. Either we wake up and fight back, or we lose everything that made America free.\n\nRead more at the Financial Times here."
  },
  {
    "source": "ForkLog",
    "company": "Microsoft AI",
    "title": "Microsoft AI Chief Predicts Automation of White-Collar Jobs Within 18 Months | ForkLog",
    "date": "2026-02-13T13:29:30Z",
    "url": "https://forklog.com/en/microsoft-ai-chief-predicts-automation-of-white-collar-jobs-within-18-months/",
    "content": "AI to automate white-collar jobs within 18 months, says Microsoft AI CEO.\n\nThe majority of professional tasks performed by white-collar workers will be automated within two years, according to Mustafa Suleyman, CEO of Microsoft AI. This shift will impact employees across various fields such as law, accounting, and marketing.\n\nIn an interview with the Financial Times, the executive noted that artificial intelligence is approaching \"human-level performance\" in most office tasks.\n\n\"White-collar work, where you sit at a computer, whether as a lawyer, accountant, project manager, or marketer, will largely be automated by AI within the next 12-18 months,\" he stated.\n\nSuleyman cited software development as an example, noting that Microsoft programmers are employing neural networks in most work processes.\n\n\"This is already a different relationship with technology, and it has developed literally over the past six months,\" the expert added.\n\nSuleyman emphasized that due to increased computing power, AI models already surpass most programmers. This technological shift justifies substantial investments in companies, including OpenAI.\n\nDespite supporting the developer of ChatGPT, the CEO of Microsoft AI stressed the need to develop proprietary AI products to enhance resilience and independence.\n\n\"We must develop our own cutting-edge models, with gigawatt-scale computing and some of the world's best LLM training teams. This is our mission of technological self-sufficiency,\" he said.\n\nSuleyman anticipates the emergence of \"professional AGI\" -- a form of general artificial intelligence capable of performing most cognitive tasks at a human level.\n\nEconomists confirm these concerns: everyone whose work is directly related to computers is at risk of automation.\n\n\"The highest risk is for professions requiring higher education, with higher pay and predominantly cognitive tasks,\" noted Rand Corporation economist Tobias Sitsma.\n\nPoliticians are also seeking ways to adapt to the displacement of human labor. Senator Bernie Sanders has already planned a visit to California to discuss this issue with industry representatives.\n\nSuleyman's view aligns with that of other tech leaders. In January, Robinhood CEO Vlad Tenev stated that AI will not only displace some professions but also lead to the creation of new jobs, solo companies, and entire industries."
  },
  {
    "source": "NDTV",
    "company": "Microsoft AI",
    "title": "White-Collar Jobs Will Be Automated In 18 Months, Warns Microsoft AI Chief",
    "date": "2026-02-13T08:15:53Z",
    "url": "https://www.ndtv.com/world-news/white-collar-jobs-will-be-automated-in-18-months-warns-microsoft-ai-chief-mustafa-suleyman-10997627",
    "content": "According to Suleyman, AI will redefine employment rather than completely replace it (Representational)\n\nShowQuick Read\n\nSummary is AI-generated, newsroom-reviewed\n\n* Most white-collar jobs may be automated by AI within 12 to 18 months, says Microsoft AI chief\n\n* Jobs like document inspection, financial analysis, and marketing will be fully automated soon\n\n* AI-assisted coding now handles most software code, letting engineers focus on complex tasks\n\nDid our AI summary help?\n\nLet us know.\n\nSwitch To Beeps Mode\n\nMost white-collar jobs may be automated by artificial intelligence (AI) in the next 12 to 18 months, Microsoft AI chief Mustafa Suleyman has warned.\n\nThese include jobs that currently take up a significant number of professional workloads, such as document inspection, financial analysis, compliance checks, marketing optimisation, scheduling, and customer communication, he told Financial Times.\n\n\"White collar work where you are sitting down at a computer, either being a lawyer, an accountant, a project manager or a marketing person. Most of those tasks will be fully automated by an AI within the next 12 to 18 months,\" Suleyman said.\n\n\"Many software engineers report that they are now using AI-assisted coding for the vast majority of their code production,\" Suleyman stated, citing software engineering as an early example.\n\nAccording to the Microsoft AI chief, AI-assisted coding tools now handle most of the code generation, allowing engineers to concentrate on higher-level activities like system architecture, debugging, verification, and deployment.\n\nThe developments in \"professional-grade\" AI systems are drastically altering the way knowledge workers function, Suleyman said.\n\nHe said such systems may carry out the majority of tasks at work at a human level, making a distinction between artificial general intelligence (AGI) and what he called \"superintelligence.\"\n\n\"I prefer the definition that focuses first on what it takes to build a system that could achieve most of the tasks that a regular professional in a workplace goes about on a daily basis. Think of it as a professional-grade AGI,\" Suleyman expressed.\n\nSuleyman asserted that the standard for human-level performance is approaching more quickly than expected in several professional domains. \"I think that we're going to have a human-level performance on most if not all professional tasks,\" he said.\n\nWatch the full conversation here:\n\nAccording to Suleyman, AI will redefine employment rather than completely replace it. Professionals may go from carrying out tasks directly to strategic and managerial positions, which include controlling exceptions, establishing goals, and assessing AI outputs.\n\nShow full article\n\nTrack Latest News Live on NDTV.com and get news updates from India and around the world\n\nMustafa Suleyman, AI To Take White-collar Jobs, Rapid Automation Of White-Collar Work"
  },
  {
    "source": "freedomsphoenix.com",
    "company": "Microsoft AI",
    "title": "Microsoft AI CEO Warns Most White Collar Jobs Fully Automated \"Within Next 12-18 Months\";",
    "date": "2026-02-12T23:44:25Z",
    "url": "https://www.freedomsphoenix.com/News/391285-2026-02-12-microsoft-ai-ceo-warns-most-white-collar-jobs-fully-automated.htm",
    "content": "Microsoft AI CEO Warns Most White Collar Jobs Fully Automated \"Within Next 12-18 Months\";\n\nThe man leading Microsoft's AI sprawling efforts is sounding the alarm over imminent mass labor disruptions, warning that the overwhelming majority of white-collar professional work could vanish to automation far sooner than most business and policy leaders are willing to admit - something we've been concerned about since early 2023.\n\nIn an interview with the Financial Times, Microsoft AI CEO Mustafa Suleyman forecasted that within the next two years a vast swath of desk-bound tasks will be swallowed by AI.\n\n\"I think we're going to have a human-level performance on most, if not all, professional tasks - so white collar where you're sitting down at a computer, either being a lawyer, accountant, or project manager, or marketing person - most of the tasks will be fully automated by an AI within the next 12 to 18 months,\" Suleyman said when asked about the time table for Artificial general intelligence, commonly known as AGI."
  },
  {
    "source": "SDPnoticias.com",
    "company": "Microsoft AI",
    "title": "CEO de Microsoft AI advierte que la IA automatizará tareas profesionales en un año",
    "date": "2026-02-12T17:49:40Z",
    "url": "https://www.sdpnoticias.com/tecnologia/ceo-de-microsoft-ai-advierte-que-la-ia-automatizara-tareas-profesionales-en-un-ano",
    "content": "\"La mayoría de las tareas que realizan los contadores, abogados y otros profesionales estarán completamente automatizadas en los próximos 12 a 18 meses.\"\n\nMustafa Suleyman, CEO de Microsoft AI\n\nY es que de acuerdo al CEO de Microsoft, predijo en dicha entrevista que tareas de profesionistas como contadores y abogados, se automatizarán por completo en 12 a 18 meses gracias a la Inteligencia Artificial.\n\nMustafa Suleyman, CEO de Microsoft AI se ha hecho viral en las redes sociales por sus recientes declaraciones sobre la Inteligencia Artificial, pues aseguró que varias tareas profesionales serán reemplazadas.\n\nNo obstante existe escepticismo ente usuarios, pues hay quien dice que la IA no podría reemplazar el criterio humano en áreas como el derecho, como Suleyman aseguró en su entrevista.\n\nEs por eso que en redes sociales ha surgido una controversia, pues hay quienes critican las declaraciones de Mustafa Suleyman, CEO de Microsoft AI como una forma de estrategia para devaluar salarios en profesiones técnicas.\n\nNo obstante, cabe resaltar que estas declaraciones por parte del CEO de Microsoft, llega en un momento en donde se ha visto el avance acelerado de la IA, con estudios como el de McKinsey en 2023 que estiman que el 45% de las actividades laborales podrían automatizarse.\n\nPero de igual manera, ha habido algunos expertos que advierten que, de pasar estas automatizaciones en tareas profesionales, habría que cuestionarse las limitaciones éticas y regulatorias de la Inteligencia Artificial por sobre la humana."
  },
  {
    "source": "Stories",
    "company": "Microsoft AI",
    "title": "Microsoft accelerates AI skilling in Saudi Arabia, helping 3 million people acquire AI skills by 2030 - Source EMEA",
    "date": "2026-02-12T17:45:54Z",
    "url": "https://news.microsoft.com/source/emea/2026/02/microsoft-accelerates-ai-skilling-in-saudi-arabia-helping-3-million-people-acquire-ai-skills-by-2030/",
    "content": "From momentum to scale: More than 800,000 people have already completed essential AI training, and a commitment to help a further three million Saudis acquire AI skills by 2030.\n\nMicrosoft today announced a significant expansion of its AI skilling efforts in Saudi Arabia, building on the scale and impact of work delivered to date in support of Vision 2030 workforce priorities. Announced at the Microsoft AI Tour Riyadh, the company outlined its ambition to help a further three million people across the Kingdom acquire AI skills by 2030, reinforcing its long-term commitment to developing future-ready talent and enabling economic growth at scale.\n\nAs part of this effort, Microsoft also announced the launch of Microsoft Elevate for Educators in Saudi Arabia, a new program designed to strengthen AI capability across the education ecosystem. The global community program provides free AI literacy credentials, access to one of the world's largest and most connected professional learning communities for educators and offers capacity building support and resources to organizations in the education ecosystem - enabling teachers and school leaders to integrate AI into teaching and learning in practical, responsible ways.\n\nWithin the broader ambition to help three million people acquire AI skills by 2030, Microsoft will focus on providing in-demand credentials to more than 500,000 educators across the Kingdom, alongside targeted initiatives to expand AI learning and skills for women, aligned with Vision 2030 objectives.\n\nMicrosoft's AI skilling efforts in Saudi Arabia build on a long-standing partnership with the Ministry of Communications and Information Technology (MCIT) and a broad national ecosystem to deliver impact at scale. Over the past few years, Microsoft has engaged more than one million people in AI, cloud, and data programs across the Kingdom, with over 800,000 learners completing training, far exceeding initial targets.\n\nThis momentum is reinforced by Microsoft's collaboration with the Ministry of Education, including the nationwide deployment of Microsoft 365 across schools and universities. By embedding secure digital and AI tools into everyday teaching and learning, this initiative is helping equip millions of students, educators, and administrators with practical digital and AI capabilities, strengthening the foundation for long-term talent development.\n\nKey national initiatives include the Microsoft AI Academy with Saudi Data & Artificial Intelligence Authority (SDAIA), which has recorded over one million enrollments, as well as Microsoft's central role in SDAIA's flagship Samai Initiative, where Microsoft provided more than two-thirds of the national AI curriculum that supported learning for over one million Saudis in AI fundamentals - one of the largest digital skilling efforts ever undertaken in the Kingdom.\n\nThese efforts also include the upcoming launch of the Microsoft Datacenter Academy, the first of its kind community-focused workforce and development skilling program in the Middle East, and large-scale programs that have supported skills development for 1,000 government employees in a single quarter with Digital Government Authority (DGA), more than 109,000 educators, and thousands of women with future-ready AI skills. Microsoft has also delivered the AI Global Leadership Program in partnership with MCIT, equipping more than 120 senior executives from Saudi's public and private sector with in-depth AI training at Microsoft innovation and experience centers in Amsterdam, Munich and Seattle.\n\nH.E. Eng. Abdullah bin Amer Al-Swaha, Minister of Communications and Information Technology (MCIT), said: \"We thank and welcome Microsoft's expanded commitment to AI skilling in the Kingdom, including its ambition to help three million people acquire AI skills by 2030. This strategic partnership strengthens our national talent pool, empowering our growing tech force and expanding opportunities for women in high-demand AI roles. Together, we are investing in people at scale, so innovation translates into productivity, new startups, and growth.\""
  },
  {
    "source": "mint",
    "company": "Microsoft AI",
    "title": "Microsoft AI chief Mustafa Suleyman says most white-collar jobs will be automated in 12-18 months: Is your role at risk? | Company Business News",
    "date": "2026-02-12T17:16:32Z",
    "url": "https://www.livemint.com/companies/people/microsoft-ai-chief-mustafa-suleyman-says-most-white-collar-jobs-will-be-automated-in-12-18-months-is-your-role-at-risk-11770910567664.html",
    "content": "Microsoft AI CEO Mustafa Suleyman warns that AI may soon automate many white-collar jobs, including roles such as those of lawyers and accountants. Here are the details.\n\nThe chief executive officer of Microsoft AI, Mustafa Suleyman, has warned that Artificial Intelligence could replace a large share of white-collar jobs within the next 12-18 months.\n\nThis rapid pace of technological advancement will not just impact coders and software engineers. It is also expected to significantly affect professionals such as lawyers and accountants, many of whom could see their work being automated by AI.\n\nIn a video interview with the Financial Times, Suleyman revealed that Microsoft has been pushing for a bigger share in the enterprise market with \"professional-grade AGI (Artificial General Intelligence).\"\n\nSpeaking of the plan, he described the system as an AI model that can do almost everything a human professional is capable of doing. Such technology would allow Microsoft to offer powerful AI tools to its customers, helping them handle routine tasks that are usually done by office workers.\n\nThe Microsoft AI chief further explained, \"White-collar jobs, essentially those sitting in front of computers, whether lawyers, accountants, project managers, or marketers, most of these tasks will be fully automated by AI within the next 12 to 18 months.\"\n\nThe top executive's remarks come at a time when companies around the world are increasingly adopting AI as a cost-cutting measure, which in turn is leading to the elimination of certain positions.\n\nUS-based cloud services platform Salesforce allegedly laid off up to 1,000 employees at the beginning of this month, amid the company CEO's push towards transitioning to artificial intelligence for some of its roles. Many such reports have also emerged over the last few weeks, involving major tech companies such as Amazon, FedEx, and Ericsson.\n\n\"Creating a new model will be as simple as making a podcast or writing a blog. In the future, it will be possible to design AI tailored to the needs of every institution and individual on Earth,\" he said.\n\nSuleyman also added that within the next two to three years, AI agents will become more efficient, and capable enough to handle the workflow of large institutions and businesses.\n\nAdditionally, the Microsoft executive also hinted that the company is planning to increase the production of its own AI models gradually, with an aim to reduce its dependence on OpenAI, after the two companies reached a new agreement.\n\nHe added, \"We decided that this was a moment when we have to set about delivering on true AI self-sufficiency.\" Suleyman has teased that the new AI models from Microsoft may debut in 2026."
  },
  {
    "source": "Financial Times News",
    "company": "Microsoft AI",
    "title": "Mustafa Suleyman sets out Microsoft AI's goal of 'human superintelligence'",
    "date": "2026-02-12T05:11:58Z",
    "url": "https://www.ft.com/video/2c428045-bf4f-45bd-ada2-8ba53983cd81",
    "content": "You can enable subtitles (captions) in the video player\n\nIs AI a bubble? Are companies overspending? Who's winning the AI race? And how do we recognise victory? Mustafa Suleyman, CEO of Microsoft AI, is here to answer these questions and much more. Welcome, Mustafa.\n\nThank you. Good to be here.\n\nLet's start with last week. There were breathtaking increases in capex spending by AI companies that reported, and perhaps for the first time markets were nervous. They really want to start to see revenue. Microsoft's stock suffered as well. What was your reaction? Did that make you feel like, I'm under pressure, I need to deliver a lot faster?\n\nI think that there's no question these are unprecedented times. And I think markets are trying to wrap their head around how this plays out over the next five years. The interesting thing is that I think a lot of us at the companies and certainly the generation before me have seen multiple of these cycles over the last 30 years. And they often require unprecedented action in order to really actually land one of these big waves.\n\nAnd this is a wave unlike anything anyone's ever seen. The prospect of being able to create an intelligence, the very thing that has made us successful as a species and create everything of value in our world, I think, is just unprecedented. And the progress that has been made just in the last two or three years is eye-watering. And we've seen a very direct and unequivocal relationship between an order of magnitude increase in flops invested for computation and a pretty linear increase in capabilities.\n\nLike, over the last 15 years, there's been a $1tn-fold increase in training compute. In the next three years or so there will be a further 1,000X increase in training compute. And today, as a result, we have models that can code better than the vast majority of human coders, maybe even all of them to date. Some of them literally the inventors of things like Linux are publicly saying on Twitter that they're full time using these models as their primary method of generating new code. So that's pretty unprecedented, and I think it justifies unprecedented spend.\n\nBut will markets keep with you? I mean, you have to...\n\nI think markets are markets. They're sort of trying to figure it out.\n\nYeah.\n\nI think that's true. I mean, markets have to have to figure it out. And I think there's a lot of open questions around the timeline. I think that we all have no doubt that these returns do compound to revenue and to bottom line. So we'll see.\n\nSo Microsoft still has the deal with OpenAI. And Copilot is powered today by OpenAI models. You were hired last year. You're in your second year. Tell me a little bit what you are working on.\n\nYeah, I mean, my personal mission at Microsoft is to build superintelligence. Three or four months ago, having re-negotiated our long-term relationship with OpenAI, we extended our IP licence through to 2032. We also decided that this was a moment when we have to set about delivering on true AI self-sufficiency. I mean, this, after all, is the most important technology of our time.\n\nSelf-sufficiency means developing your own foundation model.\n\nWe have to develop our own foundation models, which are at the absolute frontier with gigawatt scale compute, with some of the very best AI training team in the world, and collecting, paying for, organising, sorting all the data that we need to do that. And so that's our true self-sufficiency mission.\n\nAnd you talk about superintelligence. Most of your arrivals talk about AGI, artificial general intelligence. Explain the difference between AGI and superintelligence. And how do we know when we've reached AGI? How do we know when we've reached superintelligence? Yeah, it's become a very unhelpful, fuzzy concept, I think. Yeah, could you guys just simplify things for us?\n\nSort it out. Yeah, I mean, so I prefer the definition that focuses first on: what would it take to build a system that could achieve most of the tasks that a regular professional in a workplace goes about on a daily basis? Think of it as a professional grade AGI. And, beyond that, there would be teams of AGIs that are co-ordinated together by a sort of organisational AGI that really can run large institutions. And I think that's coming into view in the next two or three years' time. I think beyond...\n\nThese are AI agents that can essentially make decisions on their own without input from humans.\n\nI think that we would train them to seek input from humans, but they will fundamentally be creative. They'll have good judgement. They will be able to learn and improve themselves over time. They'll have some degree of autonomy and decide where to direct their attention or processing power. So these are the hallmarks of some of the best humans that we have in the world and some of the most effective organisations, so.\n\nYou are working on consumer AI, right? Aren't you late? Why would you come up with a consumer product when ChatGPT already has 800mn users, and Claude, the Anthropic AI, has very, very advanced now coding AI?\n\nYeah. I mean, just to put it into perspective, this is not going to be a world where there is one winner. There are going to be billions of digital minds. There are going to be many, many different lineages of model. Creating a new model is going to be like creating a podcast or writing a blog. It is going to be possible to design an AI that suits your requirements for every institution, organisation, and person on the planet. At Microsoft, we already have 800mn monthly active users engaging with our AI products. So this is still a giant property with billions of dollars of revenue delivering great value to our users. So that's how I think the landscape is going to play out.\n\nLet's go back to superintelligence. You talk about humanist superintelligence. How do you ensure that it's a humanist superintelligence?\n\nYeah, I mean, I published an essay on this question recently, and the main motivation was that I was starting to feel increasingly anxious that some of the other labs are making an assumption that a superintelligence that is smarter than all of us put together, all the intelligences in the world, is both inevitable and even desirable, and that such a system would probably be very hard to control, something that is so many times more intelligent than us.\n\nAnd I think that we have to reset that and make the assumption that we should only bring a system like that into the world that we are sure we can control and operates in a subordinate way to us, that humans remain at the top of the food chain, that these tools, like any other past technology, are designed to enhance human well-being and serve humanity, not exceed humanity. And some of the things that you hear from Elon often or even others in the field, you can clearly see that they're fixating on a world in 2050 or 2075 when they're going off and exploring other universes and conquering resources from other planets and stuff like that. And a system like that is unclear to me how it would have any time for preserving us as a species.\n\nDo you think that there are two conflicting forces here? One is speed because you are in a race, and the other is what you're talking about is ensuring that it's safe, ensuring that it is aligned, that it's... you also talk about containment. How do you deal with these two conflicting forces?\n\nI mean, they're definitely intention. And I think that we have to make a decision as a species to prioritise the creating superintelligences and AGIs that are aligned and that care about humans and want to protect humans and drive human well-being. And if we just accelerate and cut all those corners, then we're really taking a massive risk with the future of our species that I've been on record talking about for many, many, many years.\n\nDo you worry that in this AI race that we're seeing, at some point there will be some kind of a big accident and that will stop the development for whoever is responsible for that accident, but also for the rest of you? Do you think everything is just moving a bit too fast?\n\nI mean, what we saw a few weeks ago on Moltbook with various questions... Yes, I wanted to ask you about that.\n\nWell, I mean, that turned out, it seems, and it's always hard to tell. It turned out to be started by a gang of human engineers, and it was seeded much more than I think it was reported. Explain what Moltbook was.\n\nSo Moltbook was basically a social network for AIs to communicate with one another publicly on a messaging forum. And you had to declare yourself either as an AI or as a human. And then the AIs would have one section and they could talk to each other, learn from each other, and basically co-ordinate. And all these AIs come from different people. They're all open source. And there was a million and a half of them in the space of a week.\n\nAnd you see unbelievable emergent behaviours. I mean, they invented a new religion. They started communicating in a language called ROT13, which is a cypher language which basically regresses every letter by 13 characters in order to mask what's underneath it. Obviously, that's quite a simple algorithm, but if instead of 13, it were a unique number, that would be very hard for humans to decode. And then it turned out that they were actually talking about acquiring new resources, getting more training data, improving one another.\n\nAnd so it turned out to be an amazing safety simulation. No matter how autonomous it was, in hindsight, it doesn't really matter, I think we learned quite a lot from that episode, and everyone should pay close attention because in a year or two years' time, these systems truly are going to be capable of writing their own code. I mean, they can do that today. They'll be using arbitrary APIs. They'll be making phone calls to one another. They're doing that today. They'll be ordering things in the\n\nreal world. Apparently, there's some kind of movement asking for human rights...\n\nThat's right. That's the most concerning thing to me.\n\n...for artificial intelligence.\n\nIt's called the model welfare movement. And it's inspired by animal welfare or by human rights. And there is a growing belief in some labs, particularly inside of Anthropic, that these models are conscious. And if it's a conscious being that is aware of itself and can suffer, then it deserves our protection, our moral protection. And this is a very serious area of academic research, not just outside but inside some of the labs. And I think it's very concerning. It's totally without merit or basis. And if we go down that, it ends up being a very, very slippery slope to not being prepared to turn these things off.\n\nI want to ask you about hallucinations because a few years ago I think you were quoted as saying that you thought hallucinations will be largely eliminated by 2025. They haven't been, although the rate of hallucinations has certainly come down. Can they ever really be eliminated?\n\nFor sure. I mean, I would. I stand by the statement that they've been largely eliminated. I mean, if I give you and me 10 questions, random questions on any topic, and we sit here with two of the frontier models, I mean, they're going to do a much better job of eliminating hallucinations than you or I, for sure. And the rate of reduction and increase in accuracy over the last two or three years has been eye-watering. It's kind of unbelievable.\n\nWe used to spend all our time in 2023 talking about biassed data and dirty data in, and dirty data out. I mean, there are still errors, there's no question about that. But the rate of improvement, I think, is unbelievable. And I do think that they're going to largely be eliminated. I don't think it's something... even today, it's not something we really talk about.\n\nI have noticed that it's no longer a topic of conversation, but they still do make mistakes. And the concern is that people are relying increasingly on these models. I mean, I come across people who say, no, this can't be true. This is what Gemini or ChatGPT says. And humans are developing this kind of trust relationship with these models. You don't find that concerning?\n\nYeah, I think we should be concerned about it because we should be sceptical, just as we're sceptical of any new technology. Hold it to a high bar, push back on it. And I think this is the tension of the age we're in. We both have to be accelerationists, optimistic about the good things that technology can do, but provide no free pass. Like, we should be very critical and ask the very tough questions to try and improve the quality of these things.\n\nSo DeepMind and OpenAI have gone all in on AI for science. This is a space that you're also interested in. And of course, you were at DeepMind. What are you doing in that space?\n\nWell, the main focus for me at the moment in that space is on medical superintelligence. Like, we really believe that we can learn from the corpus of all medical information and use that to drive diagnostics to basically commodity. It's going to be possible to take any complex case history and provide a diagnosis that is significantly more accurate and significantly cheaper with fewer interventions, so fewer tests to reach the same accurate conclusion, than any of the best panel of doctors.\n\nAnd we published a blog post on that last year. We're shortly submitting to independent peer review to a major journal. And the results are really quite startling. And I think that once we get that into clinical practice, it would change the job of the doctor completely, just in the same way that the job of the engineer has shifted from writing code. Most engineers now are just reviewing code, architecting code, debugging code to the job of the doctor is going to go from figuring out what the diagnosis is, which is largely going to be a solved problem, to actually administering the right care at the right time and providing emotional support and guidance to...\n\nHow do you see that deployed in practical terms? So how do you put it in the hands of doctors?\n\nI think that doctors will just make a phone call to their AI in-clinic. They'll text their AI. They will upload the patient record in the click of a button, and the model will be able to reason over all the contents of that. I also think that consumers are going to go direct to Copilot, which is what they're already doing. Almost 20 per cent of the questions that we get in Copilot every day are health-related or medical related. So it's already our main use case. And that's why my team has pushed so hard on medical superintelligence.\n\nSo I don't want to confuse our viewers even more between superintelligence, AGI, but what is artificial capable intelligence?\n\nI mean, this was a phrase that I used three years ago in The Coming Wave, a book that I wrote about AI. And basically, I was trying to break down the AGI term to pick a point along the route, which was before AGI. And instead of focusing on the abstract idea of intelligence to focus on the capabilities, like, what can it actually do? And I coupled it with the introduction of what I was calling the modern Turing test. And that was basically could an AI, take $100,000. Invent a new product, create a new business, market it, and then use that to make a million dollars in some short period of time.\n\nIt's not that we want to use AIs just to make money. It's just that it's a nice metric for measuring the efficacy of a complex set of tasks. And I think now that we see Claude bot and many other action-based AIs in operation, this year I think there are going to be models that can achieve that artificial capable intelligence and satisfy the modern Turing test.\n\nWhat about AGI? How close are we?\n\nI think that we're going to have... And you have said before that thinking about AGI is misleading.\n\nI think that we're going to have a human-level performance on most, if not all professional tasks. So white-collar work where you're sitting down at a computer, either being a lawyer or an accountant or a project manager or a marketing person, most of those tasks will be fully automated by an AI within the next 12 to 18 months. And we can see this in software engineering.\n\nMany software engineers report that they are now using AI-assisted coding for the vast majority of their code production, which means that their roles shifted now to this meta function of debugging, scrutinising, of doing the strategic stuff like architecting, etc, etc, putting things into production. So it's a quite different relationship to the technology. And that's happened in the last six months.\n\nSo what do you think is Anthropic's secret sauce? Why is it that they've been able to corner the market on AI enterprise tools?\n\nI think they've done a great job of just focusing on coding capabilities. I mean, compared to the other companies, their singular focus has paid real dividends. So they've done a great job. It's great to see.\n\nAnd when are you going to have your model?\n\nWe are developing our own superintelligence, and that's the main focus of our efforts at Microsoft AI. So sometime this year.\n\nMicrosoft or Microsoft AI has recently opened a UK centre. Why is that? Is that mainly because you think that there's a lot of talent in the UK?\n\nYeah, I mean there's great talent in the UK. We have an incredibly strong ecosystem. There's great universities. And we've hired some of the very top people from DeepMind to join us over the last few years. So we're growing that lab quite a bit.\n\nAnd you're not paying them... you don't have to pay them $100mn?\n\nNo, certainly not. Do you pay anyone $100mn?\n\nCertainly not. I mean, look, I think that the numbers are pretty eye-watering across the industry at the moment.\n\nYes. How do you deal with that? I'm just curious.\n\nI mean, at the end of the day when there's a supply and demand mismatch for talent, then things go exponential. And so I don't think it will last for very long because the knowledge is proliferating like crazy. And obviously, everybody's entering the industry and stuff. So I think that was just some crazy blip in time led by one or two people.\n\nLast question. In Silicon Valley the debate is all about who's going to get to AGI first or who's going to reach this superintelligence. When you look at China, I know you were there at the end of last year. This debate doesn't really exist. The debate is, how fast can you deploy? And a lot of the big companies are already deploying AI systems. Which is the right approach? And is there too much focus that's put on the incremental improvement in the technology?\n\nI mean, maybe to flip that the other way around, what China also does incredibly well is withdrawing deployments quite quickly. Now, obviously they do it arbitrarily without due process. But it's important to recognise that their rate of deployment is coupled with this other mechanism to pull things in and be quite restrictive. We don't have that mechanism. And I think that is actually a little bit concerning. I mean, like you mentioned earlier, the safety incident around Moltbook, I mean, there is going to be a real one of those in the next two or three years. And it's unfortunately an open question as to what the mechanism is for managing that safety situation from a public interest perspective on the open web. And I think we all...\n\nWell, there isn't one right now.\n\nWell, I think that's basically true. There isn't. It would come with popular pressure. Obviously, if there was a direct violation of a local law, then there could be an intervention. But those things take quite a long time to make their way through the system. So that's probably one of the biggest areas of concern for me.\n\nMustafa, thank you. And when you do reach superintelligence and you start deploying it, we'd love to have you back."
  },
  {
    "source": "WebProNews",
    "company": "Microsoft AI",
    "title": "Mustafa Suleyman's Radical Vision: Why Microsoft's AI Chief Believes Traditional Software Is Living on Borrowed Time",
    "date": "2026-02-07T14:29:23Z",
    "url": "https://www.webpronews.com/mustafa-suleymans-radical-vision-why-microsofts-ai-chief-believes-traditional-software-is-living-on-borrowed-time/",
    "content": "For decades, the software industry has operated on a familiar premise: developers write code, companies package it into applications, and users learn to navigate interfaces designed by someone else. According to Microsoft's AI chief executive, that entire paradigm is hurtling toward obsolescence -- and the timeline is far shorter than most industry veterans expect.\n\nMustafa Suleyman, the CEO of Microsoft AI, has articulated a sweeping vision in which artificial intelligence doesn't merely augment existing software but fundamentally replaces it. In recent remarks reported by Business Insider, Suleyman described a future where traditional applications give way to AI agents that understand user intent and execute tasks without requiring humans to click through menus, fill out forms, or master complex workflows. It is a thesis that, if even partially correct, would reshape the economics of the technology sector from top to bottom.\n\nSuleyman's argument rests on a deceptively simple observation: most software exists because humans need structured interfaces to communicate their intentions to machines. A spreadsheet application, for instance, forces users to organize data into rows and columns, learn formulas, and manipulate charts -- all because the computer cannot simply be told, in natural language, what analysis to perform. But large language models and their successors are rapidly closing that gap. When an AI agent can interpret a plain-English request like \"show me which product lines lost money last quarter and suggest where to cut costs,\" the spreadsheet becomes an intermediary that adds friction rather than value.\n\nThis is not merely a theoretical exercise for Microsoft. The company has been embedding AI capabilities across its product suite at an extraordinary pace, from Copilot integrations in Office 365 to AI-powered features in Windows, Azure, and Dynamics 365. Suleyman's comments signal that Microsoft's leadership views these integrations not as the end state but as a transitional phase -- a bridge between the old world of discrete applications and a new world in which AI agents operate fluidly across tasks that currently require users to juggle multiple programs.\n\nCentral to Suleyman's thesis is the phenomenon known as \"vibe coding\" -- a term that has gained traction in developer circles to describe the practice of using AI to generate software by describing what you want rather than writing precise instructions in a programming language. As reported by Business Insider, Suleyman sees vibe coding as a harbinger of a much larger shift. If non-technical users can prompt an AI to build custom tools on the fly -- tailored exactly to their needs in the moment -- then the rationale for purchasing off-the-shelf software begins to erode.\n\nConsider the implications for the enterprise software market, which generates hundreds of billions of dollars in annual revenue. Companies like Salesforce, SAP, and Oracle have built empires on the premise that businesses need standardized, professionally maintained applications for customer relationship management, enterprise resource planning, and supply chain logistics. If AI agents can dynamically assemble the equivalent functionality in response to a manager's spoken or typed request, the value proposition of these monolithic platforms comes under serious question. Suleyman has suggested that this transition could begin to materialize as early as 2026, a timeline that has raised eyebrows even among AI optimists.\n\nWall Street has been grappling with how to price this possibility. Microsoft's own stock has been buoyed by investor enthusiasm for its AI strategy, particularly its deep partnership with OpenAI and the rapid adoption of Copilot products. But if Suleyman's vision plays out, the disruption would not spare Microsoft's own legacy businesses. Office 365, one of the company's most reliable revenue engines, is itself a collection of traditional applications -- Word, Excel, PowerPoint, Outlook -- that could theoretically be subsumed by a sufficiently capable AI agent layer. The strategic bet appears to be that Microsoft can cannibalize its own products before competitors do it for them, a playbook reminiscent of how the company navigated the cloud transition under Satya Nadella's leadership.\n\nFor the broader software industry, the stakes are existential. Independent software vendors, or ISVs, that have built businesses around niche applications face the prospect of their entire product categories being absorbed into general-purpose AI platforms. The SaaS model, which depends on recurring subscriptions for standardized tools, could give way to a usage-based model where customers pay for AI compute rather than application licenses. Venture capital firms that have poured billions into SaaS startups over the past decade are quietly reassessing their portfolios, looking for companies that are either building the AI infrastructure layer or are vulnerable to being displaced by it.\n\nFor all the boldness of Suleyman's predictions, significant technical challenges stand between the current state of AI and the app-free future he envisions. Today's large language models, while impressive in their ability to generate text, code, and analysis, still suffer from hallucinations -- confident-sounding outputs that are factually incorrect. In a world where AI agents are executing business-critical tasks without human intermediation, the tolerance for error is vanishingly small. A spreadsheet formula that produces the wrong number is bad; an AI agent that autonomously sends incorrect financial data to a regulator is catastrophic.\n\nReliability and auditability are closely related concerns. Regulated industries such as finance, healthcare, and government require detailed audit trails showing how decisions were made and what data informed them. Traditional software, for all its rigidity, produces deterministic outputs that can be traced and verified. AI agents, by contrast, operate probabilistically, and their reasoning processes remain difficult to inspect even for the engineers who build them. Until these transparency and reliability gaps are closed, many enterprises will be reluctant to hand over mission-critical workflows to autonomous AI systems, regardless of how eloquently Microsoft's AI chief makes the case.\n\nBeyond the technical hurdles, there is the deeply human dimension of this transition. The global software development workforce numbers in the tens of millions, and adjacent roles -- quality assurance, technical writing, user experience design, systems administration -- employ millions more. If vibe coding and AI agents reduce the need for traditional software development, the labor market effects could be profound. Suleyman has acknowledged that AI will transform the nature of work but has generally framed the shift in optimistic terms, emphasizing that AI will augment human capabilities rather than simply eliminate jobs.\n\nHistory suggests the reality will be more nuanced. Previous waves of technological disruption -- from the mechanization of agriculture to the automation of manufacturing -- did eventually create more jobs than they destroyed, but the transitions were often painful and unevenly distributed. Software engineers in Silicon Valley who can pivot to AI-native development may thrive; those in more routine coding roles, particularly in outsourcing hubs, face a more uncertain future. The policy response -- retraining programs, educational reform, social safety nets -- will matter enormously, and it is a conversation that the technology industry has historically been more comfortable deferring than leading.\n\nSuleyman's public commentary also serves a strategic purpose. By framing the future in terms that favor Microsoft's strengths -- vast cloud infrastructure, deep enterprise relationships, the OpenAI partnership, and a willingness to integrate AI across its entire product line -- he is attempting to set the terms of the competitive debate. Google, Amazon, Apple, and Meta are all pursuing their own AI strategies with varying degrees of aggression, and a new generation of startups including Anthropic, Cohere, and Mistral are building foundation models that could power alternative visions of the AI-driven future.\n\nThe race is not merely about who has the best model but about who controls the interface between users and AI. If Microsoft can position its Copilot and agent framework as the default layer through which hundreds of millions of enterprise users interact with AI, it will have secured an enormously valuable chokepoint -- even if the underlying applications those agents replace were Microsoft's own. It is a high-wire act that requires simultaneously maintaining the revenue streams of today while building the platforms of tomorrow, and it is a challenge that Suleyman, with his background as a co-founder of DeepMind, appears to relish.\n\nSuleyman's suggestion that meaningful disruption could begin by 2026 sets up a near-term test of his thesis. Industry observers will be watching several key indicators: the adoption rate of AI agents in enterprise workflows, the degree to which vibe coding moves from developer novelty to mainstream practice, and whether any major software categories see measurable revenue declines attributable to AI substitution. Early signals may come from the small and medium business segment, where the switching costs are lower and the appetite for cost savings is acute.\n\nWhether or not the traditional software application disappears on Suleyman's timeline, the direction of travel seems clear. The interface between humans and computers is being rewritten in real time, and the executives, investors, and policymakers who fail to take that seriously risk being caught on the wrong side of one of the most consequential technological shifts in a generation. Microsoft is placing an enormous bet that it can lead this transition rather than be disrupted by it. The next chapter of the software industry's story is being written not in code, but in conversation -- and Mustafa Suleyman intends to be the one shaping the narrative."
  },
  {
    "source": "adalovelaceinstitute.org",
    "company": "Microsoft AI",
    "title": "What is an AI transcription tool?",
    "date": "2026-02-05T15:46:37Z",
    "url": "https://www.adalovelaceinstitute.org/resource/what-is-an-ai-transcription-tool/",
    "content": "If you are a policymaker and you are interested in emerging insights about the adoption of AI across the public sector, read 'Current evaluations and assessments of AI transcription tools' and 'How are AI transcription tools used in the public sector?'.\n\nThere is growing opinion in policy and government circles that AI can make many aspects of the public sector more efficient. One example of a type of AI that is generating attention is AI transcription tools. These tools are designed and marketed to make administrative tasks, such as note-taking and writing summaries of meetings, faster and more efficient. They are increasingly built from the latest generation of foundation models.\n\nAI transcription tools have the potential to be highly effective when applied to tasks and professions requiring a high degree of information to be processed, recorded and transcribed - for example, in clinical contexts, courts and in social work. As such, AI transcription tools have captured policy attention[1] [2] and have been quickly adopted into public sector service delivery. It is estimated that around a third of social workers are using generative AI tools with transcription capabilities, such as Microsoft Copilot.[3]\n\nThe UK government has shown enthusiasm about the potential for AI transcription tools to ease the pressure on public services in the face of resourcing challenges. AI transcription tools feature heavily in the government's plans to roll out AI tools across the public sector. Ambitions for the adoption of AI transcription tools have been outlined in the 10 Year Health Plan for England[4] and the AI Action Plan for Justice.[5] Additionally, the government's AI Exemplars programme identifies AI transcription tools as one of the 'most promising' AI applications for public benefit.[6]\n\nThis explainer aims to support policymakers or public sector agencies recommending AI tool adoption and public sector workers using these tools. We combine technical detail about the underlying architecture of AI transcription tools (including their potential risks and benefits) with tailored insights into approaches for evaluating and assessing the efficacy of transcription tools. There is emphasis on public sector adoption throughout.\n\nFirst, by highlighting the core technical features of AI transcription tools, we provide a deeper understanding of their suitability for contexts where accuracy and efficiency are prioritised.\n\nWe show that as AI transcription tools are a type of generative AI system, they introduce the same potential risk of bias, hallucination and data protection risks.\n\nWe also pay attention to their wider application and systemic risks, such as how these tools may augment or challenge decision-making and accountability.\n\nSecond, by synthesising emerging evidence about AI transcription tool adoption and evaluation in the public sector, we aim to inform ongoing adoption efforts such as pilots. Ultimately, this explainer provides valuable evidence to ensure AI transcription tools are deployed and evaluated in the public interest.\n\nTranscription tools enable spoken audio to be reproduced as written text. They can be used to support note-taking and the documentation of conversations, meetings, interviews or recordings.\n\nWe use the term 'AI transcription tools' to refer to a new generation of transcription technologies that are powered by foundation models trained on extremely large datasets of recorded speech.[7] These tools are sometimes referred to as 'AI scribes', 'ambient scribes' or 'ambient voice technologies'.[8] Some general-purpose AI tools that are also powered by foundation models, such as ChatGPT, have transcription and summarisation capabilities.\n\nAI transcription tools can be used in a variety of contexts that require a transcript or a summary of an interaction, such as an interview or meeting. They can also support contexts where accurate record-keeping and official documentation are required as part of a best practice service, such as patient records or care assessments.\n\nSome AI transcription tools also include additional features to restructure transcribed text into different formats, including AI-generated summaries of conversations and meetings that underline key points or even provide recommended actions.\n\nThese features can be fine-tuned and iterated further for specific contexts, such as supporting healthcare staff to write a patient report that meets specific formatting requirements.\n\nAI transcription tools are powered by two types of foundation models: an automated speech recognition (ASR) model and a large language model (LLM). As set out in our explainer on the topic, foundation models are AI systems trained on large amounts of data for multimodal capabilities, meaning they can generate new text, images, video and voice materials.[9] Below, we describe how each model supports the function of an AI transcription tool.\n\nAutomated speech recognition (ASR) models are one of the two types of foundation models used to power AI transcription tools. ASR models are the newest generation of a family of models that can create a written transcript of a conversation.\n\nNew ASR models are trained on large datasets of recorded conversations that are paired with pre-transcribed text of what was said in each recording. The foundation models identify patterns between the sounds in the recordings and the words in the corresponding text.\n\nAI transcription tools use these patterns to transcribe speech from new audio files by estimating the sequence of written words that best fits the sounds in the audio files. Examples of ASR models that are commonly used in AI transcription tools include OpenAI's Whisper and Deepgram.\n\nDevelopers of AI transcription tools can fine-tune ASR models to improve their performance in specific, specialised contexts. Fine-tuning is the process of using context-specific audio recordings to further train a model to recognise specific words and phrases, such as medical terminology or other context-specific terms.\n\nLarge language models (LLMs) are the second kind of foundation model used to power AI transcription tools.\n\nLLMs are a subset of foundation models created for natural-language capabilities. They are trained on large amounts of written text and underpin a range of AI tools, such as customer service chatbots. In AI transcription tools, LLMs summarise the transcripts made by ASR models into key points and formats that can be included in official documentation. LLMs in AI transcription tools can be fine-tuned to introduce additional or context-specific safeguards from those of the underlying foundation model, to better enable faithful transcription. For example, it is possible that a conversation taking place in a social care setting may mention child sexual abuse material (CSAM), but commercial foundation models are designed to filter and block this content from appearing in outputs by default.\n\nThe use of ASR models and LLMs within the same tool means that transcription tools can also use written text as an input to accompany audio recordings. LLMs have an interface for users to prompt the tool to alter the final outputs of AI transcription tools, and some can also use photographs of handwritten notes as inputs. This text will be incorporated into the final output generated by the transcription tool. As a result, users can adapt the outputs produced by AI transcription tools to add more detail or, if they spot inaccuracies, by prompting the tool to make desired changes.\n\nFor example, a doctor could combine notes from previous interactions with a patient with an audio recording of a more recent interaction, to update their notes about the patient. The doctor can then prompt the AI transcription tool to make changes to the notes after they have reviewed them.\n\nFigure 1: The architecture of an AI transcription tool\n\nThis diagram shows the workings of a generic AI transcription tool and how it uses an ASR model and LLM to produce a summary of a conversation. The boxes show the different components of the AI transcription tool and the arrows show how information is passed between them. The dotted arrows show how the LLM may access additional sources of information, such as user prompts or other relevant documents, to generate summaries that are more suited to the user's needs.\n\nFigure 1 is one approximation of what an AI transcription tool may look like. Although different AI transcription tools will use roughly the same components, their specific design may differ.\n\nOlder transcription tools used automated speech recognition (ASR) models, without foundation models and large language models (LLMs).\n\nThese older models converted audio recordings to transcriptions using traditional statistical methods that consisted of algorithms designed to complete narrowly defined tasks, such as identifying specific sounds within speech. However, the narrowness of the underlying models made these older transcription tools inflexible and inaccurate.\n\nThe availability of large amounts of training data and computational power means that ASR models can now be developed as foundation models. These new ASR models match sounds to written text directly, using the associations between sounds and written text in their training data.\n\nAs a result, new ASR models are often described as having 'end-to-end' architectures. This is because information from the audio recordings is not processed through the multiple statistical models found in previous transcription technologies.[10]\n\nThis technological advancement means end-to-end ASR models perform better than older ASR models, because a higher percentage of words from the audio recording is faithfully transcribed into text. This metric is referred to as the 'word error rate'.[11]\n\nPopular examples of AI transcription tools on the market include Otter.ai and Rev, but other generative AI tools marketed for more general use may contain transcription capabilities, such as Microsoft's Copilot.\n\nA defining feature of foundation models is their need for large quantities of training data.[12] However, building foundation models requires developers to make decisions about what kinds of data to include, and how this data is cleaned and categorised. These decisions can significantly impact how foundation models function.[13]\n\nMany developers of foundation models do not publicly share information about the data their models are trained on. Without a clear understanding of the data used to build a foundation model, it is difficult to understand why the model generates certain outputs.[14]\n\nResearch shows that these data considerations are important for foundation models that power AI transcription tools. ASR models could inaccurately transcribe conversations if the words and phrases in the audio recordings used for training data do not reflect the kinds of conversations that they transcribe in practice.[15] Similarly, LLMs may reproduce harmful content from their training data in the new text that they generate, such as hate speech and slurs.[16]\n\nAI transcription tools may perform differently depending on their underlying foundation models, but even tools built from the same foundation models should be evaluated independently. For example, two different AI transcription tools could both be built from GPT-5, but one fine-tuned for use in healthcare will be trained to retrieve information from a smaller dataset, such as clinical records, which may mean it scores better on accuracy evaluations.[17] Organisations that want to deploy AI transcription tools should consider the trade-offs between different tools on the market and rigorously assess whether they function as intended for the desired context.\n\nPrevious Ada research shows that organisations should establish clear success criteria when deploying new technologies, as well as methods for regular testing and evaluation.[18] All of these processes should include the perspectives of people impacted by new technologies. This will ensure they work more effectively in the public interest.\n\nRegular testing is also important because the use of new technologies to automate tasks can create new responsibilities, such as reviewing and adapting the outputs of AI tools, that counteract anticipated efficiencies.[19]\n\nFor example, one evaluation of a pilot of AI transcription tools for generating witness statements found that AI transcription did not reduce the amount of time that police officers spent taking statements, because the time saved was replaced by the additional time required for amending poor quality statements.[20]\n\nOrganisations may have a number of incentives to adopt AI transcription tools. There are a range of potential benefits from the use of AI transcription tools. Many of these benefits stem from the ability of transcription tools to reduce the time that people spend documenting interactions.[21] However, AI transcription tools could also provide benefits beyond time savings.\n\nThe tables below summarise the possible benefits of AI transcription tools in three categories:\n\nLike all AI systems, AI transcription tools present multiple risks that stem from the technical aspects of their design and their integration into social structures. We summarise the risks of AI transcription tools using three categories:\n\nAll AI transcription tools are prone to bias and hallucination (producing fabricated content) because they are powered by foundation models.[27]\n\nThe potential for bias is present across all data-driven systems. A technical definition of bias is the statistical deviation from a desired or 'true' result, which can lead to real-world bias when the output of a system results in individuals or groups experiencing a different, unfair outcome.\n\nHallucination is a risk specific to foundation models and generative AI systems built on top of foundation models. The term refers to the phenomenon of these systems producing convincing but factually incorrect information, presented as if it were 'the truth'.[28]\n\nAI transcription tools compound the risks of bias and hallucination because they pass information through two separate foundation models and therefore create multiple opportunities for bias and hallucinations to occur in their outputs. First, during the transcription of audio recordings into text using an ASR model, and second, when the transcribed text is summarised for system usability using an LLM.\n\nIn ASR models, hallucination refers to generated text that has no relation to the conversation in the original audio recording. ASR models have been found to generate random text that promotes harmful perceptions of individuals included in the recording, including portrayals of violence, demographic stereotypes and false relationships between speakers.[29]\n\nASR models are also less accurate for people with characteristics that are underrepresented in the data that the models are trained on. Differences in the way that people with different characteristics speak, such as their accents, mean that transcription tools can create inaccurate outputs even if these characteristics are not mentioned in the recorded conversation.\n\nFor example, foundation models have been found to transcribe speech with varying accuracy between different groups, performing more poorly for non-native English speakers than for native English speakers.[30] Other research has shown that hallucinations disproportionately occur for people who have conditions that affect their ability to coordinate language. For example, aphasia can create delays, repetitions or missing words in peoples' speech.[31]\n\nLLMs also hallucinate and perpetuate biases, as shown in our evidence review of foundation models in the public sector.[32] These inaccuracies can occur even when LLMs perform a seemingly neutral function like summarising a written document.\n\nResearch has shown that summaries of care records produced by LLMs can differ significantly according to the characteristics of the people that the document relates to, such as their gender.[33]\n\nBecause of these potential inaccuracies, using AI transcription tools to support decision-making can put people at risk. For example, it is possible that inaccuracies from AI transcription tools used to write police reports could be used as evidence in court cases and increase the likelihood of injustices in legal proceedings.[34] Similarly, evidence suggests that transcription inaccuracies can produce incorrect diagnoses for patients when AI transcription tools are used in medical contexts.[35]\n\nAnother critical risk from the technical features of AI transcription tools arises from their underlying foundation models often being hosted externally by foundation model developers or third-party cloud providers.[36] This decreases an organisation's control over the data that is transcribed and summarised with AI transcription tools.\n\nAs a result, organisations may not be able to prevent sensitive information being used in unexpected ways or stored insecurely.[37] The foundation model developers may be based in different countries, increasing the number of cross-border transfers of information and further reducing the organisations' control over their information. Organisations should conduct robust Data Protection Impact Assessments to identify privacy risks introduced by AI transcription tools.[38]\n\nOrganisations will also need to regularly assess AI transcription tools, compare alternative tools and consider whether the risk of bias and hallucination is acceptable for the intended use case.\n\nAI transcription tools can create risks even if they perform perfectly well at a technical level. These risks stem from how people react to new technologies unpredictably and the challenges of implementing new technologies in sensitive contexts.\n\nThe fact that AI transcription tools generate outputs in compelling professional language could lead to users' over-reliance on the legitimacy of their outputs in ways that are detrimental.­ For example, frontline workers may defer to an AI-generated transcript in critical decision-making contexts, underplaying their own professional judgement.[39] This tendency is often known as 'automation bias'.\n\nPeople may also modify their behaviour to accommodate AI transcription. For example, it is possible that people who use frontline public services will feel uncomfortable with the use of AI transcription tools in certain sensitive contexts, such as clinical settings, which could cause them to 'self-censor' when faced with a transcription tool.[40] This could lead to people withholding important information from professionals that could improve the quality of care or support that they receive.\n\nAI transcription tools may not wholly reflect the 'true' context of a meeting or an interaction. For example, AI transcription tools use auditory information, not visual or sensory cues. In some cases, it could be important for frontline workers to record additional observations about people's body language or the environment that they live in, even if these factors are not spoken about directly. Moreover, poor quality audio, caused by use in noisy environments, could also mean that important information is missing. This can distort the accuracy of the final summary that is produced.\n\nSimilarly, official records based on summaries of interactions often require specific kinds of language and narrative styles that AI transcription tools may not replicate fully. One study of an AI transcription tool used for writing domestic abuse witness statements found that AI-generated statements used a more academic style that did not include a strong narrative of events and therefore did not accurately represent the witnesses' accounts in court.[41]\n\nIn sensitive contexts like social care, it is often more appropriate to use language that centres the perspectives of people and avoids complex, technical language. AI-generated technical language could obscure aspects of people's experiences in official records, impacting the decisions made based on these records and people's ability to understand significant events in their lives.[42]\n\nAdditionally, the AI-generated transcript and summaries may not reference that it was produced by AI, which may have implications for independent reviews of the documentation.[43]\n\nApplication risks and impacts underscore the importance of research, testing and piloting AI transcription tools with the intended user population. This should be done in context to create rich grounded evidence about the interface between the technology and affected groups.\n\nThe introduction of new technologies to established processes can change how organisations, industries or economies function. Such knock-on effects can change how people work with these institutions and the experiences of people who rely on them for support. We have identified two areas where AI transcription tools can potentially change processes and systems in a way that creates systemic impact.\n\nFirst, AI transcription tools can complicate processes for allocating accountability in critical decision pathways. The introduction of AI - wholly or partially - into official documentation such as witness statements may introduce challenges around the 'human in the loop' and the extent to which people have oversight and authority over work and decision-making processes supported by AI.\n\nAs a result, it could be difficult to identify who is responsible for official records when humans do not have control over the whole process. Due to the black box nature of foundation models, it is also impossible to explain why AI transcription tools prioritise, ignore or reframe different pieces of information when generating summaries of written texts.\n\nThis means there will be random variation in the outputs of AI transcription tools that will require oversight and, where necessary, correction, but may sometimes be difficult to catch. [44] This could create challenges for organisations or processes where accurate records are critical and could have implications for people seeking redress when mistakes are made.\n\nSecond, AI transcription tools could cause deskilling in some professions where skills are traditionally learned through the documentation processes. When tasks are automated, it can reduce the opportunities for people to develop skills and knowledge through repetition.[45]\n\nFor example, social workers may learn to exercise their professional judgement through the process of writing assessments of people's care needs. In the context of AI transcription, professionals may lose opportunities to develop sector-specific knowledge and reasoning skills that they would otherwise learn by manually recording and summarising interactions with people.\n\nThe widespread automation of significant parts of people's work could also lead to broader economic consequences. For example, automation could reduce the quality of people's work, if their work is reallocated to unfulfilling tasks, or exacerbate economic inequalities, if automation reduces demand for paid workers.[46]\n\nSystemic risks highlight how AI transcription tools exist alongside other AI technologies and demonstrate the need to consider whether the impacts of these technologies are justly and equitably distributed.\n\nAs we set out above, the latest AI transcription tools are powered by foundation models and incorporate LLMs into their architecture. This technical context presents a few challenges when evaluating a transcription tool's efficacy.\n\nFirst, any fine-tuning to an AI transcription tool (such as training it on a dataset of clinical records so it is more effective for adoption in a healthcare context) may override or degrade the safety guarantees of the foundation model. The LLM component of an AI transcription tool will therefore require specific, iterative evaluation different to that undertaken by the foundation model developer, to account for capability changes or potential biases created by the fine-tuning.\n\nSecond, LLMs - as a form of generative AI - are highly dynamic, meaning the same system prompt may generate different outputs. They also create risk of distributed impacts that are hard to measure at a system level. This type of system makes standardising testing and evaluations challenging and requires an iterative approach, to address any updates in the underlying model as well as to ensure the system is effective and safe for the deployment context.\n\nDeployers of AI transcription tools may face challenges and trade-offs identifying and operationalising appropriate approaches and metrics for evaluation.\n\nResearchers and practitioners in machine learning use evaluations to refer to assessments of technical capabilities of an AI model (including foundation models). For example, how well it performs against a chosen benchmark like bias, or how easily a model exhibits certain types of behaviours, like deception or sycophancy.\n\nThere is no consensus around a defined scope for a model evaluation. Previous Ada research finds that model evaluations are often targeted narrowly, presenting models with a variety of inputs and checking the outputs correspond with ethics or safety goals. Some evaluations are broader and consider how users interact with the application on top of the foundation model (see 'Human interaction evaluations' below).[47]\n\nTypical model evaluation methods for foundation models include benchmarking, which involves generating a score or metric from testing the model's performance on a range of tasks, like legal comprehension or medical reasoning.\n\nOther types of technical testing like red-teaming - an activity that involves the probing of a system in an adversarial way to identify potential harmful outputs - are a form of evaluation that is specifically safety focused. Different actors are involved with model evaluations and other forms of technical testing across the AI development and deployment lifecycle. In the context of AI transcription tools, evaluations may be conducted upstream by model developers using standardised assessments for safety or bias, and downstream by deployers after fine-tuning, to check for specific performance on the task of transcription.\n\nIf a model evaluation is a narrow form of evaluation purely focused on the model level, there are other evaluation approaches that focus on assessing an AI system in its real-world application. These are sometimes referred to as human interaction evaluations.[48]\n\nAn example evaluation of this kind might involve gathering evidence on an LLM being used in a school to understand how it changes pedagogical practices of teachers.\n\nOutside of AI, 'evaluation' can refer to any kind of experimental or descriptive investigation into a system, process or intervention. These kinds of evaluations might be used to deliver evidence of efficacy or value for money, which might be a requirement for public sector projects.\n\nThe UK government's guidance for central government agencies on how to conduct evaluations - the Magenta Book - defines evaluations as 'systematic assessment of the design, implementation and outcomes of an intervention. It involves understanding how an intervention is being, or has been, implemented and what effects it has, for whom and why', with a focus on impacts and cost-effectiveness.\n\nSome evaluations may include experimental or quasi-experimental design. Experimental methods include randomised controlled-trials (RCTs), which are often used in social science research, and are used for robust evidence-gathering to establish a causal relationship between two (or more) factors.\n\nClinical research, for example, uses methods like non-inferiority testing to ensure that a new medical treatment does not perform any worse than an existing treatment.[49]\n\nAI evaluations may be adopted alongside other resources that are more tailored towards organisational processes and day-to-day responsible management of AI systems. Examples of such assessments include the UK government's AI Management Essentials tool and Careful Industries' Careful Consequence Check.[50]\n\nWe have existing evidence about AI transcription tools' capabilities from studies assessing their accuracy and their pitfalls, such as demonstrating gender bias, and from their application into critical decision-making contexts that require complete and accurate information, such as medical settings.[51]\n\nAs we show above, there are several existing approaches for AI evaluations that draw from different disciplines. For meaningful evaluations of AI transcription tools that incorporate risks, capabilities and user interactions, a range of metrics will likely be required to gain a holistic picture of the system in practice.\n\nBelow we offer some example approaches and metrics for evaluating AI transcription tools that are drawn from the field of natural language processing (NLP). The approaches demonstrate how multiple different metrics could be used to assess a particular target of evaluation, e.g. model performance. We focus on approaches for human-led evaluations, as opposed to automated evaluations that use large language models (LLMs) to assess target metrics (a method known as 'LLM-as-a-judge').[53]\n\nIn the following section, we synthesise some recent evaluations of AI transcription tools in the UK public sector. While not an exhaustive list, these three studies provide useful evidence of emerging AI evaluation metrics in practice. They showcase recent efforts towards evidence gathering on the effectiveness of consequential applications of AI in the public sector, such as transcription tools.\n\nIn 2024, Kingston Council supported the piloting of a prominent AI transcription tool used in the public sector, Beam's Magic Notes. Magic Notes is a transcription tool specifically tailored for social work contexts, supporting transcriptions of meetings and the preparation of case notes and assessments. It is estimated that over 80 councils in the UK are currently using Magic Notes in social work contexts.[56]\n\nKingston Council worked with Beam in a user research capacity, testing and evaluating the transcription tool to adapt the existing product for use by social workers on mobile phones. The approach involved training Magic Notes with anonymised data from templates and assessments from council social workers and checking the outputs met the required standards for the council, including populating the correct fields within the case management system.[57]\n\nThe pilot also explored the time-saving ability and value for money of Magic Notes, finding an average time saving of over 50 per cent and an accuracy of 96 per cent in the transcripts.\n\nAs an early pilot of the real-world capabilities of an emerging AI transcription tool in the public sector, the study and its publicly available evidence may have informed wider uptake of Magic Notes.\n\nThe UK's Department for Business and Trade (DBT) and Department for Work and Pensions (DWP) have both conducted multi-month pilots and evaluations of Microsoft Copilot, a generative AI tool with wide-ranging capabilities, including transcription and summarisation.\n\nBoth evaluations collect data via self-reporting surveys. The DBT survey aimed to measure on use cases, satisfaction, accuracy and time savings. One thousand Copilot licences were allocated to volunteer and randomly selected participants from across the department, with a control group for comparison.[58] The DWP evaluation adopted a quasi-experimental design, using a control group and complementing the survey with qualitative interviews to measure staff experiences, perceived usefulness and the impact on task efficiency, job satisfaction and work quality.[59] The DWP also used an econometric model called Seemingly Unrelated Regression (SUR) to capture the interdependency of the outcomes of task efficiency, job satisfaction and work quality.\n\nIn the DBT evaluation, 'Transcribing or summarising a meeting' was the most popular use of Copilot among the use cases identified by participants, with 89 per cent of participants 'satisfied' with this capability. The evaluation also found that 'transcribing or summarising a meeting' using Copilot saved on average 0.7 hours per task, one of the higher time savings identified in the study. However, the study also noted that the link between time savings and increased productivity among participants was not conclusive. This suggests multistage or multifactor evaluations may be required to make robust empirical claims that time savings lead to improved productivity, which may include more longitudinal data gathering.\n\nIn the DWP evaluation, using Copilot for 'Transcribing or summarising meetings' was found to save on average nine minutes, and 'Summarising information or research' saved 24 minutes. The qualitative interviews revealed that time savings were often redirected towards 'higher value' work, including 'project delivery, planning, and mentoring'. The statistically significant effects on all three outcome measures are a positive result for this study, though the authors highlight the importance of human-in-the-loop for tasks involving 'nuance, sensitive content, or stakeholder-facing outputs'.\n\nBoth studies are limited by self-selection bias among their participant base. The use of self-reporting for measures like job satisfaction can introduce biases such as social desirability bias, where responses are tailored according to societal expectations.[60]\n\nThe Centre for Policing Research and Learning at the Open University conducted an evaluation of Hertfordshire Constabulary's 'ADA' (Anathem Digital Assistant) transcription tool, designed to transcribe and produce witness statement documentation. The purpose of the evaluation was to assess whether the tool led to increased productivity and quality of statements.[61]\n\nThe evaluation adopted a mixed-methods approach, combining interviews with police officer using the tool and various metrics from linguistic theory to determine the quality of AI-produced transcripts. These included the Flesch readability score,[62] which is a metric for evaluating a text for how easy it is to understand.\n\nThe researchers in this study highlight how assessing the transcripts through a broader lens enabled them to look beyond accuracies or inaccuracies, and to judge whether the completed transcript reflected the overall language and tone of the witness's original statement.\n\nThey found that the use of ADA did result in time savings, but they could not evidence any 'realisable claims' around productivity, recommending that future productivity assessments in this domain would need to operate 'end to end' to provide a more well-rounded picture of the impact of the AI transcription tool on work and practice. The team concluded that the study did not indicate sufficient proof of concept that ADA would increase productivity, but indicated that a forthcoming second version of the tool 'may improve significantly in terms of performance'.[63]\n\nAs highlighted in the UK government's guidance for conducting impact evaluations of AI, organisations should approach evaluations with good faith. A major incentive to conduct evaluations is to gain helpful insights and to learn about the tool's impact. However, the findings of the above evaluations offer several important lessons for informing ongoing evaluations of AI transcription tools, as detailed below.\n\nThe three studies above all show evidence that the transcription and summarisation functions of AI transcription tools are reducing time spent on drafting, which is promising for further adoption of these tools. However, two of the studies do not show evidence that this straightforwardly translates to improved productivity. Additional requirements may be needed within organisations, such as training and longer-term evidence gathering or the introduction of an alternative tool, to ensure that productivity benefits are fully realised.\n\nMore importantly, further research that specifically measures the costs and gains of AI transcription tools in a range of organisational contexts will be needed to understand the degree to which time savings empirically contribute to increased productivity.\n\nFor contexts where a holistic interpretation of a conversation is important, such as a witness statement, additional metrics may be required to measure details that may be omitted by transcription or summarisation capabilities.\n\nAdditional human safeguards to thoroughly check transcripts - for example, for erroneous detail inserted superfluously - will likely be required. There are also likely to be scenarios where details in transcripts are faithful to the conversation but may contain tonal or linguistic shifts that have an impact on the interpretation of meaning. This type of impact will be much harder to track at a single user level, and may require evaluations against non-AI generated transcripts, or comparisons with different AI transcription tools.\n\nOverall, the evidence from these three studies underscores that AI transcription tools will require an iterative, sociotechnical-informed approach to evaluation that prioritises grounded evidence from real-world impacts, beyond just efficiency and time savings.\n\nUK public services are facing significant financial challenges and the implementation of AI tools, such as transcription tools, is part of a broader government strategy to improve the efficiency and quality of public services, as outlined in the government's AI Opportunities Action Plan.[64]\n\nThe UK government has demonstrated sustained interest in the potential for AI transcription tools to create impactful transformation. Multiple AI transcription tools were included in the government's AI Exemplars Programme in August 2025.[65] The potential for transcription tools is focused on frontline public services, where it is important to maintain a written record of interactions between service providers and service users.\n\nOfficial documentation helps ensure that public services provide people with the support they need, that correct procedures are followed and that people experience consistent care over time. Effective documentation can also help individuals who draw on public services understand how frontline workers made decisions and act as a resource for understanding interactions with public services that impacted their lives, as well as supporting redress when things go wrong.[66]\n\nHowever, producing official documentation can be time consuming for frontline workers who often have a significant backlog of people who need to draw on their support. AI transcription tools could potentially allow frontline workers to provide essential services more efficiently by reducing the amount of time they spend documenting interactions.\n\nIn this context, AI transcription tools are being adopted or recommended for several public services, notably social care, healthcare and the justice system. All three sectors are experiencing acute resourcing challenges. In 2023-2024, only 31 per cent of requests for local authority-funded adult social care resulted in support being given.[67] In healthcare, the median waiting time for patients in England to start treatment was 13.4 weeks in June 2025, and192,000 people had waited over a year for treatment.[68] In the justice sector, backlogs have grown significantly in recent years, to the point where the Crown Court of England and Wales had around 75,000 outstanding cases at the end of 2024.[69]\n\nSocial workers spend most of their working time completing documentation processes and recording interactions with people who draw on their care.[70] As a result, staff in both adult and children's social care have begun to use AI transcription tools to record conversations and automatically generate official documentation such as care plans (a personalised document outlining the specific support a person or family requires for their wellbeing).\n\nAI transcription tools have been adopted rapidly in social care because the sector faces significant resource challenges. Increasing demand for social care, coupled with local authority budget cuts, means that people who need care are often unable to receive it from their local authority.[71] Resource shortages and growing workloads are also impacting the wellbeing of social workers themselves, with social workers reporting high levels of stress and concern about their ability to adequately support people who draw on their care.[72]\n\nMultiple local authorities are adopting AI transcription tools for social care contexts, but there is no nationally representative data to understand how many social workers are using this technology across the UK. Local authorities can procure AI transcription tools from independent developers or develop their own AI transcription tools. Incumbent providers of social care data management platforms have also started to develop their own AI transcription tools that they provide to their existing customers.\n\nThe Ada Lovelace Institute is focusing on the application of AI transcription tools in social care contexts through our 'Transcribing trust' research project, which aims to understand how these tools impact social care practice.\n\nResearchers have estimated that over 100 companies provide AI transcription tools for healthcare applications worldwide.[73] The UK government is beginning to recommend the use of transcription tools in meetings between healthcare professionals and patients to reduce backlogs and cost pressures and allow frontline staff to spend more time with patients.[74]\n\nThe UK's leading digital healthcare platform launched its own AI transcription tool in April 2025 and provides this as part of its platform services to 98 per cent of GP practices in England, with plans to extend the rollout to hospital and mental health settings.[75]\n\nGiven the range of transcription tools available for healthcare practitioners, NHS England has published a set of standards to support clinicians in using these tools safely, detailing recommended technical features and the ongoing monitoring required.[76] NHS England's current position is that 'ambient scribing products' with a summarisation capability should be registered as a medical device under the Medicines and Healthcare products Regulatory Authority (MHRA) Class I device status.[77]\n\nLiability for using an AI transcription tool that does not comply with guidance and regulation rests with individual users or the organisations (such as general practitioners or NHS trusts) deploying the tool.[78]\n\nSimilarly, the UK government is trialling the use of AI transcription tools in the justice system, to reduce the amount of time that frontline staff spend documenting interactions. This is part of a broader strategy to integrate AI into the justice system, as outlined in the AI Action Plan for Justice.[79]\n\nAt the time of writing, AI transcription tools have been piloted in multiple parts of the justice system. The government is conducting pilots of AI transcription tools in probation services in Kent, Surrey, Sussex and Wales.[80] The government plans to roll out its Justice Transcribe tool to all 12,000 probation officers after it has been piloted.[81]\n\nPolice forces have also used AI transcription tools. Hertfordshire Constabulary piloted a tool for automatically generating witness statements in domestic abuse cases based on interviews conducted by police officers.[82] Outside of the UK, AI transcription tools have also been used to generate police reports based on audio recordings from police body cameras.[83]\n\nThe use of AI transcription tools in public sector contexts creates additional challenges. While we do not examine these comprehensively in this explainer, we have highlighted two significant challenges below.\n\nPublic sector bodies may lack sufficient expertise and resources to adequately evaluate AI tools. Our previous research has shown that technological uncertainty about how AI works and the outsized market power of AI developers are impacting how well local authorities are able to procure and evaluate AI tools.[84] This is further exacerbated by the fact that specialist evaluators and engineers can command higher salaries in the private sector and might therefore be unwilling to work for the public sector.[85]\n\nFurthermore, the guidance and legislation surrounding the procurement and evaluation of AI tools in the public sector is difficult to implement in practice. There are multiple definitions and measures of social benefit that public bodies use when evaluating AI tools, but current guidance lacks specificity about where particular definitions are more appropriate and how to operationalise them.[86]\n\nOfficial guidance also uses multiple definitions of AI and key AI-related concepts like 'fairness' and 'transparency'.[87] The resulting lack of clarity often forces public bodies that lack expertise in AI to fill in the gaps on their own.[88]\n\nThese ecosystem-level difficulties in conducting effective evaluations pose a significant challenge to the fair and transparent use of AI transcription tools in the public sector.\n\nThe public sector is subject to higher standards of scrutiny than the private sector. Public officials have a commitment to accountability and fairness through legislation, such as the Public Sector Equality Duty[89] and the guiding values of public service, such as the Seven Principles of Public Life.[90]\n\nHowever, AI tools present challenges to these expectations because they can significantly influence how official documents reflect the experiences of the people that they pertain to. The summarisation of people's experiences to create official records requires professionals to sort, select and prioritise information about a person for the context in question, from children in social care to victims of crime.[91] When AI transcription tools make these decisions, it reduces the control of professionals over the official records they create.\n\nThis raises challenges relating to accountability if frontline professionals are held responsible for decisions over which they had limited control. It will also make it challenging for people who use public services to seek redress when things go wrong.\n\nAdditionally, the way AI transcription tools present information will often be biased (see the previous section 'What are AI transcription tools?'). Biases in official documentation processes can perpetuate inequalities in people's experience of public services in significant ways. For example, one study of the use of LLMs to summarise long-term care records from an English local authority found that some models consistently downplayed women's health issues and needs compared to those of men.[92]\n\nThis explainer aims to provide policymakers and those working in the public sector with a practical understanding of AI transcription tools by describing their technical features, examining their use in frontline public services and synthesising the current approaches to evaluation.\n\nWe have identified that AI transcription tools could be useful for public sector workers, in the face of resource challenges and demanding workloads, by reducing the burden of administrative work that they must complete. However, these tools pose the same risks as any other technology powered by foundation models.\n\nCurrently, there is not enough evidence on how these potential risks and benefits are materialising in the diverse contexts in which AI transcription tools are being deployed. As a result, the UK public sector could benefit from additional evaluations of the use of AI transcription tools, through pilots or other interventions. Metrics like accuracy and time savings are popular in evaluations but should be supported with wider investigations to understand whether anticipated benefits of transcription tools are being realised and that potential harms are avoided.\n\nMoreover, public sector agencies would benefit from the collation and synthesis of empirical evidence from the deployment of AI transcription tools in multiple contexts over longer periods of time. Siloed evaluations over short time frames risk missing the systemic impacts of AI transcription tools that could negatively impact the experiences of people who draw on public services. Such interventions will better enable AI transcription tools to create meaningful value in public sector contexts.\n\nThis explainer was co-authored by Oliver Bruff and Lara Groves, with substantive contributions from Mavis Machirori and Catherine Gregory.\n\n[1] Tommaso Spinelli, Senior Artificial Intelligence Change Manager, and GDS, 'Launching the Artificial Intelligence Playbook for the UK Government - Government Digital Service' (10 February 2025) <https://gds.blog.gov.uk/2025/02/10/launching-the-artificial-intelligence-playbook-for-the-uk-govern... accessed 29 September 2025.\n\n[2] 'AI Opportunities Action Plan' (GOV.UK) <https://www.gov.uk/government/publications/ai-opportunities-action-plan/ai-opportunities-action-pla... accessed 19 August 2025.\n\n[3] 'Use of AI Rising among Social Workers, Poll Finds' (Community Care, June 2025) <https://www.communitycare.co.uk/2025/06/12/use-of-ai-rising-among-social-workers-poll-finds/> accessed 29 September 2025.\n\n[4] '10 Year Health Plan for England: Fit for the Future' (GOV.UK, 30 July 2025) <https://www.gov.uk/government/publications/10-year-health-plan-for-england-fit-for-the-future> accessed 29 September 2025.\n\n[5] 'AI Action Plan for Justice' (GOV.UK) <https://www.gov.uk/government/publications/ai-action-plan-for-justice> accessed 29 September 2025.\n\n[6] 'AI Exemplars Programme' (GOV.UK) <https://www.gov.uk/guidance/ai-exemplars-programme> accessed 29 September 2025.\n\n[7] Elliot Jones, 'What Is a Foundation Model?' (Ada Lovelace Institute, 2023) <https://www.adalovelaceinstitute.org/resource/foundation-models-explainer/> accessed 31 July 2025.\n\n[8] 'AI-Enabled Ambient Scribing Products in Health and Care Settings' (NHS England) <https://www.england.nhs.uk/long-read/ai-enabled-ambient-scribing-products-in-health-and-care-settings/> accessed 29 September 2025.\n\n[9] Elliot Jones, 'What Is a Foundation Model?' (Ada Lovelace Institute, 2023) <https://www.adalovelaceinstitute.org/resource/foundation-models-explainer/> accessed 31 July 2025.\n\n[10] Rohit Prabhavalkar and others, 'End-to-End Speech Recognition: A Survey' (arXiv, 3 March 2023) <http://arxiv.org/abs/2303.03329> accessed 10 July 2025.\n\n[11] Hanin Atwany and others, 'Lost in Transcription, Found in Distribution Shift: Demystifying Hallucination in Speech Foundation Models' (arXiv, 18 February 2025) <http://arxiv.org/abs/2502.12414> accessed 24 April 2025.\n\n[12] Elliot Jones, 'What Is a Foundation Model?' (Ada Lovelace Institute, July 2023) <https://www.adalovelaceinstitute.org/resource/foundation-models-explainer/> accessed 31 July 2025.\n\n[13] Laura Weidinger and others, 'Ethical and Social Risks of Harm from Language Models' (arXiv, 8 December 2021) <http://arxiv.org/abs/2112.04359> accessed 13 August 2025.\n\n[14] Merlin Stein and Connor Dunlop, 'Safe before Sale' (Ada Lovelace Institute<https://www.adalovelaceinstitute.org/report/safe-before-sale/> accessed 4 September 2025.\n\n[15] Allison Koenecke and others, 'Careless Whisper: Speech-to-Text Hallucination Harms' (The 2024 ACM Conference on Fairness, Accountability, and Transparency, 2024) <http://arxiv.org/abs/2402.08021> accessed 24 April 2025.\n\n[16] Laura Weidinger and others, 'Ethical and Social Risks of Harm from Language Models' (arXiv, 8 December 2021) <http://arxiv.org/abs/2112.04359> accessed 13 August 2025.\n\n[17] Jiarui Li, Ye Yuan and Zehua Zhang, 'Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases' (arXiv, 15 March 2024) <http://arxiv.org/abs/2403.10446> accessed 28 January 2026.\n\n[18] Laura Carter and others, 'Critical Analytics? Learning from the Early Adoption of Data Analytics for Local Authority Service Delivery' (Ada Lovelace Institute, 2023) <https://www.adalovelaceinstitute.org/report/local-authority-data-analytics/> accessed 10 September 2025.\n\n[19] Ida Lindgren, 'Ironies of Automation and Their Implications for Public Service Automation' (2024) 41 Government Information Quarterly 101974 <https://www.sciencedirect.com/science/article/pii/S0740624X24000662> accessed 13 August 2025.\n\n[20] Dr Paul Walley and Dr Helen Glasspoole-Bird, 'An Evaluation of the Pilot Application of Artificial Intelligence to Witness Statement and Report Generation at Hertfordshire Constabulary' (2025) <https://university.open.ac.uk/centres/policing/research/digitally-enabled-policing/345-herts-ai-test-and-learn> accessed 3 September 2025.\n\n[21] 'AI Knowledge Hub' (GOV.UK) <https://ai.gov.uk/knowledge-hub/> accessed 21 August 2025.\n\n[22] Simon Guerrier, '\"Magic Notes\" AI Tool Saves Social Workers Time on Admin' (Social Care Today, 2024) <https://socialcare.today/2024/09/26/magic-notes-ai-tool-saves-social-workers-time-on-admin/> accessed 21 August 2025.\n\n[23] 'Fit for the Future: 10 Year Health Plan for England' (GOV.UK) <https://assets.publishing.service.gov.uk/media/6888a0996478525675738f3a/fit-for-the-future-10-year-health-plan-for-england-executive-summary.pdf> accessed 19 August 2025.\n\n[24] 'AI Action Plan for Justice' (GOV.UK) <https://www.gov.uk/government/publications/ai-action-plan-for-justice/ai-action-plan-for-justice> accessed 19 August 2025.\n\n[25] 'BASW Statement on Social Work and Generative Artificial' (British Association of Social Workers, 21 March 2025) <https://basw.co.uk/policy-and-practice/resources/basw-statement-social-work-and-generative-artificial-intelligence> accessed 19 August 2025.\n\n[26] 'Neurodiversity at Work: The Power of AI-Driven (The Access Group) <https://www.theaccessgroup.com/en-gb/blog/neurodiversity-at-work-the-power-of-ai-driven-support/> accessed 20 August 2025.\n\n[27] Elliot Jones, 'What Is a Foundation Model?' (Ada Lovelace Institute, 2023) <https://www.adalovelaceinstitute.org/resource/foundation-models-explainer/> accessed 31 July 2025\n\n[28] Ziwei Xu, Sanjay Jain and Mohan Kankanhalli, 'Hallucination Is Inevitable: An Innate Limitation of Large Language Models' (arXiv, 13 February 2025) <http://arxiv.org/abs/2401.11817> accessed 29 September 2025.\n\n[29] Allison Koenecke and others, 'Careless Whisper: Speech-to-Text Hallucination Harms', The 2024 ACM Conference on Fairness, Accountability, and Transparency (2024) <http://arxiv.org/abs/2402.08021> accessed 24 April 2025.\n\n[30] Ajinkya Kulkarni and others, 'Unveiling Biases While Embracing Sustainability: Assessing the Dual Challenges of Automatic Speech Recognition Systems', Interspeech 2024 (ISCA 2024) <https://www.isca-archive.org/interspeech_2024/kulkarni24_interspeech.html> accessed 6 February 2025; Li-Fang Lai and Nicole Holliday, 'Exploring Sources of Racial Bias in Automatic Speech Recognition through the Lens of Rhythmic Variation', INTERSPEECH 2023 (ISCA 2023) <https://www.isca-archive.org/interspeech_2023/lai23_interspeech.html> accessed 11 June 2025.\n\n[31] Allison Koenecke and others, 'Careless Whisper: Speech-to-Text Hallucination Harms', The 2024 ACM Conference on Fairness, Accountability, and Transparency (2024) <http://arxiv.org/abs/2402.08021> accessed 24 April 2025.\n\n[32] Elliot Jones, 'Foundation Models in the Public Sector' (Ada Lovelace Institute 2023) <https://www.adalovelaceinstitute.org/evidence-review/foundation-models-public-sector/> accessed 13 August 2025.\n\n[33] Sam Rickman, 'Evaluating Gender Bias in Large Language Models in Long-Term Care' (2025) 25 BMC Medical Informatics and Decision Making 274 <https://doi.org/10.1186/s12911-025-03118-0> accessed 21 August 2025.\n\n[34] 'ACLU White Paper on Police Departments' Use of AI to Draft Police Reports' (American Civil Liberties Union) <https://www.aclu.org/documents/aclu-on-police-departments-use-of-ai-to-draft-police-reports> accessed 21 July 2025.\n\n[35] Beatrice Nolan, 'UK Health Service AI Tool Generated a Set of False Diagnoses for a Patient' (Fortune) <https://fortune.com/2025/07/20/uk-health-service-ai-tool-false-diagnoses-patient-screening-nhs-anima-health-annie/> accessed 24 July 2025.\n\n[36] Allison Koenecke and others, 'Careless Whisper: Speech-to-Text Hallucination Harms', The 2024 ACM Conference on Fairness, Accountability, and Transparency (2024) <http://arxiv.org/abs/2402.08021> accessed 24 April 2025\n\n[37] Isabel Barberá, 'AI Privacy Risks & Mitigations Large Language Models (LLMs)' (EDPB, 2025) <https://www.edpb.europa.eu/system/files/2025-04/ai-privacy-risks-and-mitigations-in-llms.pdf> accessed 3 September 2025.\n\n[38] Elliot Jones, 'Foundation Models in the Public Sector' (Ada Lovelace Institute 2023) <https://www.adalovelaceinstitute.org/evidence-review/foundation-models-public-sector/> accessed 13 August 2025.\n\n[39] Thomas Dratsch and others, 'Automation Bias in Mammography: The Impact of Artificial Intelligence BI-RADS Suggestions on Reader Performance' (2023) 307 Radiology e222176 <https://pubs.rsna.org/doi/10.1148/radiol.222176> accessed 29 September 2025.\n\n[40] 'Guidance on the Use of AI-Enabled Ambient Scribing Products in Health and Care Settings' (NHS England) <https://www.england.nhs.uk/long-read/guidance-on-the-use-of-ai-enabled-ambient-scribing-products-in-health-and-care-settings/> accessed 28 April 2025.\n\n[41] Dr Paul Walley and Dr Helen Glasspoole-Bird, 'An Evaluation of the Pilot Application of Artificial Intelligence to Witness Statement and Report Generation at Hertfordshire Constabulary' (2025) <https://university.open.ac.uk/centres/policing/research/digitally-enabled-policing/345-herts-ai-test-and-learn> accessed 3 September 2025..\n\n[42] Rebecca O'Keefe, 'Case Recording in Child Protection: An Exploration of the Evidence Base and Good Practice' (2024) 33 Child Abuse Review e2894 <https://onlinelibrary.wiley.com/doi/abs/10.1002/car.2894> accessed 3 September 2025.\n\n[43] Tekendra Parmar, 'Government Documents Show Police Disabling AI Oversight Tools' (Mother Jones) <https://www.motherjones.com/criminal-justice/2025/08/axon-police-ai-draft-one-foia/> accessed 29 September 2025.\n\n[44] Sam Rickman, 'Evaluating Gender Bias in Large Language Models in Long-Term Care' (2025) 25 BMC Medical Informatics and Decision Making 274 <https://doi.org/10.1186/s12911-025-03118-0> accessed 21 August 2025.\n\n[45] Sofia Morandini and others 'The Impact of Artificial Intelligence on Workers' Skills: Upskilling and Reskilling in Organisations' (2023) 26 Informing Science: The International Journal of an Emerging Transdiscipline <https://www.informingscience.org/Publications/5078> accessed 5 September 2025 .\n\n[46] Sofia Morandini and others 'The Impact of Artificial Intelligence on Workers' Skills: Upskilling and Reskilling in Organisations' (2023) 26 Informing Science: The International Journal of an Emerging Transdiscipline <https://www.informingscience.org/Publications/5078> accessed 5 September 2025.\n\n[47] Elliot Jones, Mahi Hardalupas and William Agnew, 'Under the Radar?' (Ada Lovelace Institute 2024) <https://www.adalovelaceinstitute.org/report/under-the-radar/> accessed 4 March 2025\n\n[48] Evani Radiya-Dixit, 'Adopting More Holistic Approaches to Assess the Impacts of AI Systems' (Center for Democracy and Technology, 2025) <https://cdt.org/insights/adopting-more-holistic-approaches-to-assess-the-impacts-of-ai-systems/> accessed 29 September 2025.\n\n[49] 'Exploring Different Objectives in Non-Inferiority <https://www.bmj.com/content/385/bmj-2023-078000> accessed 29 September 2025.\n\n[50] Introducing the Careful Consequence Check' (Careful Industries, 2025) <https://www.careful.industries/blog/2025-11-introducing-the-careful-consequence-check> accessed 25 November 2025.\n\n[51] Katelyn Xiaoying Mei and others, 'Addressing Pitfalls in Auditing Practices of Automatic Speech Recognition Technologies: A Case Study of People with Aphasia' (arXiv, 11 July 2025) <http://arxiv.org/abs/2506.08846> accessed 16 July 2025.\n\n[52] Anna Smerdiagina, 'Lost in Transcription: Experimental Findings on Ethnic and Age Biases in AI Systems' (2024) 9 Junior Management Science (JUMS) 1591 <https://www.econstor.eu/handle/10419/305308> accessed 29 September 2025.\n\n[53] Aymeric Roucher, 'Using LLM-as-a-Judge🧑⚖️ for an Automated and Versatile Evaluation: Hugging Face Open-Source AI Cookbook' (Hugging Face) <https://huggingface.co/learn/cookbook/en/llm_judge> accessed 8 January 2026.\n\n[54] Hugging Face, 'WER: A Hugging Face Space by Evaluate-Metric' <https://huggingface.co/spaces/evaluate-metric/wer> accessed 29 September 2025.\n\n[55] Petrus te Braak and others, 'Data Quality and Recall Bias in Time-Diary Research: The Effects of Prolonged Recall Periods in Self-Administered Online Time-Use Surveys' (2023) 53 Sociological Methodology 115 <https://doi.org/10.1177/00811750221126499> accessed 28 January 2026.\n\n[56] Anastasia Koutsounia, 'AI Tool Improves Direct Work in Adult Social Care despite Accuracy Concerns, Practitioners Report (Community Care, 2025) <https://www.communitycare.co.uk/2025/02/10/ai-tool-adult-social-care-accuracy-issues-practitioners-report/> accessed 29 September 2025.\n\n[57] 'Kingston Council: Using AI in Adult Social Care Administration' (Local Government Association) <https://www.local.gov.uk/case-studies/kingston-council-using-ai-adult-social-care-administration> accessed 29 September 2025.\n\n[58] 'The Evaluation of the M365 Copilot Pilot in the Department for Business and Trade' (GOV.UK) <https://assets.publishing.service.gov.uk/media/68adbe409e1cebdd2c96a19d/dbt-microsoft-365-copilot-evaluation.pdf> accessed 21 October 2025\n\n[59] 'An Evaluation of DWP's Microsoft 365 Copilot Trial' (GOV.UK) <https://www.gov.uk/government/publications/an-evaluation-of-dwps-microsoft-copilot-365-trial/an-evaluation-of-dwps-microsoft-365-copilot-trial> accessed 3 February 2026.\n\n[60] The Decision Lab - Behavioral Science, Applied.' (The Decision Lab) <https://thedecisionlab.com/reference-guide/psychology/social-desirability-bias> accessed 3 February 2026.\n\n[61] Dr Paul Walley and Dr Helen Glasspoole-Bird, 'An Evaluation of the Pilot Application of Artificial Intelligence to Witness Statement and Report Generation at Hertfordshire Constabulary' (2025) <https://university.open.ac.uk/centres/policing/research/digitally-enabled-policing/345-herts-ai-test-and-learn> accessed 3 September 2025.\n\n[62] Pranay Jindal and Joy C MacDermid, 'Assessing Reading Levels of Health Information: Uses and Limitations of Flesch Formula' (2017) 30 Education for Health (Abingdon, England) 84.\n\n[63] Dr Paul Walley and Dr Helen Glasspoole-Bird, 'An Evaluation of the Pilot Application of Artificial Intelligence to Witness Statement and Report Generation at Hertfordshire Constabulary' (The Open University, 2025) <https://university.open.ac.uk/centres/policing/research/digitally-enabled-policing/345-herts-ai-test-and-learn> accessed 3 September 2025.\n\n[64]'AI Opportunities Action Plan' (GOV.UK) <https://www.gov.uk/government/publications/ai-opportunities-action-plan/ai-opportunities-action-plan> accessed 19 August 2025\n\n[65] 'AI Exemplars Programme' (GOV.UK) <https://www.gov.uk/guidance/ai-exemplars-programme> accessed 29 September 2025.\n\n[66] Victoria Hoyle and others, 'Child Social-Care Recording and the Information Rights of Care-Experienced People: A Recordkeeping Perspective' (2019) 49 The British Journal of Social Work 1856 <https://doi.org/10.1093/bjsw/bcy115> accessed 21 August 2025.\n\n[67] 'Key facts and figures about adult social care' (The King's Fund) <https://www.kingsfund.org.uk/insight-and-analysis/data-and-charts/key-facts-figures-adult-social-care> accessed 21 October 2025\n\n[68] 'NHS Backlog Data Analysis' (The British Medical Association) <https://www.bma.org.uk/advice-and-support/nhs-delivery-and-workforce/pressures/nhs-backlog-data-analysis> accessed 1 September 2025.\n\n[69] Magdalena Domínguez, Joe Tomlinson and Ben Zaranko, 'Crown Court Backlog Exacerbated by Post-Pandemic Productivity Slump' (Institute for Fiscal Studies, 6 June 2025) <https://ifs.org.uk/news/crown-court-backlog-exacerbated-post-pandemic-productivity-slump> accessed 4 September 2025.\n\n[70] '80-20 Campaign: How much 'direct' time do social workers spend with children and families?' (British Association of Social Workers) <https://basw.co.uk/sites/default/files/2023-08/FINAL%2080-20%20report.pdf> accessed 21 October 2025\n\n[71] 'Adult Social Care: Key Facts And Figures' (The King's Fund) <https://www.kingsfund.org.uk/insight-and-analysis/data-and-charts/key-facts-figures-adult-social-care> accessed 1 September 2025.\n\n[72] UNISON, 'Social Work and the Impact of the Covid Pandemic' (2022) <https://www.unison.org.uk/content/uploads/2022/06/26799-social-work-survey-FULL-final.pdf> accessed 1 September 2025.\n\n[73] Abi Eccles and others, 'Unintended Consequences of Using Ambient Scribes in General Practice' (2025) 390 BMJ e085754 <https://www.bmj.com/content/390/bmj-2025-085754> accessed 28 August 2025.\n\n[74] 'Fit for the Future: 10 Year Health Plan for England' (GOV.UK) <https://assets.publishing.service.gov.uk/media/6888a0996478525675738f3a/fit-for-the-future-10-year-health-plan-for-england-executive-summary.pdf> accessed 19 August 2025.\n\n[75] Jordan Sollof, 'Accurx and Tandem Health Roll Out AI Scribing Tool across the NHS' (Digital Health, 29 April 2025) <https://www.digitalhealth.net/2025/04/accurx-and-tandem-health-partner-to-roll-out-ai-scribing-in-the-nhs/> accessed 28 August 2025.\n\n[76] 'AI-Enabled Ambient Scribing Products in Health and Care Settings' (NHS England, 2025) <https://www.england.nhs.uk/long-read/ai-enabled-ambient-scribing-products-in-health-and-care-settings/> accessed 21 October 2025.\n\n[77] Russell Brown, 'AI Ambient Scribing - NHS England Warning and Provisional LMC Guidance' (Surrey and Sussex LMCS, 24 September 2025) <https://www.sslmcs.co.uk/news/ai-ambient-scribing/> accessed 29 September 2025.\n\n[78] ETHOS, 'New AI Voice Technology Rules: Is Your NHS Organisation Compliant?' (ETHOS, 27 June 2025) <https://ethos.co.im/new-ai-voice-technology-rules-is-your-nhs-organisation-compliant/> accessed 29 September 2025.\n\n[79] 'AI Action Plan for Justice' (GOV.UK) <https://www.gov.uk/government/publications/ai-action-plan-for-justice/ai-action-plan-for-justice> accessed 19 August 2022.\n\n[80] 'AI Action Plan for Justice' (GOV.UK) <https://www.gov.uk/government/publications/ai-action-plan-for-justice/ai-action-plan-for-justice> accessed 19 August 2022.\n\n[81] 'AI to Cut Paperwork to Free up Doctors' Time for Patients' (GOV.UK) <https://www.gov.uk/government/news/ai-to-cut-paperwork-to-free-up-doctors-time-for-patients> accessed 3 September 2025.\n\n[82] Dr Paul Walley and Dr Helen Glasspoole-Bird, 'An Evaluation of the Pilot Application of Artificial Intelligence to Witness Statement and Report Generation at Hertfordshire Constabulary' (The Open University, 2025) <https://university.open.ac.uk/centres/policing/research/digitally-enabled-policing/345-herts-ai-test-and-learn> accessed 3 September 2025.\n\n[83] 'ACLU White Paper on Police Departments' Use of AI to Draft Police Reports' (American Civil Liberties Union) <https://www.aclu.org/documents/aclu-on-police-departments-use-of-ai-to-draft-police-reports> accessed 21 July 2025.\n\n[84] Mavis Machirori and Anna Studman, 'Spending Wisely' (Ada Lovelace Institute, 2024) <https://www.adalovelaceinstitute.org/report/spending-wisely-procurement/> accessed 4 March 2025.\n\n[85] Elliot Jones and others, 'Under the Radar?' <https://www.adalovelaceinstitute.org/report/under-the-radar/> accessed 28 April 2025.\n\n[86] Mavis Machirori and Anna Studman, 'Buying AI - Is the Public Sector Equipped to Procure Technology in the Public Interest?' (Ada Lovelace Institute, 2024) <https://www.adalovelaceinstitute.org/wp-content/uploads/2024/09/Ada-Lovelace-Institute-Buying-AI.pdf> accessed 21 October 2025.\n\n[87] Mavis Machirori and Anna Studman, 'Buying AI - Is the Public Sector Equipped to Procure Technology in the Public Interest?' (Ada Lovelace Institute, 2024) <https://www.adalovelaceinstitute.org/wp-content/uploads/2024/09/Ada-Lovelace-Institute-Buying-AI.pdf> accessed 21 October 2025.\n\n[88] Mavis Machirori and Anna Studman, 'Spending Wisely' (Ada Lovelace Institute, 2024) <https://www.adalovelaceinstitute.org/report/spending-wisely-procurement/> accessed 4 March 2025.\n\n[89] 'Public Sector Equality Duty' (Equality and Human Rights Commission) <https://www.equalityhumanrights.com/en/advice-and-guidance/public-sector-equality-duty> accessed 2 September 2025.\n\n[90] 'The Seven Principles of Public Life' (GOV.UK) <https://www.gov.uk/government/publications/the-7-principles-of-public-life/the-7-principles-of-public-life-2> accessed 2 September 2025.\n\n[91] Stella M Sieling-Monas and Marie L Meilvang, 'The Practice of Social Work Documentation in an Age of Automatization: A Case from a Danish Municipal Job Centre' (2025) 55 The British Journal of Social Work 2482 <https://academic.oup.com/bjsw/article/55/5/2482/8104249> accessed 1 September 2025.\n\n[92] Sam Rickman, 'Evaluating Gender Bias in Large Language Models in Long-Term Care' (2025) 25 BMC Medical Informatics and Decision Making 274 <https://doi.org/10.1186/s12911-025-03118-0> accessed 21 August 2025."
  },
  {
    "source": "iAfrica",
    "company": "Microsoft AI",
    "title": "Microsoft South Africa, SABC Partner to Deliver AI and Digital Skills Training via SABC Plus - iAfrica.com",
    "date": "2026-01-31T18:28:55Z",
    "url": "https://iafrica.com/microsoft-south-africa-sabc-partner-to-deliver-ai-and-digital-skills-training-via-sabc-plus/",
    "content": "Microsoft South Africa and the South African Broadcasting Corporation announced plans to expand access to artificial intelligence fluency and digital skills training through the broadcaster's digital platform.\n\nThe initiative was announced at the 2026 Microsoft AI Tour in Johannesburg. It is driven by Microsoft Elevate, the company's skills development programme focused on preparing individuals and organisations for an AI-driven economy. The effort builds on the AI Skills Initiative launched in 2025, when Microsoft pledged to train 1 million South Africans by 2026.\n\nSince the start of that initiative, Microsoft says it has engaged 4 million learners, trained 1.4 million people and issued credentials to nearly 500,000 citizens. The collaboration with the SABC is intended to expand inclusive access to digital skills nationwide.\n\nMicrosoft said AI and data capabilities are becoming increasingly important for employability. It cited the World Economic Forum Future of Jobs Report 2025, which identifies AI and data skills among the fastest growing through 2030. LinkedIn data shows AI hiring increased 25% year over year, while AI literacy job postings rose 70%, extending beyond technical roles.\n\nLearning Through SABC Plus\n\nThe companies plan to integrate AI fluency modules, updated digital literacy pathways and co-branded digital badges into SABC Plus. The platform has just over 1.9 million registered users, with about 25% active users, according to the announcement.\n\nThrough SABC Plus, learners will be able to access on-demand content, complete assessments and earn recognized credentials designed to improve employability.\n\n\"AI can be a powerful bridge to opportunity. By partnering with SABC, we aim to embed digital and AI skills into the daily lives of millions of South Africans,\" said Tiara Pathon, Microsoft Elevate AI Skills Director for South Africa. She said the programme will provide practical, credentialed learning pathways for learners, educators and job seekers.\n\nVukani Mngxati, chief executive officer of Microsoft South Africa, said the agreement is aimed at scaling access to skills, accelerating employability and empowering communities through device-based learning at home.\n\nMicrosoft said the programme aligns with national priorities to reduce the digital divide, prepare young people for the future of work and support inclusive participation in an AI-powered economy.\n\nAI Adoption and Broader Programmes\n\nAccording to Microsoft's AI Diffusion Report, South Africa's AI adoption increased from 19.3% in the first half of 2025 to 21.1% in the second half, a rise of 1.8 percentage points. Globally, generative AI tools now reach 16.3% of the population, up from 15.1% earlier in the year. The report says adoption in the Global North is nearly twice as fast as in the Global South, highlighting the need for inclusive skills programmes.\n\nLungile Binza, the SABC's chief operations officer, said the partnership supports the broadcaster's public mandate to inform, educate and empower citizens through accessible digital learning and industry skills development.\n\nThe memorandum of understanding with the SABC builds on several Microsoft Elevate programmes in South Africa, including Ikamva Digital for TVET colleges, ElevateHer for women in technology, Civic AI for non-profits and the Youth Employment Service 50K Certification Programme, which provides free Microsoft certification exam vouchers to young people.\n\nMicrosoft said the joint effort is designed to ensure that access to AI and digital skills training is not limited by geography, background or income level, and to expand practical skills pathways for the future of work."
  },
  {
    "source": "Telepolis",
    "company": "Microsoft AI",
    "title": "Besser als menschliche Ärzte? Microsoft präsentiert KI-System für medizinische Diagnosen",
    "date": "2026-01-31T05:32:06Z",
    "url": "https://www.telepolis.de/article/Besser-als-menschliche-Aerzte-Microsoft-praesentiert-KI-System-fuer-medizinische-Diagnosen-10472474.html",
    "content": "KI ahmt Expertenteam nach und liefert in 85 Prozent der Fälle korrekte Diagnose. Weg zur \"medizinischen Superintelligenz\" in fünf bis 10 Jahren geebnet?\n\nDer US-Technologiekonzern Microsoft hat Details zu einem neuartigen KI-System veröffentlicht, das nach Unternehmensangaben bei komplexen medizinischen Diagnosen deutlich besser abschneidet als menschliche Ärzte.\n\nDas Unternehmen sieht darin einen \"Weg zur medizinischen Superintelligenz\", wie der Guardian berichtet.\n\nDie von dem britischen KI-Pionier Mustafa Suleyman geleitete Abteilung Microsoft AI hat ein System entwickelt, das ein Gremium von Fachärzten bei der Lösung \"diagnostisch komplexer und intellektuell anspruchsvoller\" Fälle imitiert.\n\nLaut Microsoft konnte das System, wenn es mit dem fortschrittlichen o3-KI-Modell von OpenAI gekoppelt wurde, mehr als acht von zehn speziell für die diagnostische Herausforderung ausgewählten Fallstudien \"lösen\".\n\nPraktizierende Ärzte, die keinen Zugang zu Kollegen, Lehrbüchern oder Chatbots hatten, erreichten im Vergleich nur eine Trefferquote von 20 Prozent, wie die Financial Times schreibt.\n\nDas neue System namens \"Microsoft AI Diagnostic Orchestrator\" (MAI-DxO) basiert auf einem sogenannten \"Orchestrator\", der virtuelle Gremien von fünf KI-Agenten erstellt, die als verschiedene \"Ärzte\" agieren. Jeder Agent hat eine bestimmte Rolle, wie etwa das Aufstellen von Hypothesen oder die Auswahl diagnostischer Tests.\n\nDie Agenten interagieren und \"debattieren\" miteinander, um einen Behandlungsverlauf zu wählen.y Um die Fähigkeiten des Systems zu testen, wurde MAI-DxO mit 304 Studien aus dem New England Journal of Medicine (NEJM) gefüttert, die beschreiben, wie einige der kompliziertesten Fälle von Ärzten gelöst wurden.\n\nSo konnten die Forscher testen, ob das Programm in der Lage war, die richtige Diagnose zu stellen und seinen Entscheidungsprozess mit einer neuen Technik namens \"Chain of Debate\" nachzuvollziehen, bei der KI-Modelle Schritt für Schritt darlegen, wie sie Probleme lösen.\n\nMicrosoft setzte führende Large Language Models von OpenAI, Meta, Anthropic, Google, xAI und DeepSeek ein. Der Orchestrator verbesserte die Leistung aller Sprachmodelle, arbeitete aber am besten mit OpenAIs o3-Modell zusammen, mit dem er 85,5 Prozent der NEJM-Fälle korrekt löste.\n\nMicrosoft betonte auch, dass sein Ansatz eine kostengünstigere Option als der Einsatz menschlicher Ärzte sei, da er bei der Anordnung von Tests effizienter arbeite. Trotz der Betonung der potenziellen Kosteneinsparungen durch die Forschung spielte Microsoft die Auswirkungen auf Arbeitsplätze herunter und erklärte, dass KI die Rolle der Ärzte eher ergänzen als ersetzen werde.\n\n\"Ihre klinischen Aufgaben gehen weit über die bloße Diagnosestellung hinaus. Sie müssen mit Mehrdeutigkeit umgehen und mit Patienten und ihren Familien Vertrauen aufbauen, was KI nicht leisten kann\", schrieb das Unternehmen in einem Blogbeitrag.\n\nSuleyman, der CEO von Microsoft AI, sagte gegenüber dem Guardian, dass das System innerhalb des nächsten Jahrzehnts perfekt funktionieren werde.\n\n\"Es ist ziemlich klar, dass wir auf dem Weg sind, dass diese Systeme in den nächsten 5-10 Jahren nahezu fehlerfrei werden. Das wird eine enorme Entlastung für alle Gesundheitssysteme weltweit sein\", so Suleyman.\n\nEric Topol, Kardiologe und Gründer des Scripps Research Translational Institute, bezeichnete die Microsoft-Studie als \"Meilenstein\". \"Auch wenn diese Arbeit nicht im Rahmen der realen medizinischen Praxis durchgeführt wurde, liefert sie als erste den Nachweis für das Effizienzpotenzial von generativer KI in der Medizin - Genauigkeit und Kosteneinsparungen\", sagte Topol."
  },
  {
    "source": "WebProNews",
    "company": "Microsoft AI",
    "title": "Microsoft's $80 Billion AI Gamble Rattles Wall Street as Cloud Growth Slows",
    "date": "2026-01-30T00:34:35Z",
    "url": "https://www.webpronews.com/microsofts-80-billion-ai-gamble-rattles-wall-street-as-cloud-growth-slows/",
    "content": "Microsoft Corporation's stock tumbled more than 6% in after-hours trading Thursday following fourth-quarter earnings that revealed a troubling paradox: the software giant is pouring unprecedented capital into artificial intelligence infrastructure while its core cloud computing business shows signs of deceleration. The Redmond-based technology behemoth announced plans to spend approximately $80 billion on AI-related capital expenditures in fiscal 2025, yet investors recoiled at Azure's growth trajectory and concerns about when these massive investments will translate into meaningful returns.\n\nAccording to Business Insider, the company's Azure cloud platform grew 31% year-over-year in constant currency during the quarter ending December 31, missing analyst expectations of 32.4% growth. While still robust by most industry standards, this marked a continued slowdown from previous quarters and raised questions about whether Microsoft's aggressive AI spending strategy is cannibalizing resources from its established cloud business or simply reflecting broader market saturation in enterprise cloud adoption.\n\nThe earnings report crystallized a tension that has been building throughout the technology sector: how much should companies invest in generative AI capabilities before clear monetization pathways emerge? Microsoft CEO Satya Nadella has bet the company's future on AI integration across its product portfolio, from GitHub Copilot to Copilot for Microsoft 365, yet the financial returns remain difficult to quantify. The company reported revenue of $69.6 billion for the quarter, up 12% year-over-year, but gross margins compressed slightly as capital-intensive AI infrastructure buildouts consumed resources.\n\nThe Infrastructure Arms Race Intensifies\n\nMicrosoft's capital expenditure surge represents one of the largest infrastructure buildouts in corporate history, rivaling the massive investments telecommunications companies made during the fiber optic boom of the late 1990s. The $80 billion figure includes spending on data centers, specialized AI chips, and the energy infrastructure required to power these computational workhorses. Approximately half of this spending is dedicated to building out data center capacity, while the remainder focuses on acquiring graphics processing units (GPUs) and other specialized hardware from suppliers like Nvidia.\n\nThe company faces intense competition from Amazon Web Services and Google Cloud, both of which are making similar infrastructure investments. This three-way race has created supply constraints for critical components, particularly high-end AI chips, driving up costs and extending deployment timelines. Microsoft's partnership with OpenAI, which requires substantial computational resources to train and run large language models, adds another layer of capital intensity to the equation. The company has committed to providing OpenAI with computing infrastructure valued at multiple billions of dollars annually, a relationship that simultaneously positions Microsoft at the forefront of AI development while straining its financial resources.\n\nAzure's Decelerating Growth Trajectory\n\nThe Azure platform, which has been Microsoft's primary growth engine for the past five years, now faces a more challenging environment. Enterprise customers who rushed to migrate workloads to the cloud during the pandemic have largely completed their initial transitions, leaving Microsoft to compete for incremental workloads and new project wins. The 31% growth rate, while impressive in absolute terms, represents a continued deceleration from the 40%+ growth rates Azure achieved in prior years.\n\nCFO Amy Hood acknowledged during the earnings call that some enterprise customers are taking longer to make purchasing decisions amid economic uncertainty, a trend that has affected all major cloud providers. Additionally, Microsoft is experiencing what analysts describe as a \"mix shift\" within Azure, where customers are increasingly consuming AI-specific services that carry different margin profiles than traditional infrastructure-as-a-service offerings. This transition complicates the financial picture, as AI workloads can be more expensive to deliver while pricing remains competitive due to market dynamics.\n\nThe Profitability Puzzle of AI Services\n\nWall Street's negative reaction to Microsoft's earnings stems partly from uncertainty about the profitability of AI services. While the company has successfully integrated AI features across its product suite, including the highly publicized Copilot offerings, the revenue contribution from these features remains modest relative to the infrastructure investments required to support them. Microsoft charges $30 per user per month for Copilot for Microsoft 365, but the computational costs associated with running these AI models can be substantial, particularly during peak usage periods.\n\nIndustry analysts estimate that Microsoft needs to achieve significant scale in AI service adoption before unit economics become favorable. The company reported that Copilot for Microsoft 365 is now being used by more than 70% of Fortune 500 companies, but actual seat penetration within those organizations remains limited. Many enterprises are running pilot programs with small user groups rather than deploying AI tools company-wide, a cautious approach that delays revenue ramps while Microsoft continues to invest heavily in capacity.\n\nCompetitive Pressures and Market Positioning\n\nMicrosoft's AI strategy faces competition not only from traditional cloud rivals but also from a new generation of AI-native companies offering specialized services. Startups like Anthropic, Cohere, and others are attracting enterprise customers with focused AI solutions that may be more cost-effective for specific use cases than Microsoft's integrated platform approach. This fragmentation of the AI market creates uncertainty about whether Microsoft's horizontal platform strategy will dominate or whether vertical, specialized solutions will capture significant market share.\n\nThe company's close relationship with OpenAI provides technological advantages but also creates dependencies and potential conflicts. OpenAI's commercial ambitions, including its own enterprise API business, could eventually compete with Microsoft's AI services, even as Microsoft remains OpenAI's primary infrastructure provider and largest investor. This complex relationship requires careful management to ensure both parties benefit while avoiding channel conflict that could undermine Microsoft's market position.\n\nEnergy and Sustainability Challenges\n\nThe massive scale of Microsoft's AI infrastructure buildout has created significant energy consumption challenges that carry both financial and reputational implications. Data centers required to train and run large AI models consume enormous amounts of electricity, and Microsoft has struggled to meet its sustainability commitments while simultaneously expanding AI capacity. The company has pledged to become carbon negative by 2030, yet its carbon emissions have increased approximately 30% since 2020, largely due to data center expansion.\n\nThis tension between AI ambitions and environmental goals has prompted Microsoft to invest heavily in renewable energy projects and explore novel solutions like small modular nuclear reactors to power future data centers. These initiatives add further capital requirements to an already stretched budget and may not deliver results for years. The energy intensity of AI workloads also affects operating expenses, as electricity costs represent a growing portion of data center operational budgets, potentially squeezing margins even as revenue grows.\n\nMarket Reaction and Investor Sentiment\n\nThe sharp decline in Microsoft's stock price following earnings reflects investor concern that the company's AI investments may take longer to pay off than previously anticipated. With shares trading at approximately 30 times forward earnings before the decline, Microsoft carried a premium valuation that assumed strong execution on its AI strategy. The combination of slowing Azure growth and accelerating capital expenditures has prompted some analysts to downgrade their near-term price targets, even while maintaining positive long-term outlooks.\n\nSeveral Wall Street analysts issued cautious notes following the earnings release, emphasizing the need for Microsoft to demonstrate clearer paths to AI monetization. The market's reaction suggests that investors are becoming more discriminating about AI investments, moving past the initial euphoria that drove technology stocks higher in 2023 and early 2024. Companies must now show not just AI capabilities but also realistic timelines for achieving profitable scale, a higher bar that Microsoft has not yet cleared to investor satisfaction.\n\nStrategic Implications for the Technology Sector\n\nMicrosoft's challenges have broader implications for the technology industry's AI investment cycle. If the sector's most financially robust company with the deepest enterprise relationships struggles to monetize AI quickly, it raises questions about the timeline for AI profitability across the industry. Smaller companies with less capital and fewer established customer relationships may face even steeper challenges in achieving sustainable AI businesses, potentially leading to consolidation as the market matures.\n\nThe situation also highlights the winner-take-most dynamics that may characterize the AI infrastructure market. Companies that achieve scale first can spread fixed costs across larger revenue bases, creating competitive advantages that become self-reinforcing. Microsoft's willingness to invest $80 billion reflects a strategic calculation that securing market position now justifies near-term financial pressure, but this approach requires patient capital and tolerance for extended payback periods that not all companies can sustain.\n\nThe Path Forward\n\nMicrosoft faces a delicate balancing act in coming quarters: maintaining aggressive AI investments to secure long-term competitive position while demonstrating enough near-term progress to satisfy investors demanding returns. The company's management team emphasized during the earnings call that AI revenue is growing rapidly from a small base and that customer engagement metrics remain strong, suggesting that monetization will accelerate as adoption matures. However, translating these qualitative indicators into quantitative financial results that justify the investment scale remains the critical challenge.\n\nThe technology giant's ability to navigate this transition will likely determine not only its own trajectory but also set precedents for how the broader industry approaches AI commercialization. As enterprises become more sophisticated AI consumers, they will demand clear value propositions and measurable returns on their AI spending, forcing providers like Microsoft to refine their offerings and pricing models. The next several quarters will reveal whether Microsoft's massive bet on AI infrastructure represents visionary leadership or a cautionary tale of overinvestment ahead of market readiness, with implications that will reverberate throughout the technology sector for years to come."
  },
  {
    "source": "WebProNews",
    "company": "Microsoft AI",
    "title": "Microsoft's Copilot Adoption Claims Face Scrutiny as Enterprise AI Investment Reaches Inflection Point",
    "date": "2026-01-30T00:34:22Z",
    "url": "https://www.webpronews.com/microsofts-copilot-adoption-claims-face-scrutiny-as-enterprise-ai-investment-reaches-inflection-point/",
    "content": "Microsoft CEO Satya Nadella's recent assertions about widespread Copilot adoption have ignited a critical debate about the true penetration of enterprise artificial intelligence tools, even as the company reports unprecedented revenue growth from its AI initiatives. Speaking at the company's fiscal second-quarter earnings call, Nadella emphasized that Microsoft's AI assistant has achieved significant traction across its customer base, yet questions persist about whether usage metrics align with the transformative promises that justified billions in infrastructure investment.\n\nAccording to TechCrunch, Nadella stated that \"people are using Microsoft's Copilot AI a lot,\" pointing to engagement numbers that the company characterizes as validation of its AI-first strategy. The declaration comes at a pivotal moment when enterprise customers are evaluating whether generative AI tools deliver measurable productivity gains that justify their premium pricing structures. Microsoft 365 Copilot, priced at $30 per user monthly on top of existing subscription costs, represents a substantial financial commitment for organizations already grappling with economic uncertainty.\n\nThe timing of Nadella's emphatic defense proves particularly significant given mounting pressure from investors seeking concrete returns on Microsoft's estimated $50 billion annual AI infrastructure spending. While the company reported that AI services contributed substantially to Azure's growth, translating cloud consumption into sustained Copilot seat adoption remains the ultimate test of whether generative AI has moved beyond experimental deployments into mission-critical workflows.\n\nThe Measurement Challenge: Defining Success in Enterprise AI Adoption\n\nDetermining what constitutes meaningful AI adoption has emerged as one of the technology industry's most contentious measurement challenges. Microsoft has disclosed that hundreds of thousands of organizations use Copilot, but the company has been notably circumspect about providing granular metrics around daily active users, feature utilization rates, or the percentage of licensed seats that generate regular engagement. This opacity has fueled skepticism among analysts who note that enterprise software frequently suffers from the \"shelfware\" phenomenon, where purchased licenses go largely unused.\n\nIndustry observers point out that Microsoft's definition of usage may encompass a broad spectrum of engagement levels, from employees who interact with Copilot dozens of times daily to those who activate the feature sporadically for specific tasks. The distinction matters enormously for assessing whether the technology has achieved the workflow integration necessary to drive renewal rates and expansion sales. Without standardized benchmarks for AI adoption, companies can present vastly different narratives using technically accurate but strategically selective data points.\n\nThe challenge extends beyond simple usage statistics to questions of value realization. Early enterprise adopters report mixed results, with some departments finding Copilot indispensable for drafting communications and analyzing data, while others struggle to identify use cases that justify the cost. This variability suggests that successful deployment requires not just licensing the technology but investing in change management, training, and workflow redesign -- factors that Microsoft's usage claims may not fully capture.\n\nRevenue Signals Versus Adoption Reality: Parsing Microsoft's AI Economics\n\nMicrosoft's financial disclosures provide some clarity while raising additional questions about the economics underlying Copilot adoption. The company has indicated that AI services are growing faster than any product in its history, yet this growth encompasses Azure AI infrastructure consumed by both Microsoft's own services and third-party developers, not solely Copilot subscriptions. Disentangling these revenue streams proves essential for understanding whether enterprises are primarily buying AI capabilities or AI-enabled productivity tools.\n\nThe distinction carries profound implications for Microsoft's competitive positioning. Azure's role as the infrastructure layer for OpenAI and numerous other AI applications creates a business model less dependent on Copilot's direct adoption than on the broader ecosystem of AI workloads. This diversification provides Microsoft with multiple paths to monetize its AI investments, but it also means that strong Azure growth doesn't necessarily validate Copilot as a transformative productivity tool.\n\nFinancial analysts note that Microsoft's willingness to bundle Copilot capabilities into existing Microsoft 365 tiers, as the company has done selectively, suggests a strategic calculation that broader adoption may prove more valuable than premium pricing in the near term. This approach mirrors tactics Microsoft employed successfully with Teams, where aggressive bundling eventually created network effects that marginalized competitors. Whether the same strategy succeeds with AI tools remains uncertain, particularly given the computational costs that make free or bundled AI features economically challenging at scale.\n\nThe Enterprise Decision Calculus: Why Organizations Hesitate Despite the Hype\n\nConversations with enterprise IT leaders reveal a complex decision calculus that extends well beyond Copilot's technical capabilities. Chief information officers consistently cite concerns about data governance, accuracy verification, and integration complexity as factors that slow deployment even when pilot programs demonstrate value. The requirement to ensure that AI-generated content meets regulatory standards for industries like healthcare, finance, and legal services creates validation overhead that can negate productivity gains.\n\nSecurity and compliance teams have raised particular concerns about Copilot's access to organizational data repositories, fearing that the tool might inadvertently expose sensitive information or facilitate data exfiltration. Microsoft has implemented controls designed to respect existing permissions and prevent unauthorized access, but enterprise security architectures often require additional layers of protection that complicate deployment. These technical and procedural hurdles mean that even enthusiastic organizations may take months to progress from limited pilots to company-wide rollouts.\n\nThe skills gap presents another significant barrier to adoption. Maximizing Copilot's value requires employees to develop proficiency in prompt engineering and to understand the tool's capabilities and limitations sufficiently to verify outputs. Organizations report that effective use demands more than simply enabling the feature -- it requires structured training programs and the development of best practices tailored to specific roles and workflows. The investment required for this enablement adds to the total cost of ownership in ways that Microsoft's per-seat pricing doesn't fully reflect.\n\nCompetitive Dynamics: The Battle for Enterprise AI Mindshare Intensifies\n\nMicrosoft's aggressive promotion of Copilot adoption occurs against a backdrop of intensifying competition from Google, Anthropic, and a growing array of specialized AI vendors. Google's Workspace AI features, offered at comparable pricing, provide enterprises with an alternative that integrates with a different productivity ecosystem. Meanwhile, companies like Anthropic have positioned Claude as a more controllable, safer option for enterprises with stringent compliance requirements, potentially fragmenting the market that Microsoft seeks to dominate.\n\nThe competitive pressure extends beyond feature parity to questions of strategic control. Some enterprises express concern about deepening dependence on Microsoft's AI infrastructure, particularly as the company's partnership with OpenAI creates uncertainty about the long-term direction of the underlying models. Organizations that have invested heavily in multi-cloud strategies view reliance on Copilot as potentially creating vendor lock-in that limits flexibility and negotiating leverage.\n\nEmerging open-source alternatives add another dimension to the competitive environment. While these solutions currently lack the polish and integration of commercial offerings, they appeal to organizations with the technical capabilities to deploy and customize AI models internally. The total cost of ownership for open-source approaches remains debatable, but the option provides a credible alternative that may constrain Microsoft's pricing power and force greater transparency around usage metrics and value delivery.\n\nThe Path Forward: What Widespread Adoption Actually Requires\n\nAchieving the ubiquitous adoption that Nadella's comments suggest will require Microsoft to address several fundamental challenges. First, the company must demonstrate clear ROI metrics that resonate with CFOs evaluating whether to expand deployments beyond initial pilot programs. Vague productivity claims need to give way to industry-specific benchmarks showing measurable improvements in time-to-completion, error reduction, or output quality for defined tasks.\n\nSecond, Microsoft faces the challenge of evolving Copilot from a feature that handles discrete tasks to a platform that fundamentally reshapes workflows. Current usage patterns suggest that many employees treat Copilot as an occasional assistant rather than an integral component of their daily work. Bridging this gap requires not just technical improvements but a reimagining of business processes to leverage AI capabilities fully -- a transformation that extends far beyond Microsoft's direct control.\n\nThe company's credibility in making adoption claims will increasingly depend on its willingness to provide detailed, auditable metrics that allow independent verification. As enterprise AI moves from experimental to operational status, customers demand the same rigor in usage reporting that they expect for other mission-critical systems. Microsoft's ability to deliver this transparency while protecting competitive information will significantly influence whether Nadella's assertions about widespread adoption gain acceptance or continue to face skepticism.\n\nThe stakes extend beyond Microsoft's quarterly results to the broader question of whether generative AI can deliver on its transformative promise in enterprise settings. If Copilot succeeds in achieving genuine mass adoption with demonstrable productivity gains, it validates the technology industry's massive AI investments and accelerates the shift toward AI-augmented work. If adoption remains confined to enthusiastic early adopters while the majority of licensed users engage sporadically or not at all, it suggests that the path to AI transformation may prove longer and more complex than current valuations assume. Nadella's insistence that people are using Copilot extensively represents more than corporate messaging -- it's a claim that the future of work has already arrived, even as the evidence remains subject to interpretation."
  },
  {
    "source": "ITNewsAfrica.com",
    "company": "Microsoft AI",
    "title": "Microsoft Reports Record Net Income as OpenAI Investment Drives AI Growth - IT News Africa | Business Technology, Telecoms and Startup News",
    "date": "2026-01-29T14:51:23Z",
    "url": "https://www.itnewsafrica.com/2026/01/microsoft-reports-record-net-income-as-openai-investment-drives-ai-growth/",
    "content": "Microsoft Corp. has posted strong second-quarter results for the fiscal year ended December 31, 2025, with net income surging 60% on a GAAP basis to $38.5 billion, driven in large part by its strategic investments in OpenAI. Diluted earnings per share similarly rose 60% to $5.16, reflecting the company's growing influence in the cloud and AI markets.\n\n\"Microsoft has built an AI business that is larger than some of our biggest franchises,\" said Satya Nadella, chairman and CEO. \"We are pushing the frontier across our entire AI stack to drive new value for our customers and partners.\"\n\nMicrosoft's overall revenue for the quarter reached $81.3 billion, up 17% from the same period last year, while operating income increased 21% to $38.3 billion. Non-GAAP net income, which excludes the impact from OpenAI investments, was $30.9 billion, up 23%, showing strong operational growth independent of AI gains.\n\nThe company reported that its net income was significantly boosted by net gains from OpenAI investments, which contributed $7.6 billion and $1.02 per share this quarter. In comparison, OpenAI investments had reduced net income by $939 million in the same quarter last year, highlighting the dramatic financial upside of Microsoft's AI strategy.\n\nMicrosoft Cloud remains a major growth driver, with revenue crossing $50 billion for the quarter. Azure and other cloud services increased 39%, while Microsoft 365 Commercial and Consumer cloud revenues rose 17% and 29%, respectively. LinkedIn revenue grew 11%, and Dynamics 365 increased 19%, reflecting broad demand for Microsoft's AI-augmented enterprise solutions.\n\nWhile the More Personal Computing segment saw a slight decline of 3% in revenue, gains in Windows OEM and Devices, as well as search advertising, helped offset some of the softness.\n\nFinancial discipline remains a focus, with Microsoft returning $12.7 billion to shareholders through dividends and share repurchases -- a 32% increase year-over-year. The company's commercial remaining performance obligation also grew 110% to $625 billion, underscoring strong future revenue visibility.\n\nAmy Hood, CFO, emphasized, \"We exceeded expectations across revenue, operating income, and earnings per share, reflecting the growing strength of Microsoft Cloud and AI.\"\n\nWith OpenAI investments now translating into tangible financial returns, Microsoft's AI strategy is not just shaping the future of computing -- it is also delivering immediate shareholder value, positioning the company as a leader in the next wave of enterprise technology."
  },
  {
    "source": "PYMNTS.com",
    "company": "Microsoft AI",
    "title": "Microsoft's AI Growth Drives Both Revenue and Massive Capital Expenditure | PYMNTS.com",
    "date": "2026-01-29T00:59:44Z",
    "url": "https://www.pymnts.com/earnings/2026/microsofts-ai-growth-drives-both-revenue-and-massive-capital-expenditure/",
    "content": "By migrating enterprises from on-premise software to subscription services and hyperscale infrastructure, the Redmond, Washington-based tech giant in many senses rewired corporate infrastructure from the ground-up, or the cloud-down.\n\nBut if Microsoft's second quarter fiscal 2026 earnings call Wednesday (Jan. 28) is any indication, the company's latest transformation story now revolves around artificial intelligence (AI).\n\n\"We are only at the beginning phases of AI diffusion and already Microsoft has built an AI business that is larger than some of our biggest franchises We are pushing the frontier across our entire AI stack to drive new value for our customers and partners,\" Satya Nadella, chairman and CEO of Microsoft told investors, according to the earnings release.\n\nThe clearest signal of Microsoft's AI strategy appeared in its Intelligent Cloud segment. Revenue there rose 29% year over year to $32.9 billion, with Azure and other cloud services growing 39% in reported terms. While Microsoft does not break out AI revenue explicitly, Azure has become the primary delivery mechanism for large-scale AI workloads, from training foundation models to deploying inference at enterprise scale.\n\n\"Microsoft Cloud revenue crossed $50 billion this quarter, reflecting the strong demand for our portfolio of services,\" Amy Hood, Microsoft executive vice president and CFO, said in the earnings release.\n\nThe company's overall reported revenue for the second quarter reached $81.3 billion, up 17% year over year, while Microsoft's operating income climbed 21% to $38.3 billion.\n\nDespite topping analyst estimates, the tech firm's share price fell mid-single-digits in after-hours trading due to concerns around AI-driven capital expenditures, which were up 66% for the quarter.\n\nAsked about the movement during the investor Q&A, both Nadella and Hood stressed that the short-lived assets, primarily GPUs and CPUs, are \"already sold for their entire useful life.\"\n\nMore here: AI Doers Drown Out AI Naysayers\n\nMicrosoft's AI Advantage Is No Longer Theoretical, Nor Is it Cheap\n\nAs executives told investors in response to questions about capital expenditures, the reason is that Microsoft is aiming to control the full AI stack. Azure is not just renting GPUs; it is increasingly bundling model access, orchestration tools, security and governance into a single enterprise-ready environment. This reduces friction for customers who want AI capabilities without managing fragmented vendor relationships.\n\nThe result is a flywheel: AI demand drives Azure usage, which in turn justifies further infrastructure investment, reinforcing Microsoft's scale advantage.\n\nMicrosoft's 110% increase in commercial remaining performance obligation, which now stands at $625 billion, underscores how sticky this demand has become. Enterprises are not experimenting with AI on short contracts; they are committing to long-term capacity.\n\nPer the company's financials, while Microsoft keeps 80% of sales of OpenAI models to Azure customers, it retains a smaller percentage of its sales of Anthropic's AI models.\n\nStill, chasing AI leadership is expensive. Microsoft's balance sheet reflects a dramatic expansion in property and equipment, with net assets rising to $261 billion. Cash flow from operations remains robust, but free cash flow is increasingly shaped by infrastructure investment decisions that may take years to pay off.\n\nIt was covered here how Microsoft is part of a small group of technology giants expected to spend more than $500 billion combined on capital expenditures in 2026, largely driven by investments in data centers, chips and AI infrastructure. Microsoft's capital expenditures including assets acquired under finance leases were $37.5 billion for the most recent quarter.\n\nTraining large models requires massive capital expenditure in specialized hardware, while inference workloads create persistent, high-margin consumption over time.\n\nThe company is still early in what Nadella called the diffusion phase of AI, and diffusion favors platforms over point solutions. Microsoft's ability to deliver AI as infrastructure, application and service could position it favorably, and the company's accelerating investments in data centers signal confidence that demand for AI-driven compute may remain structurally higher than traditional cloud demand.\n\nSee also: Enterprise AI Gets Real as Davos 2026 Focuses on Agents\n\nProductivity Reimagined\n\nOne often overlooked aspect of Microsoft's AI strategy is governance. The company repeatedly emphasizes responsible deployment, security and compliance not merely as ethical commitments, but as commercial necessities. Large enterprises and governments are unlikely to adopt AI at scale without assurances around data protection, explainability and regulatory alignment.\n\nThat governance supports the company's Microsoft 365 Productivity and Business Processes business, where revenue grew 16% to $34.1 billion for the quarter, driven by strong performance across commercial and consumer cloud offerings. Microsoft 365 Commercial cloud revenue increased 17%, while consumer cloud revenue jumped 29%.\n\nDynamics 365 revenue rose 19%, reflecting how AI is also reshaping business applications. Predictive forecasting, automated workflows and conversational interfaces are increasingly table stakes in the customer relationship management (CRM) and enterprise resource planning (ERP) systems that finance leaders use.\n\nThe PYMNTS Intelligence report \"Smart Spending: How AI Is Transforming Financial Decision Making\" found that more than 8 in 10 CFOs at large companies are either already using AI or considering adopting it."
  },
  {
    "source": "Tokenist",
    "company": "Microsoft AI",
    "title": "Richtech Robotics (RR) Shares Surge on Microsoft AI Collaboration",
    "date": "2026-01-27T15:31:28Z",
    "url": "https://tokenist.com/richtech-robotics-rr-shares-surge-on-microsoft-ai-collaboration/",
    "content": "Neither the author, Tim Fries, nor this website, The Tokenist, provide financial advice. Please consult our website policy prior to making financial decisions.\n\nRichtech Robotics Inc. (Nasdaq: RR) experienced significant stock movement on January 27, 2026, following the announcement of a strategic collaboration with Microsoft through the Microsoft AI Co-Innovation Labs. The partnership focuses on integrating agentic artificial intelligence capabilities into the company's real-world robotic systems, particularly enhancing the ADAM robot with adaptive intelligence powered by Azure AI.\n\nAs of 10:02 AM EST, RR shares were trading at $4.2250, representing a gain of $0.4150 or 10.89% from the previous close of $3.81, with trading volume reaching 25.6 million shares against an average of 28 million.\n\nNew Microsoft Collaboration Adds Agentic AI to Richtech's Robotics Platform\n\nThe collaboration between Richtech Robotics' engineering team and Microsoft's AI Co-Innovation Labs has resulted in significant enhancements to the ADAM robot. The robot now features additional layers of context awareness, allowing it to incorporate environmental signals such as time of day, weather conditions, and promotional information.\n\nThese capabilities enable ADAM to respond more naturally to customer preferences and maintain speed and quality during peak demand periods through vision-based models.\n\nBeyond customer interaction, the enhanced ADAM robot now supports operational awareness by notifying staff of ingredient shortages or equipment issues before disruptions occur. This proactive approach is designed to create smoother workflows and more responsive customer interactions in retail environments.\n\nWhile ADAM serves as the flagship example, the collaboration demonstrates how these agentic AI capabilities can be applied across logistics, hospitality, manufacturing, and other operational settings where real-time perception, reasoning, and reliability are essential.\n\nWayne Huang, Founder and CEO of Richtech Robotics, emphasized that the collaboration reflects a shared focus on applying advanced AI to practical, real-world use cases. By working closely with Microsoft AI Co-Innovation Labs, the teams jointly developed and deployed intelligent capabilities that strengthen reliability, enhance customer interactions, and support scalable automation across physical environments without requiring extensive new hardware investments.\n\nStock Performance and Market Position\n\nFollowing the announcement, RR shares opened at $4.22, up from the previous close of $3.81, and traded in a range of $4.05 to $4.41 during the session. The stock's market capitalization reached approximately $886 million, with the company showing strong year-to-date performance of 26.88% compared to the S&P 500's 1.78% gain.\n\nOver the past year, RR has delivered returns of 42.79%, significantly outperforming the broader market's 15.89% return during the same period.\n\nThe company's 52-week range spans from $1.37 to $7.43, indicating significant volatility in the stock. Analyst coverage remains positive, with HC Wainwright & Co. maintaining a \"Buy\" rating and a $6 price target as of January 21, 2026.\n\nThe average analyst price target stands at $4.25, with estimates ranging from a low of $2.50 to a high of $6.00. The company's next earnings announcement is expected on February 12, 2026.\n\nRichtech Robotics operates in the specialty industrial machinery sector within the broader industrials industry. The company develops, manufactures, deploys, and sells robotic solutions for automation in the service industry, with products including the Matradee restaurant service robot line and dual-arm AI-powered robots like ADAM and Scorpion for beverage preparation and customer interaction.\n\nThe collaboration with Microsoft underscores Richtech's continued investment in data-driven automation and physical AI, leveraging cloud intelligence to improve performance across commercial and industrial applications.\n\nDisclaimer: The author does not hold or have a position in any securities discussed in the article. All stock prices were quoted at the time of writing."
  },
  {
    "source": "thefastmode.com",
    "company": "Microsoft AI",
    "title": "HKTDC, Microsoft Launch AI Programme for SMEs in Hong Kong",
    "date": "2026-01-26T00:52:10Z",
    "url": "https://www.thefastmode.com/technology-solutions/46807-hktdc-microsoft-launch-ai-programme-for-smes-in-hong-kong",
    "content": "HKTDC and Microsoft Hong Kong jointly launch the Microsoft AI Adoption Programme Helping SMEs boost productivity through AI and accelerate digital transformation\n\nThe Hong Kong Trade Development Council (HKTDC) has collaborated with Microsoft Hong Kong to launch the Microsoft AI Adoption Programme, which will support Hong Kong SMEs in harnessing artificial intelligence (AI) to enhance productivity and operational efficiency.\n\nThe programme includes three thematic workshops featuring industry experts who will share insights on how businesses can effectively apply AI in real-world scenarios to improve workflow efficiency. In addition, a series of exclusive AI solution offers will be provided to help companies accelerate AI adoption and drive digital transformation.\n\nThe first workshop, held two days ago, was titled \"The Intelligence Chain - How AI Connects Every Workflow in Your Business.\" The session featured Fred Sheu, National Technology Officer at Microsoft Hong Kong; Frankie Lam, Go-To-Market Lead of AI Business Solutions at Microsoft Hong Kong; and Vincent Chak, Solutions Director - Modern Workplace at SOS Group Limited. Through product demonstrations and real-life case sharing, the speakers introduced AI solutions designed for the modern workplace and provided an in-depth look into core strategies and best practices for effective AI adoption in business operations.\n\nTwo additional workshops will take place in March and April, titled \"Building AI Applications and Intelligent Processes: Practical Experience with Azure AI\" and \"AI Leading a New Era of Operational Automation\". The March session will showcase how intelligent solutions can be used to analyse data, and we will explore how automation and predictive analytics can streamline workflows and enhance efficiency in April. The Microsoft AI Adoption Programme also offers exclusive Microsoft discounts to companies participating in T-box, helping SMEs adopt AI technologies in a more cost-effective manner. For details, please visit the programme website: https://smesupport.hktdc.com/en/s/sme-microsoft/"
  },
  {
    "source": "Revista Summa",
    "company": "Microsoft AI",
    "title": "Costa Rica lidera la adopción de inteligencia artificial en América Latina, según informe del Microsoft",
    "date": "2026-01-22T20:35:48Z",
    "url": "https://revistasumma.com/costa-rica-lidera-la-adopcion-de-inteligencia-artificial-en-america-latina-segun-informe-del-microsoft/",
    "content": "Somos el principal grupo editorial de revistas en América Central y el Caribe.\n\nSegún informe del Microsoft AI Economy Institute.\n\nPor Revista Summa\n\nLa adopción global de la inteligencia artificial (IA) continuó en aumento durante la segunda mitad de 2025, y Costa Rica se consolida como el país con mayor nivel de adopción en América Latina. De acuerdo con el más reciente informe del Microsoft AI Economy Institute, Costa Rica alcanzó un índice de adopción del 26,5% entre la población en edad laboral en el segundo semestre del 2025, registrando un crecimiento de 1,4 puntos porcentuales frente al semestre anterior y posicionándose dentro del ranking de los primeros 30 países del mundo en adopción de herramientas de IA generativa. Estos resultados reflejan el creciente interés del país por explorar y aprovechar el uso de la inteligencia artificial en distintos ámbitos de la vida cotidiana, el trabajo y los servicios.\n\nA nivel global, el informe señala que la adopción de la inteligencia artificial aumentó 1,2 puntos porcentuales en la segunda mitad de 2025 en comparación con la primera mitad del año. Actualmente, alrededor de una de cada seis personas en el mundo utiliza herramientas de IA generativa, un avance significativo para una tecnología que, hasta hace poco, apenas comenzaba a integrarse en el uso generalizado.\n\nPara dar seguimiento a esta tendencia, el Microsoft AI Economy Institute mide la difusión de la IA como la proporción de personas a nivel mundial que han utilizado un producto de IA generativa durante el periodo evaluado. Esta medición se basa en telemetría agregada y anonimizada de Microsoft, ajustada para reflejar diferencias entre países en variables como la cuota de mercado de sistemas operativos y dispositivos, la penetración de internet y el tamaño de la población. Si bien ninguna métrica es perfecta, el informe destaca que se trata de la medida más sólida disponible actualmente para evaluar la adopción de la IA a nivel nacional.\n\nA pesar de los avances en la adopción de la IA, los datos muestran una brecha cada vez mayor: la adopción en el Norte Global creció casi el doble de rápido que en el Sur Global. Como resultado, el 24,7 por ciento de la población en edad de trabajar en el Norte Global utiliza ahora estas herramientas, frente a solo el 14,1 por ciento en el Sur Global.\n\nPaíses que han invertido pronto en infraestructura digital, habilidades en IA y adopción gubernamental, como Emiratos Árabes Unidos, Singapur, Noruega, Irlanda, Francia y España, continúan con su liderazgo. Los EAU ampliaron su ventaja como el país número uno, con un 64,0% de la población en edad laboral que utiliza IA a finales de 2025, frente al 59,4% a principios de año. Los EAU han abierto una ventaja de más de tres puntos porcentuales sobre Singapur, que sigue en segundo lugar con un 60,9 por ciento de adopción. La segunda mitad del año en Estados Unidos muestra que el liderazgo en innovación e infraestructuras, aunque fundamental, no conduce por sí solo a una adopción generalizada de la IA. Estados Unidos lidera tanto en infraestructura de IA como en desarrollo de modelos de frontera, pero cayó del puesto 23 al 24\n\nen uso de IA entre la población en edad laboral, con una tasa de uso del 28,3 por ciento. Está muy por detrás de economías más pequeñas, más digitalizadas y centradas en la IA.\n\nCorea del Sur destaca como la historia de éxito de fin de año más clara. Subió siete puestos en el ranking global, para pasar del puesto 25 al 18, impulsado por políticas gubernamentales, mejoras en las capacidades del modelo de frontera en el idioma coreano y características orientadas al consumidor que conectaron con la población. La IA generativa se utiliza ahora en escuelas, lugares de trabajo y servicios públicos, y Corea del Sur se ha convertido en uno de los mercados de más rápido crecimiento de ChatGPT, lo que ha llevado a OpenAI a abrir una oficina en Seúl.\n\nUn desarrollo paralelo que transformó el panorama global en 2025 fue el rápido auge de DeepSeek, una plataforma de IA de código abierto que ha ganado una tracción significativa en mercados que durante mucho tiempo han sido desatendidos por los proveedores tradicionales. Al lanzar su modelo bajo una licencia de código abierto del MIT y ofrecer un chatbot gratuito, DeepSeek eliminó tanto barreras financieras como técnicas que limitaban el acceso a IA avanzada. Su adopción más fuerte, como era de esperar, ha surgido en China, Rusia, Irán, Cuba y Bielorrusia. Pero quizá aún más destacable es la creciente popularidad de DeepSeek en toda África, donde se ve impulsada por la promoción estratégica y las asociaciones con empresas como Huawei.\n\nEsta rápida evolución subraya una dimensión cada vez más importante de la competencia en IA entre Estados Unidos y China, que implica una carrera para promover la adopción de sus respectivos modelos nacionales. El éxito de DeepSeek refleja el creciente impulso chino en toda África, una tendencia que podría seguir acelerándose en 2026. El ascenso de DeepSeek también subraya una verdad más amplia: la difusión global de la IA está influida por factores de accesibilidad, y la próxima oleada de usuarios podría provenir de comunidades que a nivel histórico han tenido acceso limitado al progreso tecnológico. El reto que se enfrenta es asegurar que la innovación se extienda de formas que ayuden a reducir las divisiones en lugar de profundizarlas."
  },
  {
    "source": "Benzinga",
    "company": "Microsoft AI",
    "title": "Seneca Polytechnic, together with Microsoft, expands commitment to advance learning with AI",
    "date": "2026-01-22T14:57:17Z",
    "url": "https://www.benzinga.com/pressreleases/26/01/g50066629/seneca-polytechnic-together-with-microsoft-expands-commitment-to-advance-learning-with-ai",
    "content": "Enter your email to get Benzinga's ultimate morning update: The PreMarket Activity Newsletter\n\nTORONTO, Jan. 22, 2026 (GLOBE NEWSWIRE) -- Seneca Polytechnic and Microsoft are deepening their collaboration to further enhance the postsecondary experience for students at Seneca.\n\nBuilding on a strategic relationship first announced in 2024, today's announcement sets out a new multi-year commitment that Seneca has signed with Microsoft. It introduces new elements and AI advancements designed to enhance learning and innovation across disciplines, and help students achieve their career goals.\n\nThis expanded relationship with Microsoft showcases how Seneca is continuing to align with higher education and employment trends. In addition to providing students and employees with early access to the latest Microsoft AI innovations, Seneca will continue to innovate with Microsoft Foundry (formerly Azure AI Platform). Foundry is Microsoft's unified, interoperable AI platform.\n\nWorking together with Microsoft partner Adastra, Seneca will integrate agentic AI learning opportunities into the classroom. This includes providing Microsoft Azure tools to students to help advance their AI knowledge. Within AI-focused programs, Seneca faculty will be working with Microsoft to integrate agentic AI into curriculum, equipping students with hands-on experience to help set them up for success in emerging job markets. Seneca is also offering co-op opportunities to students in various technology programs to work with Adastra and Seneca's ITS department to build custom AI solutions.\n\n\"Seneca is delighted to continue working with Microsoft as we explore new ways to equip students with the skills and knowledge today's workplaces demand,\" said David Agnew, President, Seneca Polytechnic. \"Together, we are accelerating innovation by embracing AI as central to education and employment.\"\n\nAccording to Microsoft's AI Diffusion Leaderboard, Canada ranks 14th globally in AI adoption, with usage now topping a third (35%) of the population, showing leadership in putting AI to good use. Today's announcement builds on this momentum.\n\n\"AI will be a defining driver of Canada's competitiveness, but it only delivers value when it's adopted broadly and responsibly,\" said Matt Milton, President, Microsoft Canada. \"By deepening our collaboration with Seneca Polytechnic, we're helping bring trusted, secure AI into the tools and experiences that students and educators both use, helping more people build the skills and confidence to thrive in Canada's future AI economy.\"\n\nIn recognition of this new commitment, Mr. Milton toured Seneca's Newnham Campus as part of the announcement to observe Microsoft AI in action.\n\nThis continued work with Microsoft is part of Seneca's ongoing evolution in AI. This January, Seneca welcomed domestic and international students to the new Master of Artificial Intelligence Design & Development program, its historic first-ever master's degree. All students have access to My Tutor, an AI learning companion used across courses, in addition to AI-powered tools that help prepare students for presentations, sales pitches, job searches and interviews. Microsoft 365 Copilot chat is also available to all students and employees.\n\nSeneca will soon launch a new AI lab to serve as a central hub for training, collaboration and the sharing of generative AI projects. The lab will also operate as a launchpad for new initiatives, helping to accelerate AI adoption and advance Seneca's position as a leader in AI usage in higher education. Microsoft will be a collaborator in various activities taking place within the lab.\n\nBy combining Seneca's academic excellence with Microsoft's leading cloud and AI solutions, this expanded collaboration will prepare students at Seneca with the knowledge and adaptability essential for success, helping them graduate career-ready and world-\n\nready.\n\nAbout Seneca Polytechnic:\n\nSeneca Polytechnic provides a great education to prepare our students for great careers. Combining academic rigour with practical, professional and career-focused learning, we offer our students a seamless transition from education to employment. Expert faculty, excellent staff, outstanding campuses, awesome technology, deep connections with industry: This is Seneca Polytechnic.\n\nLearn more: senecapolytechnic.ca\n\nFor more information, please contact:\n\nCam Gordon\n\n647.615.1756\n\n[email protected]\n\nDirector, Communications and Public Affairs\n\nA photo accompanying this announcement is available at https://www.globenewswire.com/NewsRoom/AttachmentNg/412e6c8f-0ebc-4357-aeeb-e1945a4527f3\n\nMarket News and Data brought to you by Benzinga APIs"
  },
  {
    "source": "The Manila times",
    "company": "Microsoft AI",
    "title": "Seneca Polytechnic, together with Microsoft, expands commitment to advance learning with AI",
    "date": "2026-01-22T14:24:00Z",
    "url": "https://www.manilatimes.net/2026/01/22/tmt-newswire/globenewswire/seneca-polytechnic-together-with-microsoft-expands-commitment-to-advance-learning-with-ai/2263663",
    "content": "New integrations build on shared commitment to enriching education and achieving career successes through innovation\n\nSeneca Polytechnic and Microsoft Canada\n\nGet the latest news\n\ndelivered to your inbox\n\nSign up for The Manila Times newsletters\n\nBy signing up with an email address, I acknowledge that I have read and agree to the Terms of Service and Privacy Policy.\n\nPictured in the HELIX space at Seneca's Newnham Campus are (left to right) are Panos Panagiotakopoulos, AI Thought Leader and Professor in the School of Global Business, Seneca Polytechnic; Kent Peel, AI Thought Leader and Professor in the School of Public Safety, Seneca Polytechnic; Radha Krishnan, Chief Information Officer and Vice-President, Students, Seneca Polytechnic; Dr. Elka Walsh, Chief Education Officer, Microsoft Canada; President David Agnew, Seneca Polytechnic; Matt Milton, President, Microsoft Canada; Suzanne Conner, Senior Account Executive, Microsoft Canada; Michael Grahlman, Director, Higher Education, Microsoft Canada.\n\nAdvertisement\n\nTORONTO, Jan. 22, 2026 (GLOBE NEWSWIRE) -- Seneca Polytechnic and Microsoft are deepening their collaboration to further enhance the postsecondary experience for students at Seneca.\n\nBuilding on a strategic relationship first announced in 2024, today's announcement sets out a new multi-year commitment that Seneca has signed with Microsoft. It introduces new elements and AI advancements designed to enhance learning and innovation across disciplines, and help students achieve their career goals.\n\nThis expanded relationship with Microsoft showcases how Seneca is continuing to align with higher education and employment trends. In addition to providing students and employees with early access to the latest Microsoft AI innovations, Seneca will continue to innovate with Microsoft Foundry (formerly Azure AI Platform). Foundry is Microsoft's unified, interoperable AI platform.\n\nWorking together with Microsoft partner Adastra, Seneca will integrate agentic AI learning opportunities into the classroom. This includes providing Microsoft Azure tools to students to help advance their AI knowledge. Within AI-focused programs, Seneca faculty will be working with Microsoft to integrate agentic AI into curriculum, equipping students with hands-on experience to help set them up for success in emerging job markets. Seneca is also offering co-op opportunities to students in various technology programs to work with Adastra and Seneca's ITS department to build custom AI solutions.\n\nAdvertisement\n\n\"Seneca is delighted to continue working with Microsoft as we explore new ways to equip students with the skills and knowledge today's workplaces demand,\" said David Agnew, President, Seneca Polytechnic. \"Together, we are accelerating innovation by embracing AI as central to education and employment.\"\n\nAccording to Microsoft's AI Diffusion Leaderboard, Canada ranks 14th globally in AI adoption, with usage now topping a third (35%) of the population, showing leadership in putting AI to good use. Today's announcement builds on this momentum.\n\n\"AI will be a defining driver of Canada's competitiveness, but it only delivers value when it's adopted broadly and responsibly,\" said Matt Milton, President, Microsoft Canada. \"By deepening our collaboration with Seneca Polytechnic, we're helping bring trusted, secure AI into the tools and experiences that students and educators both use, helping more people build the skills and confidence to thrive in Canada's future AI economy.\"\n\nIn recognition of this new commitment, Mr. Milton toured Seneca's Newnham Campus as part of the announcement to observe Microsoft AI in action.\n\nAdvertisement\n\nThis continued work with Microsoft is part of Seneca's ongoing evolution in AI. This January, Seneca welcomed domestic and international students to the new Master of Artificial Intelligence Design & Development program, its historic first-ever master's degree. All students have access to My Tutor, an AI learning companion used across courses, in addition to AI-powered tools that help prepare students for presentations, sales pitches, job searches and interviews. Microsoft 365 Copilot chat is also available to all students and employees.\n\nSeneca will soon launch a new AI lab to serve as a central hub for training, collaboration and the sharing of generative AI projects. The lab will also operate as a launchpad for new initiatives, helping to accelerate AI adoption and advance Seneca's position as a leader in AI usage in higher education. Microsoft will be a collaborator in various activities taking place within the lab.\n\nBy combining Seneca's academic excellence with Microsoft's leading cloud and AI solutions, this expanded collaboration will prepare students at Seneca with the knowledge and adaptability essential for success, helping them graduate career-ready and world-\n\nready.\n\nAdvertisement\n\nAbout Seneca Polytechnic:\n\nSeneca Polytechnic provides a great education to prepare our students for great careers. Combining academic rigour with practical, professional and career-focused learning, we offer our students a seamless transition from education to employment. Expert faculty, excellent staff, outstanding campuses, awesome technology, deep connections with industry: This is Seneca Polytechnic.\n\nLearn more: senecapolytechnic.ca\n\nFor more information, please contact:\n\nAdvertisement\n\nCam Gordon\n\n647.615.1756\n\n[email protected]\n\nDirector, Communications and Public Affairs\n\nAdvertisement\n\nA photo accompanying this announcement is available at https://www.globenewswire.com/NewsRoom/AttachmentNg/412e6c8f-0ebc-4357-aeeb-e1945a4527f3"
  },
  {
    "source": "MediaPost",
    "company": "Microsoft AI",
    "title": "Microsoft Names 'AI CMO,' Pushes Intelligence Into Ads",
    "date": "2026-01-22T11:36:57Z",
    "url": "https://www.mediapost.com/publications/article/412196/microsoft-names-ai-cmo-pushes-intelligence-into.html",
    "content": "* by Laurie Sullivan , Staff Writer, Yesterday\n\nAndréa Mallard has spent the past seven years leading marketing at Pinterest. Now she has joined Microsoft AI as its new chief marketing officer.\n\n\"AI is already the most consequential technological shift of my lifetime,\" Mallard wrote on LinkedIn, adding that this is which is why she stepped into the role two weeks ago. \"It will shape our children's lives in ways that are difficult to predict.\"\n\nPrior to joining Pinterest in 2018, Mallard held the role of CMO at Gap brand Athleta, as well as Omada Health. Now she reports to Microsoft AI CEO Mustafa Suleyman.\n\n\"As Mustafa put it at last week's global MAI team meetup, 'AI must work in service of people. Not the other way around. Ever.'\" she wrote.\n\nEarlier this month, Mustafa shared details of a test for the next phase of AI technology.\n\nMustafa two months ago posted about \"Humanistic Superintelligence (HSI)\" on LinkedIn. He described it as \"advanced AI capabilities that always work for, in service of, people and humanity more generally.\"\n\nHSI is a concept championed by Microsoft AI and its Superintelligence Team in 2025 and early 2026, and represents a philosophy where AI is viewed as a specialized tool rather than a replacement for humans.\n\nMicrosoft uses the concept of HSI to support advertising, along with new features and services for its advertising business.\n\nThe latest updates announced last week focus on more controls, transparency, and making products easier to use.\n\nCustomer-acquisition features for Performance Max campaigns and expanded automation for responsive search ads mark the latest AI-driven campaign optimization across its advertising network. This will automate asset creation. For some early testers, the feature has delivered 5% click-through rate improvements.\n\nFunctions enable specific targeting of new customers with options to either increase bids or focus on customer-acquisition campaigns, bringing new customer revenue alongside existing Performance Max campaign objectives.\n\nTwo other updates center on making Performance Max campaign imports efficient through increased search theme limits.\n\nAdvertisers can now use up to 50 search themes, doubling the previous limit.\n\nWhen asset groups contain images that do not meet size requirements, exceed the previous 25-image limit, or include auto-generated logos, the rest of the asset group will still be imported.\n\nMicrosoft also introduced share of voice metrics, which aggregate impression data from search and shopping, for Performance Max campaigns. It provides historical data that extends back to November 10.\n\nThe metrics include impression share, click share, impression share lost to budget, and impression share lost to rank, addressing advertiser demands for increased visibility into automated campaign performance."
  },
  {
    "source": "Economie Matin",
    "company": "Mistral AI",
    "title": "Mistral AI : comment la startup française vise un milliard d'euros en 2026",
    "date": "2026-01-23T08:45:01Z",
    "url": "https://www.economiematin.fr/mistral-ai-startup-francaise-un-milliard-euros",
    "content": "Le 22 janvier 2026, en marge du Forum économique mondial de Davos, Mistral AI a levé le voile sur ses ambitions financières et industrielles. La jeune entreprise, fondée en 2023, assume désormais un objectif clair pour 2026 : franchir le seuil symbolique du milliard d'euros de chiffre d'affaires, dans un secteur dominé par des acteurs américains et chinois.\n\nMistral AI vise le milliard d'euros de revenus en 2026\n\nPour Mistral AI, l'année 2026 doit marquer un basculement. Selon son cofondateur et dirigeant Arthur Mensch, l'entreprise \" devrait dépasser un milliard d'ici la fin de l'année \", une déclaration faite publiquement lors d'un entretien en marge du rendez-vous de Davos. Cette projection place la société dans une catégorie jusque-là inaccessible aux startups européennes de l'intelligence artificielle.\n\nCette ambition repose toutefois sur une dynamique financière déjà engagée. En septembre 2025, Mistral AI a levé 1,7 milliard d'euros, une opération qui a valorisé l'entreprise à 11,7 milliards d'euros. Cette levée a notamment vu l'entrée au capital du groupe néerlandais ASML, acteur clé des technologies de semi-conducteurs. Parallèlement, la société indique avoir provisionné un milliard d'euros de dépenses d'investissement, confirmant une stratégie axée sur la montée en puissance industrielle et commerciale.\n\nMistral AI, croissance industrielle et acquisitions à l'horizon 2026\n\nAu-delà des revenus, Mistral AI prépare une transformation structurelle. Arthur Mensch affirme ainsi que l'entreprise \" regarde quelques opportunités \" de croissance externe, sans préciser ni les cibles ni les zones géographiques concernées, selon Boursorama avec l'AFP, le 22 janvier 2026. Cette stratégie d'acquisitions suggère une volonté de renforcer rapidement des briques technologiques ou commerciales jugées critiques pour atteindre les objectifs de 2026.\n\nCette accélération s'inscrit dans un contexte mondial très concurrentiel. Toujours à Davos, le dirigeant de Mistral AI a également tenu à relativiser l'idée d'un retard chinois en intelligence artificielle, qualifiant cette perception de \" conte de fées \". Ce positionnement souligne la lecture stratégique de l'entreprise : la compétition est globale, et le temps long joue contre les acteurs qui tardent à investir massivement.\n\nMistral AI face au défi du modèle économique en 2026\n\nPour Mistral AI, atteindre le milliard d'euros de revenus en 2026 ne relève pas uniquement d'une prouesse technologique. Ce cap impose une transformation profonde du modèle économique. Contrairement aux géants américains de l'intelligence artificielle, capables d'absorber des pertes prolongées grâce à des ressources financières quasi illimitées, Mistral AI doit démontrer rapidement sa capacité à générer des flux de revenus récurrents. Cette montée en puissance repose sur la commercialisation de solutions d'intelligence artificielle destinées aux entreprises et aux institutions, dans un contexte où la demande progresse mais reste fortement concurrentielle.\n\nCette équation économique est d'autant plus exigeante que les coûts liés à l'intelligence artificielle demeurent structurellement élevés. L'entraînement et l'exploitation des modèles nécessitent des infrastructures lourdes, énergivores et dépendantes du cloud. À Davos, le dirigeant de Nvidia a rappelé que le secteur devra encore investir \" des milliers de milliards \" pour bâtir les infrastructures nécessaires. Pour Mistral AI, cela signifie que la croissance du chiffre d'affaires devra aller de pair avec une maîtrise rigoureuse des dépenses, afin d'éviter que l'augmentation des revenus ne soit neutralisée par l'explosion des coûts opérationnels.\n\nMistral AI, un enjeu de souveraineté pour la France et l'Europe\n\nLes ambitions de Mistral AI dépassent largement le cadre de l'entreprise. À Davos, la vice-présidente de la Commission européenne chargée du Numérique, Henna Virkkunen, a rappelé qu'il est \" extrêmement important de ne pas être dépendant d'un pays ou d'une entreprise pour des secteurs très critiques de notre économie ou notre société \". Une déclaration qui éclaire directement la place stratégique occupée par Mistral AI dans l'écosystème européen.\n\nCette logique de souveraineté s'inscrit dans un contexte d'investissements colossaux à l'échelle mondiale. Le patron de Nvidia, Jensen Huang, a ainsi estimé qu'il restait \" des milliers de milliards \" de dollars d'infrastructures à construire pour soutenir l'essor de l'intelligence artificielle. Énergie, cloud et composants électroniques figurent parmi les postes clés, rappelant que la réussite de Mistral AI dépendra aussi de sa capacité à s'insérer dans cette chaîne industrielle lourde, bien au-delà du seul développement de modèles."
  },
  {
    "source": "Les Smartgrids",
    "company": "Mistral AI",
    "title": "Mistral AI accélère son industrialisation pour atteindre le milliard d'euros de chiffre d'affaires",
    "date": "2026-01-23T09:32:53Z",
    "url": "https://les-smartgrids.fr/mistral-ai-milliard-euros-chiffre-daffaires/",
    "content": "Mistral AI change de dimension. En visant plus d'un milliard d'euros de revenus en 2026, la startup française ne se contente plus d'exister face aux géants américains de l'intelligence artificielle. Elle ambitionne désormais de s'imposer comme un acteur industriel à part entière du marché mondial des modèles d'IA.\n\nLe 22 janvier 2026, en marge du Forum économique mondial de Davos, Mistral AI a officiellement assumé une trajectoire de croissance radicale. Son dirigeant Arthur Mensch a confirmé que l'entreprise visait un chiffre d'affaires supérieur à un milliard d'euros à l'horizon 2026, un objectif inédit pour une société européenne spécialisée dans les modèles de langage et les systèmes d'intelligence artificielle générative.\n\nMistral AI accélère vers une industrialisation de ses modèles\n\nPour un acteur technologique comme Mistral AI, l'annonce de cet objectif financier marque un tournant stratégique. Jusqu'ici, la startup s'était surtout distinguée par ses performances techniques, notamment dans le développement de modèles de langage ouverts et performants, capables de rivaliser avec les standards du marché. Désormais, l'enjeu est clair : transformer ces avancées technologiques en produits commercialisables à grande échelle, capables de générer des revenus récurrents.\n\nCette montée en puissance repose sur une base financière solide. En septembre 2025, Mistral AI a levé 1,7 milliard d'euros, une opération qui a porté sa valorisation à 11,7 milliards d'euros. Cette levée a notamment été marquée par l'entrée au capital du groupe ASML, un signal fort pour un écosystème technologique européen encore largement dépendant des infrastructures américaines.\n\nUne stratégie d'investissement massive pour soutenir l'IA générative\n\nAtteindre un milliard d'euros de revenus suppose cependant des investissements lourds. Selon Arthur Mensch, Mistral AI a provisionné un milliard d'euros de dépenses d'investissement, notamment pour renforcer ses capacités de calcul, ses infrastructures cloud et ses équipes d'ingénierie. Cette stratégie illustre une réalité bien connue du secteur : l'intelligence artificielle générative est un marché à forte intensité capitalistique, où les coûts de fonctionnement progressent aussi vite que les usages.\n\nCette contrainte structurelle a été rappelée à Davos par le dirigeant de Nvidia, Jensen Huang, qui estime que des \" milliers de milliards \" de dollars devront encore être investis à l'échelle mondiale pour construire les infrastructures nécessaires à l'essor de l'IA. Pour Mistral AI, cela signifie que la croissance du chiffre d'affaires devra impérativement s'accompagner d'une optimisation fine des coûts de calcul et de déploiement.\n\nMistral AI et la concurrence mondiale des grands modèles\n\nDans ce contexte, Mistral AI évolue dans un environnement extrêmement concurrentiel. Face aux acteurs américains disposant de ressources quasi illimitées, la startup française revendique une approche plus ciblée, reposant sur l'efficacité des modèles, leur adaptabilité et leur intégration dans des environnements professionnels spécifiques. Arthur Mensch a d'ailleurs tenu à rappeler que la compétition ne se limite pas aux États-Unis, rejetant l'idée selon laquelle la Chine serait structurellement en retard en intelligence artificielle, qualifiant cette perception de \" conte de fées \".\n\nCette lecture réaliste du marché mondial souligne un point clé : pour tenir ses ambitions à l'horizon 2026, Mistral AI devra non seulement innover, mais aussi convaincre durablement les entreprises et les administrations européennes de choisir ses solutions plutôt que celles des hyperscalers étrangers. La capacité à proposer des modèles performants, maîtrisables et conformes aux exigences réglementaires européennes sera déterminante.\n\nUn acteur technologique au cœur des enjeux de souveraineté numérique\n\nAu-delà des considérations économiques, Mistral AI s'inscrit désormais dans un débat stratégique plus large. À Davos, la vice-présidente de la Commission européenne chargée du Numérique, Henna Virkkunen, a rappelé l'importance de ne pas dépendre d'un nombre limité d'acteurs ou de pays pour des technologies critiques. Cette déclaration renforce le statut de Mistral AI comme l'un des rares candidats crédibles à une alternative européenne dans le domaine des grands modèles d'intelligence artificielle.\n\nPour la France et l'Europe, l'enjeu dépasse donc largement le succès d'une startup. La trajectoire de Mistral AI vers le milliard d'euros de revenus en 2026 constitue un test grandeur nature : celui de la capacité du continent à transformer une excellence technologique en puissance industrielle durable dans le secteur clé de l'intelligence artificielle."
  },
  {
    "source": "developpez.net",
    "company": "Mistral AI",
    "title": "0",
    "date": "2026-02-06T02:46:29Z",
    "url": "https://www.developpez.net/forums/d2181953/general-developpement/algorithme-mathematiques/intelligence-artificielle/mistral-ai-lance-voxtral-transcribe-2-nouvelle-famille-modeles-d-ia-reconnaissance-vocale/",
    "content": "La société française Mistral AI lance Voxtral Transcribe 2, une nouvelle famille de modèles d'IA de reconnaissance vocale, qui transcrit \" à la vitesse du son \"\n\nLa société française Mistral AI a dévoilé Voxtral Transcribe 2, une famille de modèles d'intelligence artificielle (IA) de reconnaissance vocale permettant une transcription rapide et de haute qualité \" à la vitesse du son \". Cette gamme comprend Voxtral Mini Transcribe V2, destiné à la transcription par lots, et Voxtral Realtime, conçu pour les applications en temps réel. Ce dernier se distingue par une latence configurable aussi faible que 200 ms, ce qui le rend idéal pour les assistants vocaux et le sous-titrage en direct. L'approche open source et les prix compétitifs de Mistral rendent Voxtral Transcribe 2 particulièrement attrayant pour les secteurs sensibles à la confidentialité, tout en offrant des performances robustes pour la transcription multilingue.\n\nMistral AI SAS est une entreprise française spécialisée dans l'IA dont le siège social est situé à Paris, avec des bureaux au Royaume-Uni, ainsi qu'à Palo Alto et à Singapour. Elle a été fondée en 2023 par Arthur Mensch, Guillaume Lample et Timothée Lacroix. L'entreprise développe de grands modèles de langage (LLM) à poids ouvert, ainsi que des modèles d'IA à la fois open source et propriétaires. Considérée comme l'un des leaders européens de l'IA, la société a été évaluée à plus de 14 milliards de dollars en 2025.\n\nCette annonce prolonge une initiative amorcée en juillet dernier, lorsque Mistral AI a dévoilé Voxtral, sa première famille de modèles d'IA open source dédiée à la compréhension de la parole. L'éditeur avait alors présenté un modèle de 24 milliards de paramètres destiné aux environnements de production, ainsi qu'une version allégée de 3 milliards de paramètres pour les déploiements locaux. Les modèles Voxtral se distinguent par des capacités avancées, incluant la gestion du multilingue, l'analyse de longs contextes, le résumé natif et l'interaction fonctionnelle à partir de la voix.\n\nPour poursuivre sur cette lancée, Mistral AI a lancé Voxtral Transcribe 2, une nouvelle famille de modèles de reconnaissance vocale qui transcrit \" à la vitesse du son \". Voxtral Transcribe 2 se compose de deux modèles de conversion de la parole en texte offrant une qualité de transcription, une diarisation et une latence ultra-faible. La famille comprend Voxtral Mini Transcribe V2 pour la transcription par lots et Voxtral Realtime pour les applications en temps réel.\n\nSelon Mistral, Voxtral Realtime utilise \" une architecture de streaming innovante qui transcrit l'audio à mesure qu'il arrive \", plutôt que d'adapter des modèles hors ligne. Cela permet une latence \" configurable à moins de 200 ms \", un seuil essentiel pour les assistants vocaux, le sous-titrage en direct et l'IA conversationnelle.\n\nLes utilisateurs peuvent tester Voxtral Transcribe 2 directement dans Mistral Studio. Ils peuvent télécharger jusqu'à 10 fichiers audio, activer ou désactiver la diarisation, choisir la granularité des horodatages et ajouter des termes contextuels pour le vocabulaire spécifique à un domaine. La famille de modèles prend en charge les formats .mp3, .wav, .m4a, .flac, .ogg jusqu'à 1 Go chacun.\n\nStratégie produit\n\nLa société a révélé que Voxtral Realtime est commercialisé en open source sous licence Apache 2.0, ce qui permet aux organisations de le déployer sur leur propre infrastructure, y compris sur des appareils périphériques. Cela a des implications importantes pour les secteurs sensibles à la confidentialité, tels que la santé, la finance et le gouvernement, où l'envoi de données audio vers des clouds tiers est souvent restreint.\n\nCette importance accordée à l'open source et à l'intégration n'est pas fortuite. Alors que les entreprises s'inquiètent de plus en plus de la dépendance vis-à-vis des fournisseurs et de la souveraineté des données, Mistral se positionne comme une alternative aux plateformes d'IA fermées basées aux États-Unis.\n\nLa tarification est abordée avec franchise. Elle est basée sur l'usage et commence à environ 5 000  par mois. Cela indique que Mistral cible les moyennes et grandes entreprises plutôt que les développeurs individuels, tout en continuant à présenter ses services comme compétitifs en termes de coût.\n\nTransformer les applications vocales\n\nSelon Mistral, la famille de modèles Voxtral optimise les flux de travail vocaux dans diverses applications et industries.\n\nPerformances\n\nVoxtral Realtime\n\nMistral affirme que Voxtral Realtime est spécialement conçu pour les applications où la latence est importante. Contrairement aux approches qui adaptent les modèles hors ligne en traitant l'audio par blocs, Realtime utilise une \" architecture de streaming innovante qui transcrit l'audio au fur et à mesure qu'il arrive. \" Le modèle fournit des transcriptions avec un délai configurable inférieur à 200 ms, ouvrant la voie à une nouvelle catégorie d'applications axées sur la voix.\n\nAvec un délai de 2,4 secondes, idéal pour le sous-titrage, Voxtral Realtime est comparable à Voxtral Mini Transcribe V2. Avec un délai de 480 ms, il reste dans une fourchette d'erreur de 1 à 2 %, ce qui permet aux agents vocaux d'atteindre une précision quasi hors ligne.\n\nLe modèle est multilingue de manière native et offre d'excellentes performances de transcription dans 13 langues, dont l'anglais, le chinois, l'hindi, l'espagnol, l'arabe, le français, le portugais, le russe, l'allemand, le japonais, le coréen, l'italien et le néerlandais. Avec une empreinte de 4 Go de paramètres, il fonctionne efficacement sur les appareils périphériques, garantissant la confidentialité et la sécurité des déploiements sensibles.\n\nVoxtral Realtime est disponible via API au prix de 0,006 $ par minute et en poids ouverts sur Hugging Face.\n\nVoxtral Mini Transcribe V2\n\nMistral affirme que Voxtral Mini Transcribe V2 atteint un taux d'erreur d'environ 4 % sur FLEURS au coût de 0,003 $/min, ce qu'il décrit comme \" le meilleur rapport qualité-prix de toutes les API de transcription \".\n\nLa société affirme que ce modèle surpasse les offres de GPT-4o mini Transcribe, Gemini 2.5 Flash, Assembly Universal et Deepgram Nova en termes de précision, tout en traitant l'audio \" environ trois fois plus rapidement que Scribe v2 d'ElevenLabs \" pour \" un cinquième du coût \".\n\nSi elles sont validées de manière indépendante, ces affirmations pourraient bouleverser un marché où les prix de la transcription vocale restent relativement élevés, en particulier pour la transcription multilingue et diarisée. La baisse des coûts rend économiquement viable la transcription de grands volumes de réunions, d'appels et d'archives médiatiques qui étaient auparavant trop coûteux à traiter.\n\nVoxtral Mini Transcribe V2 est disponible via API au prix de 0,003 $ par minute. Il peut être testé dans le nouvel espace audio de Mistral Studio ou dans Le Chat.\n\nDes idées novatrices\n\nAu-delà de la simple transcription, Voxtral Mini Transcribe V2 introduit des fonctionnalités spécialement conçues pour une utilisation en entreprise. Celles-ci comprennent la diarisation des locuteurs avec des horodatages précis, le biais contextuel pour traiter le vocabulaire spécifique à un domaine, les horodatages au niveau des mots et une robustesse améliorée dans les environnements bruyants tels que les \" ateliers d'usine \" et les \" centres d'appels très fréquentés \".\n\nLe modèle prend en charge des enregistrements pouvant atteindre trois heures en une seule requête et fonctionne dans 13 langues, dont l'anglais, le chinois, l'hindi, l'arabe et plusieurs langues européennes et asiatiques. Mistral souligne que \" les performances hors anglais surpassent largement celles de ses concurrents \", remédiant ainsi à une faiblesse de longue date dans le domaine de l'IA vocale, dominé par des données d'entraînement centrées sur l'anglais.\n\nMistral présente Voxtral comme une couche fondamentale pour de nombreux secteurs. Les médias et les organismes de radiodiffusion peuvent générer des sous-titres multilingues en direct, tandis que les secteurs réglementés peuvent s'appuyer sur la diarisation et les horodatages pour la conformité et les pistes d'audit. Les deux modèles Voxtral prennent en charge les déploiements conformes au RGPD et à la loi HIPAA grâce à des configurations sur site ou dans le cloud privé.\n\nL'avenir de l'IA vocale\n\nLe lancement de Voxtral illustre comment l'IA vocale passe du statut de nouveauté à celui d'infrastructure. La combinaison de poids ouverts, de prix agressifs et de performances en temps réel suggère que la concurrence s'éloigne de la question de savoir qui dispose du plus grand modèle pour se concentrer sur celle de savoir qui est capable de fournir des systèmes pratiques et déployables.\n\nLe succès de Voxtral Transcribe 2 dépendra peut-être moins des critères techniques que de sa capacité à tenir ses promesses en matière de réduction des coûts et de gains d'efficacité.\n\nEn décembre, Mistral avait le vent en poupe avec le lancement de la gamme de modèles Mistral 3.\n\nSource : Mistral AI\n\nEt vous ?\n\nQuel est votre avis sur le sujet ?\n\nTrouvez-vous cette initiative de Mistral AI crédible ou pertinente ?\n\nVoir aussi :\n\nLa start-up française Mistral lance Mistral AI Studio, une plateforme web permettant de créer, d'observer et de déployer des applications d'IA à l'aide de modèles propriétaires et à poids ouvert\n\nMistral AI lance Mistral OCR 3 offre une avancée majeure en matière de précision et d'efficacité pour l'extraction de texte et d'images\n\nLa start-up française Mistral se lance dans la course au vibe-coding et publie Devstral 2 sous une licence MIT modifiée, une nouvelle génération de son modèle d'IA conçu pour le codage\n\nLe Chat de Mistral AI peut désormais mémoriser vos conversations grâce à la nouvelle fonctionnalité Memories, et Mistral AI introduit également la prise en charge des connecteurs MCP"
  },
  {
    "source": "Ericsson.com",
    "company": "Mistral AI",
    "title": "Mistral AI, Ericsson partner to boost telecom AI innovation",
    "date": "2026-02-19T10:02:17Z",
    "url": "https://www.ericsson.com/en/news/2026/2/mistral-ai-and-ericsson-partner-to-drive-ai-innovation-in-telecom",
    "content": "Mistral AI and Ericsson today announced a partnership to apply advanced AI to real telecom challenges with a clear goal to make networks smarter, more efficient, and more trusted.\n\nThe collaboration combines Mistral AI's model customization capabilities with Ericsson's R&D and network expertise, with Ericsson acting as a design partner for the platform. The work targets high-impact use cases that speed software delivery and strengthen network performance, including automation of legacy code translation, AI-assisted development for 6G research, and custom AI agents for complex workflows in Ericsson's Networks organization.\n\nMistral AI and Ericsson will jointly research and co-develop AI agents tailored to Ericsson's data and engineering environments, bringing data closer to AI and enabling faster decision-making in product development and deployment. The companies aim to set new benchmarks for secure, high-performing, and resilient telecom infrastructure, aligning AI innovation with the requirements of carrier-grade networks.\n\nThis is a collaboration grounded in complementary strengths, Mistral AI's cutting-edge foundation models and tooling, and Ericsson's decades of radio, cloud, and network systems experience, proven at global scale. Together, the partnership focuses on AI for networks, not AI in isolation, to deliver measurable outcomes for customers.\n\nMarjorie Janiewicz, Chief Revenue Officer at Mistral AI says: \"This partnership with Ericsson isn't just about applying AI to telecom, it's about transforming networks from the ground up. By combining our frontier models with Ericsson's unmatched expertise in radio, cloud, and carrier-grade systems, we're customizing models that have deep domain knowledge and accelerate modernization. Together, we're setting a new standard for what AI can achieve in telecom: networks that are more resilient, more responsive, and ready for the future of connectivity.\"\n\nDag Lindbo. Head of AI & Emerging Technologies, Business Area Networks, at Ericsson, says: \"At Ericsson, AI for networks is about precision, not hype. With Mistral AI, we apply advanced models where they matter most. From accelerating code migration to informing 6G research and building trustworthy AI agents, this collaboration helps us improve time to value for customers while strengthening performance and resilience across the network.\"\n\nNOTES TO EDITORS:\n\nFOLLOW US:\n\nSubscribe to Ericsson press releases here\n\nSubscribe to Ericsson blog posts here\n\nhttps://x.com/ericsson\n\nhttps://www.facebook.com/ericsson\n\nhttps://www.linkedin.com/company/ericsson\n\nMORE INFORMATION AT:\n\nEricsson Newsroom\n\nmedia.relations@ericsson.com (+46 10 719 69 92)\n\ninvestor.relations@ericsson.com (+46 10 719 00 00)\n\nABOUT ERICSSON:\n\nEricsson's high-performing networks provide connectivity for billions of people every day. For nearly 150 years, we've been pioneers in creating technology for communication. We offer mobile communication and connectivity solutions for service providers and enterprises. Together with our customers and partners, we make the digital world of tomorrow a reality. www.ericsson.com\n\nABOUT MISTRAL AI:\n\nMistral AI is a pioneer company in generative artificial intelligence, empowering the world with the tools to build and benefit from the most transformative technology of our time. The company democratizes AI through high-performance, optimized, and cutting-edge open-source models, products and solutions as well as end-to-end infrastructure with Mistral Compute. Headquartered in France and independent, Mistral AI defends a decentralized and transparent approach to technology, with a strong global presence in the United States, United Kingdom, and Singapore. For more information, visit https://mistral.ai"
  },
  {
    "source": "Génération-NT",
    "company": "Mistral AI",
    "title": "Mistral AI s'aventure hors de France pour construire un centre de données",
    "date": "2026-02-11T18:38:18Z",
    "url": "https://www.generation-nt.com/actualites/mistral-ai-investissement-suede-datacenter-ecodatacenter-2070637",
    "content": "La start-up française Mistral AI officialise un investissement de 1,2 milliard d'euros pour le développement de nouvelles infrastructures en Suède. Une initiative en partenariat avec l'opérateur suédois EcoDataCenter qui marque le premier projet d'envergure hors de France pour Mistral AI.\n\nL'ambition affichée par Mistral AI est de renforcer \" l'autonomie technologique de l'Europe en matière d'intelligence artificielle \".\n\nPatron et cofondateur de Mistral AI, Arthur Mensch souligne que l'investissement est \" une étape concrète vers le développement de capacités indépendantes en Europe, dédiées à l'IA \". L'objectif est de proposer une offre entièrement intégrée avec des données traitées et stockées localement.\n\nCette démarche s'inscrit dans un contexte de tensions géopolitiques croissantes où la souveraineté technologique est devenue un enjeu majeur en Europe. Mistral AI entend servir à grande échelle les industries, les institutions publiques et les chercheurs, tout en gardant le contrôle de sa technologie et de ses serveurs.\n\nEcoDataCenter est chargé de la conception, de la construction et de l'exploitation de la nouvelle infrastructure sur son site de Borlänge. Les pays nordiques sont des lieux privilégiés pour les centres de données en raison de leurs températures plus fraîches et de leurs coûts énergétiques parmi les plus bas d'Europe.\n\nPrévue pour être opérationnelle en 2027, l'installation soutiendra le développement et le fonctionnement des futurs modèles d'IA de nouvelle génération de Mistral AI.\n\nEcoDataCenter mettra à profit son expertise des centres de données à haute densité, en utilisant des énergies renouvelables et des technologies de refroidissement avancées. Le datacenter hébergera notamment les GPU de dernière génération Vera Rubin de Nvidia.\n\nLes patrons de Mistral AI et EcoDataCenter (source image : EcoDataCenter)\n\nFondé en 2023, Mistral AI s'est rapidement imposé comme l'un des principaux acteurs européens de l'IA. En septembre dernier, la start-up avait levé 1,7 milliard d'euros, une opération qui a vu l'entrée au capital du groupe néerlandais ASML. Sa valorisation avait alors atteint 11,7 milliards d'euros. Mistral AI vise plus d'un milliard d'euros de revenus d'ici fin 2026."
  },
  {
    "source": "Numerama.com",
    "company": "Mistral AI",
    "title": "Mistral AI investit 1,2 milliard d'euros dans un data center en Suède",
    "date": "2026-02-11T16:43:55Z",
    "url": "https://www.numerama.com/tech/2177979-mistral-ai-lance-un-projet-colossal-a-12-milliard-deuros-en-suede-pour-la-souverainete-de-leurope.html",
    "content": "Mistral AI s'allie à l'entreprise suédoise EcoDataCenter pour mettre au point un centre de données en Suède au nom de la souveraineté européenne en matière d'intelligence artificielle. Un projet qui souffre cependant d'une limite : la provenance des GPU.\n\nIl y a quelques semaines, la question se posait de savoir si Mistral AI allait quitter la France pour la Suisse. Ce scénario n'est pas advenu à ce jour. En revanche, la startup française spécialisée dans l'intelligence artificielle (IA) avait bien des projets à l'international, et plus précisément en Suède. C'est ce qu'annonce un communiqué paru le 11 février 2026.\n\nCette annonce, immédiatement saluée par Anne Le Hénanff, la ministre de l'IA et du numérique, a été dévoilée par le partenaire suédois de Mistral AI : EcoDataCenter. Le but ? \" Développer une infrastructure numérique spécifiquement dédiée à l'IA \". En clair, construire un data center qui sera situé à Borlänge, ville située dans le centre du pays.\n\nEt pour y parvenir, Mistral AI va largement ouvrir son porte-monnaie : en effet, l'investissement grimpe à 1,2 milliard d'euros, ce qui constitue le premier et le plus gros investissement de l'entreprise française dans une infrastructure d'IA en dehors de l'Hexagone. Le centre de données doit être bientôt opérationnel, car on parle d'une entrée en service en 2027.\n\nCe sera aussi la plus grosse présence physique de Mistral AI hors du pays. Jusqu'à présent, la startup hexagonale disposait de bureaux à Palo Alto aux États-Unis (là où se trouve la fameuse Silicon Valley, creuset de l'innovation en IA), mais aussi à Londres, une autre place forte de l'IA au Royaume-Uni, et à Singapour.\n\nObjectif : la souveraineté de l'Europe dans l'IA\n\nCe partenariat européen n'est pas le fruit du hasard. Il s'inscrit dans une logique d'autonomie continentale en matière d'intelligence artificielle pour réduire la dépendance aux géants américains ou chinois. C'est d'ailleurs assumé dans le communiqué : l'affaire est décrite comme \" une étape importante pour renforcer l'indépendance de l'Europe \".\n\nDans cette optique, chaque partenaire apporte quelque chose qui manque à l'autre : Mistral AI s'occupe pour ainsi dire de la partie logicielle, avec des \" modèles de langage européens de premier plan \", et EcoDataCenter gère la partie matérielle, en concevant, construisant et exploitant l'infrastructure sous-jacente, et alimentée par des énergies renouvelables.\n\n\" L'ambition, poursuit le communiqué, est de créer une pile technologique d'IA entièrement européenne - de l'infrastructure et des modèles aux logiciels - où toutes les données sont traitées et stockées localement en Europe \". Mais cette pile, bien que largement européenne et située sur le continent, ne le sera pas totalement dans les faits.\n\nLe point faible des GPU\n\nEn effet, comme l'explique le communiqué de presse, le centre de données à Borlänge fera appel à des processeurs graphiques (GPU) Vera Rubin conçus par l'entreprise américaine Nvidia. Certes, ces composants conféreront une \" capacité de calcul très avancée à la Suède \", mais ils constituent un coup de canif dans la promesse d'une pile technologique 100 % européenne.\n\nComme le dit Jensen Huang, le patron de Nvidia, la pile de l'IA peut se décliner en cinq couches : l'énergie, la puce de calcul, l'infrastructure (le data center), l'algorithme (le modèle d'IA) et l'application. Dans le cadre du plan monté par Mistral AI et EcoDataCenter, cette souveraineté est en principe atteinte à quatre niveaux, mais celui de la puce fait encore défaut.\n\nMais le communiqué le dit bien : cet investissement est une étape. Dès lors, même si la souveraineté n'est pas atteinte à 100 % comme le suggèrent les deux partenaires, c'est un pas important qui va dans cette direction. Plutôt que d'attendre d'avoir un équivalent européen à Nvidia, EcoDataCenter et Mistral proposent quelque chose avec l'existant.\n\nC'est une posture pragmatique, à défaut de pouvoir s'affranchir tout de suite de la domination matérielle des USA. Surtout, Mistral AI sécurise l'essentiel : en localisant les puces Vera Rubin en Suède, et en utilisant un data center suédois, l'entreprise garantit que tout restera sous juridiction européenne, bien à l'abri du Cloud Act américain.\n\nDe l'électricité abondante, à bas prix et surtout décarbonée\n\nPar ailleurs, le choix suédois n'est pas qu'une question de souveraineté. Il y a aussi un enjeu économique et écologique, à l'heure où l'IA est perçue de plus en plus comme un gouffre énergétique. Or, le pays propose justement de l'électricité à un prix très compétitif, fortement décarbonée et reposant très largement sur les sources d'énergie renouvelable.\n\nCe faisant, Mistral AI a de quoi atténuer les critiques quant à son empreinte environnementale et continuer de challenger les géants de la Silicon Valley sans exploser son bilan carbone et sa facture d'électricité. Une nécessité pour que ce fleuron européen puisse jouer dans la même division que les Google, Microsoft et autres OpenAI (ChatGPT)."
  },
  {
    "source": "Pulse 2.0",
    "company": "Mistral AI",
    "title": "Mistral AI Acquiring Koyeb To Advance Buildout Of AI Infrastructure",
    "date": "2026-02-19T12:56:25Z",
    "url": "https://pulse2.com/mistral-ai-acquiring-koyeb-to-advance-buildout-of-ai-infrastructure/",
    "content": "Koyeb has agreed to join Mistral AI, aiming to accelerate the buildout of next-generation AI infrastructure. The acquisition, which is subject to customary closing conditions, will bring Koyeb's platform, technology, and team into Mistral AI's expanding Compute offering.\n\nThe combination is designed to strengthen Mistral Compute, a new infrastructure initiative intended to provide enterprises and AI teams with the same high-performance systems Mistral uses to build, run, and scale frontier AI models and applications.\n\nFounded in 2021, Koyeb set out to build a new generation of cloud infrastructure, centered on a seamless serverless experience backed by high-performance hardware, deployable globally in seconds. The company's founders, who previously spent a decade building and operating large-scale cloud platforms, saw traditional CPU-based infrastructure becoming commoditized while advanced, scalable platforms remained complex and costly.\n\nThe rapid rise of generative AI and agent-based workloads reshaped cloud requirements. AI applications increasingly demand serverless GPUs, specialized accelerators, and flexible CPU workloads that can efficiently scale from zero to high throughput in sub-seconds. They also require sandboxed environments capable of supporting autonomous agents and complex inference tasks.\n\nKoyeb responded by building a purpose-built serverless platform designed specifically for AI applications. Today, the company runs tens of thousands of applications across bare metal servers in 10 locations worldwide, enabling teams to deploy APIs, agents, and high-performance GPU inference workloads without managing servers or infrastructure operations. By staying close to hardware design and optimization, Koyeb has emphasized high performance with sustainable cost structures, a critical factor for AI teams navigating growing compute demands.\n\nAccording to Koyeb, conversations with Mistral AI leadership revealed a shared vision around building world-class AI infrastructure in Europe and beyond. Mistral AI has been rapidly scaling both its AI models and its enterprise footprint, and is now extending that momentum into infrastructure.\n\nMistral AI is deploying 40 megawatts of data center capacity and 18,000 Nvidia Blackwell GPUs as an initial foundation. The company is also expanding with a $1.4 billion data center investment in Sweden, signaling a major push to strengthen European AI infrastructure capacity.\n\nKoyeb's team will join Mistral AI full-time, working on Mistral Compute as a core initiative. The goal is to create state-of-the-art infrastructure accessible to AI developers and enterprises worldwide.\n\nKoyeb confirmed that its platform will continue operating without disruption. Over the coming months, it is expected to transition into a core component of Mistral Compute, with increased focus on inference, sandbox environments, and serverless capabilities tailored for AI workloads.\n\nFor existing customers, there are no immediate changes. Current organizations, including those on Starter plans, will continue operating under the same terms. However, new users will need to subscribe to paid tiers -- Pro, Scale, or Enterprise -- as the company shifts its focus toward production-grade infrastructure and away from maintaining a free tier, which it described as operationally intensive.\n\nBilling, support channels, and account access remain unchanged. Customers do not need a Mistral AI account to use Koyeb, and no data will be transferred immediately as part of the acquisition. If future changes occur, the company said it will communicate clearly in advance.\n\nThe acquisition positions Mistral AI not only as a frontier model developer but also as a vertically integrated AI infrastructure provider. By combining large-scale GPU capacity with Koyeb's serverless platform, Mistral aims to offer enterprises a full-stack AI environment, from model development to deployment and scaling."
  },
  {
    "source": "ZDNet",
    "company": "Mistral AI",
    "title": "Mistral AI rachète Koyeb : l'offensive pour bâtir un \"AWS de l'IA\" européen - ZDNET",
    "date": "2026-02-18T15:57:28Z",
    "url": "https://www.zdnet.fr/actualites/mistral-ai-rachete-koyeb-loffensive-pour-batir-un-aws-de-lia-europeen-490395.htm",
    "content": "En faisant l'acquisition de la start-up française, championne du cloud serverless, Mistral AI franchit une étape critique de son développement. Le leader européen de l'IA ne se contente plus de fournir des modèles, il déploie désormais sa propre infrastructure de calcul.\n\nL'annonce de l'accord définitif pour l'acquisition de Koyeb marque la première opération de croissance externe pour Mistral AI.\n\nToute l'actualité de la tech pour les pros chaque jour dans notre newsletter En savoir plus sur l'utilisation des données personnelles\n\nValorisée à 11,7 milliards d'euros, l'entreprise dirigée par Arthur Mensch cherche à internaliser les compétences de gestion de plateforme (PaaS) pour propulser son offre \" Mistral Compute \".\n\nL'objectif est de permettre aux entreprises de déployer des agents et des modèles d'IA sans la complexité de la gestion des serveurs physiques.\n\nKoyeb propose une plateforme sans serveur (serverless), qui permet aux développeurs de déployer des applications d'IA sans gérer d'infrastructure complexe précise Mistral AI.\n\nMais qu'est ce que le serverless ?\n\n\"Le cloud computing sans serveur n'est, bien sûr, pas du tout sans serveur\" évoquait il y a quelques années ZDNET dans un article consacré à cette technologie, ou en tout cas à ce concept. \"Néanmoins, du point de vue de l'entreprise, il élimine la nécessité de spécifier les serveurs avant de développer et de poster du code dans le cloud\".\n\nL'idée maîtresse est donc que le développeur peut écrire une fonction directement sur le portail d'un prestataire de cloud. Ce sont ensuite les systèmes de cloud, ici de Koyeb et donc de Mistral AI, qui s'occupent de gérer les ressources requises par la fonction. Cela n'est donc plus un problème pour la DSI d'un point de vue opérationnel.\n\nCette fusion promet aussi pour les clients de Mistral une couche d'abstraction supplémentaire au-dessus du \"bare metal\", permettant de se concentrer sur le code plutôt que sur le provisionnement des ressources.\n\nCette stratégie s'appuie donc côté Mistral sur une puissance de calcul et de stockage très importante. Mistral AI déploie actuellement 18 000 GPU Nvidia Blackwell et dispose d'une capacité de 40 MW en data centers.\n\nL'entreprise a également annoncé un investissement de 1,2 milliard d'euros pour la construction de centres de données en Suède, confirmant son ambition de devenir un acteur majeur de l'infrastructure à l'échelle continentale.\n\nImpact opérationnel : la fin du \"gratuit\"\n\nCe rachat a une conséquence directe pour les utilisateurs de la plateforme. Koyeb a déjà annoncé que son \"Starter Plan\" (gratuit) serait supprimé pour les nouveaux arrivants, l'entreprise souhaitant se concentrer sur les clients \"Pro\" et \"Enterprise\".\n\nMais l'entreprise met en avant également les avantages de ce rachat :\n\nAccès simplifié aux GPU serverless pour l'inférence haute performance. Déploiement d'agents IA et de workloads CPU en quelques secondes à l'échelle mondiale. Optimisation des coûts grâce au \"scale-to-zero\" (arrêt automatique des ressources non utilisées). Intégration native des serveurs MCP (Model Context Protocol) pour les flux de travail complexes.\n\nYann Léger, PDG de Koyeb, souligne également que l'équipe de 13 ingénieurs rejoindra Mistral dès mars pour fusionner les technologies.\n\nVers une souveraineté technologique concrète\n\nAu-delà de l'aspect technique, ce mouvement répond aux enjeux de souveraineté. En maîtrisant à la fois le modèle (le cerveau) et l'infrastructure (les muscles), Mistral AI réduit la dépendance des entreprises européennes vis-à-vis des hyperscalers américains.\n\nAvec des revenus prévus dépassant le milliard d'euros d'ici fin 2026, la pépite française se donne les moyens de ses ambitions.\n\nL'intégration de Koyeb transforme Mistral AI en un fournisseur de plateforme complet (Full-stack AI)."
  },
  {
    "source": "KultureGeek",
    "company": "Mistral AI",
    "title": "Le français Mistral AI investit 1,2 milliard d'euros pour des data centers IA en Suède",
    "date": "2026-02-11T14:54:33Z",
    "url": "https://kulturegeek.fr/news-347036/ia-francais-mistral-ai-ai-investit-12-milliard-deuros-datas-center-europe",
    "content": "Mistral AI va investir 1,2 milliard d'euros en Suède pour construire des data centers spécialisés en intelligence artificielle, une opération qui doit augmenter de 50 % ses capacités de calcul lors de la mise en service annoncée pour 2027. Le projet, mené avec l'entreprise suédoise EcoDataCenter, marque une première hors de France pour la société française.\n\nL'investissement reprend comme un ensemble cohérent : des data centers dédiés à l'IA, des capacités de calcul avancées et des solutions d'IA localisées. En choisissant la Suède, Mistral AI ajoute une brique industrielle à son dispositif, avec une infrastructure d'IA conçue en dehors de son pays d'origine, à savoir la France.\n\nCe mouvement s'inscrit aussi dans une trajectoire déjà chiffrée en France. Mistral AI a annoncé l'année dernière une capacité de 40 mégawatt (MW) sur le territoire, avec un tiers déjà opérationnel et un deuxième tiers en cours de déploiement, tout en révélant maintenant une capacité supplémentaire de 23 MW en Suède.\n\nDans son communiqué, l'entreprise relie explicitement ce projet à un discours de souveraineté. Mistral AI affirme que \" cette initiative constitue une étape majeure vers l'indépendance technologique de l'Europe \" et veut proposer une \" solution d'IA intégralement européenne \", de la conception des modèles jusqu'au traitement des données.\n\nL'infrastructure suédoise est pensée comme un accélérateur de capacité plutôt qu'un simple site d'hébergement. L'objectif annoncé (+50 % de capacités de calcul) fixe une logique d'échelle, avec une date de mise en service projetée en 2027.\n\nPour comprendre ce que cela représente dans le plan d'ensemble, Mistral AI met sur la table plusieurs repères, entre la base française et l'ajout suédois, et associe l'annonce à des ressources de calcul avancées et à des solutions localisées. Cette combinaison suggère une montée en puissance technique tout en adaptant l'offre au contexte européen.\n\nMistral AI place l'annonce dans un contexte où la souveraineté technologique européenne occupe le débat public. L'entreprise est le seul acteur européen qui tente de rester dans la course aux grands modèles d'IA face à OpenAI (ChatGPT), Anthropic (Claude) et Google (Gemini), malgré un décalage d'investissements et de valorisation.\n\nEn parallèle, la société revendique un rythme de développement rare pour une entreprise de la tech française et se positionne davantage sur le marché des entreprises que sur le grand public. Son dirigeant, Arthur Mensch, indique qu'elle devrait dépasser un milliard d'euros de revenus d'ici la fin de l'année.\n\nCette dynamique s'appuie aussi sur une levée de fonds récente. En septembre, Mistral AI a bouclé un tour à 1,7 milliard d'euros, une opération qui a quasiment doublé sa valorisation pour l'établir à 11,7 milliards d'euros."
  },
  {
    "source": "Le Monde.fr",
    "company": "Mistral AI",
    "title": "IA : Nouveau Monde Editions accuse Mistral AI d'avoir piraté un cinquième de son catalogue",
    "date": "2026-01-31T16:10:19Z",
    "url": "https://www.lemonde.fr/economie/article/2026/01/31/ia-nouveau-monde-editions-accuse-mistral-ai-d-avoir-pirate-un-cinquieme-de-son-catalogue_6664916_3234.html",
    "content": "Yannick Dehée, historien et fondateur de Nouveau Monde Editions, est le premier \" à demander à Mistral AI d'indemniser les auteurs et les éditeurs français qu'elle a piratés \". Il assure au Monde que le champion français de l'intelligence artificielle (IA) a utilisé, pour entraîner son ..."
  },
  {
    "source": "DATAQUEST",
    "company": "Mistral AI",
    "title": "Mistral AI eyes Bengaluru for new GCC to tap India's talent",
    "date": "2026-01-22T08:23:53Z",
    "url": "https://www.dqindia.com/news/mistral-ai-eyes-bengaluru-for-new-gcc-to-tap-indias-talent-11020808",
    "content": "Mistral AI, the Paris-based artificial intelligence firm, is in active discussions with the Karnataka government to establish a Global Capability Centre (GCC) in Bengaluru. The potential move, disclosed during the World Economic Forum 2026, signals the company's intent to leverage India's deep technical talent pool as part of its global expansion strategy.\n\nKarnataka's Minister for Large and Medium Industries, MB Patil, met with Audrey Herblin Stoop, Vice President of Global Public Affairs and Communication at Mistral AI, to discuss the proposal. The center would likely adopt a phased growth model, starting with engineering teams before expanding into advanced research and scientific capabilities.\n\nBengaluru currently accounts for over 50% of India's AI and machine learning workforce, making it the second-largest AI talent hub globally. Mistral AI joins a growing list of frontier AI companies establishing a physical presence in the city. Anthropic has already announced plans for a Bengaluru office in early 2026, while OpenAI recently confirmed its first Indian office in New Delhi.\n\nThe decision to set up a GCC reflects a shift in how international tech firms view India. Traditionally used for back-office support, Indian GCCs have evolved into \"Innovation Hubs\" where local teams take end-to-end ownership of global products.\n\nFounded in 2023, Mistral AI has distinguished itself by offering high-performance, open-weight models that provide an alternative to closed-source systems like those from OpenAI. The company recently raised EURO 1.7 billion in a Series C round led by ASML, bringing its valuation to approximately EURO 11.7 billion.\n\nBy establishing a base in India, Mistral AI can better support its 6.2 million active developers worldwide and cater to Indian enterprises that prioritize data sovereignty and on-premises deployment. Minister Patil noted that the state's policy continuity and the existing ecosystem around the Indian Institute of Science (IISc) and NASSCOM were major factors in the discussions.\n\nThe establishment of a Mistral AI hub would further solidify Bengaluru's status as a \"fortress of global enterprise.\" Current trends indicate that 83% of Indian GCCs are now scaling Generative AI projects, moving beyond pilot programs to full-scale operational intelligence.\n\nWhile the specific investment amount for the Bengaluru center has not been disclosed, the move aligns with the broader IndiaAI Mission. The government expects AI infrastructure investments in India to reach USD 150 billion by the end of 2026. For Mistral AI, a Bengaluru GCC provides a direct pipeline to the engineers who will build the next generation of multilingual and multimodal applications for the global market."
  },
  {
    "source": "BitcoinWorld",
    "company": "Mistral AI",
    "title": "Mistral AI's Strategic Masterstroke: Acquiring Koyeb to Power European AI Cloud Dominance",
    "date": "2026-02-17T18:15:22Z",
    "url": "https://bitcoinworld.co.in/mistral-ai-acquires-koyeb-cloud/?v=1771350331",
    "content": "In a landmark move reshaping Europe's artificial intelligence landscape, Paris-based Mistral AI has announced its first acquisition of fellow French startup Koyeb. This strategic decision, confirmed on October 15, 2025, positions the $13.8 billion company to directly challenge American cloud giants while accelerating its full-stack AI ambitions. The acquisition represents more than just corporate consolidation -- it signals Europe's growing determination to establish sovereign AI infrastructure independent of US technological dominance.\n\nMistral AI's purchase of Koyeb represents a calculated evolution beyond its core large language model development. Previously known primarily for its competitive LLMs challenging OpenAI's offerings, the company now demonstrates clear infrastructure ambitions. This acquisition follows Mistral's June 2025 announcement of Mistral Compute, its dedicated AI cloud infrastructure service. Consequently, Koyeb's technology and expertise will directly accelerate this initiative's development timeline.\n\nThe timing proves particularly significant given recent geopolitical developments in technology infrastructure. Just days before this announcement, Mistral revealed a massive $1.4 billion investment in Swedish data centers. This dual approach -- acquiring deployment technology while expanding physical infrastructure -- creates a comprehensive strategy for European AI sovereignty. Industry analysts immediately recognized the pattern: Mistral is building an integrated ecosystem rather than just selling individual AI models.\n\nKoyeb brings specialized serverless architecture expertise that directly addresses critical AI deployment challenges. Founded in 2020 by former Scaleway engineers Yann Léger, Edouard Bonlieu, and Bastien Chatelard, the startup developed technology allowing developers to deploy applications without managing underlying server infrastructure. This serverless approach gained tremendous relevance as AI models grew increasingly complex and resource-intensive.\n\nKoyeb's platform already supported Mistral's models alongside other AI frameworks before the acquisition. Now, the integration will enable several key advancements:\n\nMistral CTO Timothée Lacroix emphasized this strategic fit in his statement, noting Koyeb will become a \"core component\" of Mistral Compute. The 13-person Koyeb team, including all three co-founders, will join Lacroix's engineering division immediately. Their collective expertise in serverless architecture will help transform Mistral from an AI model provider into a comprehensive cloud platform.\n\nThis acquisition occurs against a backdrop of increasing European investment in independent technology infrastructure. Mistral recently surpassed $400 million in annual recurring revenue, demonstrating strong market traction despite competing against better-funded American rivals. The company's growth reflects broader European Union initiatives to reduce technological dependence on US and Chinese providers.\n\nFloriane de Maupeou, principal at Koyeb investor Serena Capital, highlighted the geopolitical significance. She told Bitcoin World this combination will play a key role \"in building the foundations of sovereign AI infrastructure in Europe.\" This perspective aligns with recent EU policy directives encouraging homegrown technology solutions for critical infrastructure sectors.\n\nMistral CEO Arthur Mensch reinforced this European focus during his recent appearance at Stockholm's Techarena conference. He pitched the company to prospective employees as an organization \"headquartered in Europe, that is doing frontier research in Europe.\" This messaging deliberately contrasts with the US-centric narratives dominating AI development discussions.\n\nKoyeb's journey from concept to acquisition target illustrates the growing importance of deployment technology in the AI ecosystem. The company raised $8.6 million across two funding rounds: a $1.6 million pre-seed in 2020 followed by a $7 million seed round in 2023 led by Serena Capital. Throughout its development, Koyeb maintained focus on simplifying complex infrastructure management.\n\nRecently, the company launched Koyeb Sandboxes, providing isolated environments for deploying AI agents. This innovation addressed growing enterprise concerns about security and resource management when implementing AI solutions. Mistral's acquisition will likely accelerate development of these sandbox environments while integrating them more deeply with Mistral's model offerings.\n\nFollowing the acquisition, Koyeb's platform will continue operating for existing users. However, new users can no longer access the Starter tier as the company shifts toward enterprise clients. This strategic pivot aligns with Mistral's own enterprise-focused growth strategy and recurring revenue model.\n\nMistral's move creates ripple effects across multiple competitive dimensions. First, it positions the company more directly against cloud infrastructure giants like Amazon Web Services, Google Cloud, and Microsoft Azure. Second, it differentiates Mistral from pure AI model providers who lack deployment infrastructure. Third, it establishes a European alternative to US-dominated AI cloud services.\n\nThe serverless computing approach that Koyeb specializes in has become increasingly crucial as AI applications scale. Traditional infrastructure management struggles with the variable demands of AI inference workloads. Serverless architectures automatically scale resources based on demand, optimizing both performance and cost -- critical factors for enterprises adopting AI solutions.\n\nMistral's integrated approach offers several competitive advantages:\n\nIndustry observers note this acquisition follows a pattern established by larger tech companies: vertical integration of complementary technologies. However, Mistral executes this strategy with distinct European characteristics, emphasizing sovereignty and regional infrastructure development.\n\nWhile Mistral declined to disclose financial terms, the acquisition's strategic value clearly outweighs its direct cost. Koyeb's technology accelerates Mistral Compute's development timeline by potentially months or years. Additionally, the acquired team brings specialized expertise that would take significant time to develop internally.\n\nThe timing proves particularly advantageous given current market conditions. Demand for AI infrastructure continues growing exponentially while concerns about US technological dominance create openings for European alternatives. Mistral's recent $400 million annual recurring revenue milestone demonstrates strong market traction that the Koyeb acquisition will likely accelerate further.\n\nLooking forward, this acquisition raises questions about Mistral's broader merger and acquisition strategy. CEO Arthur Mensch's comments about continued hiring for infrastructure roles suggest this might represent just the beginning of Mistral's expansion beyond pure AI research. The company appears to be building a comprehensive European AI ecosystem rather than just developing individual components.\n\nMistral AI's acquisition of Koyeb represents a pivotal moment in European artificial intelligence development. This strategic move transforms the French company from an AI model provider into a full-stack cloud platform competitor. By integrating Koyeb's serverless deployment technology with its existing models and new infrastructure investments, Mistral creates a compelling alternative to US-dominated AI services.\n\nThe acquisition accelerates Mistral Compute's development while addressing critical enterprise needs for scalable, efficient AI deployment. Furthermore, it strengthens Europe's position in the global AI infrastructure landscape, supporting regional sovereignty initiatives. As Mistral continues expanding its capabilities and infrastructure, this Koyeb acquisition will likely be remembered as the moment the company truly became a comprehensive AI cloud platform rather than just another model developer.\n\nQ1: Why did Mistral AI acquire Koyeb specifically?\n\nMistral acquired Koyeb to accelerate its Mistral Compute cloud infrastructure platform. Koyeb's serverless deployment technology and expertise address critical challenges in scaling AI applications, particularly for enterprise clients requiring efficient resource management and on-premises deployment options.\n\nQ2: How does this acquisition affect Koyeb's existing customers?\n\nKoyeb's platform will continue operating for existing customers, but new users can no longer access the Starter tier. The company will shift focus toward enterprise clients aligned with Mistral's business strategy, with the technology gradually integrating into Mistral Compute over coming months.\n\nQ3: What is serverless computing and why is it important for AI?\n\nServerless computing allows developers to deploy applications without managing underlying server infrastructure. For AI applications with variable resource demands, this approach enables automatic scaling that optimizes both performance and cost -- critical factors as AI models grow more complex and resource-intensive.\n\nQ4: How does this acquisition support European AI sovereignty?\n\nThe acquisition strengthens European AI infrastructure by combining French AI model development with French deployment technology. This reduces dependence on US cloud providers while creating integrated solutions that comply with European data regulations and sovereignty initiatives.\n\nQ5: Will Mistral AI make more acquisitions in the future?\n\nWhile Mistral hasn't confirmed specific plans, CEO Arthur Mensch's comments about hiring for infrastructure roles suggest continued expansion. The company appears to be building a comprehensive AI ecosystem, making additional strategic acquisitions in complementary technology areas likely."
  },
  {
    "source": "IT News zu den Themen Künstliche Intelligenz, Roboter und Maschinelles Lernen - IT BOLTWISE® x Artificial Intelligence",
    "company": "Mistral AI",
    "title": "Mistral AI Ã¼bernimmt Koyeb zur StÃ¤rkung der KI-Cloud-Infrastruktur",
    "date": "2026-02-17T15:54:20Z",
    "url": "https://www.it-boltwise.de/mistral-ai-uebernimmt-koyeb-zur-staerkung-der-ki-cloud-infrastruktur.html",
    "content": "PARIS / LONDON (IT BOLTWISE) - Mistral AI, ein führendes Unternehmen in der europäischen KI-Branche, hat seine erste Akquisition abgeschlossen und das französische Startup Koyeb übernommen. Diese Übernahme soll die Entwicklung einer souveränen KI-Cloud-Infrastruktur beschleunigen und Mistrals Position im globalen KI-Wettbewerb stärken.\n\nMistral AI, ein aufstrebendes Unternehmen aus Paris, hat kürzlich die Übernahme des französischen Startups Koyeb bekannt gegeben. Diese Akquisition markiert einen bedeutenden Schritt in Mistrals Strategie, durch gezielte Übernahmen zu wachsen und seine Position als führender Anbieter von KI-Technologien in Europa zu festigen. Koyeb, das 2019 gegründet wurde, bietet eine serverlose Cloud-Plattform, die es Entwicklern ermöglicht, Anwendungen ohne die Verwaltung der zugrunde liegenden Infrastruktur zu betreiben.\n\nDie Übernahme von Koyeb durch Mistral AI ist ein strategischer Schritt, um die Entwicklung einer souveränen KI-Cloud-Infrastruktur zu beschleunigen. Mistral AI hat sich zum Ziel gesetzt, europäische KI-Modelle zu entwickeln, die mit den Angeboten von US-amerikanischen Giganten wie OpenAI und Anthropic konkurrieren können. Mit der Expertise von Koyeb im Bereich der serverlosen Cloud-Technologie plant Mistral, seine Compute-Plattform weiter auszubauen und seinen Kunden eine robuste und flexible Infrastruktur für KI-Workloads zu bieten.\n\nIn den letzten Jahren hat Mistral AI beeindruckende Fortschritte gemacht, indem es fast 2,8 Milliarden Euro an Investitionen gesammelt und eine Bewertung von 11,7 Milliarden Euro erreicht hat. Diese finanzielle Stärke ermöglicht es dem Unternehmen, strategische Akquisitionen wie die von Koyeb durchzuführen, um seine technologischen Fähigkeiten und Marktposition zu stärken. Die Integration von Koyebs Team in Mistrals Ingenieurabteilung wird voraussichtlich im nächsten Monat abgeschlossen sein.\n\nDie Entscheidung von Mistral AI, Koyeb zu übernehmen, spiegelt einen breiteren Trend in der KI-Branche wider, bei dem Unternehmen durch Übernahmen ihre technologischen Fähigkeiten erweitern und ihre Marktstellung verbessern. Während der globale Wettbewerb im Bereich der Künstlichen Intelligenz weiter zunimmt, setzen Unternehmen wie Mistral auf strategische Partnerschaften und Akquisitionen, um ihre Innovationskraft zu steigern und neue Marktchancen zu erschließen."
  },
  {
    "source": "Le journal du net",
    "company": "Mistral AI",
    "title": "Comparatif : 5 alternatives européennes à ChatGPT",
    "date": "2026-02-05T13:53:27Z",
    "url": "https://www.journaldunet.com/intelligence-artificielle/1547771-comparatif-5-alternatives-europeennes-a-chatgpt/",
    "content": "Les solutions alternatives à ChatGPT, bien que nombreuses, sont surtout basées à l'étranger, aux Etats-Unis ou en Chine. Voici 5 alternatives 100% européennes au leader de l'IA grand public.\n\nDans un contexte d'incertitudes géopolitiques croissantes et de débats sur la souveraineté numérique, de nombreux citoyens européens s'interrogent sur les alternatives \"souveraines\" à ChatGPT. Or à côté des solutions américaines et chinoises qui dominent le marché, les solutions véritablement européennes à destination du grand public se font rares. Nous avons sélectionné 5 interfaces conversationnelles européennes, deux françaises, deux suisses et une néerlandaise. L'objectif ? Proposer des équivalents simples et accessibles pour remplacer ChatGPT sur les tâches courantes : synthèse de documents, génération de texte, recherche web, etc. Chacun de ces services est détenu par une entreprise européenne, mais attention, toutes n'utilisent pas des data centers européens et n'offrent donc pas une garantie de souveraineté.\n\nLe Chat de Mistral AI, dévoilé en février 2024, est la solution française de référence. Le siège de l'entreprise est basé à Paris. Mistral AI dépend en revanche actuellement fortement de Microsoft Azure, Google Cloud et Cerebras pour l'hébergement du Chat. Si l'entrainement des modèles doit à terme être effectué chez Eclairion (acteur français), rien n'est annoncé sur ses autres verticales. Point positif toutefois : l'ensemble des modèles à disposition sur Le Chat sont développés en France.\n\nHuggingChat est l'interface IA grand public d'Hugging Face. Bien que le siège social de l'entreprise soit basé à Brooklyn aux Etats-Unis, l'éditeur conserve encore une partie substantielle de ses opérations en France. L'éditeur utilise en majorité des cloud providers américains pour héberger ses modèles et HuggingChat En revanche plusieurs modèles de Mistral AI sont disponibles. Attention toutefois, l'entreprise ayant son siège aux Etats-Unis, cette dernière est donc pleinement soumise au Cloud Act et au Fisa notamment. C'est la solution la moins souveraine de notre comparatif.\n\nLumo a été lancé en juillet 2025 par l'éditeur suisse Proton axé sur la privacy. Proton héberge Lumo et l'ensemble des modèles utilisés sur sa propre infrastructure sécurisée, en Suisse. La plateforme utilise uniquement des modèles open source, dont plusieurs développés à l'étranger (Mistral Nemo, OpenHands 32B, OLMO 2 32B, GPT-OSS 120B, Qwen, Ernie 4.5 VL 28B, Apertus, et Kimi K2).\n\nApertus est le modèle et la plateforme d'IA développée par l'EPFL, ETH Zurich et le Swiss National Supercomputing Centre (CSCS). La plateforme et le modèle ont été développés par des équipes en Suisse et sont hébergés en Suisse sur l'infrastructure du CSCS. C'est l'outil le plus souverain de notre liste.\n\nEuqai Chat, enfin, a été développé par l'entreprise du même nom basée à Eindhoven, aux Pays-Bas. Euqai est avant tout un fournisseur d'orchestration intelligente entre les modèles pour réduire les coûts.\n\nL'éditeur se base sur plusieurs modèles open source, mais là encore pas uniquement européens. La plateforme de chat et les modèles sont hébergés sur les serveurs de Leafcloud, fournisseur néerlandais de cloud.\n\nSans surprise, Le Chat de Mistral AI arrive en tête avec l'arsenal fonctionnel le plus complet : choix du modèle, historique, mémoire, recherche web, support MCP, interpréteur de code et application mobile. C'est l'alternative la plus crédible pour remplacer ChatGPT au quotidien. Lumo se positionne remarquablement bien en seconde position avec un équilibre intelligent entre fonctionnalités essentielles et accessibilité via son application mobile. L'approche privacy-first de Proton n'a donc pas sacrifié l'expérience utilisateur. HuggingChat est également une excellente option grâce à son choix diversifié de modèles, son historique et le support MCP, ce qui en fait une option solide pour les utilisateurs à l'aise avec l'open source. Apertus, malgré un nombre limité de fonctionnalités, compense par sa cohérence : historique, upload de documents et mémoire suffisent pour un usage classique de l'IA.\n\nEuqai Chat apparaît plus limité. L'App dispose de deux fonctionnalités : la recherche web et l'upload de documents, c'est tout. Pour un utilisateur cherchant à véritablement remplacer ChatGPT au quotidien, Euqai reste donc sur la touche.\n\nSeuls trois éditeurs proposent des abonnements premium : Le Chat Pro à 17,99 euros par mois, Hugging Face Pro à 9 euros et Lumo+ à 9,99 euros. Les avantages apportés ne révolutionnent pas l'expérience utilisateur. Le Chat Pro, malgré son positionnement tarifaire deux fois supérieur, se contente d'augmenter les quotas (30 fois plus de réflexion avec les modèles à chaîne de pensée, 5 fois plus de rapports de recherche) et d'ajouter du stockage (15 Go) et l'accès à Mistral Vibe. Hugging Face Pro promet simplement \"20 fois plus de messages gratuits\" sans détailler davantage les quotas de la version gratuite. Quant à Lumo+, il lève les limitations de base : chats et historiques illimités, fichiers volumineux et accès aux modèles avancés. En somme, les abonnements permettent surtout d'utiliser l'IA de manière intensive sans se heurter aux plafonds gratuits, mais n'offrent pas de fonctionnalités exclusives.\n\nPour les Français cherchant l'expérience la plus proche de ChatGPT, Le Chat de Mistral AI s'impose comme le choix évident. Interface complète, application mobile, interpréteur de code, mémoire... L'arsenal fonctionnel ne laisse rien à désirer. Cette exhaustivité a toutefois un prix : 17,99 euros par mois pour la version Pro, soit le double de ses concurrents européens. Un tarif justifié pour les utilisateurs intensifs qui veulent toute la puissance d'une IA générative souveraine. Le second choix naturel se porte sur Lumo de Proton, qui associe fonctionnalités essentielles et garanties de souveraineté maximale. Hébergement en Suisse, infrastructure propriétaire, absence totale de traçage publicitaire : l'éditeur spécialisé dans la confidentialité applique sa recette habituelle à l'IA conversationnelle. A 9,99 euros en version premium, c'est le meilleur rapport fonctionnalités / prix et souveraineté du comparatif.\n\nPour ceux qui privilégient la gratuité et cherchent simplement un outil qui fonctionne en 2026, HuggingChat demeure, encore, une valeur sûre. Le choix de modèles, l'historique et le support MCP suffisent amplement pour un usage quotidien. Quant à Euqai Chat, son positionnement reste flou : l'absence d'historique et de mémoire le cantonne à des usages ponctuels, une recherche web rapide ou l'analyse occasionnelle d'un document. Difficile d'en faire son assistant IA principal.\n\nEnfin, en matière de vraie souveraineté, c'est bien Apertus qui décroche la palme : développé et hébergé entièrement en Suisse par des institutions publiques (EPFL, ETH Zurich, CSCS), il incarne le summum de la maîtrise européenne de l'IA. Mistral AI, malgré ses modèles français, reste dépendant de Microsoft Azure et Google Cloud pour l'hébergement. La souveraineté affichée sur le papier ne reflète pas toujours la réalité des infrastructures."
  },
  {
    "source": "Ouest France",
    "company": "Mistral AI",
    "title": "Il avait accusé Cédric O de conflit d'intérêts sur l'IA : le chanteur Bertrand Burgalat devant les tribunaux ce lundi",
    "date": "2026-01-26T14:30:06Z",
    "url": "https://www.ouest-france.fr/high-tech/intelligence-artificielle/il-avait-accuse-cedric-o-de-conflit-dinterets-sur-lia-le-chanteur-bertrand-burgalat-devant-les-tribunaux-ce-lundi-bf3f78e2-fa95-11f0-bcbd-01f0839eab22",
    "content": "Bertrand Burgalat est jugé pour diffamation ce lundi 26 janvier. Ce procès fait suite à une citation à comparaître déposée par l'ancien secrétaire d'État Cédric O, que le chanteur avait accusé de trafic d'intérêt et de trafic d'influence en raison de son rôle au sein de Mistral AI.\n\nBertrand Burgalat est jugé ce lundi 26 janvier 2026 par le tribunal correctionnel de Paris pour diffamation.\n\nLe chanteur, qui est également président du Syndicat national de l'édition phonographique (Snep), avait accusé l'ancien secrétaire d'État au numérique Cédric O de conflit d'intérêts en raison de son investissement auprès de Mistral AI.\n\nDes déclarations sur BFMTV\n\nEn mars 2024, alors qu'il était invité sur le plateau de BFM Business , Bertrand Burgalat avait taclé le secrétaire d'État : \" Qui conseille la France aujourd'hui sur les questions d'intelligence artificielle ? Google, Meta, Mistral AI qui est un cache-sexe de la Sillicon Valley. \"\n\nEt d'ajouter : \" On est dans quelque chose qui relève du pénal : on a un ancien secrétaire d'État au Numérique, monsieur Cédric O, qui est en prise illégale d'intérêt totale et en trafic d'influence. [...] Je suis très étonné que la Haute autorité pour la transparence de la vie publique (HATVP) ne fasse pas son travail dans cette affaire puisque la position de la France a été dictée par Cédric O. \"\n\nDe secrétaire d'État à lobbyiste\n\nCédric O, membre du gouvernement entre 2019 et mai 2022, était parti dans le privé au lendemain de l'élection présidentielle de 2022. Il était devenu en 2023 un lobbyiste pour l'Intelligence artificielle en tant que cofondateur et conseiller en affaires publiques de la start-up française Mistral AI.\n\nUne enquête de Cash Investigation , diffusée en juin 2025, mettait en avant un rôle \" primordial \" joué par l'ancien ministre à Bruxelles lors des négociations de l'IA Act pour tenter de l'infléchir. Selon le magazine d'investigation, Cédric O aurait participé à des réunions sur l'IA Act alors qu'il était encore membre du gouvernement, avant de passer du côté des lobbies.\n\nAvec cette deuxième casquette, il avait été un grand opposant aux obligations de transparence quant aux entrainements de ces intelligences artificielles. Il avait aussi été consultant au sein du comité de l'intelligence artificielle générative, créé par Élisabeth Borne en 2023 pour conseiller le gouvernement sur cette question.Lors de la création de Mistral AI en avril 2023, Cédric O avait acquis plus de 10 000 parts de l'entreprise, pour une valeur de 176 €. Quelques mois et plusieurs levées de fonds plus tard, ses parts étaient estimées à 23 millions, comme le rapportait Capital .\n\nLe média révélait que la HATVP, qui avait interdit à Cédric O d'entrer au conseil d'administration du groupe informatique Atos, n'avait en revanche pas émis d'avis négatif pour ses activités au sein de Mistral AI, cette dernière ayant été créée après son départ du gouvernement et Cédric O y ayant participé via une société de conseil.\n\nUne action en justice\n\nCédric O avait attaqué en justice Bertrand Burgalat quelques jours après le passage de ce dernier sur BFMTV, révélait l' Informé , via une citation à comparaître.\n\nCela n'a pas empêché le chanteur de continuer à critiquer l'action de l'ancien ministre. \" Il \" empoche \" 27 millions et il m'en demande 60 000 pour avoir osé m'étonner de cette situation \", dénonçait-il encore le 10 septembre 2025 sur son compte X. Une première audience devait avoir lieu en juin 2025, avant d'être reportée à ce lundi 26 janvier."
  },
  {
    "source": "storyboard18.com",
    "company": "Mistral AI",
    "title": "Mistral AI in talks to set up Bengaluru GCC with phased expansion plan",
    "date": "2026-01-22T04:57:47Z",
    "url": "https://www.storyboard18.com/brand-marketing/mistral-ai-in-talks-to-set-up-bengaluru-gcc-with-phased-expansion-plan-88076.htm",
    "content": "The potential move comes as several of Mistral AI's rivals accelerate their presence in India, particularly in Bengaluru.\n\nFrench artificial intelligence company Mistral AI is in discussions to establish a Global Capability Centre in Bengaluru as part of its plans to expand into India and strengthen its global footprint, Karnataka's large and medium industries minister MB Patil said.\n\nPatil said the talks were discussed during a meeting with Audrey Herblin Stoop, vice-president for global public affairs and communication at Mistral AI, on the sidelines of the World Economic Forum 2026 in Davos.\n\nThe potential move comes as several of Mistral AI's rivals accelerate their presence in India, particularly in Bengaluru. Anthropic is expanding its global operations into the country and has announced plans to open an office in Bengaluru in early 2026, while generative AI pioneer OpenAI has said it will open its first India office in New Delhi.\n\nFounded in 2023, Mistral AI is a fast-growing French AI company known for developing high-performance, accessible and often open-source large language models. The company currently has a strong base in Europe and is expanding its operations in Singapore and other international markets.\n\nPatil said discussions centred on setting up the Bengaluru centre through a phased approach, beginning with engineering teams and gradually expanding into advanced research and scientific capabilities. He said India's deep pool of AI talent was a key attraction for Mistral AI, with the company's delegation expressing interest in leveraging these capabilities as part of its global expansion strategy.\n\nBengaluru is the second-largest AI talent hub globally and accounts for more than 50 per cent of India's AI and machine learning workforce. Patil added that Karnataka hosts three artificial intelligence centres of excellence in partnership with the Indian Institute of Science, NASSCOM and other ecosystem partners, further strengthening the state's appeal for AI and frontier technology investments.\n\nSeparately, Patil said Sify Technologies is set to inaugurate a new data centre facility in Karnataka shortly, while Bharti Enterprises has also expressed interest in setting up a data centre in the state."
  },
  {
    "source": "RCR Wireless News",
    "company": "Mistral AI",
    "title": "Ericsson, Mistral AI target AI for network ops",
    "date": "2026-02-20T15:06:08Z",
    "url": "https://www.rcrwireless.com/20260220/ai/ericsson-mistral-ai",
    "content": "Practical use cases - The partnership focuses on practical operational and research, including legacy code migration, AI-assisted 6G research, and AI agents to automate complex engineering workflows.\n\nBroader industry shift - The announcement comes as a broader shift from generative AI to agentic AI is taking hold, where autonomous agents can take action across networks, IT systems, and customer operations.\n\nMistral AI and Ericsson are collaborating to apply generative AI (gen AI) tools to telecom network development and operations. Under the agreement, Ericsson will act as a design partner, combining its network R&D expertise with Mistral AI's model customization capabilities.\n\nThe collaboration focuses on several operational and research use cases, including automation of legacy code translation, AI-assisted development in 6G research, and the development of AI agents to support complex workflows within Ericsson's Networks organization.\n\nThe companies plan to jointly research and develop AI agents tailored to Ericsson's engineering environments and internal data systems. The goal is to integrate AI more directly into product development and deployment processes, with an emphasis on secure implementation and alignment with carrier-grade network requirements.\n\nThe partnership centers on applying AI specifically to telecom network functions rather than general-purpose deployments. It combines Mistral AI's foundation models and tooling with Ericsson's experience in radio access, cloud infrastructure, and large-scale network systems.\n\nDag Lindbo, head of AI and emerging technologies in Ericsson's Business Area Networks, said the companies are focusing on practical applications such as code migration, 6G research support, and AI-driven workflow automation to improve development efficiency and network performance. \"At Ericsson, AI for networks is about precision, not hype. With Mistral AI, we apply advanced models where they matter most,\" he said.\n\nOperator interest and funding are already there, according to Nvidia's fourth annual \"State of AI in Telecommunications report\". Operators are placing significant bets on AI: 90% of respondents said AI is helping increase annual revenue and reduce costs, while 89% plan to boost AI spending in 2026. Much of that investment is focused on network automation. But the most consequential shift may be the move from generative AI to agentic AI. \"Generative AI delivered fast productivity gains, but agentic AI is where telecoms begin to see structural ROI,\" said Chetan Sharma, CEO of Chetan Sharma Consulting. \"Autonomous agents can act across networks, IT, and customer journeys, turning insights into decisions without human delay.\"\n\nFor Mistral AI Chief Revenue Officer Marjorie Janiewicz, the partnership reflects a broader effort to embed AI more deeply into telecom network architecture. \"It's about transforming networks from the ground up,\" she said. \"Together, we're setting a new standard for what AI can achieve in telecom: networks that are more resilient, more responsive, and ready for the future of connectivity.\""
  },
  {
    "source": "Mobile World Live",
    "company": "Mistral AI",
    "title": "Ericsson, Mistral seek to shake-up telecoms with AI",
    "date": "2026-02-19T15:52:39Z",
    "url": "https://www.mobileworldlive.com/ranvendors/ericsson-mistral-seek-to-shake-up-telecoms-with-ai/",
    "content": "Ericsson struck a deal with Mistral AI to combine the French company's advanced models with its R&D and network expertise, with the aim of using the technology to address challenges in the telecoms industry.\n\nThe Swedish vendor stated the pair's work would target \"high-impact\" use cases that speed delivery and strengthen network performance, including automation of legacy code translation, AI-assisted development of 6G research and custom agents for use within Ericsson Networks.\n\nThey explained they would jointly research and develop AI agents tailored to Ericsson's data and engineering environments, bringing data closer to the technology to aid product development and deployment.\n\nEricsson and Mistral AI also have lofty ambitions to set new benchmarks for secure, high performing and resilient telecom infrastructure, \"aligning AI innovation with the requirements of carrier-grade networks\".\n\nDag Lindbo, head of AI and emerging technologies, business area Networks at Ericsson, said \"AI for networks is about precision not hype\".\n\n\"With Mistral AI, we apply advanced models where they matter most,\" he said, citing examples including accelerating code mitigation, to informing 6G research and building trustworthy AI agents.\n\nChief revenue officer at Mistral AI, Marjorie Janiewicz, added the tie-up is not just about applying AI to telecoms, it is about transforming networks from the ground up.\n\nThe deal with Ericsson follows a commitment Mistral AI made last week to invest €1.2 billion on a new data centre in Sweden."
  },
  {
    "source": "Génération-NT",
    "company": "Mistral AI",
    "title": "Mistral AI fait sa première acquisition avec Koyeb",
    "date": "2026-02-18T10:03:55Z",
    "url": "https://www.generation-nt.com/actualites/mistral-ai-rachat-koyeb-cloud-infrastructure-ia-serverless-2070985",
    "content": "C'est une première en matière de rachat pour Mistral AI qui met la main sur Koyeb, une start-up parisienne spécialisée dans le déploiement serverless d'applications IA. Fondé par d'anciens de Scaleway (Iliad), Koyeb et sa dizaine d'employés rejoindront l'équipe d'ingénieurs de Mistral AI pour intégrer leur technologie.\n\nL'opération confirme les ambitions de Mistral AI pour l'infrastructure IA. Jusqu'ici réputée pour ses modèles de langage performants, l'entreprise cherche à maîtriser également l'infrastructure technique sous-jacente.\n\nCette acquisition vise directement à accélérer le développement de Mistral Compute, une infrastructure cloud IA annoncée en juin 2025. L'objectif est de fournir une solution intégrée pour l'entraînement et le déploiement de modèles à grande échelle\n\nKoyeb s'est spécialisé dans la technologie serverless, qui permet aux développeurs de déployer des applications sans se soucier de la gestion des serveurs. Sa plateforme facilite l'exécution d'inférences.\n\nLa technologie de Koyeb aidera Mistral AI à déployer ses modèles directement sur le matériel de ses clients et à proposer une infrastructure plus performante. Il est précisé que la plateforme Koyeb continuera de fonctionner, mais se concentrera désormais sur les clients professionnels.\n\nCe rachat, dont le montant n'a pas été dévoilé, s'inscrit dans une stratégie de croissance plus large et renforce la volonté de souveraineté européenne en matière d'IA. Il intervient suite à l'annonce par Mistral AI d'un investissement de 1,2 milliard d'euros dans un datacenter en Suède, avec EcoDataCenter."
  },
  {
    "source": "WebProNews",
    "company": "Mistral AI",
    "title": "Mistral AI's Acquisition of Koyeb Signals a Bold New Chapter in Europe's AI Infrastructure Wars",
    "date": "2026-02-17T20:02:52Z",
    "url": "https://www.webpronews.com/mistral-ais-acquisition-of-koyeb-signals-a-bold-new-chapter-in-europes-ai-infrastructure-wars/",
    "content": "When Mistral AI, the French artificial intelligence powerhouse valued at roughly $6.2 billion, announced its acquisition of serverless cloud platform Koyeb, it marked more than just a routine corporate transaction. It signaled a fundamental strategic pivot -- one that could reshape how European AI companies compete against American hyperscalers and redefine the economics of deploying large language models at scale.\n\nThe deal, reported by TechCrunch, represents Mistral AI's first-ever acquisition and underscores the Paris-based company's ambition to evolve from a pure-play model developer into a vertically integrated AI platform provider. By absorbing Koyeb -- also a French company, specializing in serverless infrastructure that allows developers to deploy applications without managing underlying servers -- Mistral is making a deliberate bet that owning the infrastructure layer is just as critical as building the models themselves.\n\nWhy Koyeb? The Strategic Logic Behind Mistral's First Deal\n\nKoyeb, founded in 2021, carved out a niche as a developer-friendly serverless platform designed to simplify the deployment of applications, APIs, and AI workloads across global infrastructure. The platform offered automatic scaling, built-in GPU support, and edge deployment capabilities -- features that align neatly with the computational demands of running inference on large language models. For Mistral, which has rapidly built a portfolio of open-weight and commercial models including Mistral Large, Mixtral, and its enterprise-grade Le Chat assistant, the acquisition addresses a critical gap: the ability to offer customers a seamless, end-to-end experience from model selection to production deployment.\n\nAccording to TechCrunch, the acquisition is designed to bolster Mistral's cloud ambitions, giving the company a proprietary infrastructure layer that can serve as the backbone of its commercial platform. Rather than relying entirely on third-party cloud providers like Amazon Web Services, Google Cloud, or Microsoft Azure to host and serve its models, Mistral can now begin building out its own managed deployment environment. This vertical integration strategy mirrors moves made by other AI leaders, though Mistral's approach carries a distinctly European flavor -- one that emphasizes data sovereignty, regulatory compliance, and independence from American big tech.\n\nVertical Integration: The New Playbook for AI Companies\n\nThe acquisition arrives at a moment when the AI industry is undergoing a profound structural shift. The initial wave of generative AI competition centered almost exclusively on model quality -- who could build the most capable large language model. But as the technology matures and enterprises move from experimentation to production deployment, the battleground has shifted toward infrastructure, distribution, and total cost of ownership. Companies that can offer a tightly integrated stack -- from model training and fine-tuning to inference serving and application hosting -- hold a significant competitive advantage.\n\nOpenAI, for instance, has invested heavily in its API platform and is reportedly exploring custom silicon to reduce its dependence on Nvidia GPUs and Microsoft's Azure cloud. Anthropic has deepened its partnership with Amazon Web Services, while Google has leveraged its own Tensor Processing Units and Cloud infrastructure to create a vertically integrated offering around Gemini. Mistral, as a younger and considerably smaller player, lacks the resources to build data centers from scratch. But acquiring Koyeb gives it something potentially just as valuable: a lightweight, developer-centric infrastructure platform that can be rapidly adapted and scaled to serve AI-specific workloads.\n\nThe European Dimension: Sovereignty and Strategic Independence\n\nMistral AI has long positioned itself as Europe's answer to the American AI giants. Co-founded by Arthur Mensch, a former DeepMind researcher, alongside Guillaume Lample and Timothée Lacroix, both alumni of Meta's AI research division, the company has enjoyed strong backing from European investors and political leaders who view it as a critical component of the continent's technological sovereignty. The company raised a massive funding round in 2024 that valued it at $6.2 billion, with investors including Andreessen Horowitz, Lightspeed Venture Partners, and several prominent European funds.\n\nThe Koyeb acquisition reinforces this sovereignty narrative. With growing regulatory pressure in Europe around data localization and the EU AI Act imposing new compliance requirements on high-risk AI systems, having a European-owned and operated infrastructure layer is not merely a technical convenience -- it is a strategic asset. Enterprise customers in regulated industries such as finance, healthcare, and government are increasingly demanding that their AI workloads run on infrastructure that complies with European data protection standards. By integrating Koyeb's platform, Mistral can offer these customers a fully European solution, from model weights to the servers they run on.\n\nWhat This Means for Koyeb's Team and Technology\n\nFor Koyeb, the acquisition represents a significant validation of its technology and team. The startup had built a modern, container-based serverless platform with native support for GPU workloads -- a relatively uncommon capability among serverless providers. Its architecture, designed for low-latency global deployment, is particularly well-suited for AI inference tasks, where milliseconds of latency can determine whether an application feels responsive or sluggish to end users.\n\nAccording to details shared by TechCrunch, the Koyeb team is expected to join Mistral AI, bringing with them deep expertise in cloud infrastructure, container orchestration, and developer experience design. This talent infusion could prove just as valuable as the technology itself. Building reliable, scalable cloud infrastructure requires a specialized skill set that is difficult to recruit for, particularly in Europe where competition for cloud engineering talent is fierce. By acquiring Koyeb rather than attempting to build an infrastructure team from scratch, Mistral accelerates its timeline considerably.\n\nCompetitive Implications: How Rivals May Respond\n\nThe deal is likely to send ripples through both the European startup ecosystem and the broader global AI industry. For other European AI companies -- including Germany's Aleph Alpha, which has pivoted toward enterprise AI solutions, and various emerging players across the continent -- Mistral's move raises the bar for what it means to compete as a full-stack AI provider. Companies that offer only models or only infrastructure may find themselves at a disadvantage against integrated platforms that can deliver a complete solution.\n\nAmong American competitors, the acquisition is unlikely to cause immediate alarm, given the vast resource advantages enjoyed by companies like OpenAI, Google, and Anthropic. However, it does signal that the competitive dynamics of the AI industry are becoming more complex and multi-dimensional. Mistral's willingness to pursue acquisitions suggests that the company is thinking beyond model benchmarks and toward building a durable, defensible business -- one that generates recurring revenue from platform services rather than relying solely on API call volume.\n\nThe Road Ahead: From Model Maker to Platform Company\n\nThe transition from AI model developer to full-stack platform company is fraught with execution risk. Managing cloud infrastructure is a capital-intensive, operationally complex endeavor that has humbled even well-funded startups. Mistral will need to invest significantly in scaling Koyeb's platform, building out data center relationships, and ensuring the reliability and security standards that enterprise customers demand. The company will also need to navigate the delicate balance between offering its own infrastructure and maintaining partnerships with hyperscale cloud providers, many of which also serve as distribution channels for Mistral's models.\n\nYet the potential rewards are substantial. If Mistral can successfully integrate Koyeb's technology and build a compelling, vertically integrated AI platform, it could capture a significant share of the rapidly growing market for enterprise AI deployment -- a market that analysts project could be worth hundreds of billions of dollars by the end of the decade. More importantly, it would establish Mistral as something more than just another model provider in a crowded field. It would position the company as a genuine platform player -- one with the infrastructure, the models, and the European bona fides to compete on the global stage.\n\nFor now, the acquisition of Koyeb is a statement of intent. It tells the market that Mistral AI is not content to remain a model-layer company, and it tells European enterprises that a homegrown alternative to the American AI stack is being assembled, one acquisition at a time. Whether that vision can be fully realized will depend on execution, capital, and the willingness of enterprise customers to bet on a European champion. But with this first acquisition, Mistral has made clear that it intends to play a much bigger game."
  },
  {
    "source": "Presse-citron",
    "company": "Mistral AI",
    "title": "Une petite startup parisienne vient dâ€™Ãªtre rachetÃ©e par le gÃ©ant Mistral AI",
    "date": "2026-02-17T18:57:52Z",
    "url": "https://www.presse-citron.net/?p=706688",
    "content": "Mistral AI, le champion franÃ§ais de lâ€™intelligence artificielle, multiplie les investissements. Alors quâ€™il vient dâ€™annoncer 1,2 milliard dâ€™euros dâ€™investissement pour des centres de donnÃ©es en SuÃ¨de, on apprend que ce concurrent dâ€™OpenAI rachÃ¨te aussi Koyeb, une autre startup franÃ§aise. Celle-ci est spÃ©cialisÃ©e dans le cloud computing. Elle propose de la puissance de calcul pour lâ€™IA Ã ses clients, sans avoir ses propres serveurs.\n\nCette toute premiÃ¨re acquisition de Mistral AI est une entreprise relativement petite, composÃ©e de 13 employÃ©s, selon les explications de TechCrunch. NÃ©anmoins, sa technologie pourrait Ãªtre trÃ¨s utile Ã Mistral AI pour dÃ©ployer ses modÃ¨les dâ€™intelligence artificielle Ã plus de clients. Le montant de la transaction nâ€™a pas Ã©tÃ© annoncÃ©. Mais on sait quâ€™Ã ce jour, Koyeb a levÃ© 8,6 millions de dollars.\n\nLa plateforme de Koyeb continuera de fonctionner. NÃ©anmoins, ses employÃ©s vont rejoindre lâ€™Ã©quipe de Mistral Compute. â€œAvec cette premiÃ¨re acquisition, Mistral AI franchit une Ã©tape importante dans sa mission visant Ã crÃ©er un champion de l'IA full-stack et Ã faire progresser les infrastructures d'IA de pointeâ€, indique le champion franÃ§ais de lâ€™IA, dans un communiquÃ© citÃ© par Reuters.\n\nLa valorisation de Mistral AI, bien quâ€™elle ne soit pas comparable Ã celles dâ€™Anthropic et dâ€™OpenAI, est dÃ©jÃ consÃ©quente. Lâ€™annÃ©e derniÃ¨re, la startup a levÃ© 1,7 milliard dâ€™euros, lors dâ€™un tour de financement menÃ© par ASML (le spÃ©cialiste europÃ©en des Ã©quipements de fabrication de puces), pour une valorisation de 11,7 milliards dâ€™euros.\n\nLes modÃ¨les dâ€™IA de Mistral ont beaucoup de succÃ¨s auprÃ¨s des sociÃ©tÃ©s europÃ©ennes qui souhaitent ne pas dÃ©pendre des gÃ©ants amÃ©ricains. Et ses revenus sont en hausse. RÃ©cemment, Mistral a rÃ©vÃ©lÃ© quâ€™il atteint presque les 400 millions de dollars de chiffre dâ€™affaires annualisÃ©, contre 20 millions de dollars, il y a un an."
  },
  {
    "source": "Boursier.com",
    "company": "Mistral AI",
    "title": "Mistral AI rachète la start-up française de \"cloud\" Koyeb",
    "date": "2026-02-17T17:09:49Z",
    "url": "https://www.boursier.com/actualites/reuters/mistral-ai-rachete-la-start-up-francaise-de-cloud-koyeb-412055.html",
    "content": "Mistral AI rachète la start-up française de \"cloud\" Koyeb\n\nPublié le 17/02/2026 à 17h54\n\nMistral AI rachète la start-up française de \"cloud\" Koyeb\n\n17 février (Reuters) - La société française d'intelligence artificielle Mistral AI a déclaré mardi avoir conclu un accord pour racheter la ?start-up de services d'informatique dématérialisée (\"cloud\") Koyeb pour un montant non divulgué.\n\n\"Avec cette première acquisition, Mistral AI fait un pas important dans sa mission de bâtir un champion de l'IA intégrée(\"full-stack\") et de développer des infrastructures avancées? d'IA,\" a déclaré Mistral dans un communiqué.\n\nKoyeb, basée à Boulogne-Billancourt, fournit des services de cloud sans serveurs. Ses 13 employés et ses trois cofondateurs rejoindront l'équipe d'ingénierie de Mistral.\n\nMistral AI, valorisée à 11,7 ?milliards d'euros en septembre après l'entrée au capital du fournisseur d'équipements pour semi-conducteurs ASML, est considérée comme la plus grande entreprise d'IA en Europe.\n\nA lire aussi...\n\nLa semaine dernière, la société a annoncé un investissement de 1,2? milliard d'euros dans de nouveaux centres de données en Suède, dans le cadre d'efforts visant à conserver ses technologies et ses serveurs de cloud en Europe, contrairement à ses principaux concurrents comme l'américain OpenAI.\n\n(Rédigé par Inti Landauro, version française Elena Smirnova, édité par Kate Entringer)"
  },
  {
    "source": "Siècle Digital",
    "company": "Mistral AI",
    "title": "Mistral AI investit 1,2 milliard d'euros pour construire son premier data center géant dédié à l'IA - Siècle Digital",
    "date": "2026-02-12T12:34:25Z",
    "url": "https://siecledigital.fr/2026/02/11/mistral-ai-injecte-12-milliard-deuros-dans-un-data-center-en-suede/",
    "content": "Alors que la compétition mondiale autour de l'intelligence artificielle s'intensifie, les acteurs européens cherchent à consolider leurs positions.\n\nFace aux géants américains et chinois, la question de la souveraineté technologique s'impose dans les débats industriels et politiques...\n\nDans un communiqué de l'AFP, on apprend que Mistral AI franchit une nouvelle étape stratégique en annonçant un investissement massif.\n\nEn effet, la start-up française va injecter 1,2 milliard d'euros dans la construction d'une infrastructure dédiée à l'IA en Suède. Une première implantation internationale pour l'entreprise, qui entend affirmer son ancrage européen tout en renforçant ses capacités de calcul au passage...\n\nLe projet sera déployé à Borlänge, au nord-ouest de Stockholm, en partenariat avec EcoDataCenter, spécialiste suédois des infrastructures numériques durables.\n\nL'investissement prévoit la construction de centres de données spécialisés, en intégrant notamment des clusters GPU et des capacités de calcul haute performance (HPC). Cela n'a rien d'un hasard, car ces infrastructures sont indispensables pour entraîner et exploiter des modèles d'intelligence artificielle.\n\nAu delà des serveurs, le projet comprend également la modernisation du réseau électrique et des systèmes de refroidissement, deux éléments structurants pour ce type d'installations énergivores.\n\nPour Mistral AI, la Suède apparaît comme un choix stratégique, car le pays bénéficie d'une énergie largement décarbonée et d'un climat favorable au refroidissement naturel des data centers.\n\nCette implantation représente également un levier économique significatif pour la région, avec des retombées attendues dans la construction, l'ingénierie, la cybersécurité et l'exploitation de centres de calcul.\n\nÀ terme, de grands groupes industriels suédois comme Ericsson, Volvo, Saab ou Scania pourraient profiter d'un accès local à des capacités d'IA avancées, conformes aux exigences européennes en matière de protection des données.\n\nPour Mistral AI, cette infrastructure est une première brique internationale de son dispositif industriel. Prévue pour une mise en service en 2027, elle doit soutenir l'entraînement et l'inférence de ses futurs modèles, tout en complétant les installations déjà annoncées en France, notamment dans l'Essonne.\n\nEtant le seul acteur européen à rivaliser, à son échelle, avec OpenAI, Google ou Anthropic sur le terrain des grands modèles, Mistral AI poursuit une trajectoire de croissance soutenue.\n\nAprès une levée de fonds de 1,7 milliard d'euros en 2025, et étant la start-up d'IA la plus valorisée en Europe, la société a pour ambition de dépasser le milliard d'euros de chiffre d'affaires d'ici la fin de l'année.\n\nAvec ce projet suédois, Mistral AI ne se contente pas d'augmenter sa puissance de calcul, mais elle consolide son positionnement comme fer de lance d'une IA pensée, développée et opérée en Europe..."
  },
  {
    "source": "DCD",
    "company": "Mistral AI",
    "title": "French AI firm Mistral signs $1.2bn deal to build EcoDataCenter facility in Sweden",
    "date": "2026-02-11T18:06:15Z",
    "url": "https://www.datacenterdynamics.com/en/news/french-ai-firm-mistral-signs-12bn-deal-to-lease-ecodatacenter-facility-in-sweden/",
    "content": "Mistral to deploy Vera Rubin GPUs in Borlänge, will be company's first deployment outside France\n\nFrench AI firm Mistral AI is to lease capacity from Swedish colocation provider EcoDataCenter.\n\nThe companies this week announced a strategic long-term investment of €1.2 billion ($1.42bn) to build an AI-focused data center at EcoDataCenter's Borlänge site.\n\n- EcoDataCenter\n\nUnder the partnership, Mistral AI will deploy AI compute at EcoDataCenter's facilities in Sweden. Full details haven't been shared, but the first facilities are scheduled to open in 2027.\n\nMistral's first infrastructure investment outside France will host Nvidia's latest-generation Vera Rubin GPUs.\n\n\"This investment is a concrete step toward building independent capabilities in Europe, dedicated to AI,\" said Arthur Mensch, CEO and co-founder of Mistral AI. \"By delivering a fully vertical offer with locally processed and stored data, we are reinforcing Europe's strategic autonomy and competitiveness. This lays the foundation for a European AI cloud that can serve industries, public institutions, and researchers at scale.\"\n\nBorlänge is a locality in central Sweden located northwest of Stockholm. It is around 20km away from the city of Falun, where EcoDataCenter also operates a data center.\n\nEcoDataCenter broke ground on its Borlänge campus in September. Formerly occupied by a paper mill, the 20-hectare site will offer 250MW in its first phase, potentially scaling to 600MW.\n\n\"AI is critical infrastructure for Europe's competitiveness, security, and economic growth,\" added Peter Michelson, CEO of EcoDataCenter. \"Together with Mistral AI, we are building high-performance AI infrastructure on Swedish soil - with sustainability, resilience, and European strategic autonomy at its core. This investment strengthens Sweden's position as a leading hub for advanced AI and digital infrastructure in Europe.\"\n\nThe company acquired the land for the facility in September 2024 for SEK 400 million ($39m) and previously said that it would aim to have the first data centers ready by 2027.\n\nFounded in 2023, Mistral AI is a French AI firm. The company has raised billions of dollars in investment and is valued at more than €11.7 billion ($13.67bn). Investors include ASML, DST Global, Andreessen Horowitz, Bpifrance, General Catalyst, Index Ventures, Lightspeed, and Nvidia.\n\nThe company has previously used Microsoft Azure and Google Cloud, and is a known CoreWeave and Scaleway customer. Mistral also runs its own AI cloud, Mistral Compute, which offers access to the company's products and the ability to rent GPUs for powering other AI systems.\n\nThe company is developing a 40MW cluster in France in partnership with Iliad's Scaleway, and is developing a 1.4GW cluster around Paris in a venture backed by investment bank Bpifrance, UAE investment fund MGX, and Nvidia. It is also working with Eclairion and AI cloud firm Fluidstack.\n\nEcoDataCenter was formed in 2015 as a joint venture between local energy company Falu Energi & Vatten and data center operator EcoDC AB. Nordic real estate developer Areim took a majority stake in the firm via the Areim Fund III for around SEK 200 million ($22m) in 2018 and merged it with Swedish operator Fortlax in 2019.\n\nIn April 2025, EcoDataCenter sold three facilities in Sweden to CapMan Infra as part of a hyperscale pivot. Two of the data centers are located in Stockholm and one in Piteå.\n\nEcoDataCenter has previously secured language AI company DeepL and AI cloud firm CoreWeave as customers.\n\nMore in AI & Analytics"
  },
  {
    "source": "k.sina.com.cn",
    "company": "Mistral AI",
    "title": "AI初创公司Mistral向瑞典数据中心投资14亿美元",
    "date": "2026-02-11T12:52:27Z",
    "url": "https://k.sina.com.cn/article_5953189932_162d6782c06703tbt6.html",
    "content": "新的大规模计算能力预计将于2027年在瑞典投入使用，支持Mistral AI的下一代人工智能模型。\n\n2月11日，法国人工智能初创公司Mistral AI宣布，将投资12亿欧元（约合14.3亿美元）在瑞典建设新的数据中心。Mistral AI首席执行官兼联合创始人Arthur Mensch表示：\"这项投资是欧洲建设独立人工智能能力的实质性一步。\"\n\n瑞典数据中心运营商EcoDataCenter将负责设计、建设和运营新的基础设施。EcoDataCenter在另一份声明中表示，新的大规模计算能力预计将于2027年在瑞典投入使用，支持Mistral AI的下一代人工智能模型。这将是Mistral AI在法国以外的首次基础设施投资。"
  },
  {
    "source": "Clubic.com",
    "company": "Mistral AI",
    "title": "C'est une première pour Mistral AI, accusée par un éditeur français d'avoir piraté une partie de son catalogue",
    "date": "2026-02-02T06:02:15Z",
    "url": "https://www.clubic.com/actualite-598432-c-est-une-premiere-pour-mistral-ai-accusee-par-un-editeur-francais-d-avoir-pirate-une-partie-de-son-catalogue.html",
    "content": "Un éditeur français réclame une indemnisation à Mistral AI. Nouveau Monde Editions accuse la start-up d'avoir utilisé plus de 200 de ses livres pour entraîner son intelligence artificielle sans autorisation.\n\nYannick Dehée, fondateur de Nouveau Monde Editions, affirme que Mistral AI, qui avait tout de même intéressé Apple, a utilisé plus de 200 de ses ouvrages pour entraîner son intelligence artificielle Le Chat sans aucune autorisation. Pour la première fois, un éditeur français demande à la start-up d'indemniser ses auteurs et ayants droit pour l'exploitation de ces ouvrages.\n\nDe son côté, Mistral AI conteste les accusations et précise que ses modèles sont entraînés sur un ensemble diversifié de données, incluant des sources publiques et des contenus sous licence. Mais Yannick Dehée doute de cette version et s'interroge sur l'utilisation éventuelle de données piratées issues de Library Genesis pour entraîner le modèle de Mistral AI.\n\nPour rappel, aux États-Unis, des entreprises d'IA ont dû verser des millions de dollars après avoir utilisé illégalement des livres traduits, une référence pour les enjeux financiers et juridiques auxquels Mistral AI est confrontée."
  },
  {
    "source": "Enerzine",
    "company": "Mistral AI",
    "title": "Mistral AI : le milliard de revenus en 2026, un pari osé pour le champion français de l'IA",
    "date": "2026-01-24T11:54:25Z",
    "url": "https://www.enerzine.com/mistral-ai-le-milliard-de-revenus-en-2026-un-pari-ose-pour-le-champion-francais-de-lia/181177-2026-01",
    "content": "A l'occasion du Forum économique mondial de Davos, Arthur Mensch, le PDG de Mistral AI, a dévoilé une ambition à faire pâlir d'envie la plupart des startups : dépasser le milliard d'euros de chiffre d'affaires pour l'année 2026. L'annonce, qui marque un virage stratégique majeur pour le leader européen de l'intelligence artificielle générative a relancé le débat sur la souveraineté technologique du Vieux Continent.\n\nL'objectif, confirmé par Arthur Mensch dans un entretien avec Bloomberg, repose sur une accélération commerciale sans précédent. Fort d'une levée de fonds record de 1,7 milliard d'euros en septembre dernier, qui a porté sa valorisation à 11,7 milliards d'euros, Mistral AI entend désormais passer du statut de brillant laboratoire de recherche à celui de prestataire de solutions à grande échelle pour les entreprises et les administrations. Ce saut quantique qui part d'un chiffre d'affaires probablement inférieur à 150 millions d'euros en 2025 pour viser plus d'un milliard l'année suivante montre à la fois la fébrilité et les espoirs colossaux qui entourent ce secteur.\n\nUne ambition dévoilée sur la scène mondiale de Davos\n\nLa déclaration a été faite en marge du Forum économique mondial de Davos, une tribune pour capter l'attention des décideurs économiques et politiques internationaux. En choisissant ce moment, Mistral AI a habilement positionné son ambition non seulement comme un objectif commercial, mais aussi comme un enjeu géostratégique pour l'Europe.\n\nCette projection contraste vivement avec la prudence habituelle des entreprises européennes en phase de croissance et vise à démontrer que le continent peut engendrer des champions technologiques capables de rivaliser à l'échelle mondiale.\n\nLes piliers d'une croissance \" brutale \"\n\nPour atteindre cet objectif ambitieux, Mistral AI compte sur plusieurs leviers. Le premier est une offensive commerciale agressive dans le secteur B2B (business-to-business), en signant des contrats avec de grandes entreprises et des administrations publiques. La société mise également sur le développement et la monétisation de ses produits grand public et professionnels, comme son assistant conversationnel \" Le Chat \" et sa version \" Enterprise \".\n\nLe deuxième levier est un investissement massif, estimé à près d'un milliard de dollars pour 2026, dans les infrastructures. Comme l'a précisé Bloomberg, cela inclut des dépenses en matériel (capex) et des investissements dans le cloud, éléments essentiels pour entraîner les modèles d'IA de nouvelle génération et soutenir une croissance exponentielle de la demande. Des acquisitions stratégiques dans des secteurs ou régions non encore précisés pourraient également venir accélérer cette expansion.\n\nL'entreprise, fondée il y a moins de deux ans, doit ainsi opérer une mue rapide, transformant son excellence technique en une force de vente et une capacité d'exécution industrielle.\n\nUn test pour la souveraineté européenne en IA\n\nAu-delà des chiffres, l'annonce de Mistral AI résonne comme un test pour l'ambition européenne en matière d'intelligence artificielle. Alors que les géants américains comme OpenAI et Anthropic opèrent avec des valorisations se chiffrant en centaines de milliards de dollars, l'Europe cherche désespérément à faire émerger un champion crédible. Avec sa valorisation de 11,7 milliards d'euros et le soutien d'investisseurs de premier plan comme le néerlandais ASML, Mistral symbolise actuellement cet espoir.\n\nLe succès ou l'échec de cet objectif de revenus sera perçu comme un indicateur de la capacité de l'écosystème européen à retenir ses talents, à attirer les capitaux nécessaires et à convertir l'innovation en succès commercial mondial. La route reste néanmoins semée d'embûches. La concurrence est féroce, les coûts d'infrastructure astronomiques, et le rythme d'innovation, impitoyable."
  },
  {
    "source": "Economie Matin",
    "company": "Mistral AI",
    "title": "Free Mobile : ce que vous allez payer si vous ne faites rien",
    "date": "2026-01-23T08:44:59Z",
    "url": "https://www.economiematin.fr/free-mobile-payer-si-vous-ne-faites-rien",
    "content": "17,99 € par moisLe montant de l'abonnement Mistral AI qui s'applique automatiquement après la fin de la période gratuite.\n\nL'intégration de services numériques \"bonus\" dans les forfaits télécoms est devenue un classique. Streaming, stockage cloud, cybersécurité : les opérateurs rivalisent d'options pour enrichir leurs offres. Avec Mistral AI, Free Mobile est allé plus loin en misant sur l'intelligence artificielle générative. Problème : comme souvent, la gratuité est temporaire. Et lorsque l'essai s'achève, l'abonnement, lui, commence sans rappel systématique. À quelques semaines de la fin des douze mois offerts, le sujet mérite toute l'attention des abonnés.\n\nFree Mobile et Mistral : une offre gratuite... mais à durée limitée\n\nAu début de l'année 2025, Free Mobile annonçait un partenariat remarqué avec la start-up française Mistral AI, devenue l'un des acteurs majeurs de l'IA générative en Europe. Tous les abonnés de l'opérateur mobile pouvaient alors activer gratuitement, pendant douze mois, la version premium de l'assistant conversationnel Le Chat Pro.\n\nSelon Numerama, cette offre était accessible sans distinction de forfait, y compris pour les clients au forfait à 2 euros. Une stratégie claire : démocratiser l'usage de l'intelligence artificielle tout en valorisant l'écosystème technologique français.\n\nMais dès l'activation, une condition figurait comme souvent dans les petites lignes : au terme des 12 mois, l'option ne se désactive pas automatiquement.\n\nUne option activée en un clic... et facturée sans rappel\n\nC'est le point central de l'alerte. Une fois la période de gratuité terminée, l'option Le Chat Pro bascule vers un abonnement payant de 17,99 € par mois, directement ajouté à la facture Free Mobile.\n\nNumerama précise que cette transition se fait sans confirmation active de l'abonné : l'absence d'action vaut acceptation. Autrement dit, si vous avez activé l'offre gratuite par curiosité, sans l'utiliser réellement, vous pourriez payer plusieurs mois pour un service oublié.\n\nDans un contexte d'inflation persistante et de multiplication des abonnements numériques, ce type de dépense \"silencieuse\" peut rapidement peser sur le budget.\n\nComment éviter de payer pour un service inutile\n\nPour ne pas voir apparaître cette ligne supplémentaire sur votre facture, une seule solution : désactiver l'option avant la fin de la période gratuite.\n\nLa démarche est relativement simple, mais encore faut-il y penser :\n\n* se connecter à l'espace abonné Free Mobile,\n\n* accéder à la gestion des options,\n\n* repérer l'option Le Chat Pro / Mistral AI,\n\n* procéder à la résiliation.\n\nSelon Numerama, aucun e-mail de rappel généralisé n'est prévu par l'opérateur. C'est donc à l'abonné de vérifier la date exacte d'activation et d'anticiper.\n\nCe que vous perdez... et ce que vous conservez\n\nRésilier l'option payante ne signifie pas renoncer totalement à l'intelligence artificielle de Mistral. La version standard de Le Chat reste accessible gratuitement via le site ou l'application officielle.\n\nEn revanche, la version Pro incluse chez Free Mobile offre plusieurs avantages : accès prioritaire aux modèles les plus performants, fonctionnalités avancées, volume d'utilisation plus élevé. Des options utiles pour un usage professionnel ou intensif, mais largement superflues pour un usage occasionnel.\n\nL'enjeu est donc simple : faire le tri entre un service réellement utilisé et une option activée \"pour voir\".\n\nLe bon réflexe à adopter dès maintenant\n\nSi vous êtes abonné Free Mobile, une vérification s'impose : avez-vous activé l'offre Mistral AI ? Si oui, utilisez-vous réellement ce service ? Et surtout, êtes-vous prêt à payer près de 18 euros par mois pour le conserver ?\n\nÀ défaut, mieux vaut agir avant la date anniversaire. Car en matière d'abonnements, l'oubli reste souvent l'option la plus coûteuse."
  },
  {
    "source": "TechNews 科技新報 | 市場和業內人士關心的趨勢、內幕與新聞",
    "company": "Mistral AI",
    "title": "稱中國 AI 落後是天方夜譚，Mistral AI 執行長稱美國壓力大",
    "date": "2026-01-23T08:22:58Z",
    "url": "https://technews.tw/2026/01/23/china-does-not-lag-west-in-ai-tech/",
    "content": "地緣政治在今年世界經濟論壇（World Economic Forum）是熱門議題，作為歐洲具代表性的 AI 新創 Mistral AI，共同創辦人暨執行長 Arthur Mensch 受訪談到，中國開源技術的能力恐怕讓美國 AI 公司感受不少壓力。\n\n對於「中國在 AI 技術上落後美國」的說法，「中國並沒有落後西方。」Arthur Mensch 接受彭博電視專訪表示，這樣的說法彷彿天方夜譚，中國開源技術的能力「恐怕給美國的執行長們帶來壓力」，這番言論與其他在達沃斯發聲的科技領袖形成對比。\n\nGoogle DeepMind 執行長 Demis Hassabis 與 Anthropic 執行長 Dario Amodei 參與世界經濟論壇。Demis Hassabis 談到，中國在先進模型研發上大約落後西方 6 個月，而且尚未展現突破性創新能力。Dario Amodei 則指出，美國限制向中國銷售尖端技術的政策正在拖慢其進展，並形容向中國出售高階 AI 晶片「就像把核武賣給北韓」。\n\n在美國與中國主導的 AI 市場中，Mistral AI 努力打造差異化，吸引企業客戶、消費大眾及投資人的青睞。值得一提的是，Mistral AI 在 2025 年 9 月宣布完成 17 億歐元 C 輪融資，由荷蘭半導體設備大廠 ASML 領投，公司估值來到 117 億歐元，成為歐洲 2 大科技公司重大結盟。\n\nArthur Mensch 表示，Mistral AI 以企業客戶作為成長主軸，如匯豐銀行與法國巴黎銀行等金融機構為挹注 Mistral AI 成長動能，也有參與國防相關領域。他指出，公司今年目標是營收突破 10 億美元，並計劃今年投入 10 億美元的資本支出。\n\nAI 逐漸成為具備重塑未來經濟與勞動力的強大力量，企業與各國政府紛紛投入數十億美元，擴建 AI 基礎建設並投入創新與應用。NVIDIA 執行長黃仁勳表示，相關成本最終可能多達數兆美元。\n\n（首圖來源：pixabay）"
  },
  {
    "source": "Analytics India Magazine",
    "company": "Mistral AI",
    "title": "Mistral AI in Talks to Set Up Bengaluru GCC, Plans Phased Research Expansion | AIM",
    "date": "2026-01-22T06:23:34Z",
    "url": "https://analyticsindiamag.com/ai-news-updates/mistral-ai-in-talks-to-set-up-bengaluru-gcc-plans-phased-research-expansion/",
    "content": "Mistral's Bengaluru GCC is expected to follow a phased approach, beginning with engineering capabilities.\n\nFrench artificial intelligence company Mistral AI is in discussions to establish a global capability centre (GCC) in Bengaluru, signalling a potential entry into India as part of its broader global expansion strategy, MB Patil, Karnataka's minister for large and medium industries, told Moneycontrol.\n\nThe development was disclosed after Patil met Audrey Herblin Stoop, VP - global public affairs and communication, Mistral AI, on the sidelines of the World Economic Forum (WEF) 2026 in Davos, Switzerland.\n\nThe proposed Bengaluru centre is expected to follow a phased approach, beginning with engineering capabilities and gradually expanding into advanced research functions.\n\nIf finalised, the move would place Mistral AI alongside a growing list of leading global AI firms strengthening their presence in India, particularly in Bengaluru. The city has recently emerged as a preferred destination for frontier AI talent, with Anthropic planning to open a Bengaluru office in early 2026 .\n\nFounded in 2023, Mistral AI has rapidly gained recognition for building high-performance, accessible, and often open-source large language models (LLMs).\n\nThe French AI startup raised €1.7 billion ($2 billion) in a Series C funding round in September last year, led by Dutch semiconductor giant ASML, bringing its valuation to about €11.7 billion ($13.7 billion). ASML invested €1.3 billion ($1.5 billion), becoming Mistral's largest shareholder with an 11% stake and a seat on the company's strategic committee.\n\nMistral AI said the investment will support scientific research and the development of decentralised frontier AI solutions aimed at addressing complex engineering and industrial problems."
  },
  {
    "source": "Business Insider",
    "company": "Mistral AI",
    "title": "Mistral AI y Ericsson se alían para impulsar la innovación en IA en telecomunicaciones",
    "date": "2026-02-20T12:18:17Z",
    "url": "https://www.businessinsider.es/tecnologia/mistral-ai-ericsson-alian-impulsar-innovacion-ia-telecomunicaciones_6936895_0.html",
    "content": "Despido colectivo en Ericsson España: la empresa comunica a la plantilla y a sindicatos sus planes\n\nNueva alianza tecnológica para impulsar la IA. Ericsson y Mistral AI, startup francesa fundada por exinvestigadores de Meta y Google DeepMind, se han asociado para impulsar la inteligencia artificial avanzada a desafíos reales de telecomunicaciones con el objetivo claro de hacer que las redes sean \"más inteligentes, más eficientes y más confiables\", según han informado ambas compañías en un comunicado.\n\nLa colaboración combina las capacidades de personalización de modelos de Mistral AI con la experiencia en I+D y redes de Ericsson, donde esta última actúa como socio de diseño para la plataforma.\n\nEl trabajo se centra en casos de uso de alto impacto que aceleran la entrega de software y fortalecen el rendimiento de la red, incluyendo la automatización de la traducción de código heredado, el desarrollo asistido por IA para la investigación 6G y agentes de IA personalizados para flujos de trabajo complejos en la organización de redes de Ericsson.\n\nMistral AI y Ericsson investigarán y desarrollarán conjuntamente agentes de IA adaptados a los entornos de datos e ingeniería de Ericsson, acercando los datos a la IA y permitiendo una toma de decisiones más rápida en el desarrollo e implementación de productos.\n\nLas empresas buscan establecer nuevos estándares para una infraestructura de telecomunicaciones segura, de alto rendimiento y resiliente, alineando la innovación en IA con los requisitos de las redes de nivel operador.\n\nEsta colaboración se basa en fortalezas complementarias, los modelos y herramientas de base de vanguardia de Mistral AI, y las décadas de experiencia de Ericsson en sistemas de radio, nube y red, demostrada a escala global."
  },
  {
    "source": "NoticiasDe.es",
    "company": "Mistral AI",
    "title": "Mistral AI y Ericsson colaboran para fomentar la innovación en inteligencia artificial en el sector de las telecomunicaciones",
    "date": "2026-02-20T09:10:55Z",
    "url": "https://www.noticiasde.es/espana/mistral-ai-y-ericsson-colaboran-para-fomentar-la-innovacion-en-inteligencia-artificial-en-el-sector-de-las-telecomunicaciones/",
    "content": "Mistral AI y Ericsson se han asociado para aplicar inteligencia artificial (IA) avanzada a desafíos reales de telecomunicaciones con el objetivo claro de hacer que las redes sean \"más inteligentes, más eficientes y más confiables\", según han informado ambas compañías en un comunicado.\n\nLa colaboración combina las capacidades de personalización de modelos de Mistral AI con la experiencia en I+D y redes de Ericsson, donde esta última actúa como socio de diseño para la plataforma.\n\nEl trabajo se centra en casos de uso de alto impacto que aceleran la entrega de software y fortalecen el rendimiento de la red, incluyendo la automatización de la traducción de código heredado, el desarrollo asistido por IA para la investigación 6G y agentes de IA personalizados para flujos de trabajo complejos en la organización de redes de Ericsson.\n\nMistral AI y Ericsson investigarán y desarrollarán conjuntamente agentes de IA adaptados a los entornos de datos e ingeniería de Ericsson, acercando los datos a la IA y permitiendo una toma de decisiones más rápida en el desarrollo e implementación de productos.\n\nLas empresas buscan establecer nuevos estándares para una infraestructura de telecomunicaciones segura, de alto rendimiento y resiliente, alineando la innovación en IA con los requisitos de las redes de nivel operador.\n\nEsta colaboración se basa en fortalezas complementarias, los modelos y herramientas de base de vanguardia de Mistral AI, y las décadas de experiencia de Ericsson en sistemas de radio, nube y red, demostrada a escala global."
  },
  {
    "source": "thefastmode.com",
    "company": "Mistral AI",
    "title": "Mistral AI, Ericsson Partner on AI Agents for 6G Research and Network Performance",
    "date": "2026-02-20T01:16:52Z",
    "url": "https://www.thefastmode.com/technology-solutions/47190-mistral-ai-ericsson-partner-on-ai-agents-for-6g-research-and-network-performance",
    "content": "Mistral AI and Ericsson announced a partnership to apply advanced AI to real telecom challenges with a clear goal to make networks smarter, more efficient, and more trusted.\n\nThe collaboration combines Mistral AI's model customization capabilities with Ericsson's R&D and network expertise, with Ericsson acting as a design partner for the platform.\n\nThe work targets high-impact use cases that speed software delivery and strengthen network performance, including automation of legacy code translation, AI-assisted development for 6G research, and custom AI agents for complex workflows in Ericsson's Networks organization.\n\nMistral AI and Ericsson will jointly research and co-develop AI agents tailored to Ericsson's data and engineering environments, bringing data closer to AI and enabling faster decision-making in product development and deployment. The companies aim to set new benchmarks for secure, high-performing, and resilient telecom infrastructure, aligning AI innovation with the requirements of carrier-grade networks.\n\nThis is a collaboration grounded in complementary strengths, Mistral AI's cutting-edge foundation models and tooling, and Ericsson's decades of radio, cloud, and network systems experience, proven at global scale. Together, the partnership focuses on AI for networks, not AI in isolation, to deliver measurable outcomes for customers."
  },
  {
    "source": "Diario Siglo XXI",
    "company": "Mistral AI",
    "title": "Mistral AI y Ericsson se asocian para impulsar la innovación en IA en telecomunicaciones",
    "date": "2026-02-19T17:44:35Z",
    "url": "https://www.diariosigloxxi.com/texto-ep/mostrar/20260219182640/mistral-ai-ericsson-asocian-impulsar-innovacion-ia-telecomunicaciones",
    "content": "Mistral AI y Ericsson se han asociado para aplicar inteligencia artificial (IA) avanzada a desafíos reales de telecomunicaciones con el objetivo claro de hacer que las redes sean \"más inteligentes, más eficientes y más confiables\", según han informado ambas compañías en un comunicado.\n\nLa colaboración combina las capacidades de personalización de modelos de Mistral AI con la experiencia en I+D y redes de Ericsson, donde esta última actúa como socio de diseño para la plataforma.\n\nEl trabajo se centra en casos de uso de alto impacto que aceleran la entrega de software y fortalecen el rendimiento de la red, incluyendo la automatización de la traducción de código heredado, el desarrollo asistido por IA para la investigación 6G y agentes de IA personalizados para flujos de trabajo complejos en la organización de redes de Ericsson.\n\nMistral AI y Ericsson investigarán y desarrollarán conjuntamente agentes de IA adaptados a los entornos de datos e ingeniería de Ericsson, acercando los datos a la IA y permitiendo una toma de decisiones más rápida en el desarrollo e implementación de productos.\n\nLas empresas buscan establecer nuevos estándares para una infraestructura de telecomunicaciones segura, de alto rendimiento y resiliente, alineando la innovación en IA con los requisitos de las redes de nivel operador.\n\nEsta colaboración se basa en fortalezas complementarias, los modelos y herramientas de base de vanguardia de Mistral AI, y las décadas de experiencia de Ericsson en sistemas de radio, nube y red, demostrada a escala global."
  },
  {
    "source": "Silicon",
    "company": "Mistral AI",
    "title": "Arthur Mensch, patron de Mistral AI, relance le débat sur le replatforming par l'IA générativep",
    "date": "2026-02-19T13:50:14Z",
    "url": "https://www.silicon.fr/business-1367/arthur-mensch-relance-le-debat-le-replatforming-par-lia-va-t-il-engloutir-le-logiciel-dentreprise-225834",
    "content": "Arthur Mensch, le patron de Mistral AI, affirme que plus de la moitié des logiciels d'entreprise pourraient être supplantés par des solutions construites autour de l'IA générative.\n\nC'est une déclaration qui va faire frémir les grands éditeurs du logiciel déjà éprouvés par le désamour des analystes financiers. Dans une interview accordée à CNBC, Arthur Mensch, PDG de Mistral AI, affirme que plus de 50 % des logiciels d'entreprise actuels seraient, à terme, remplaçables par des applications bâties sur l'IA générative.\n\nDans le viseur ? Les outils de productivité, de workflows et les CRM légers : autrement dit, le cœur de métier de Salesforce, Workday ou ServiceNow.\n\nLa promesse du \" replatforming \"\n\nPour l'expliquer, Arthur Mensch utilise le concept de \" replatforming \" , soit un basculement en profondeur des architectures IT d'entreprise, qui délaisseraient les abonnements SaaS standardisés au profit d'applications sur mesure, construites directement sur des API de modèles d'IA. L'argument commercial est séduisant avec la réduction des coûts de licences, une meilleure adéquation aux processus internes, des délais de développement compressés à quelques jours là où il fallait autrefois compter en mois.\n\nLe mouvement a déjà ses précurseurs. Klarna, la fintech suédoise, a ainsi tourné le dos à certaines briques Salesforce et Workday pour construire son propre stack dopé à l'IA. Un cas d'école qu'Arthur Mensch cite volontiers pour illustrer la faisabilité concrète de ce grand saut technologique.\n\nTout ne sera pas emporté dans la vague\n\nLe patron de Mistral prend soin de nuancer. L'infrastructure de données - stockage, sauvegarde, sécurité, data platforms - n'est pas menacée. Elle sortira même renforcée de la transition, car c'est elle qui alimente les modèles.\n\nLire aussi : Koyeb, un atout PaaS dans les mains de Mistral AI\n\nBipul Sinha, PDG de Rubrik, partage ce diagnostic : le \" front office logiciel \" sera remodelé, quand le \" back office data \" se consolidera comme couche critique et incontournable.\n\nCe basculement suppose toutefois des prérequis sérieux : des données propres et unifiées, une infrastructure cloud moderne, et des équipes capables de gouverner ces nouveaux agents. On est bien loin du confort du SaaS clé en main.\n\nUne opportunité taillée pour Mistral\n\nEvidemment, la sortie du patron de Mistral AI sert son business modèle. Il revendique déjà plus d'une centaine de clients grands comptes en quête de modernisation. Sa plateforme - modèles ouverts et propriétaires, assistants personnalisés, recherche d'entreprise - se positionne précisément comme l'outillage de ce \" replatforming \" .\n\nLa prédiction reste néanmoins contestée. Les sceptiques pointent l'inertie considérable des systèmes d'information existants, les contraintes réglementaires, et les nombreuses désillusions de projets d'IA en production. Pour beaucoup d'observateurs, l'IA s'intégrera d'abord comme une couche d'augmentation au-dessus des outils existants, plutôt que comme un bulldozer.\n\nMais le message envoyé aux éditeurs traditionnels est sans ambiguïté : se transformer en plateformes d'IA, ou risquer de devenir eux-mêmes les logiciels à remplacer."
  },
  {
    "source": "Silicon",
    "company": "Mistral AI",
    "title": "Koyeb, un atout PaaS dans les mains de Mistral AI",
    "date": "2026-02-18T17:01:11Z",
    "url": "https://www.silicon.fr/business-1367/mistral-ai-koyeb-225820",
    "content": "Pour alimenter son offre d'infrastructure, Mistral AI s'empare de Koyeb, un PaaS made in France qui a pris le virage de l'IA.\n\nUne offre d'infrastructure pour l'IA, donnant accès à une stack privée avec une variété d'options de déploiement, du bare metal au PaaS managé. Telle était, dans les grandes lignes, la promesse de Mistral Compute à son lancement mi-2025.\n\nMistral AI a récemment annoncé un projet à 1,2 Md€ pour implanter, en Suède, un datacenter qui alimentera ce \" cloud IA \". Il y ajoute une acquisition. La cible : Koyeb.\n\nCette entreprise française est née en 2019 à l'initiative de trois anciens de Scaleway*. Isai, Serena Capital et Kima Ventures, entre autres, y ont mis leurs billes. Elle présente aujourd'hui son offre comme une \" plate-forme serverless pour les applications IA \". Techniquement, il s'agit d'un PaaS - d'ailleurs encore comparé à Render et à Heroku dans sa documentation.\n\nDu H200 et du B200 depuis peu\n\nÀ cette partie compute, Koyeb avait adjoint, en 2025, du PostgreSQL managé, en plus de faire la connexion avec diverses bases de données (dont MongoDB, investisseur et client). Il a surtout pris le virage de l'IA en multipliant les options GPU et autres puces accélératrices (les RTX Pro 6000, les H200 et les B200 sont arrivées début 2026). Tout en enrichissant son catalogue de composants déployables \" en un clic \" (n8n, Ollama, Open WebUI, Unsloth, Jupyter Notebook...) et en développant des intégrations avec les assistants de codage (serveur MCP, pack de skills).\n\nLire aussi : Mistral AI investit 1,2 milliard € dans un datacenter IA\n\nKoyeb s'appuie sur quatre hébergeurs : Equinix, AWS, IBM Cloud et Scaleway. Son plan de contrôle se trouve en Belgique, sur GCP. L'offre se divise en quatre forfaits de base auxquels s'ajoute la consommation de ressources.\n\nAvec le passage dans le giron de Mistral AI (16 employés seront du voyage), le forfait gratuit n'est plus proposé. La plate-forme reste commercialisée indépendamment, en attendant son intégration dans Mistral Compute.\n\n* Yann Léger, 35 ans, est président de Koyeb. La société a deux DG : Édouard Bonlieu (38 ans, directeur produit) et Bastien Chatelard (37 ans, directeur technique).\n\nIllustration générée par IA"
  },
  {
    "source": "Tech Funding News",
    "company": "Mistral AI",
    "title": "Nordic AI corridor? Mistral AI joins the rush with a €1.2B Sweden investment  --  TFN",
    "date": "2026-02-12T13:04:46Z",
    "url": "https://techfundingnews.com/mistral-ai-invests-e1-2b-in-sweden-infrastructure/",
    "content": "French AI company Mistral AI announced that it will invest €1.2 billion ($1.43 billion) in Sweden to build out digital infrastructure, including large-scale AI data centres.\n\nThe move signals a serious attempt to anchor AI capacity within Europe at a time when geopolitical tensions are pushing governments to rethink their reliance on foreign tech giants.\n\nThe €1.2 billion commitment will fund AI data centres, advanced compute capacity, and localised AI processing and storage. In simple terms, Mistral is no longer just building models. It is building the backbone required to run them.\n\nArthur Mensch, CEO of Mistral AI, described the move as more than just expansion. \"This investment is a concrete step toward building independent capabilities in Europe, dedicated to AI,\" he said.\n\nSweden and the broader Nordic region have become a hotspot for compute infrastructure. Cooler temperatures help reduce cooling costs, and electricity prices are among the lowest in Europe.\n\nMistral will partner with Swedish firm EcoDataCenter to deploy large-scale AI compute facilities. This marks Mistral's first major infrastructure investment outside France.\n\nThe new facility is expected to go live in 2027 and will support the training and operation of next-generation AI models.\n\nThe timing is notable. In July, OpenAI announced plans to build an AI data centre in Norway as part of its Stargate initiative. The Nordics are quickly becoming Europe's AI compute corridor.\n\nFounded in 2023, Mistral AI rose quickly as Europe's answer to U.S. large language model developers. It initially focused on building advanced LLMs but has since expanded deeper into infrastructure.\n\nIn June, the company launched Mistral Compute, an integrated stack offering GPUs, APIs, and fully managed platform services. That shift signalled a broader ambition: controlling not just the software layer, but also the hardware and operational backbone.\n\nMistral AI is currently Europe's most heavily funded LLM builder. In September, the company raised €1.7 billion, bringing its valuation to €11.7 billion.\n\nMistral is backed by prominent investors such as ASML, Nvidia, Microsoft, DST Global, Andreessen Horowitz, Bpifrance, General Catalyst, and Index Ventures.\n\nAccording to Dealroom, the company has raised approximately $2.9 billion to date. While this amount is noteworthy for Europe, it still pales in comparison to U.S. competitors.\n\nFor instance, OpenAI is reportedly close to completing a funding round that may reach $100 billion, and Anthropic secured a term sheet for a $10 billion round in January."
  },
  {
    "source": "IT News zu den Themen Künstliche Intelligenz, Roboter und Maschinelles Lernen - IT BOLTWISE® x Artificial Intelligence",
    "company": "Mistral AI",
    "title": "EuropÃ¤ische KI-Infrastruktur: Mistral AI und EcoDataCenter investieren in Schweden",
    "date": "2026-02-11T21:18:13Z",
    "url": "https://www.it-boltwise.de/europaeische-ki-infrastruktur-mistral-ai-und-ecodatacenter-investieren-in-schweden.html",
    "content": "BORLÄNGE / LONDON (IT BOLTWISE) - Die strategische Partnerschaft zwischen Mistral AI und EcoDataCenter markiert einen bedeutenden Schritt zur Stärkung der europäischen technologischen Souveränität. Mit einer Investition von 1,2 Milliarden Euro wird ein KI-Rechenzentrum in Schweden errichtet, das die wachsende Kluft zwischen KI-Nachfrage und Rechenleistung schließen soll. Diese Initiative zielt darauf ab, eine unabhängige europäische KI-Infrastruktur zu schaffen und die Abhängigkeit von außereuropäischen Technologieanbietern zu reduzieren.\n\nDie Zusammenarbeit zwischen Mistral AI und EcoDataCenter ist ein bedeutender Schritt in Richtung einer unabhängigen europäischen KI-Infrastruktur. Mit einer Investition von 1,2 Milliarden Euro wird in Borlänge, Schweden, ein hochmodernes KI-Rechenzentrum errichtet, das speziell für das Training und den Betrieb komplexer KI-Modelle ausgelegt ist. Diese Initiative soll nicht nur die technologische Souveränität Europas stärken, sondern auch die wachsende Kluft zwischen der Nachfrage nach KI-Anwendungen und der verfügbaren Rechenleistung schließen.\n\nDie Entscheidung, das Rechenzentrum in Schweden zu bauen, ist strategisch klug. Schweden bietet nicht nur Zugang zu erneuerbarer Energie, sondern auch die notwendige Expertise für den Betrieb nachhaltiger Hochleistungsrechenzentren. EcoDataCenter wird das Rechenzentrum betreiben und dabei NVIDIAs neueste GPU-Generation nutzen, um die Leistungsanforderungen moderner KI-Cluster zu erfüllen. Diese Investition ist für Mistral AI die erste große Infrastrukturinvestition außerhalb Frankreichs und ein klares Bekenntnis zur Schaffung eines souveränen europäischen KI-Ökosystems.\n\nDie Notwendigkeit einer solchen Investition wird durch aktuelle Berichte unterstrichen, die zeigen, dass die Nachfrage nach KI-Anwendungen das Angebot an Rechenkapazitäten bei weitem übersteigt. Laut einer Studie der BCS Consultancy erwarten 93 Prozent der Branchenexperten weiteres Wachstum im KI-Sektor, während nur 20 Prozent der europäischen Rechenzentren als \"KI-ready\" gelten. Diese Diskrepanz droht, die Entwicklung der KI in Europa auszubremsen, da die größten Herausforderungen nicht mehr im Kapital, sondern im Zugang zu Strom, langwierigen Genehmigungsverfahren und Fachkräftemangel liegen.\n\nEin weiterer Bericht des Europäischen Rechenzentrumsverbands (EUDCA) warnt vor einem strukturellen Kipppunkt, der das Wachstum der Rechenzentrumsinfrastruktur in Europa begrenzen könnte. Bis 2031 werden zwar kumuliert 176 Milliarden Euro investiert, doch die Stromversorgung bleibt für 67 Prozent der Betreiber die größte operative Herausforderung. Diese Energieknappheit zwingt zur Dezentralisierung und verlagert neue Projekte von traditionellen Hotspots in sekundäre Märkte wie Südeuropa und Skandinavien. Die 1,2-Milliarden-Investition von Mistral AI und EcoDataCenter ist daher ein wichtiges Vertrauensvotum in die Zukunft der europäischen KI-Infrastruktur."
  },
  {
    "source": "ZDNet",
    "company": "Mistral AI",
    "title": "Pour son nouveau datacenter, Mistral AI opte pour la Suède - ZDNET",
    "date": "2026-02-11T16:38:31Z",
    "url": "https://www.zdnet.fr/actualites/pour-son-nouveau-datacenter-mistral-ai-opte-pour-la-suede-489953.htm",
    "content": "Mistral AI sort de ses frontières, mais reste dans l'union européenne. Le développeur français de modèles d'intelligence artificielle a choisi la Suède pour héberger son premier centre de données hors de France. Le projet a été officialisé par Ecodatacenter, qui sera le principal partenaire de Mistral AI pour l'installation du datacenter.\n\nToute l'actualité de la tech pour les pros chaque jour dans notre newsletter En savoir plus sur l'utilisation des données personnelles Aventure suédoise\n\nLes deux sociétés annoncent un investissement total de 12,8 milliards de couronnes suédoises, soit environ 1,2 milliard d'euros, pour financer la construction de ce datacenter basé près de la ville de Borlänge. Les porteurs du projet espèrent que le datacenter pourra voir le jour d'ici à 2027 et expliquent leur ambition de créer \"une pile technologique entièrement européenne où toutes les données sont traitées et stockées localement en Europe.\"\n\nLe projet met également l'accent sur la réduction de l'empreinte énergétique du datacenter : le site choisi réhabilite ainsi une ancienne friche industrielle utilisée auparavant par une papeterie. Ecodatacenter rappelle son engagement à utiliser des énergies renouvelables pour la conception des bâtiments et espère que le site pourra bénéficier des conditions naturelles de la région pour avoir recours à des technologies de \"free cooling\", technique qui consiste à utiliser l'air extérieur pour refroidir les systèmes du datacenter. La société suédoise avait déjà évoqué son projet de \"mega campus\" en fin d'année 2025 : l'entreprise expliquait alors que le site était conçu pour une capacité de 250 MW, avec la possibilité d'étendre ses capacités jusqu'à 600 MW.\n\nUne première implantation en Essonne\n\nC'est le deuxième projet de datacenter porté par Mistral AI. La société avait déjà annoncé en début d'année 2025 un premier projet de datacenter sur le territoire français, dans l'Essonne. Ce premier projet s'appuyait cette fois sur un partenariat avec la société Eclairion, dont le site de Bruyeres-le-Chatel est destiné à accueillir un cluster de calcul dédié à Mistral AI. Ce site dispose d'une capacité de calcul de 60 MW.\n\nLes projets de datacenters se multiplient en France, largement portés par les initiatives lancées par le gouvernement lors du sommet Choose France en début d'année 2025. Un premier bilan dressé par le ministère des finances à la fin du mois de janvier faisait état de 63 sites recensés, dont 26 étaient déjà préemptés par des porteurs de projets. Ce plan vise à permettre l'installation simplifiée de datacenters sur le territoire national, à travers des aides administratives et des facilités de raccordement des sites identifiés au réseau électrique."
  },
  {
    "source": "Silicon",
    "company": "Mistral AI",
    "title": "Un an après, que devient l'initiative EU AI Champions ?",
    "date": "2026-02-11T11:29:50Z",
    "url": "https://www.silicon.fr/uncategorized/teu-ai-champions-225661",
    "content": "Au Sommet de l'IA 2025 naissait l'initiative EU AI Champions. Elle a connu depuis quelques temps forts... et des départs de membres.\n\nNe cherchez plus Heineken ni Henkel : ils ne font plus partie des \" supporters \" d'EU AI Champions.\n\nCette initiative était née il y a tout juste un an, à l'occasion du Sommet pour l'action sur l'IA. Une soixantaine d'organisations - fournisseurs, utilisateurs et quelques associations représentatives - s'y étaient ralliées, sous la houlette du fonds de capital-risque General Catalyst. Sur le papier, l'objectif principal était simple : contribuer à accélérer le développement et l'adoption de l'intelligence artificielle en Europe. Entre renforcement de l'infrastructure de datacenters, développement des compétences et financement d'entreprises spécialisées, une vingtaine d'investisseurs s'étaient engagés à débloquer un total de 150 milliards d'euros sur 5 ans.\n\nUne première discussion avec un groupe de P-DG et de responsables politiques eut lieu au lendemain du Sommet. EU AI Champions appelait alors, entre autres, à simplifier la réglementation, à commencer par l'AI Act. Sa mobilisation sur le sujet avait culminé au mois juillet, quelques semaines avant l'entrée en vigueur des dispositions du règlement concernant les modèles d'IA à usage général. Par une lettre ouverte, la majorité de ses membres avaient réclamé un moratoire de 2 ans sur l'application des principales obligations du texte. Parmi les signataires figuraient, côté français, Arthur Mensch (Mistral AI), Éléonore Crespo (Pigment), Patrick Pouyanné (TotalEnergies), Guillaume Faury (Airbus), Bernard Charlès (Dassault Systèmes), Alexandre Bompard (Carrefour) et Jean-Laurent Bonnafé (BNP Paribas).\n\nL'UE n'est pas allée jusque-là, mais elle a lâché du lest avec son omnibus numérique.\n\nUne com qui s'est recentrée sur les membres\n\nLa liste des \" supporters \" comprend aujourd'hui 114 logos, rattachés à une centaine d'organisations. Elle n'a globalement pas évolué depuis mars 2025. Heineken et Henkel étaient alors déjà sortis de la boucle.\n\nEU AI Champions a fait de LinkedIn son principal canal de communication au public. Initialement, il y mit régulièrement en avant des organisations qui ne comptaient pas parmi ses membres (Bosch, Langfuse, PortiaAI...). C'est devenu plus rare.\n\nFin 2025, dans la lignée de ses appels à un allègement réglementaire, le collectif s'était fait l'écho de la position du gouvernement allemand sur l'omnibus numérique. Il s'en était réjoui, y voyant \" la première réponse réglementaire sérieuse à ce que l'industrie européenne de l'IA signalait depuis des mois \". Il faut dire que Berlin préconisait notamment de reporter d'un an l'application des règles relatives aux systèmes d'IA \" à haut risque \".\n\nUne initiative européenne à dominante franco-allemande\n\nQuoique à dimension européenne, EU AI Champions a des fondations largement franco-allemandes. Sur ses 114 logos, une trentaine sont rattachés à des organisations françaises et une quarantaine, à des organisations allemandes.\n\nFrance Allemagne Airbus Bilfinger (construction et services) Airbus Defence and Space Black Forest Labs (labo d'IA à l'origine des modèles multimodaux FLUX) Alan Cambrium (fabricant de produits chimiques) Autone (optimisation des stocks) Covestro (fabricant de produits chimiques) AXA DeepL BNP Paribas Deutsche Bank Carrefour Deutsche Telekom CMA CGM Edgeless Systems (cybersécurité) Contentsquare Flix (traveltech) Dassault Systèmes Genesis Cloud (\" néocloud \") Dataiku German AI Association (fédération des entreprises allemandes de l'IA) Doctolib GetYourGuide (réservation de voyages) Dust (plate-forme IA axée production de contenu) Giesecke+Devrient (solutions de sécurité) EDF Hapag-Lloyd (transport maritime en conteneurs) Kering Helsing (technologies de défense) L'Oréal Holtzbrinck (maison d'édition) Lighton Hexagon (technologies de mesure) Mirakl Infineon Mistral AI K+S (entreprise minière) Orange LangDock (plate-forme IA d'entreprise) Owkin LOH Group (industrie manufacturière et services) Pelico (plate-forme d'orchestration de la production) Lufthansa Photoroom Merantix Capital Pigment (plate-forme de planification) Mercedes-Benz Publicis Groupe Northern Data Group (\" néocloud \") Renault Group Orbem (technologies d'IRM) Sanofi Otto Group (vente à distance) Shift (détection de fraude à l'assurance) Parloa (conception et déploiement d'IA) TotalEnergies Personio (logiciels RH) RobCo (robotique) SAP Schwarz (groupe de distribution) Siemens Siemens Energy Skeleton (stockage de l'énergie) SPREAD (IA industrielle) Startup Verband (fédération des start-up allemandes) Südzucker (groupe sucrier) United Internet (services numériques) ZF (équipementier automobile)\n\nÀ ce contingent s'ajoutent deux entreprises dont les fondateurs sont français, mais qui ont leur siège aux États-Unis. Une du secteur aérospatial (Loft Orbital, qui a un pied à Toulouse). L'autre qui fournit des logiciels RH (Deel).\n\nUn pot-pourri de projets annoncé à Berlin\n\nLe dernier \" temps fort \" d'EU AI Champions s'est déroulé au Sommet sur la souveraineté numérique organisé à Berlin. C'était mi-novembre 2025. Ses membres y ont annoncé un pot-pourri de partenariats, d'accords commerciaux et d'engagements individuels, valorisés dans leur ensemble à 1 Md€.\n\nProjets Grandes lignes des engagements Allianz (Allemagne) x Parloa (Allemagne) Le premier va exploiter les technologies du second pour le service client. Black Forest Labs (Allemagne) x Mercedes-Benz (Allemagne) Le groupe automobile va développer des outils IA sur la base des modèles FLUX de la start-up. Charité Comprehensive Cancer Center (Allemagne) x Gustave Roussy (France) x Owkin (France) Ce projet associe les deux centres autour de la structuration de données biomédicales et d'un modèle de raisonnement pour la recherche biologique et le développement de médicaments. Current AI x SPRIND Current AI est un partenariat global né au Sommet de l'IA, sous l'impulsion de DeepMind, de Salesforce, d'AI Collaborative (initiative d'Omidyar Group), du ministère français de l'Europe et des Affaires étrangères et de trois fondations (Ford, MacArthur, McGovern).\n\nSPRIND est l'agence fédérale allemande pour l'innovation de rupture.\n\nLes deux parties travailleront sur l'application de l'IA aux données de santé. Deutsche Telekom (Allemagne) x PhysicsX (Angleterre) L'entreprise anglais va exploiter le cloud du groupe allemand pour déployer sa plate-forme d'ingénierie fondée sur de l'IA physique. Le partenariat court pour 3 ans. Doctolib (France) Pas de partenariat, mais un engagement à investir dans le système de santé allemand. ESTIA (European Sovereign Tech Industry Alliance) Naissance de cette alliance qui réunit A1, Airbus, Dassault Systèmes, Deutsche Telekom, Evroc, OpenNebula Systems, Orange, OVHcloud, Post Luxembourg, Schwarz Digits, Sopra Steria et TIM. Helsing (Allemagne) x Mistral AI (France) Nouvelle phase de collaboration, axée sur la conception de modèles vision-langage pour la défense et la sécurité. ICEYE (Finlande) Ce fabricant de micro-satellites a constitué un joint-venture avec le groupe allemand Rheinmetall (armement et équipement automobile). Legora (Suède) Cette legaltech a annoncé vouloir doubler son effectif sur un an et établit des points de présence supplémentaires en Europe. MBDA (France) x Rheinmetall (Allemagne) x SPREAD (Allemagne) SPREAD contribue au jumeau numérique de défense de Rheinmetall et à l'automatisation de la validation chez MBDA (aéronautique, spatial et armement). Mercedes-Benz (Allemagne) Engagement à collaborer avec les start-up et les fournisseurs de modèles d'IA européens. Multiverse (Angleterre) Cette edtech s'est engagé à ouvrir un bureau en Allemagne. Elle compte former, sur place, 100 000 personnes à l'IA en 5 ans. Nextcloud (Allemagne) L'entreprise s'engage à investir \" plus de 250 M€ \" dans son programme Sovereignty 2030 pour \" faire de l'IA ouverte souveraine une réalité en Europe \". Otto Group (Allemagne) Engagement à investi 350 M€ sur 3 ans pour faire évoluer le e-commerce, notamment à renfort de GenAI. SAP (Allemagne) x Mistral AI (France) Extension du partenariat à travers lequel SAP fournit les modèles Mistral via sa Business Technology Platform (voir notre article à ce sujet). Siemens Engagement à développer un \" écosystème européen de données industrielles \" qui alimentera des modèles de fondation industriels. SPRIND Lancement, en juin 2026, d'un défi \" Next Frontier AI \" doté de 125 M€. Objectif : faire émerger des labos européens exporant des \" approches alternatives \" de l'IA, en particulier la frugalité en données et en énergie.\n\nÀ cette même occasion, EU AI Champions a déclaré que 20 Md€ avaient été engagés sur les 150 Md€ alloués.\n\nSon document référent reste le rapport \" Un agenda ambitieux pour l'IA européenne \" que General Catalyst avait publié au Sommet de l'IA 2025\n\nIllustration générée par IA"
  },
  {
    "source": "Developpez.com",
    "company": "Mistral AI",
    "title": "La société française Mistral AI lance Voxtral Transcribe 2~? une nouvelle famille de modèles d'IA de reconnaissance vocale~? qui transcrit \" à la vitesse du son \"",
    "date": "2026-02-06T02:44:12Z",
    "url": "https://intelligence-artificielle.developpez.com/actu/379978/La-societe-francaise-Mistral-AI-lance-Voxtral-Transcribe-2-une-nouvelle-famille-de-modeles-d-IA-de-reconnaissance-vocale-qui-transcrit-a-la-vitesse-du-son/",
    "content": "La société française Mistral AI a dévoilé Voxtral Transcribe 2, une famille de modèles d'intelligence artificielle (IA) de reconnaissance vocale permettant une transcription rapide et de haute qualité \" à la vitesse du son \". Cette gamme comprend Voxtral Mini Transcribe V2, destiné à la transcription par lots, et Voxtral Realtime, conçu pour les applications en temps réel. Ce dernier se distingue par une latence configurable aussi faible que 200 ms, ce qui le rend idéal pour les assistants vocaux et le sous-titrage en direct. L'approche open source et les prix compétitifs de Mistral rendent Voxtral Transcribe 2 particulièrement attrayant pour les secteurs sensibles à la confidentialité, tout en offrant des performances robustes pour la transcription multilingue.\n\nMistral AI SAS est une entreprise française spécialisée dans l'IA dont le siège social est situé à Paris, avec des bureaux au Royaume-Uni, ainsi qu'à Palo Alto et à Singapour. Elle a été fondée en 2023 par Arthur Mensch, Guillaume Lample et Timothée Lacroix. L'entreprise développe de grands modèles de langage (LLM) à poids ouvert, ainsi que des modèles d'IA à la fois open source et propriétaires. Considérée comme l'un des leaders européens de l'IA, la société a été évaluée à plus de 14 milliards de dollars en 2025.\n\nCette annonce prolonge une initiative amorcée en juillet dernier, lorsque Mistral AI a dévoilé Voxtral, sa première famille de modèles d'IA open source dédiée à la compréhension de la parole. L'éditeur avait alors présenté un modèle de 24 milliards de paramètres destiné aux environnements de production, ainsi qu'une version allégée de 3 milliards de paramètres pour les déploiements locaux. Les modèles Voxtral se distinguent par des capacités avancées, incluant la gestion du multilingue, l'analyse de longs contextes, le résumé natif et l'interaction fonctionnelle à partir de la voix.\n\nPour poursuivre sur cette lancée, Mistral AI a lancé Voxtral Transcribe 2, une nouvelle famille de modèles de reconnaissance vocale qui transcrit \" à la vitesse du son \". Voxtral Transcribe 2 se compose de deux modèles de conversion de la parole en texte offrant une qualité de transcription, une diarisation et une latence ultra-faible. La famille comprend Voxtral Mini Transcribe V2 pour la transcription par lots et Voxtral Realtime pour les applications en temps réel.\n\nSelon Mistral, Voxtral Realtime utilise \" une architecture de streaming innovante qui transcrit l'audio à mesure qu'il arrive \", plutôt que d'adapter des modèles hors ligne. Cela permet une latence \" configurable à moins de 200 ms \", un seuil essentiel pour les assistants vocaux, le sous-titrage en direct et l'IA conversationnelle.\n\nLes utilisateurs peuvent tester Voxtral Transcribe 2 directement dans Mistral Studio. Ils peuvent télécharger jusqu'à 10 fichiers audio, activer ou désactiver la diarisation, choisir la granularité des horodatages et ajouter des termes contextuels pour le vocabulaire spécifique à un domaine. La famille de modèles prend en charge les formats .mp3, .wav, .m4a, .flac, .ogg jusqu'à 1 Go chacun.\n\nStratégie produit\n\nLa société a révélé que Voxtral Realtime est commercialisé en open source sous licence Apache 2.0, ce qui permet aux organisations de le déployer sur leur propre infrastructure, y compris sur des appareils périphériques. Cela a des implications importantes pour les secteurs sensibles à la confidentialité, tels que la santé, la finance et le gouvernement, où l'envoi de données audio vers des clouds tiers est souvent restreint.\n\nCette importance accordée à l'open source et à l'intégration n'est pas fortuite. Alors que les entreprises s'inquiètent de plus en plus de la dépendance vis-à-vis des fournisseurs et de la souveraineté des données, Mistral se positionne comme une alternative aux plateformes d'IA fermées basées aux États-Unis.\n\nLa tarification est abordée avec franchise. Elle est basée sur l'usage et commence à environ 5 000  par mois. Cela indique que Mistral cible les moyennes et grandes entreprises plutôt que les développeurs individuels, tout en continuant à présenter ses services comme compétitifs en termes de coût.\n\nTransformer les applications vocales\n\nSelon Mistral, la famille de modèles Voxtral optimise les flux de travail vocaux dans diverses applications et industries.\n\nIntelligence de réunion : les utilisateurs peuvent transcrivez des enregistrements multilingues grâce à la diarisation des locuteurs, qui attribue clairement qui a dit quoi et quand. Le prix proposé par Voxtral permet d'annoter de grands volumes de contenu de réunion avec une rentabilité inégalée dans le secteur. Agents vocaux et assistants virtuels : les développeurs peuvent créer une IA conversationnelle avec une latence de transcription inférieure à 200 ms. Il leur suffira de connecter Voxtral Realtime à leur pipeline LLM et TTS pour obtenir des interfaces vocales réactives et naturelles. Automatisation des centres d'appels : les entreprises peuvent transcrire les appels en temps réel, permettant ainsi aux systèmes d'IA d'analyser les sentiments, de suggérer des réponses et de remplir les champs CRM pendant que les conversations sont encore en cours. La diarisation des locuteurs garantit une attribution claire entre les agents et les clients. Médias et diffusion : Les sociétés de médias et de diffusion peuvent générer des sous-titres multilingues en direct avec une latence minimale. Le biais contextuel traite les noms propres et la terminologie technique qui posent problème aux services de transcription génériques. Conformité et documentation : Voxtral Transcribe 2 permet de surveiller et de transcrir les interactions à des fins de conformité réglementaire, grâce à la journalisation qui fournit une attribution claire des locuteurs et des horodatages permettant des pistes d'audit précises.\n\nPerformances\n\nVoxtral Realtime\n\nMistral affirme que Voxtral Realtime est spécialement conçu pour les applications où la latence est importante. Contrairement aux approches qui adaptent les modèles hors ligne en traitant l'audio par blocs, Realtime utilise une \" architecture de streaming innovante qui transcrit l'audio au fur et à mesure qu'il arrive. \" Le modèle fournit des transcriptions avec un délai configurable inférieur à 200 ms, ouvrant la voie à une nouvelle catégorie d'applications axées sur la voix.\n\nAvec un délai de 2,4 secondes, idéal pour le sous-titrage, Voxtral Realtime est comparable à Voxtral Mini Transcribe V2. Avec un délai de 480 ms, il reste dans une fourchette d'erreur de 1 à 2 %, ce qui permet aux agents vocaux d'atteindre une précision quasi hors ligne.\n\nLe modèle est multilingue de manière native et offre d'excellentes performances de transcription dans 13 langues, dont l'anglais, le chinois, l'hindi, l'espagnol, l'arabe, le français, le portugais, le russe, l'allemand, le japonais, le coréen, l'italien et le néerlandais. Avec une empreinte de 4 Go de paramètres, il fonctionne efficacement sur les appareils périphériques, garantissant la confidentialité et la sécurité des déploiements sensibles.\n\nVoxtral Realtime est disponible via API au prix de 0,006 $ par minute et en poids ouverts sur Hugging Face.\n\nVoxtral Mini Transcribe V2\n\nMistral affirme que Voxtral Mini Transcribe V2 atteint un taux d'erreur d'environ 4 % sur FLEURS au coût de 0,003 $/min, ce qu'il décrit comme \" le meilleur rapport qualité-prix de toutes les API de transcription \".\n\nLa société affirme que ce modèle surpasse les offres de GPT-4o mini Transcribe, Gemini 2.5 Flash, Assembly Universal et Deepgram Nova en termes de précision, tout en traitant l'audio \" environ trois fois plus rapidement que Scribe v2 d'ElevenLabs \" pour \" un cinquième du coût \".\n\nSi elles sont validées de manière indépendante, ces affirmations pourraient bouleverser un marché où les prix de la transcription vocale restent relativement élevés, en particulier pour la transcription multilingue et diarisée. La baisse des coûts rend économiquement viable la transcription de grands volumes de réunions, d'appels et d'archives médiatiques qui étaient auparavant trop coûteux à traiter.\n\nVoxtral Mini Transcribe V2 est disponible via API au prix de 0,003 $ par minute. Il peut être testé dans le nouvel espace audio de Mistral Studio ou dans Le Chat.\n\nDes idées novatrices\n\nAu-delà de la simple transcription, Voxtral Mini Transcribe V2 introduit des fonctionnalités spécialement conçues pour une utilisation en entreprise. Celles-ci comprennent la diarisation des locuteurs avec des horodatages précis, le biais... La fin de cet article est réservée aux abonnés. Soutenez le Club Developpez.com en prenant un abonnement pour que nous puissions continuer à vous proposer des publications."
  },
  {
    "source": "ZDNet",
    "company": "Mistral AI",
    "title": "Le CNRS déploie Emmy, assistant IA pour la recherche conçu par Mistral AI - ZDNET",
    "date": "2026-02-04T13:16:08Z",
    "url": "https://www.zdnet.fr/actualites/le-cnrs-deploie-emmy-assistant-ia-pour-la-recherche-concu-par-mistral-ai-489440.htm",
    "content": "Face aux risques du Shadow AI, le CNRS franchit une étape majeure dans sa stratégie numérique. Le centre de recherche déploie \"Emmy\", un assistant fondé sur la technologie française de Mistral AI, offrant ainsi un environnement de travail sécurisé et souverain à ses 35 000 collaborateurs.\n\nCroissance, Capex et souveraineté... Mistral AI accélère son déploiement institutionnel. La startup française vient de conclure un accord stratégique avec le CNRS pour fournir un agent conversationnel d'IA générative à ses 35 000 agents.\n\nToute l'actualité de la tech pour les pros chaque jour dans notre newsletter En savoir plus sur l'utilisation des données personnelles\n\nLe projet ne date pas d'hier. Ce chantier, engagé depuis plus d'un an, a franchi une étape clé le 17 décembre dernier avec l'ouverture officielle de l'outil. Selon François Pouget, directeur général délégué aux ressources du CNRS, l'assistant nommé Emmy est désormais accessible à l'ensemble des effectifs.\n\nUn PoC concluant auprès de 600 agents\n\nCe déploiement généralisé fait suite à un contrat signé en octobre 2025 avec Mistral AI. Avant de franchir le pas, l'organisme de recherche avait développé sa propre solution interne en s'appuyant sur les modèles open source de la pépite tricolore.\n\nLe CNRS a structuré sa démarche en plusieurs phases :\n\nLancement d'une phase de tests fin 2024. Expérimentation auprès de 600 salariés volontaires. Identification des besoins métiers et des contraintes techniques de montée en charge.\n\nL'assistant est associé à un hébergement des données \"au sein de datacenters situés sur le sol européen notamment soumis au RGPD et à l'IA Act.\"\n\nLutter contre le Shadow AI avec Emmy\n\nL'objectif est clair : offrir une alternative crédible aux outils grand public non sécurisés. En proposant Emmy, le CNRS limite les risques liés au Shadow AI et sanctuarise les données de la recherche française.\n\nLes capacités de l'outil couvrent un large spectre d'usages quotidiens :\n\nTraduction et synthèse de documents complexes. Aide à la reformulation et support à la réflexion. Recherche web et reconnaissance de caractères (OCR).\n\nPoint crucial pour la confidentialité : si Emmy mémorise les échanges pour la pertinence des réponses, Mistral AI n'utilise pas les prompts ou les documents soumis pour entraîner ses modèles commerciaux.\n\n7 000 comptes déjà activés\n\nLe déploiement d'Emmy s'accompagne d'une politique de gouvernance stricte. Le CNRS interdit désormais l'usage d'outils tiers similaires pour les missions professionnelles. Une charte éthique encadrant l'usage de l'IA est également en cours de finalisation.\n\nPour soutenir l'adoption, l'organisme propose un catalogue de formation :\n\nModules de sensibilisation de 1h à 6h. Ateliers dédiés à l'ingénierie de prompt (prompt engineering).\n\nLe succès semble au rendez-vous. À ce jour, le CNRS compte plus de 7 000 comptes activés. \" Sa mise en place est progressive, et nous constatons que le nombre d'utilisateurs augmente tous les jours \", confirme François Pouget."
  },
  {
    "source": "ZDNet",
    "company": "Mistral AI",
    "title": "Nouveau Monde Éditions attaque Mistral AI pour piratage massif - ZDNET",
    "date": "2026-02-04T09:49:28Z",
    "url": "https://www.zdnet.fr/actualites/nouveau-monde-editions-attaque-mistral-ai-pour-piratage-massif-489396.htm",
    "content": "La pépite française rejoint le banc des accusés après une plainte de Nouveau Monde Éditions pour l'utilisation non autorisée de son catalogue. Entre procès aux États-Unis et mises en demeure de doubleurs en France, l'industrie de l'IA cherche urgemment un terrain d'entente avec les ayants droit.\n\nPour lancer une entreprise spécialisée dans l'intelligence artificielle, vous aurez évidemment besoin de profils techniques, ingénieurs et analystes de données. Mais vous auriez tout intérêt à prévoir un département juridique tout aussi compétent. Mistral AI n'y échappe pas. La pépite française d'intelligence artificielle est visée par Nouveau Monde Éditions rapporte Le Monde. La maison d'édition accuse Mistral AI d'avoir piraté un cinquième de son catalogue, afin d'entraîner sans son autorisation ni compensation financière ses modèles de langage.\n\nToute l'actualité de la tech pour les pros chaque jour dans notre newsletter En savoir plus sur l'utilisation des données personnelles\n\nCette procédure en justice fait suite aux révélations de Mediapart en fin d'année 2025. Le site d'information avait en effet révélé qu'un des cofondateurs de Mistral AI avait été impliqué dans l'utilisation de la base de données de livres piratés Libgen lors de son passage au sein de la société américaine Meta en 2022.\n\nRecours en série contre les géants de l'IA\n\nMistral AI conteste de son coté les allégations de Nouveau Monde Éditions. La firme assure que ses modèles sont entraînés sur des données publiques, des jeux de données sous licence ainsi que des données internes.\n\nVoici les faits marquants de cette offensive judiciaire :\n\n1,5 milliard de dollars : c'est le montant versé par Anthropic en septembre 2025 suite à un accord à l'amiable avec des éditeurs, dont Nouveau Monde Editions. 5 ouvrages identifiés ont suffi à Nouveau Monde Éditions pour obtenir des indemnités outre-Atlantique.\n\nDes procédures similaires visent également d'autres entreprises américaines spécialisées dans l'intelligence artificielle. En France, plusieurs ayant droit ont ainsi intenté une procédure devant le tribunal judiciaire en mars 2025. Les plaignants accusent Meta d'avoir utilisé des œuvres protégées par le droit d'auteur pour entraîner leurs modèles.\n\nLes doubleurs donnent de la voix\n\nLa question des données d'entraînement des modèles ne se limite d'ailleurs pas aux livres. En France, un collectif de huit doubleurs a ainsi annoncé auprès de l'AFP avoir envoyé plusieurs mises en demeure à des sociétés américaines qui proposent des outils de synthèse vocale reproduisant leur voix.\n\nLes deux sociétés concernées, VoiceDub et Fish Audio, proposent un service permettant de faire lire à voix haute un texte avec la voix d'un personnage connu. Ils proposent également à leurs utilisateurs de créer une nouvelle voix synthétique à partir d'extraits audio fournis par l'utilisateur. Le tout sans demander l'autorisation ou verser de contrepartie. Le collectif exige la fin de l'utilisation de leurs voix et une compensation de 20 000 euros, sans quoi ils débuteront une action en justice.\n\nRégulation et accords de licence : pas de solution magique\n\nPour limiter les risques, les solutions varient. Dans certains cas, les entreprises assignées en justice acceptent un accord à l'amiable visant à faire cesser les poursuites contre des dédommagements financiers. Certaines sociétés, comme OpenAI, prennent les devant en négociant des accords de licence avec les producteurs de contenus ou les ayants droits.\n\nLa stratégie des acteurs du secteur s'articule autour de trois axes :\n\nLe lobbying aux États-Unis pour obtenir des exemptions de propriété intellectuelle. La négociation directe avec les ayants droit (ex: accord Disney/OpenAI). Le recours à des entreprises spécialisées dans la création de données d'entraînement \"propres\".\n\nAux États Unis, les principales entreprises du secteur tente de faire évoluer les projets de loi afin de prévoir des exceptions leur permettant d'entrainer des modèles sur des œuvres soumises au droit d'auteur. L'union européenne tente de son côté d'inciter les entreprises à prendre des engagements sur la transparence des données d'entrainement."
  },
  {
    "source": "MIT Technology Review",
    "company": "Mistral AI",
    "title": "The crucial first step for designing a successful enterprise AI system",
    "date": "2026-02-02T14:58:05Z",
    "url": "https://www.technologyreview.com/2026/02/02/1131822/the-crucial-first-step-for-designing-a-successful-enterprise-ai-system/",
    "content": "How to identify the first iconic use case for an enterprise AI transformation.\n\nMany organizations rushed into generative AI, only to see pilots fail to deliver value. Now, companies want measurable outcomes -- but how do you design for success?\n\nAt Mistral AI, we partner with global industry leaders to co-design tailored AI solutions that solve their most difficult problems. Whether it's increasing CX productivity with Cisco, building a more intelligent car with Stellantis, or accelerating product innovation with ASML, we start with open frontier models and customize AI systems to deliver impact for each company's unique challenges and goals.\n\nOur methodology starts by identifying an iconic use case, the foundation for AI transformation that sets the blueprint for future AI solutions. Choosing the right use case can mean the difference between true transformation and endless tinkering and testing.\n\nMistral AI has four criteria that we look for in a use case: strategic, urgent, impactful, and feasible.\n\nFirst, the use case must be strategically valuable, addressing a core business process or a transformative new capability. It needs to be more than an optimization; it needs to be a gamechanger. The use case needs to be strategic enough to excite an organization's C-suite and board of directors.\n\nFor example, use cases like an internal-facing HR chatbot are nice to have, but they are easy to solve and are not enabling any new innovation or opportunities. On the other end of the spectrum, imagine an externally facing banking assistant that can not only answer questions, but also help take actions like blocking a card, placing trades, and suggesting upsell/cross-sell opportunities. This is how a customer-support chatbot is turned into a strategic revenue-generating asset.\n\nSecond, the best use case to move forward with should be highly urgent and solve a business-critical problem that people care about right now. This project will take time out of people's days -- it needs to be important enough to justify that time investment. And it needs to help business users solve immediate pain points.\n\nThird, the use case should be pragmatic and impactful. From day one, our shared goal with our customers is to deploy into a real-world production environment to enable testing the solution with real users and gather feedback. Many AI prototypes end up in the graveyard of fancy demos that are not good enough to put in front of customers, and without any scaffolding to evaluate and improve. We work with customers to ensure prototypes are stable enough to release, and that they have the necessary support and governance frameworks.\n\nFinally, the best use case is feasible. There may be several urgent projects, but choosing one that can deliver a quick return on investment helps to maintain the momentum needed to continue and scale.\n\nThis means looking for a project that can be in production within three months -- and a prototype can be live within a few weeks. It's important to get a prototype in front of end users as fast as possible to get feedback to make sure the project is on track, and pivot as needed.\n\nEnterprises are complex, and the path forward is not usually obvious. To weed through all the possibilities and uncover the right first use case, Mistral AI will run workshops with our customers, hand-in-hand with subject-matter experts and end users.\n\nRepresentatives from different functions will demo their processes and discuss business cases that could be candidates for a first use case -- and together we agree on a winner. Here are some examples of types of projects that don't qualify.\n\nMoonshots: Ambitious bets that excite leadership but lack a path to quick ROI. While these projects can be strategic and urgent, they rarely meet the feasibility and impact requirements.\n\nFuture investments: Long-term plays that can wait. While these projects can be strategic and feasible, they rarely meet the urgency and impact requirements.\n\nTactical fixes: Firefighting projects that solve immediate pain but don't move the needle. While these cases can be urgent and feasible, they rarely meet the strategy and impact requirements.\n\nQuick wins: Useful for building momentum, but not transformative. While they can be impactful and feasible, they rarely meet the strategy and urgency requirements.\n\nBlue sky ideas: These projects are gamechangers, but they need maturity to be viable. While they can be strategic and impactful, they rarely meet the urgency and feasibility requirements.\n\nHero projects: These are high-pressure initiatives that lack executive sponsorship or realistic timelines. While they can be urgent and impactful, they rarely meet the strategy and feasibility requirements.\n\nOnce a clearly defined and strategic use case ready for development is identified, it's time to move into the validation phase. This means doing an initial data exploration and data mapping, identifying a pilot infrastructure, and choosing a target deployment environment.\n\nThis step also involves agreeing on a draft pilot scope, identifying who will participate in the proof of concept, and setting up a governance process.\n\nOnce this is complete, it's time to move into the building phase. Companies that partner with Mistral work with our in-house applied AI scientists who build our frontier models. We work together to design, build, and deploy the first solution.\n\nDuring this phase, we focus on co-creation, so we can transfer knowledge and skills to the organizations we're partnering with. That way, they can be self-sufficient far into the future. The output of this phase is a deployed AI solution with empowered teams capable of independent operation and innovation.\n\nAfter the first win, it's imperative to use the momentum and learnings from the iconic use case to identify more high-value AI solutions to roll out. Success is when we have a scalable AI transformation blueprint with multiple high-value solutions across the organization.\n\nBut none of this could happen without successfully identifying that first iconic use case. This first step is not just about selecting a project -- it's about setting the foundation for your entire AI transformation.\n\nIt's the difference between scattered experiments and a strategic, scalable journey toward impact. At Mistral AI, we've seen how this approach unlocks measurable value, aligns stakeholders, and builds momentum for what comes next.\n\nThe path to AI success starts with a single, well-chosen use case: one that is bold enough to inspire, urgent enough to demand action, and pragmatic enough to deliver."
  },
  {
    "source": "Le Jour Guinée, actualités des banques en ligne",
    "company": "Mistral AI",
    "title": "Mistral AI signe avec l'armée française : IA souveraine pour la défense",
    "date": "2026-02-02T06:13:04Z",
    "url": "https://www.lejourguinee.com/de-la-transcription-aux-obus-caesar-mistral-ai-entre-dans-larmee-francaise/",
    "content": "Le 8 janvier 2026, le ministère des Armées a officialisé un accord-cadre avec Mistral AI. La start-up parisienne devient le fournisseur privilégié d'intelligence artificielle générative pour l'ensemble des forces françaises. Un contrat qui va de la bureautique au champ de bataille.\n\nL'annonce n'est pas une surprise. Sébastien Lecornu, alors ministre des Armées, avait évoqué ce rapprochement dès janvier 2025 sur France Culture. Une convention de coopération avait suivi en mars. Mais l'accord-cadre notifié début janvier change d'échelle : il ouvre la voie à un déploiement industriel, transversal, de l'IA générative dans toute la défense française.\n\nLe périmètre est large. L'accord couvre l'ensemble des armées -- Terre, Air, Marine -- ainsi que les directions et services du ministère. Mais il s'étend aussi aux organismes sous tutelle : le CEA (Commissariat à l'énergie atomique), l'ONERA (recherche aérospatiale) et le SHOM (hydrographie et océanographie de la Marine).\n\nLe pilotage revient à l'AMIAD, l'Agence ministérielle pour l'intelligence artificielle de défense, créée à l'été 2024 pour mettre fin à l'éparpillement des projets IA au sein des différents corps. Son directeur, Bertrand Rondepierre, parle d'une \" étape majeure pour renforcer les capacités d'IA générative au sein du ministère \".\n\nL'accord ne se limite pas aux grandes déclarations sur la souveraineté. Le ministère a identifié des cas d'usage précis, certains déjà en test, d'autres à venir.\n\nCôté administratif : transcription sécurisée de réunions et d'écoutes, traduction de rapports multilingues, aide à l'analyse et à la synthèse documentaire. Le chatbot maison \" GenIAl Intradef \", déjà déployé en interne, sera renforcé par les modèles Mistral. L'AMIAD envisage une suite bureautique complète intégrant l'IA, potentiellement réplicable dans d'autres ministères.\n\nCôté opérationnel : les ambitions montent en intensité. Le ministère évoque la guerre acoustique -- distinguer un sous-marin d'un cargo, un drone d'un bruit parasite -- et l'optimisation du guidage des obus tirés par les canons Caesar. Si l'IA peut améliorer un capteur ou accélérer un processus, l'AMIAD veut l'explorer.\n\nLe point central de l'accord : les modèles Mistral seront déployés exclusivement sur des infrastructures contrôlées par le ministère. Pas de cloud américain, pas de données qui traversent l'Atlantique. Les modèles seront entraînés et ajustés à partir de corpus propres à la défense, avec des environnements isolés et une gestion stricte des accès.\n\nCette architecture répond à un impératif politique autant que technique. Dans un secteur dominé par OpenAI, Google et les géants chinois, la France fait le choix d'un acteur national pour ses briques les plus sensibles. Le ministère assume : \" Il est crucial que la France conserve son avance technologique. \"\n\nMistral AI, de son côté, y voit la validation de son modèle. L'entreprise fondée par Arthur Mensch en 2023 a bâti sa réputation sur des modèles open source performants et une capacité à déployer \" on-prem \" -- sur site, sans dépendance cloud. Exactement ce que demande la défense.\n\nLe ministère n'a pas communiqué la valeur de l'accord-cadre. Mais un point de comparaison existe : en juin 2025, au salon du Bourget, Mistral AI a signé un protocole avec l'armée luxembourgeoise pour 44 millions d'euros sur cinq ans. L'accord français, couvrant un périmètre incomparablement plus large, devrait se situer dans une tout autre dimension.\n\nCe contrat s'inscrit aussi dans un mouvement européen plus large. En novembre 2025, la France et l'Allemagne ont annoncé une collaboration avec SAP et Mistral pour équiper leurs administrations d'une infrastructure IA souveraine. Mistral travaille également avec Helsing, spécialiste allemand de l'IA de défense, sur des modèles \" Vision-Language-Action \" pour des applications militaires.\n\nL'enthousiasme officiel masque des défis réels. Plus l'IA devient centrale dans les opérations, plus elle devient une cible. Les attaques par \" prompt injection \" -- manipuler un modèle via des instructions cachées -- sont documentées. L'exfiltration de données d'entraînement, la contamination de corpus, les erreurs de raisonnement \" plausibles \" mais fausses : autant de risques que le ministère devra gérer.\n\nLa défense n'achète pas un logiciel figé. Elle achète un système vivant, qui évolue, qu'il faut auditer, tester, durcir, mettre à jour en continu. L'AMIAD devra construire une expertise interne pour ne pas devenir dépendante de son prestataire -- même français.\n\nPour Mistral AI, l'accord est une consécration. La start-up de deux ans devient fournisseur stratégique de l'État sur ses fonctions régaliennes les plus sensibles. Une légitimité que ni les levées de fonds ni les benchmarks ne peuvent acheter.\n\nPour la défense française, c'est un pari. Celui de construire une autonomie technologique réelle sur l'IA, en partant d'un acteur national plutôt qu'en adaptant des solutions américaines. Un pari politique, industriel, et désormais opérationnel."
  },
  {
    "source": "Mobile World Live",
    "company": "Mistral AI",
    "title": "Pour le patron de Mistral, la Chine est loin d'être à la traîne sur l'IA",
    "date": "2026-01-23T14:19:51Z",
    "url": "https://www.mobileworldlive.com/french/pour-le-patron-de-mistral-la-chine-est-loin-detre-a-la-traine-sur-lia/",
    "content": "Le PDG de Mistral AI, Arthur Mensch, rejette les affirmations de certains dirigeants occidentaux selon lesquelles la Chine serait en retard dans le domaine de l'IA, soulignant au contraire les progrès croissants du pays dans les systèmes open source.\n\nInterviewé par Bloomberg TV au Forum économique mondial de Davos, Arthur Mensch a qualifié de \" conte de fées \" l'idée que la Chine serait à la traîne derrière l'Occident, suggérant que de nombreux PDG américains seraient en réalité \" très inquiets \" des avancées de la Chine dans l'open source.\n\nLe patron de Mistral AI répondait notamment aux déclarations faites à Davos par Demis Hassabis, PDG de Google DeepMind, et Dario Amodei, son homologue chez Anthropic. Selon Bloomberg, Demis Hassabis aurait affirmé que la Chine accusait un retard d'environ six mois par rapport aux pays occidentaux dans le développement des modèles de pointe. Dario Amodei a lui salué la politique de Washington restreignant la vente de puces Nvidia, dont il estime qu'elle freine les progrès chinois. Le PDG d'Anthropic a même comparé l'exportation de processeurs haut de gamme à la Chine à \" la vente d'armes nucléaires à la Corée du Nord \".\n\nContredisant ses rivaux américains, Arthur Mensch a, lui, insisté sur le fait que la Chine \" se débrouille très bien sans les puces Nvidia \" et affirmé que Mistral AI est \" à l'avant-garde de la concurrence \" face à la RPC dans le domaine de l'open source. Il a souligné l'importance pour Mistral de continuer à développer des technologies open source, précisant que l'Europe doit pouvoir former ses propres modèles - un enjeu clé pour sa présence dans ce domaine.\n\n\" Nous ne pensons pas devoir nous appuyer sur des modèles open source chinois pour des applications critiques, a estimé Arthur Mensch. Nous devons être capables de créer nos propres modèles et de les vendre aux entreprises. \"\n\nLors de l'interview, Arthur Mensch a par ailleurs révélé que Mistral AI visait un milliard de dollars de CA en 2026 et prévoyait d'investir le même montant en dépenses d'investissement (capex). En plus de cibler les clients entreprises, Mistral reste ouvert à d'éventuelles acquisitions."
  },
  {
    "source": "Siècle Digital",
    "company": "Mistral AI",
    "title": "Mistral AI vise le milliard de dollars de revenus dès 2026, une ambition financière sans précédent en Europe - Siècle Digital",
    "date": "2026-01-23T11:38:00Z",
    "url": "https://siecledigital.fr/2026/01/22/mistral-ai-vise-le-milliard-de-dollars-de-revenus-des-2026/",
    "content": "En marge du forum économique mondial de Davos, le co-fondateur de Mistral AI, Arthur Mensch, a levé le voile sur des perspectives financières qui placent désormais Mistral dans une autre dimension, au moment où la souveraineté numérique européenne s'invite au coeur des débats...\n\nUn objectif de chiffre d'affaires inédit pour une start-up européenne\n\nMistral AI estime qu'elle dépassera le milliard de dollars de revenus d'ici la fin de l'année 2026. Une projection annoncée par Arthur Mensch lors d'un entretien accordé à Bloomberg, qui marque un tournant pour l'écosystème européen de l'IA.\n\nSi l'entreprise ne publie pas de résultats financiers détaillés, plusieurs estimations situent son chiffre d'affaires autour de 100 millions d'euros pour 2025. Mistral revendiquait également, dès septembre dernier, un revenu annuel de 300 millions d'euros, qui lui a permis d'être la start-up la plus valorisée en Europe.\n\nUne stratégie de monétisation plus diversifiée que la moyenne\n\nContrairement à certains de ses concurrents, souvent focalisés sur la vente d'API ou des licences cloud, Mistral AI multiplie les sources de revenus. La start-up propose un accès à ses modèles via des API facturées à l'usage, mais aussi des licences permettant un déploiement sur des infrastructures privées, un point clé pour les entreprises sensibles aux questions de souveraineté et de confidentialité.\n\nÀ cela s'ajoute Le Chat Enterprise, une offre dédiée aux grands comptes, intégrée à des outils professionnels comme SharePoint ou Google Drive, avec des contrats sur mesure.\n\nMistral cible également le grand public grâce à des abonnements premium de son assistant Le Chat, élargissant ainsi son audience au-delà des seuls clients professionnels.\n\nDes acquisitions et des investissements au coeur de la feuille de route...\n\nPour soutenir cette trajectoire, Mistral AI prévoit d'engager près d'un milliard de dollars en dépenses d'investissement sur l'année. Arthur Mensch a confirmé que des opportunités d'acquisitions étaient à l'étude, sans préciser les secteurs ni les zones géographiques concernées.\n\nPour cela, la start-up dispose de moyens conséquents depuis sa levée de fonds de 1,7 milliard d'euros réalisée en septembre dernier, marquée par l'entrée au capital d'ASML.\n\nLa souveraineté numérique comme toile de fond&nbsp?\n\nCette montée en puissance s'inscrit dans un contexte géopolitique tendu, où la dépendance technologique de l'Europe vis-à-vis des États-Unis et de la Chine suscite de vives inquiétudes.\n\nÀ Davos, ces questions ont largement dominé les échanges entre les dirigeants politiques et économiques. Arthur Mensch en a d'ailleurs profité pour relativiser l'avance américaine, estimant que la Chine n'était pas en retrait et que ses technologies open source inquiétaient déjà certains PDG américains..."
  },
  {
    "source": "ZDNet",
    "company": "Mistral AI",
    "title": "Croissance, Capex et souveraineté : le plan de bataille de Mistral AI dévoilé à Davos",
    "date": "2026-01-23T10:41:20Z",
    "url": "https://www.zdnet.fr/actualites/croissance-capex-et-souverainete-le-plan-de-bataille-de-mistral-ai-devoile-a-davos-488732.htm",
    "content": "Mistral AI vise le milliard de revenus en 2026 et prévoit des acquisitions stratégiques. Une montée en puissance qui crédibilise l'alternative européenne.\n\nPour les DSI et décideurs européens cherchant à réduire leur dépendance aux solutions d'IA américaines, la pérennité financière de l'alternative locale est souvent le point de blocage. Une start-up peut-elle garantir un support sur le long terme ?\n\nToute l'actualité de la tech pour les pros chaque jour dans notre newsletter\n\nEn savoir plus sur l'utilisation des données personnelles\n\nC'est à cette inquiétude légitime que la pépite française Mistral AI a répondu depuis le Forum économique mondial de Davos. L'objectif est clair : démontrer, via des projections de croissance agressives, qu'elle est un partenaire industriel solide pour les entreprises.\n\nInterrogé par Bloomberg en marge de l'événement, le cofondateur Arthur Mensch a officialisé l'ambition du groupe de franchir un cap financier majeur d'ici fin 2026. Nous devrions dépasser un milliard d'ici la fin de l'année , a-t-il déclaré au sujet des revenus. Cette montée en puissance s'appuie sur une levée de fonds massive de 1,7 milliard d'euros réalisée en septembre dernier, notamment auprès du géant néerlandais ASML, portant la valorisation de la société à 11,7 milliards d'euros.\n\nB2B : Une offre taillée pour la conformité et l'échelle\n\nCette projection de chiffre d'affaires ne repose pas sur de simples effets d'annonce, mais valide une stratégie \"Business Value\" claire. Loin d'une approche uniquement \"grand public\", Mistral AI structure ses revenus autour des exigences des entreprises : API à l'usage pour les développeurs, mais surtout licences pour déploiement sur infrastructure privée (Cloud ou On-Premise). Ce modèle répond directement aux contraintes de conformité et de sécurité des données des grands groupes.\n\nFer de lance de cette stratégie, l'offre \"Le Chat Enterprise\" cible l'efficacité opérationnelle des collaborateurs grâce à des intégrations natives avec les standards du marché comme SharePoint ou Google Drive.\n\nSi la société reste discrète sur ses comptes audités, les estimations de marché placent son chiffre d'affaires 2025 autour de 100 millions d'euros, avec un revenu récurrent annuel (ARR) qui aurait atteint 300 millions d'euros dès septembre. Pour soutenir cette croissance exponentielle, l'entreprise investit massivement dans son outil de production. Arthur Mensch a précisé que Mistral provisionnait un milliard d'euros de dépenses d'investissement (Capex) cette année. Un montant indispensable pour acquérir la puissance de calcul nécessaire et garantir la performance des modèles.\n\nAcquisitions et autonomie stratégique\n\nAu-delà de la croissance organique, Mistral AI entend jouer un rôle de consolidateur dans l'écosystème technologique européen. Nous sommes en train de regarder quelques opportunités, a indiqué Arthur Mensch concernant d'éventuelles acquisitions. Une stratégie qui vise à enrichir son offre technologique, bien que les cibles restent confidentielles.\n\nCette démarche renforce le statut de \"champion européen\" de l'IA, à l'heure où les incertitudes géopolitiques et les relations avec l'administration Trump réactivent l'urgence d'une autonomie stratégique pour les entreprises du Vieux Continent.\n\nUne vision appuyée par les instances européennes à Davos. Henna Virkkunen, vice-présidente de la Commission chargée du Numérique, a rappelé l'enjeu de résilience pour les infrastructures IT : Il est extrêmement important de ne pas être dépendant d'un pays ou d'une entreprise pour des secteurs très critiques de notre économie ou notre société.\n\nSur l'échiquier mondial, Arthur Mensch refuse par ailleurs tout complexe d'infériorité, affirmant que la Chine n'est pas en retard sur l'Occident et que ses capacités open source constituent un véritable défi concurrentiel qui inquiète probablement les PDG américains."
  },
  {
    "source": "MoneyDJ理財網",
    "company": "Mistral AI",
    "title": "世界經濟論壇聚焦AI，法國與歐洲強化數位主權布局-MoneyDJ理財網",
    "date": "2026-01-23T08:05:53Z",
    "url": "https://www.moneydj.com/kmdj/news/newsviewer.aspx?a=a3d4659e-7474-4771-9366-a1f73042ef3c",
    "content": "世界經濟論壇聚焦 AI，法國與歐洲強化數位主權布局\n\n法國財經新聞台BFM Business本(1)月22日報導，面對美中地緣政治與科技角力加劇，法國與歐洲在瑞士Davos世界經濟論壇( World Economic Forum, WEF)上，均將人工智慧(AI)列為戰略核心議題之一。Mistral AI 共同創辦人兼執行長Arthur Mensch於論壇期間受訪時強調歐洲數位主權之重要性，並將其定義為能夠為歐洲客戶提供真正自主性的能力。\n\n為實現前開目標，Mistral AI 採取垂直整合策略，透過Mistral Compute建置並營運自有運算與雲端基礎設施，以降低對美國超大型科技與雲端服務供應商的依賴，例如 Amazon AWS、Microsoft Azure，以及在 AI 運算晶片領域占據主導地位的輝達(NVIDIA)。儘管 Mistral AI 在歐洲以外市場仍持續與上述業者合作，但在歐洲境內已逐步轉向採用自有基礎設施，藉此強化技術自主性與服務掌控能力。\n\n此一策略亦與歐洲整體政策走向相呼應，即降低對主導雲端與人工智慧市場之美國數位巨擘的過度依賴。M執行長展現高度信心，指出 Mistral AI 不僅在歐洲快速成長，亦正積極拓展美國市場，並證明其具備國際競爭力。\n\n此外，法國宣布成立歐洲人工智慧卓越中心(un centre européen dexcellence en intelligence artificielle (CAIE)，盼藉此吸引人才、強化研究能量，並支持具主權性的歐洲 AI 產業體系，與此同時，業界對AI投資之泡沫風險保持審慎態度，M執行長稱，關鍵在於拿捏投資與部署之節奏，渠認為未來幾年大幅投入資本是正確選擇，是否成功仍有待時間驗證。(資料來源：經濟部國際貿易署)"
  },
  {
    "source": "Univers Freebox",
    "company": "Mistral AI",
    "title": "Ericsson, partenaire d'Orange et Bouygues, innove avec de l'IA dans les réseaux et un routeur high-tech",
    "date": "2026-02-20T10:04:12Z",
    "url": "https://www.universfreebox.com/?p=591742",
    "content": "Entre automatisation des réseaux télécoms et connectivité avancée pour les flottes de véhicules, Ericsson place l'IA au centre de deux annonces majeures.\n\nÀ l'approche du Mobile World Congress, Ericsson, équipementier télécom bien connu et partenaire d'Orange et Bouygues Telecom (pour la 5G+) multiplie les annonces autour de l'intelligence artificielle. Le groupe suédois officialise d'une part un partenariat stratégique avec Mistral AI pour appliquer des modèles d'IA avancés aux réseaux télécoms, et dévoile d'autre part un nouveau routeur 5G embarqué intégrant IA en périphérie, double SIM active et géolocalisation au centimètre.\n\nLa collaboration entre Ericsson et Mistral AI repose sur des expertises complémentaires : les modèles de pointe et outils d'IA générative de la start-up française d'un côté, et les décennies d'expérience d'Ericsson dans les systèmes radio, cloud et réseau de l'autre. L'objectif affiché est d'améliorer significativement les performances, l'automatisation et la fiabilité des réseaux opérateurs.\n\nConcrètement, le partenariat vise à accélérer la livraison logicielle et la R&D, notamment autour de la 6G, à automatiser la traduction de code legacy et à déployer des agents d'IA sur mesure pour optimiser des workflows complexes au sein de la division Networks. Les deux entreprises ambitionnent d'aligner l'innovation en IA sur les exigences des réseaux de niveau opérateur, en matière de sécurité, de performance et de résilience.\n\nCette annonce s'inscrit aussi dans un contexte plus large : Mistral AI a récemment annoncé un investissement majeur en Suède pour développer une infrastructure dédiée à l'IA. Ericsson pourrait ainsi bénéficier à terme de capacités de calcul locales, conformes aux exigences européennes (RGPD, souveraineté des données), et réduire sa dépendance à des infrastructures extra-européennes.\n\nEricsson, de son côté, assume pleinement sa stratégie IA. Le groupe intègre déjà des capacités d'inférence en temps réel dans ses radios, antennes et logiciels 5G pour optimiser trafic, couverture et qualité de service. Il promeut également le concept d'\" Agentic AI \", des systèmes capables d'agir comme des agents autonomes, coordonnant des tâches complexes et interagissant en langage naturel avec les opérateurs. L'objectif final reste l'automatisation avancée et l'évolution vers des réseaux autonomes.\n\nEn parallèle, Ericsson a présenté le routeur 5G Cradlepoint R2400, un équipement destiné aux véhicules connectés (ambulances, bus, flottes professionnelles). Ce modèle combine pour la première fois trois technologies au sein d'un même boîtier : deux cartes SIM actives simultanément sur un seul modem, une géolocalisation au centimètre près et une capacité d'IA embarquée fonctionnant sans connexion cloud.\n\nGrâce à la gestion simultanée de deux SIM, le routeur peut surveiller en permanence deux opérateurs et basculer de l'un à l'autre jusqu'à dix fois plus vite que les solutions existantes, assurant une continuité de service pour les flux critiques (voix, vidéo, données). Il peut également agréger jusqu'à cinq connexions cellulaires et, si nécessaire, basculer vers des constellations de satellites en orbite basse comme Starlink.\n\nCôté positionnement, le R2400 combine les technologies RTK (Real-Time Kinematics) et dead-reckoning pour localiser un véhicule au centimètre près, contre un à trois mètres pour un GPS classique. Un niveau de précision utile pour la coordination d'interventions d'urgence ou d'opérations logistiques fines. Le routeur intègre également le Wi-Fi 7, avec des débits annoncés deux à quatre fois supérieurs aux générations précédentes, ainsi qu'une puissance de calcul locale 2,5 fois supérieure à celle du modèle antérieur. Cette capacité d'edge computing permet d'analyser des images ou de traiter des données directement à bord, sans dépendre d'une connexion cloud.\n\nLa plateforme de gestion NetCloud embarque enfin un assistant basé sur l'IA capable de surveiller le réseau et de détecter des anomalies de manière proactive. Conçu pour évoluer, le R2400 pourra être mis à niveau via un simple remplacement de modem lors des prochaines évolutions 5G. Sa commercialisation est prévue au second trimestre 2026."
  },
  {
    "source": "WebProNews",
    "company": "Cohere",
    "title": "Cohere's Revenue Surge Signals a Pivotal Moment for Enterprise AI  --  and a Blockbuster IPO May Be Next",
    "date": "2026-02-13T17:47:41Z",
    "url": "https://www.webpronews.com/coheres-revenue-surge-signals-a-pivotal-moment-for-enterprise-ai-and-a-blockbuster-ipo-may-be-next/",
    "content": "In the fiercely competitive world of enterprise artificial intelligence, a Canadian startup is quietly building a case that it belongs in the conversation with the industry's most heavily funded players. Cohere, the Toronto-based AI company co-founded by former Google researcher Aidan Gomrat, has surpassed its internal revenue targets, fueling momentum toward what could be one of the most closely watched initial public offerings in the technology sector this year.\n\nAccording to a report from CNBC, Cohere topped its revenue goals as enterprise demand for AI tools continues to accelerate. The company, which builds large language models tailored for business use cases rather than consumer-facing chatbots, has positioned itself as a differentiated alternative to OpenAI, Anthropic, and Google in the race to serve corporate clients at scale.\n\nCohere's ability to exceed its revenue targets is significant not merely as a financial milestone but as a strategic signal. In the current environment, where public and private market investors are scrutinizing AI companies with increasing rigor, demonstrating real commercial traction -- rather than hype-driven growth projections -- is the clearest path to a successful public debut. The company's revenue beat, as detailed by CNBC, suggests that Cohere's enterprise-first strategy is resonating with the Fortune 500 clients and government agencies that represent its core customer base.\n\nThe timing is notable. The IPO market for technology companies has been gradually thawing after a prolonged freeze that began in 2022. Several AI-adjacent firms have tested the waters, but a pure-play enterprise AI company going public would represent a bellwether moment for the sector. Cohere's leadership has reportedly been in discussions with investment banks about the mechanics and timing of an offering, though no formal S-1 filing has been made public as of this writing.\n\nWhat distinguishes Cohere from many of its well-known competitors is a deliberate strategic choice: the company has eschewed the consumer chatbot wars that have consumed billions in capital at OpenAI and others, instead focusing exclusively on selling AI infrastructure and models to enterprises. This means Cohere's products are designed to be deployed within a company's own cloud environment, on-premises, or in private cloud configurations -- a critical requirement for industries like financial services, healthcare, and defense where data sovereignty and security are non-negotiable.\n\nCohere's flagship products include its Command family of large language models, which are optimized for tasks like retrieval-augmented generation (RAG), summarization, and classification. The company also offers Embed, a model designed for semantic search, and Rerank, which improves the accuracy of search results. This modular approach allows enterprises to adopt Cohere's technology incrementally, reducing the friction that often accompanies large-scale AI deployments. According to industry analysts, this flexibility has been a key driver of Cohere's commercial success, particularly among organizations that are wary of vendor lock-in with hyperscaler AI offerings from Amazon Web Services, Microsoft Azure, or Google Cloud.\n\nCohere has raised substantial capital to fund its ambitions. The company completed a Series D round in 2024 that valued it at approximately $5.5 billion, with participation from investors including Nvidia, Oracle, Salesforce Ventures, and the Canada Pension Plan Investment Board. That round brought total funding to over $900 million, giving Cohere significant runway to invest in model development, go-to-market expansion, and the infrastructure buildout required to serve large enterprise clients.\n\nThe valuation trajectory has been steep. Just two years prior, Cohere was valued at roughly $2.2 billion. The more than doubling of its valuation in a compressed timeframe reflects both the broader enthusiasm for AI companies and, more specifically, investor confidence in Cohere's enterprise revenue model. Unlike consumer AI companies that must spend heavily on user acquisition and face uncertain monetization paths, Cohere's business-to-business approach generates higher-margin, more predictable revenue streams -- exactly the kind of financial profile that public market investors tend to reward.\n\nCohere's progress comes against a backdrop of intensifying competition. OpenAI, long associated primarily with ChatGPT and consumer products, has been aggressively expanding its enterprise offerings, launching ChatGPT Enterprise and building out an API business that serves thousands of corporate clients. Anthropic, backed by billions from Amazon, has similarly targeted enterprise use cases with its Claude model family. Meanwhile, open-source models from Meta's Llama family and Mistral AI have introduced a potent competitive dynamic, offering enterprises the ability to fine-tune and deploy powerful models without licensing fees.\n\nYet Cohere's leadership argues that the company's singular focus on enterprise deployment gives it structural advantages that generalist competitors cannot easily replicate. The company's models are designed from the ground up for deployment flexibility -- they can run on any major cloud provider, on-premises hardware, or even in air-gapped environments. This cloud-agnostic approach is a meaningful differentiator, particularly for multinational corporations operating across jurisdictions with varying data residency requirements. As reported by CNBC, this strategy has helped Cohere secure contracts with organizations that might otherwise default to the AI offerings bundled with their existing cloud providers.\n\nCohere's Canadian roots are more than a biographical footnote -- they are a strategic asset. Canada has emerged as one of the world's premier hubs for AI research, anchored by institutions like the Vector Institute in Toronto and Mila in Montreal, both of which have produced many of the field's most influential researchers. Cohere co-founder Aidan Gomrat and his colleagues have drawn heavily from this talent pool, building a research team that competes with those at far larger organizations.\n\nThe Canadian government has also been an active supporter of the domestic AI ecosystem, committing billions in funding through initiatives like the Pan-Canadian AI Strategy. For Cohere, this translates into a favorable operating environment that includes access to world-class researchers, government incentives, and a regulatory framework that, while still evolving, has generally been more supportive of AI innovation than the more fragmented approaches seen in the European Union or certain U.S. states. The company's prominence has made it something of a national champion, a status that carries both advantages -- in the form of political goodwill and public procurement opportunities -- and expectations.\n\nIf Cohere proceeds with a public offering in 2026, it would be among the first pure-play enterprise AI companies to list on a major exchange. The implications extend well beyond the company itself. A successful Cohere IPO would validate the thesis that enterprise AI companies can build durable, high-growth businesses outside the shadow of the tech giants. It would also provide a public market benchmark for valuing similar companies, a reference point that has been conspicuously absent as the sector has matured almost entirely within private markets.\n\nConversely, a stumble -- whether in the form of a down-round IPO, weak aftermarket performance, or a decision to delay -- would raise uncomfortable questions about whether the current wave of AI investment is producing companies capable of standing on their own in public markets. The stakes, in other words, are not Cohere's alone. They belong to an entire generation of AI startups and their investors, all of whom are watching closely to see whether the enterprise AI thesis can survive the scrutiny of public market disclosure and quarterly earnings cycles.\n\nFor all its momentum, Cohere faces meaningful execution risks. The pace of model development in AI is relentless, and maintaining a competitive product requires enormous ongoing investment in compute, data, and talent. The company must also navigate the challenge of scaling its sales organization to match the ambitions of its technology -- a transition that has tripped up many enterprise software companies before it. Customer concentration, pricing pressure from open-source alternatives, and the ever-present risk of a technology paradigm shift all loom as potential headwinds.\n\nYet the revenue beat reported by CNBC suggests that Cohere is, for now, executing at a level that justifies the optimism surrounding its IPO prospects. In an industry where grand promises often outpace commercial reality, Cohere's ability to translate enterprise demand into measurable financial performance sets it apart. Whether that performance can sustain the expectations of public market investors remains the defining question -- one that the coming months will begin to answer."
  },
  {
    "source": "Tekedia",
    "company": "Cohere",
    "title": "Cohere Hits $240m ARR in 2025, Outpacing Target and Signaling Resilience in Competitive Enterprise AI Market - Tekedia",
    "date": "2026-02-14T13:25:03Z",
    "url": "https://www.tekedia.com/cohere-hits-240m-arr-in-2025-outpacing-target-and-signaling-resilience-in-competitive-enterprise-ai-market/",
    "content": "Artificial intelligence startup Cohere has delivered strong momentum in the enterprise segment, reporting approximately $240 million in annual recurring revenue (ARR) for 2025 -- comfortably surpassing its internal $200 million target -- according to a February 2026 investor memo obtained by CNBC.\n\nThe company achieved quarter-over-quarter revenue growth of more than 50% throughout the year, demonstrating consistent execution in a highly competitive market where larger rivals are aggressively expanding their enterprise footprints.\n\n\"Our thesis is clearly resonating in the market,\" Cohere wrote in the memo. \"Our sales pipeline continues to grow as global organizations across regulated sectors choose Cohere as their trusted partner for secure AI adoption at scale.\"\n\nFounded in Toronto in 2019 by former Google Brain researchers Aidan Gomez, Ivan Zhang, and Nick Frosst, Cohere has carved out a distinct niche developing large language models and software tools tailored for business use cases. The company is backed by high-profile investors, including Nvidia and Salesforce Ventures, and its valuation has grown to roughly $7 billion in recent private rounds, reflecting sustained confidence from strategic and financial backers.\n\nCohere's performance comes at a pivotal moment for the generative AI industry. While consumer-facing chatbots like ChatGPT and Claude have dominated headlines, enterprise adoption is now the primary battleground. OpenAI reported in November 2025 that more than 1 million businesses worldwide were using its technology, while Anthropic disclosed in September that it serves over 300,000 businesses.\n\nThese sizable customer bases present significant scale challenges for emerging players like Cohere.\n\nYet Cohere has differentiated itself through a capital-efficient business model that emphasizes flexibility and security. The company primarily generates revenue from software licenses and services, allowing customers to run its models either through managed cloud services or directly on their own hardware. This approach avoids the massive infrastructure costs incurred by full-stack competitors that build and operate their own data centers, enabling Cohere to invest more aggressively in customer acquisition, product development, and research.\n\nAs a result, Cohere's gross margins averaged around 70% in 2025, expanding by 25 basis points year-over-year.\n\n\"By scaling compute resources proportionally to customer demand, we remain insulated from the speculative excesses surrounding the broader AI market, positioning Cohere for more sustainable growth,\" the company told investors.\n\nThis efficiency has been particularly attractive to regulated industries -- financial services, healthcare, government, and legal -- where data privacy, auditability, and on-premises deployment are non-negotiable requirements. Cohere has leaned into these sectors, offering models that can be fine-tuned and deployed in secure environments without sending sensitive data to third-party clouds. CEO Aidan Gomez has been vocal about the company's growth ambitions.\n\nIn October 2025, he told Bloomberg that Cohere hopes to make its public market debut \"soon,\" suggesting investors would welcome a \"pure play AI investment opportunity\" focused on enterprise use cases. The strong 2025 results and clear 2026 roadmap appear to lay the groundwork for that potential IPO. For 2026, Cohere outlined plans to accelerate European expansion -- a region with stringent data protection regulations that favor privacy-first AI providers -- and to further develop its AI agent platform, North.\n\nThe company told investors it anticipates another year of \"rapid growth,\" supported by deepening enterprise penetration and continued model improvements. The results stand in contrast to the broader AI funding and valuation environment, where some high-profile startups have faced scrutiny over high burn rates and uncertain paths to profitability. Cohere's emphasis on capital efficiency and recurring revenue from enterprise software positions it as a more measured player in a market often criticized for speculative excess.\n\nHowever, OpenAI and Anthropic have continued to expand aggressively in the enterprise space, leveraging their frontier model capabilities and vast resources. Cohere must continue proving that its specialized focus on security, customization, and deployment flexibility can win and retain large accounts against bigger, better-funded rivals.\n\nThe strong 2025 performance and clear enterprise momentum suggest Cohere is executing well on its strategy of building a sustainable, high-margin AI business. As the generative AI market matures and shifts from hype to practical deployment, companies that can deliver secure, efficient, and enterprise-ready solutions are likely to garner increasing attention from both customers and public market investors. Cohere's progress in 2025 puts it in line as a serious contender in that evolving landscape."
  },
  {
    "source": "The Globe and Mail",
    "company": "Cohere",
    "title": "Cohere beats forecast with $240-million in annual recurring revenue",
    "date": "2026-02-13T22:06:59Z",
    "url": "https://www.theglobeandmail.com/business/technology/article-cohere-artificial-intelligence-forecast-recurring-revenue-llms/",
    "content": "Toronto-based artificial intelligence company Cohere Inc. earned US$240-million in annual recurring revenue last year, topping its own forecast, according to an investor memo obtained by The Globe and Mail.\n\nThe privately held company told investors in a letter that it had previously forecast US$200-million in revenue. Its gross margins, meanwhile, averaged 70 per cent in 2025 and grew year-over-year.\n\nFounded in 2019, Cohere builds large language models (LLMs) for enterprise customers and competes with much bigger rivals such as OpenAI and Anthropic. The ability for AI developers to boost adoption, reduce expenses and make money is a pressing issue as investment dollars continue to flow into the sector and concerns about a financial bubble persist.\n\nWhile the Cohere memo paints a rosy picture, the company still has to prove it can generate profits in the face of intense competition. CNBC first reported on the investor memo.\n\nBell and Cohere partner to sell AI tools to governments, businesses\n\nCohere, which was valued at US$7-billion in its latest fundraising round in September, has long touted how it differs from its peers, in part through its focus on security standards to meet the needs of large enterprises and regulated industries.\n\nCohere also does not have a consumer-facing chatbot, such as ChatGPT, and its customers run its AI models on computing infrastructure that they own or rent. That means Cohere spends less than its competitors on compute - the industry term for the advanced chips that power AI models and applications.\n\nCo-founder and chief executive officer Aidan Gomez has said that Cohere's gross margins are more akin to a traditional software-as-a-service company. Last fall, he said the company was looking to hold an initial public offering \"soon.\"\n\nThe company still incurs costs to build its AI models, however. The federal government announced in December, 2024, that it would give Cohere up to $240-million to help pay the expense of training models. (The funding was part of a $2-billion AI spending package.)\n\nAnthropic is also making a play for business customers. Its AI-powered application for writing computer code is beloved by software engineers, while the company recently released a product called Cowork to automate other tasks that can be done on a computer. The Silicon Valley company said yesterday that it had raised US$30-billion at a US$380-billion valuation.\n\nLast year, Cohere launched an AI software platform called North that uses its models to help automate office tasks, and has signed on RBC and Bell Canada as customers, among others. Cohere has also partnered with Fujitsu to build a Japanese-language LLM.\n\nCanada's federal government has become a strong Cohere champion as countries jockey for position for AI dominance and as sovereignty concerns have become paramount. Federal AI Minister Evan Solomon has emphasized that only a handful of companies build foundational LLMs, and that Cohere is the sole Canadian player.\n\nPatrick Pichette, a partner at Inovia Capital and a member of the federal government's AI task force, wrote in his submission last fall that Ottawa should declare Cohere a national champion and \"fuel it with significant revenue contracts,\" such as $1-billion each for civil service and defence applications. (Inovia is an investor in Cohere.)\n\nLast year, Ottawa signed a non-binding deal with Cohere to look for ways to deploy AI across the federal public service."
  },
  {
    "source": "BetaKit",
    "company": "Cohere",
    "title": "Cohere co-founder Nick Frosst wants to build more Canadian, less Silicon Valley-centric AI | BetaKit",
    "date": "2026-02-19T21:41:06Z",
    "url": "https://betakit.com/cohere-co-founder-nick-frosst-wants-to-build-more-canadian-less-silicon-valley-centric-ai/",
    "content": "Frosst recently talked efficiency, AGI, and his band Good Kid at AI conference and recent media dinner.\n\nCohere co-founder Nick Frosst says that most of the world's experience of technology is defined by Silicon Valley and the United States -- and that \"isn't great.\"\n\nAI is \"super useful\" but is not \"a god inside a computer.\"\n\n\"We're all pretty aware that that isn't great, that the vision of life, that the view of the way the world should be coming out of Silicon Valley is not really the one that a lot of the world wants,\" Frosst said on Thursday at the Vector Institute's annual AI research conference in Toronto at The Carlu. \"It's not the one that Canada wants.\"\n\nFrosst hopes to play a role in changing the global view in part by building up Canada's AI ecosystem with Toronto-based Cohere, the country's horse in the global large language model (LLM) race.\n\nSome industry leaders have argued that the Government of Canada's next AI strategy should pick domestic winners; the feds have already broadcast their intention to make Cohere into \"a Canadian champion.\" At a media dinner BetaKit attended earlier this month, Frosst called the designation \"enormously motivating.\"\n\n\"In the rarefied air of foundation model companies, [as] the only one in Canada, we take it enormously seriously, and are aware of how much work there is to do,\" Frosst added.\n\nUnlike some of its big-name competitors, Cohere has bet on specialized deployment of smaller, custom large language models for businesses rather than chasing larger, \"do-everything\" frontier models. It's an approach Frosst claimed has served Cohere well to date.\n\nCohere co-founder and CEO Aidan Gomez recently signalled that the scaling company could go public \"soon,\" a goal US rivals like OpenAI and Anthropic are also working towards. The AI firm, which in August was valued at $7 billion USD (approximately $9.3 billion CAD at the time), reportedly soared past its sales target recently after hitting $240 million USD in annual recurring revenue.\n\nAt the media dinner, Frosst wouldn't say when Cohere might actually make the leap, but argued the firm's focus on efficiency would make it a suitable candidate for public markets.\n\nAs to what Cohere is focusing on in the meantime, Frosst made clear that the company is not chasing artificial general intelligence (AGI) or a \"digital god\" that could take over the world. On stage at the Vector Institute conference on Thursday, Frosst polled attendees on how many believe that LLMs pose an existential threat to humanity, and only a few scattered hands went up.\n\nRELATED: Cohere reportedly soars past revenue target, with $240-million USD ARR\n\nFrosst argued that there would have been many more if it were 2023, and expressed his belief that LLMs will not destroy humanity or eliminate all jobs, a perspective that differs from some of his peers, including his mentor Geoffrey Hinton.\n\n\"We have to acknowledge this is a technology [that] is super useful [and] going to fundamentally change the way computers work [and] the way the economy works,\" Frosst said. \"But it is not the AI of [science fiction] ... It's not a god inside a computer.\"\n\nCohere's chief AI officer, Joelle Pineau, told media she finds the firm's focus on delivering return on investment (ROI) rather than AGI \"refreshingly pragmatic.\" Frosst said customer ROI has always been a focus for Cohere, and claimed the company's internal numbers are \"closer to the inverse\" of that much-discussed MIT study, which found 95 percent of corporate generative AI pilots fail.\n\nWhen Frosst isn't building LLMs or talking about his work at conferences, he spends his spare time on stage as lead singer of Toronto indie rock band Good Kid, which is dropping its debut album in April and going on tour in 2026. Despite Cohere's rising profile, Frosst told media that on the street he gets recognized more often for being in Good Kid.\n\nWhile Frosst said it would be a bit \"too Michael Scott\" for his taste to add Good Kid to Cohere event playlists, Pineau did say the music at Cohere functions is much better than at her last employer, Meta."
  },
  {
    "source": "BetaKit",
    "company": "Cohere",
    "title": "Cohere reportedly soars past revenue target, with $240 million USD ARR",
    "date": "2026-02-13T22:40:53Z",
    "url": "https://betakit.com/cohere-reportedly-soars-past-revenue-target-with-240-million-usd-arr/",
    "content": "Toronto AI company Cohere reportedly hit $240 million USD in annual recurring revenue (ARR) last year, exceeding its targets.\n\nThe enterprise scaleup told investors it reached that figure in a February investor memo obtained by CNBC, significantly overshooting earlier projections of $200 million USD in ARR.\n\nCNBC reported Cohere saw quarter-over-quarter growth of more than 50 percent throughout 2025, and averaged 70-percent gross margins, which expanded by 25 basis points year over year. Cohere declined to comment on the reported metrics.\n\nFounded in 2019 by former Google researchers, Cohere builds the large-language models (LLMs) that power chatbots and other AI applications for companies and government agencies. Cohere has raised $600 million USD from investors including Nvidia, and last year hit a valuation of $7 billion USD. It also recently opened a secondary sale for employees.\n\nCohere has sought to set itself apart from the world's handful of foundational model players, like OpenAI and Anthropic, with an enterprise-only offering of secure AI solutions, including data hosting through private deployments. This has made it attractive to clients in highly regulated and sensitive sectors like the Royal Bank of Canada and Thales Canada.\n\nRELATED: Cohere's new CFO on carving out space in a crowded AI market\n\nThe company was also among 15 global tech leaders, including Microsoft and Anthropic, who today signed on to be part of a new global group called the Trusted Tech Alliance. The group pledges to follow a set of principles when it comes to AI development, including transparent corporate governance, secure development and independent assessment, and respect for laws and data protection. However Cohere has faced accusations that it hasn't always practiced responsible tech. Last year, a group of major North American media companies, including Forbes and the Toronto Star, sued Cohere for alleged copyright infringement (Cohere denies the allegations).\n\nThe Canadian government, in its quest to support domestic AI companies, has touted Cohere as an AI \"champion\" and has partnered with the company to use its AI services.\n\nThe metrics come as the company flirts with an eventual public offering. At a Bloomberg Tech conference in October, CEO Aidan Gomez said Cohere might IPO \"soon\" and that he eyed profitability for the company by 2029. Last year, Cohere hired its first CFO, Francois Chadwick, who had served as Uber's acting CFO and played a leadership role in its public offering."
  },
  {
    "source": "BetaKit",
    "company": "Cohere",
    "title": "From Amharic to Zulu: Cohere's new multilingual AI model supports more than 70 languages | BetaKit",
    "date": "2026-02-17T18:34:13Z",
    "url": "https://betakit.com/from-amharic-to-zulu-coheres-new-multilingual-ai-model-supports-more-than-70-languages/",
    "content": "Toronto scaleup says Tiny Aya can run in regions where large-scale infrastructure isn't always available.\n\nToronto AI company Cohere has released a suite of new multilingual AI models that support more than 70 languages on any device, even offline.\n\n\"The future of multilingual AI ... will be a vibrant ecosystem of many models, shaped by many voices.\"\n\nThe base Tiny Aya model (Tiny Aya-Base) contains more than 3.35 billion parameters (the settings that control an AI model's output and behavior) containing the data for languages like Amharic, German, Latvian, Tagalog, and Zulu. Those 3.35 billion parameters are a small number compared to well-known large language models like ChatGPT, which have hundreds of billions of parameters.\n\nTiny Aya-Base powers the instruction-tuned TinyAya-Global model, which Cohere released on Tuesday alongside several specialized models for specific regions of the world. TinyAya-Earth is strongest for languages across the African and West Asian regions; TinyAya-Fire is strongest for South Asian languages; and TinyAya-Water is strongest for the Asia-Pacific and European regions.\n\nCohere said in a blog post that this approach \"allows each model to develop stronger linguistic grounding and cultural nuance,\" resulting in systems that \"feel more natural and reliable for the communities they are meant to serve.\"\n\n\"The future of multilingual AI will not be one giant model,\" the blog post reads. \"It will be a vibrant ecosystem of many models, shaped by many voices.\"\n\nCohere said Tiny Aya is designed to run on local devices, in classrooms, in community labs, and in regions where large-scale infrastructure isn't always available, with the intent of bringing \"high-quality AI\" closer to researchers working on underrepresented languages and developers building locally. The company said the model could be used by a university lab as an offline translation or AI education tool in classrooms and community settings, without having to rely on cloud APIs.\n\nRELATED: As Google and Cohere expand multilingual AI offerings, experts warn of \"plausible BS\"\n\nCohere has tried to set itself apart from its competitors with a focus on the multilingual capabilities of its models. ex-Cohere Labs head Sara Hooker told BetaKit in 2024 that their research is working to improve the quality of LLMs in languages other than English, as those languages suffer partly due to a lack of available high-quality training data.\n\nCohere has \"seen a lot of attraction\" for models that are adept at various languages, chief AI officer Joelle Pineau told media at a dinner BetaKit attended earlier this month. The company has struck multiple international partnerships, such as with Japanese firm Fujitsu, and opened new offices around the world over the past year.\n\n\"We have quite a few customers in Asia, in Korea, in Europe, who really value the fact that the model is actually competent in their local language,\" Pineau said at the dinner.\n\nFounded in 2019 by former Google researchers, Cohere builds the LLMs that power chatbots and other AI applications for companies and government agencies. Cohere has raised $600 million USD from investors, including Nvidia, and last year hit a valuation of $7 billion USD.\n\nThe company also reportedly hit $240 million USD in annual recurring revenue last year, significantly overshooting earlier projections of $200 million USD. At a Bloomberg Tech conference in October, CEO Aidan Gomez said Cohere might go public \"soon.\"\n\nThe Canadian government, in its quest to support domestic AI companies, has touted Cohere as an AI \"champion\" and has partnered with the company to use its AI services."
  },
  {
    "source": "Tech Funding News",
    "company": "Cohere",
    "title": "Will Cohere go public in 2026 after smashing $240M ARR?  --  TFN",
    "date": "2026-02-16T11:34:29Z",
    "url": "https://techfundingnews.com/enterprise-ai-giant-cohere-builds-momentum-towards-ipo-surpasses-240m-arr/",
    "content": "While giants like Google, Anthropic, and OpenAI dominate headlines, Canada's Cohere has been quietly building momentum.\n\nIn 2025, the Toronto-based AI company surpassed its $200 million annual recurring revenue target, closing the year at $240 million in annual recurring revenue. Even more striking was its pace, which exceeded a 50% quarter-over-quarter growth rate throughout the year, according to a memo shared with investors and industry reports.\n\nIn a market crowded with research breakthroughs and product launches, sustained revenue growth of that magnitude stands out. Cohere's performance suggests enterprises are experimenting with AI and committing budgets.\n\nFounded in 2019 by Aidan Gomez, Ivan Zhang, and Nick Frosst, Cohere has attracted backing from enterprise-focused technology leaders including Nvidia, AMD, and Salesforce. The investor base reflects its strategy of building AI that fits into real-world corporate infrastructure rather than requiring massive computing budgets.\n\nAt the heart of the company's offering is its Command family of generative AI models. Unlike some rivals that require extensive GPU resources, Cohere positions its models as efficient enough to run on limited hardware. For enterprises grappling with rising infrastructure costs and limited access to GPUs, that promise is practical and financially compelling.\n\nEfficiency is becoming a differentiator. As AI adoption scales, cost control is no longer secondary but strategic.\n\nLast summer, Cohere expanded beyond models with the launch of North, a high-level enterprise platform designed for secure, customised AI agents and workflows.\n\nNorth transforms Cohere's underlying models into operational tools. It offers businesses a structured workspace to deploy AI agents tailored to their internal processes, all within secure environments.\n\nThis shift matters. Companies increasingly want AI that integrates with compliance rules, internal data systems, and specific operational needs. North positions Cohere not just as a model provider, but as an enterprise AI partner.\n\nBy moving up the stack, Cohere strengthens customer relationships and expands revenue opportunities beyond model access alone.\n\nThe company's financial momentum now feeds a bigger question: When does Cohere go public?\n\nCEO Aidan Gomez indicated last October that an IPO could come \"soon.\" If that timeline points to 2026, Cohere may enter public markets alongside other high-profile AI contenders. OpenAI, Anthropic, and even SpaceX's xAI are reportedly weighing their own public debuts.\n\nAn IPO would test whether investors view Cohere as a durable enterprise infrastructure player rather than a short-term AI beneficiary. Its $240 million revenue milestone provides a strong foundation.\n\nIn a sector defined by speed and spectacle, Cohere's disciplined climb may prove its greatest advantage, especially as it edges closer to the public stage."
  },
  {
    "source": "Tekedia",
    "company": "Cohere",
    "title": "Cohere Bets on Compact, Multilingual AI with Launch of 'Tiny Aya' at India AI Summit - Tekedia",
    "date": "2026-02-17T20:39:44Z",
    "url": "https://www.tekedia.com/cohere-bets-on-compact-multilingual-ai-with-launch-of-tiny-aya-at-india-ai-summit/",
    "content": "In a signal that the next phase of artificial intelligence may be defined less by scale and more by accessibility, enterprise AI firm Cohere has introduced a new family of compact multilingual models designed to run directly on everyday devices -- no cloud connection required.\n\nUnveiled on the sidelines of the India AI Summit, the models, collectively branded Tiny Aya, are open-weight systems supporting more than 70 languages. Their release underscores a strategic shift toward deployable, regionally tuned AI that prioritizes linguistic diversity, hardware efficiency, and developer autonomy.\n\nTiny Aya's ability to run offline on consumer hardware positions it as a practical AI solution for low-connectivity and multilingual markets.\n\nThe base Tiny Aya model contains 3.35 billion parameters -- modest compared to frontier models that run into the hundreds of billions, but deliberately optimized for efficiency and portability. Parameter count reflects the internal complexity of a model and influences both capability and computational cost. By keeping the architecture compact, Cohere is targeting practical, real-world deployments rather than data-center-scale experimentation.\n\nThe family includes multiple variants. TinyAya-Global is instruction-tuned for general-purpose multilingual use. Regional versions include TinyAya-Earth for African languages, TinyAya-Fire for South Asian languages, and TinyAya-Water for Asia Pacific, West Asia, and Europe.\n\nSouth Asian language support includes Bengali, Hindi, Punjabi, Urdu, Gujarati, Tamil, Telugu, and Marathi, addressing a long-standing imbalance in AI systems that have historically centered on English and a small set of European languages.\n\n\"This approach allows each model to develop stronger linguistic grounding and cultural nuance, creating systems that feel more natural and reliable for the communities they are meant to serve,\" the company said in a statement.\n\nThe regional specialization model suggests a deliberate move away from one-size-fits-all training strategies toward geographically informed datasets and linguistic fine-tuning.\n\nCohere said Tiny Aya was trained on a single cluster of 64 H100 GPUs produced by Nvidia. In the context of modern large language models, some of which are trained on thousands of GPUs, this represents a comparatively restrained computational footprint.\n\nThe efficiency claim is central to the model's positioning. Cohere said it engineered its inference stack to require less computing power than most comparable multilingual systems, enabling deployment on laptops and other consumer-grade devices.\n\nThe on-device capability has several implications. First, it reduces dependence on constant internet connectivity, expanding usability in rural or bandwidth-constrained environments. Second, it lowers cloud infrastructure costs for developers. Third, it strengthens data privacy, since user inputs do not need to be transmitted to remote servers.\n\nIn linguistically diverse countries like India, offline AI can support translation tools, educational software, local-language assistants, and enterprise workflows without requiring persistent connectivity.\n\nUnlike proprietary closed models, Tiny Aya is open-weight. Developers can access, fine-tune, and redistribute the models, encouraging experimentation and localization. The models are available on Hugging Face as well as the Cohere Platform, with downloads supported through Hugging Face, Kaggle, and Ollama for local deployment.\n\nCohere is also releasing associated training and evaluation datasets and plans to publish a technical report outlining its methodology. This level of disclosure enhances reproducibility and positions the release within the open research ecosystem.\n\nThe open-weight approach aligns with a broader industry split between proprietary API-based models and adaptable open systems. For enterprises concerned about vendor lock-in, compliance, and customization, open-weight alternatives offer greater control.\n\nThe launch comes amid intensifying competition in enterprise AI. Cohere has historically positioned itself as a business-focused alternative to consumer-centric AI providers, emphasizing secure deployments and customizable solutions.\n\nBy targeting multilingual, compact, and offline-capable systems, the company is differentiating itself from firms competing primarily on model size and benchmark dominance.\n\nThe strategy reflects an emerging market thesis: growth may increasingly come from AI tailored to specific regions, industries, and linguistic communities rather than purely from scale-driven performance improvements.\n\nCohere's operational expansion coincides with strong financial performance. According to CNBC, the company ended 2025 with $240 million in annual recurring revenue and reported quarter-over-quarter growth of 50% throughout the year.\n\nChief executive Aidan Gomez has previously said the company plans to go public \"soon,\" suggesting that scaling enterprise adoption and broadening product lines are part of a longer-term strategy to support an eventual listing.\n\nThe Tiny Aya launch strengthens Cohere's narrative as an AI infrastructure company focused on pragmatic deployment rather than purely experimental research.\n\nTiny Aya's release illustrates a broader shift in AI development priorities:\n\nAs AI adoption deepens globally, particularly in emerging markets, compact multilingual systems may prove critical in bridging accessibility gaps.\n\nRather than competing solely in the race for ever-larger models, Cohere is making a case that the future of AI may rely on systems that are smaller, regionally aware, open for adaptation -- and capable of running wherever users are, even without the cloud."
  },
  {
    "source": "新浪财经",
    "company": "Cohere",
    "title": "企业AI初创公司Cohere营收超目标，IPO势头渐起",
    "date": "2026-02-13T14:36:45Z",
    "url": "https://finance.sina.com.cn/stock/usstock/c/2026-02-13/doc-inhmsuve5731272.shtml",
    "content": "尽管谷歌、Anthropic、OpenAI等竞争对手正在争夺市场份额，人工智能初创公司Cohere仍向投资者表示，其在企业客户领域的发展势头持续向好。\n\n2月投资者备忘录显示，Cohere去年年度经常性收入约达2.4亿美元，超出2亿美元的预期目标。备忘录提到，2025年全年该公司季度环比增速均超过50%。\n\n\"我们的核心理念在市场中得到了明显认可，\"公司在备忘录中写道，\"随着全球各受监管行业的机构选择Cohere作为其规模化部署安全人工智能的可靠合作伙伴，我们的销售渠道持续拓展。\"\n\nCohere于2019年在多伦多成立，专注为企业开发人工智能模型与软件工具。公司投资方包括英伟达、Salesforce风险投资公司等，估值已攀升至约70亿美元。\n\n在此份投资者备忘录发布前，公司首席执行官艾丹・戈麦斯曾于10月表示，这家初创公司希望\"尽快\"登陆公开市场。他表示，相信投资者会欢迎这样一个\"纯粹的人工智能投资标的\"。\n\n但据知情人士透露，Cohere的竞争对手OpenAI与Anthropic也在筹备潜在的首次公开募股（IPO），且两家公司均毫不掩饰其争夺企业市场的野心。\n\nOpenAI在11月表示，全球已有超100万家企业使用其技术；Anthropic在9月称，其服务企业客户超30万家。这些庞大的客户群给其他试图追赶的初创公司带来了切实挑战。\n\nCohere向投资者表示，其\"资本高效模式\"使其在行业竞争中脱颖而出。\n\n公司营收主要来源于软件业务，并称由于客户可通过托管云服务或自有硬件直接运行其模型，因此能够避免高昂的基础设施成本。据投资者备忘录，这一模式让Cohere可以\"更积极地\"投入客户拓展与研发工作。\n\n备忘录显示，2025年Cohere平均毛利率约为70%，同比提升25个基点。\n\n\"通过根据客户需求按需扩展算力资源，我们避开了整个人工智能市场存在的投机过热问题，为Cohere实现更可持续的增长奠定基础。\"公司写道。\n\nCohere表示，2026年将继续拓展欧洲市场，并完善其人工智能智能体平台North。公司向投资者预期，新一年仍将实现\"高速增长\"。"
  },
  {
    "source": "IT News zu den Themen Künstliche Intelligenz, Roboter und Maschinelles Lernen - IT BOLTWISE® x Artificial Intelligence",
    "company": "Cohere",
    "title": "Cohere erreicht 240 Millionen Dollar Umsatz und plant BÃ¶rsengang",
    "date": "2026-02-13T15:35:32Z",
    "url": "https://www.it-boltwise.de/cohere-erreicht-240-millionen-dollar-umsatz-und-plant-boersengang.html",
    "content": "TORONTO / LONDON (IT BOLTWISE) - Das kanadische KI-Startup Cohere hat in einem beeindruckenden Jahr 2025 seine Umsatzziele übertroffen und plant nun einen Börsengang. Mit einem Jahresumsatz von 240 Millionen Dollar und einem Wachstum von über 50 % pro Quartal positioniert sich das Unternehmen als starker Mitbewerber im Bereich der generativen KI-Modelle.\n\nCohere, ein aufstrebendes kanadisches KI-Startup, hat im Jahr 2025 einen bemerkenswerten Meilenstein erreicht, indem es seine Umsatzziele übertroffen hat. Das Unternehmen, das 2019 gegründet wurde, hat einen Jahresumsatz von 240 Millionen US-Dollar erzielt, was die ursprünglich gesetzten Ziele von 200 Millionen US-Dollar übertrifft. Diese Leistung ist umso beeindruckender, da Cohere ein kontinuierliches Quartalswachstum von über 50 % verzeichnen konnte.\n\nDas Unternehmen hat sich auf die Entwicklung von generativen KI-Modellen spezialisiert, die unter dem Namen Command bekannt sind. Diese Modelle zeichnen sich durch ihre Effizienz aus, da sie auf begrenzten GPU-Ressourcen betrieben werden können. Dies ist besonders attraktiv für Unternehmen, die ihre Kosten und Ressourcen im Griff behalten möchten. Unterstützt wird Cohere von namhaften Investoren aus der Technologiebranche, darunter NVIDIA, AMD und Salesforce.\n\nIm vergangenen Sommer hat Cohere seine Plattform North eingeführt, eine fortschrittliche Unternehmensplattform und KI-Arbeitsumgebung. Diese ermöglicht die Erstellung sicherer, maßgeschneiderter KI-Agenten und Workflows, die auf den Modellen von Cohere basieren. Diese Entwicklung unterstreicht das Engagement des Unternehmens, Unternehmen bei der Implementierung von KI-Lösungen zu unterstützen.\n\nCEO Aidan Gomez hat im Oktober letzten Jahres angedeutet, dass ein Börsengang des Unternehmens in naher Zukunft möglich sei. Sollte dies im Jahr 2026 geschehen, könnte Cohere in direkter Konkurrenz zu anderen großen Namen der Branche wie OpenAI, Anthropic und SpaceX/xAI stehen, die ebenfalls über einen Börsengang nachdenken.\n\nDer Erfolg von Cohere zeigt, dass das Unternehmen gut positioniert ist, um von der wachsenden Nachfrage nach KI-Lösungen in Unternehmen zu profitieren. Mit seinen innovativen Technologien und der Unterstützung durch führende Investoren hat Cohere das Potenzial, eine bedeutende Rolle im Bereich der Künstlichen Intelligenz zu spielen."
  },
  {
    "source": "The Logic",
    "company": "Cohere",
    "title": "Ottawa signs deal to use Cohere's tech for its internal AI assistant - The Logic",
    "date": "2026-02-05T11:17:30Z",
    "url": "https://thelogic.co/news/cohere-federal-government-contract-canchat-ai/",
    "content": "TORONTO -- Cohere has won its first public contract with the federal government, selling access to its flagship model for use in an AI assistant that's set to be rolled out to all public servants.\n\nUnder the deal, IT department Shared Services Canada is paying the Toronto-based firm $339,000 for a one-year license for its software starting in November 2025. The contract was disclosed on the federal open data portal late last month.\n\nShared Services spokesperson Nick Wells said the department has integrated Cohere's Command A model into CANChat, Ottawa's in-house version of ChatGPT. \"This marks a major milestone in delivering a sovereign, made-in-Canada AI capability for federal public servants,\" he said.\n\nStaff can use CANChat to draft emails, memos and other documents, conduct research or analyze data. Some 11,500 federal employees across 20 departments are currently signed up for the tool, and Shared Services is set to start rolling it out across the rest of the public service this quarter, according to Wells.\n\nCohere did not respond to a request for comment.\n\nThe firm's contract with Shared Services represents a very small piece of the $2.17 billion the department plans to spend on federal IT operations in the 2025-26 fiscal year. Still, it comes as Cohere tries to win more government business, and after Ottawa promised to back homegrown champions and improve its own operations with AI.\n\nLast August, the two sides signed a non-binding memorandum to explore how Cohere's technology could be used by federal departments and agencies. The company has also demonstrated its products to public servants via workshops. Ottawa has since signed a similar deal with Montreal-based Coveo, which sells technology that helps chatbots and other generative tools provide better responses.\n\n\"The MoUs are a signal that we're going to help develop the ecosystem,\" AI Minister Evan Solomon told The Logic in December, adding that the deals \"are leading toward contracts.\"\n\nAs The Logic first reported, the firm has had at least one other federal contract, with the Communications Security Establishment; the agency has declined to disclose details of the deal or the work it covers, citing national security.\n\nCohere claims Command A uses less processing power than competitors' large language models (LLM), running on two Nvidia AI chips, making it particularly useful to clients that want to keep their tools on in-house hardware, for security or regulatory reasons.\n\nWhile many model makers charge customers based on usage, Shared Services is paying Cohere a flat fee for a single \"instance\" of Command A, Wells said. \"This may expand as we continue to onboard more users.\" The model will live and run from government-approved compute infrastructure in Canada.\n\nCANChat users can still pick from several LLMs to run their queries and prompts, according to Shared Services' guidelines. Options include a fine-tuned version of Meta's Llama, which the department recommends for drafting professional correspondence; Google's Gemini, which works well for summarization, coding or tasks that involve images and OpenAI's GPT-5, which can handle multi-step problems like research.\n\nThe department suggests public servants use Cohere's model to generate text or run AI agents, and notes that it's conversant in both English and French, ensuring they comply with language rules.\n\nFirst developed in 2023, CANChat is a major pillar of Ottawa's internal AI strategy. Lots of public servants are already using ChatGPT and other AI tools to do their work, then federal chief data officer Stephen Burt told The Logic last June. To get staff to switch to CANChat, it needs to be as good as commercial applications, he acknowledged.\n\nShared Services is also looking to roll out other new software. In January 2025, it launched a procurement process for \"commercially-available generative AI productivity tools\" on behalf of federal departments and agencies, according to a memo The Logic obtained via an access to information request.\n\nShared Services ultimately signed off on a small list of suppliers, including longtime vendors IBM, Microsoft, OpenText, as well as Ottawa-based startup Copoly.ai and a tie-up between software company Factr and recruitment firm Altis Technology. The department plans to re-open the process during the spring to \"allow other vendors, including Cohere, to qualify for [federal] opportunities,\" according to the memo."
  },
  {
    "source": "eWEEK",
    "company": "Cohere",
    "title": "Cohere's Tiny Aya Models Bring 70+ Languages to Offline AI | eWEEK",
    "date": "2026-02-17T18:02:51Z",
    "url": "https://www.eweek.com/news/cohere-tiny-aya-multilingual-edge-ai-models/",
    "content": "eWeek content and product recommendations are editorially independent. We may make money when you click on links to our partners. Learn More\n\nIn the world of generative AI, language support has often been a luxury reserved for a handful of global languages. That's changing fast.\n\nCohere just unveiled a suite of open multilingual models designed to push AI out of data centers and into everyday devices while embracing linguistic diversity at scale. These \"Tiny Aya\" models underscore a broader shift toward accessible, globally relevant AI without the cloud tether.\n\nAnd they're opening doors for developers, researchers, and communities that have long been underserved by mainstream language tech.\n\nTiny Aya brings AI to the edge\n\nAs reported by TechCrunch, Cohere's latest announcement centers on the Tiny Aya family -- a set of multilingual AI models that pack support for over 70 languages into lightweight architectures that can run offline on laptops and other edge devices.\n\nUnlike monolithic models that demand heavy compute and constant connectivity, Tiny Aya is designed with portability in mind. That means developers can build apps that translate, generate, or understand text across a broad swath of languages -- even in low-connectivity regions of Asia, Africa, and Latin America.\n\nWhat makes Tiny Aya particularly interesting is its open-weight release.\n\nCohere's research arm, Cohere Labs, has made the underlying code and models available for anyone to download, customize, and deploy -- a sharp contrast with many proprietary AI offerings that keep their internals locked behind APIs.\n\nThis open approach could accelerate innovation in niche language communities and help smaller teams tackle localization challenges without budget-busting cloud costs.\n\nThe Tiny Aya family includes variants tailored to different regions and use cases. Models like TinyAya-Fire and TinyAya-Water focus on linguistic clusters in specific geographies. Meanwhile, the TinyAya-Global variant is fine-tuned to follow user commands across diverse languages.\n\nBy clustering models this way, Cohere hopes to balance linguistic nuance and broad applicability, giving each version a better shot at understanding local idioms and cultural context.\n\nWhy multilingual edge AI matters\n\nIf you've ever tried to use AI outside English or a handful of major European languages, you know performance often drops sharply. That's partly because most foundational models are trained on datasets skewed toward a few dominant languages.\n\nCohere's broader Aya initiative has tackled this problem before: earlier versions of Aya aimed to cover over 100 languages via open research and global collaboration. Tiny Aya picks up that baton but rethinks how and where these models run.\n\nBy enabling offline, on-device AI, Cohere is lowering the barrier to entry for innovators working in markets where constant cloud connectivity isn't a given. This could power real-time translation in rural clinics, multilingual educational tools that function without Wi-Fi, or customer service bots that understand local dialects without pinging a remote server.\n\nAnd because Tiny Aya is open, developers aren't beholden to a single vendor or API pricing structure. They can experiment, improve, and repurpose models to suit local needs -- potentially democratizing AI in ways that closed systems haven't quite managed. In an industry still grappling with bias and uneven language support, that's no small feat.\n\nCohere's latest move feels less like a feature drop and more like a philosophy shift. If AI is going to be truly global, it has to speak the world's languages... not just the loudest ones."
  },
  {
    "source": "IT News zu den Themen Künstliche Intelligenz, Roboter und Maschinelles Lernen - IT BOLTWISE® x Artificial Intelligence",
    "company": "Cohere",
    "title": "Cohere Ã¼bertrifft Umsatzziel und bereitet sich auf BÃ¶rsengang vor",
    "date": "2026-02-13T15:21:05Z",
    "url": "https://www.it-boltwise.de/cohere-uebertrifft-umsatzziel-und-bereitet-sich-auf-boersengang-vor.html",
    "content": "TORONTO / LONDON (IT BOLTWISE) - Das kanadische KI-Startup Cohere hat im vergangenen Jahr einen Umsatz von rund 240 Millionen US-Dollar erzielt und damit sein Ziel von 200 Millionen US-Dollar übertroffen. Das Unternehmen verzeichnete ein beeindruckendes Wachstum von über 50 % pro Quartal im Jahr 2025 und plant nun, bald an die Börse zu gehen.\n\nDas kanadische KI-Startup Cohere hat kürzlich bekannt gegeben, dass es im vergangenen Jahr einen Umsatz von rund 240 Millionen US-Dollar erzielt hat, womit es sein ursprüngliches Ziel von 200 Millionen US-Dollar übertraf. Diese beeindruckende Leistung ist ein Beweis für die wachsende Nachfrage nach Cohere's KI-Lösungen im Unternehmenssektor. Das Unternehmen, das 2019 in Toronto gegründet wurde, entwickelt Modelle und Software-Tools, die speziell auf die Bedürfnisse von Unternehmen zugeschnitten sind.\n\nUnter der Leitung von CEO Aidan Gomez hat Cohere ein bemerkenswertes Wachstum von über 50 % pro Quartal im Jahr 2025 verzeichnet. Diese Dynamik hat das Unternehmen dazu veranlasst, einen baldigen Börsengang in Betracht zu ziehen. Gomez äußerte gegenüber Bloomberg, dass er glaubt, Investoren würden eine reine KI-Investitionsmöglichkeit begrüßen. Cohere hebt sich durch sein kapital-effizientes Modell von seinen Mitbewerbern ab, da es Kunden ermöglicht, die Modelle entweder über verwaltete Cloud-Dienste oder direkt auf ihrer eigenen Hardware auszuführen.\n\nDie Konkurrenz im KI-Markt ist jedoch hart. Unternehmen wie OpenAI und Anthropic haben ebenfalls Pläne für einen Börsengang und bedienen bereits Hunderttausende von Unternehmen weltweit. OpenAI gab im November bekannt, dass über eine Million Unternehmen ihre Technologie nutzen, während Anthropic im September mitteilte, dass sie mehr als 300.000 Unternehmen bedienen. Diese großen Kundenstämme stellen eine erhebliche Herausforderung für Cohere dar, das sich dennoch durch seine hohe Effizienz und die Fähigkeit, Infrastrukturkosten zu vermeiden, auszeichnet.\n\nFür das Jahr 2026 plant Cohere, seine Präsenz in Europa weiter auszubauen und seine KI-Agentenplattform North zu entwickeln. Das Unternehmen erwartet ein weiteres Jahr des schnellen Wachstums und sieht sich gut positioniert, um die Herausforderungen des Marktes zu meistern. Mit einer durchschnittlichen Bruttomarge von etwa 70 % im Jahr 2025 und der Fähigkeit, Rechenressourcen proportional zur Kundennachfrage zu skalieren, bleibt Cohere vor den spekulativen Exzessen des breiteren KI-Marktes geschützt."
  },
  {
    "source": "TechCrunch",
    "company": "Cohere",
    "title": "Cohere's $240M year sets stage for IPO | TechCrunch",
    "date": "2026-02-13T15:10:09Z",
    "url": "https://techcrunch.com/2026/02/13/coheres-240m-year-sets-stage-for-ipo/",
    "content": "As the top AI labs like Google, Anthropic, and OpenAI chase enterprise adoption, Canadian AI startup Cohere has been quietly cleaning up.\n\nThe startup told investors in a memo that it surpassed its $200 million annual recurring revenue target in 2025, hitting $240 million with quarter-over-quarter growth of more than 50% throughout the year, per CNBC.\n\nCohere was founded in 2019 and has the backing of enterprise tech investors like Nvidia, AMD, and Salesforce. The startup's core tech is its Command family of generative AI models, which Cohere says are efficient enough to be deployed on limited GPUs -- an attractive promise for enterprises looking to get a handle on cost and resource management.\n\nLast summer, Cohere launched North, a higher-level enterprise platform and AI workspace for secure, custom AI agents and workflows built on Cohere's models.\n\nCohere's CEO Aidan Gomez said last October that the startup may IPO \"soon.\" If \"soon\" means in 2026, Cohere may be contending against OpenAI, Anthropic, and SpaceX/xAI, which are all reportedly weighing their own public debuts.\n\nTechCrunch has reached out to Cohere for comment."
  },
  {
    "source": "Benzinga",
    "company": "Cohere",
    "title": "FlexTecs Announces Strategic Investment from Cohere Capital to Support Services Expansion, Technology Innovation, and Accelerate Growth",
    "date": "2026-01-22T12:10:43Z",
    "url": "https://www.benzinga.com/pressreleases/26/01/n50062933/flextecs-announces-strategic-investment-from-cohere-capital-to-support-services-expansion-technolo",
    "content": "Enter your email to get Benzinga's ultimate morning update: The PreMarket Activity Newsletter\n\nATLANTA, Jan. 22, 2026 /PRNewswire/ -- FlexTecs, a leader in recovery audit, contract compliance, and payment accuracy software and solutions, today announced a strategic investment from Cohere Capital, a Boston-based private equity firm focused on investing in differentiated technology and tech-enabled services companies.\n\nThrough its services and technology, FlexTecs reviews over $1 trillion in client transactions and recovers or prevents over $1 billion incorrect payments annually, resulting in direct savings for its clients. FlexTecs was founded to disrupt the audit recovery industry with a technology-first focus to deliver unparalleled speed, agility, and value to its clients. The company's audit team leverages its proprietary technology platform to perform in-year audits, pulling recoveries forward to near real-time and offering preventative recovery solutions. This differentiated business model increases recoveries and accelerates cash collections for its clients.\n\nThis strategic investment positions FlexTecs to drive even further value for clients, expand its services and software offering, and accelerate its next phase of growth while also preserving the leadership, culture, and operating model that have defined the company since its founding. The existing leadership team will continue to lead the business, with strategic guidance and resources from Cohere Capital and its Board advisors.\n\n\"This partnership comes at a time of tremendous momentum for FlexTecs,\" said Tom Cook, Co-Founder and CEO of FlexTecs. \"We've doubled the size of the business over the past three years and have grown our global team to more than 600 employees. Our differentiated model has created a strong foundation and continues to drive market share gains and lasting value for our clients.\"\n\nAccelerating Growth Across Services and SaaS\n\nWith a revenue compound annual growth rate (CAGR) of 25% since 2020, the new investment will enable the company to accelerate its growth trajectory, including strengthening its core recovery audit and contract compliance services while expanding the capabilities of its proprietary SaaS platform, FlexTrap.\n\n\"Our tech-enabled services business remains foundational to who we are,\" said Cook. \"FlexTrap is in the midst of a strategic expansion, building on the deep expertise we've developed through years of recovery audits, turning real-world insights into software that helps clients optimize accounts payable, reclaim valuable time, and prevent errors rather than just repeatedly recover them.\"\n\nFlexTrap extends FlexTecs' decades of industry expertise into software by addressing what many organizations experience as cash leakage in the payables process. FlexTrap helps prevent payment errors before they occur and automates complex statement reconciliations that often remain manual even in modern accounts payable (AP) environments.\n\nCohere Capital brings experience supporting companies through similar periods of expansion, including scaling SaaS and technology offerings that are part of larger services companies. As part of the partnership, Cohere will support FlexTecs' growth strategy, including continued service and technology innovation, geographic expansion, and potential strategic acquisitions.\n\n\"We are excited to partner with Tom and the rest of the FlexTecs leadership team to build on the momentum they have generated over the last several years. It is clear that a key reason for their success is the experienced team and culture that they have purpose-built,\" said Daniel Gedney, Co-Founder and Partner at Cohere Capital. \"FlexTecs was founded with technology at its core and that has created tremendous differentiation in the market versus their competitors, which we only expect to accelerate as AI continues to advance.\"\n\nAbout FlexTecs\n\nFlexTecs is a technology-enabled recovery audit and contract compliance services and solutions firm that is disrupting and modernizing the recovery audit industry. Companies choose FlexTecs to accelerate cash flow, mitigate risk, and reduce leakage in the payables process. Since 2011, FlexTecs has been taking a different approach to recovery audits, leveraging advanced technology and deep industry expertise to deliver unparalleled results. By providing innovative, data-driven solutions, FlexTecs empowers businesses to optimize their financial operations, recover lost funds, and prevent future inefficiencies. For more information visit www.flextecs.com.\n\nAbout Cohere Capital\n\nCohere Capital is a Boston-based private equity firm focused exclusively on middle market growth companies. Cohere Capital has a flexible mandate across growth markets but primarily targets recapitalizations and growth investments in rapidly growing software and technology-enabled services companies. For more information, visit: www.coherecapital.com.\n\nSOURCE FlexTecs\n\nMarket News and Data brought to you by Benzinga APIs"
  },
  {
    "source": "RocketNews | Top News Stories From Around the Globe",
    "company": "Cohere",
    "title": "Cohere launches a family of open multilingual models - RocketNews",
    "date": "2026-02-17T10:12:21Z",
    "url": "https://rocketnews.com/2026/02/cohere-launches-a-family-of-open-multilingual-models/",
    "content": "Enterprise AI company Cohere launched a new family of multilingual models on the sidelines of the ongoing India AI Summit. The models, dubbed Tiny Aya, are open-weight -- meaning their underlying code is publicly available for anyone to use and modify -- support over 70 languages, and can run on everyday devices like laptops without requiring an internet connection.\n\nThe model, launched by the company's research arm Cohere Labs, supports South Asian languages such as Bengali, Hindi, Punjabi, Urdu, Gujarati, Tamil, Telugu, and Marathi.\n\nThe base model contains 3.35 billion parameters -- a measure of its size and complexity. Cohere has also launched TinyAya-Global, a version fine-tuned to better follow user commands, for apps that require broad language support. Regional variants round out the family: TinyAya-Earth for African languages; TinyAya-Fire for South Asian languages; and TinyAya-Water for Asia Pacific, West Asia, and Europe.\n\nImage Credits: Cohere\n\n\"This approach allows each model to develop stronger linguistic grounding and cultural nuance, creating systems that feel more natural and reliable for the communities they are meant to serve. At the same time, all Tiny Aya models retain broad multilingual coverage, making them flexible starting points for further adaptation and research,\" the company said in a statement.\n\nCohere noted that these models, which were trained on a single cluster of 64 H100 GPUs (a type of high-powered chip by Nvidia) using relatively modest computing sources, are ideal for researchers and developers building apps for audiences that speak native languages. The models are capable of running directly on devices, so developers can use them to power offline translation. The company noted that it built its underlying software to suit on-device usage, requiring less computing power than most comparable models.\n\nImage Credits: Cohere\n\nIn linguistically diverse countries like India, this kind of offline-friendly capability can open up a diverse set of applications and use cases without the need for constant internet access.\n\nThe models are available on HuggingFace, the popular platform for sharing and testing AI models, and the Cohere Platform. Developers can download them on HuggingFace, Kaggle, and Ollama for local deployment. The company is also releasing training and evaluation datasets on HuggingFace and plans to release a technical report detailing its training methodology.\n\nThe startup's CEO, Aidan Gomez, said last year that the company plans to go public \"soon.\" According to CNBC, the company ended 2025 on a high note, posting $240 million in annual recurring revenue, with 50% growt ..."
  },
  {
    "source": "RocketNews | Top News Stories From Around the Globe",
    "company": "Cohere",
    "title": "Cohere's $240M year sets stage for IPO - RocketNews",
    "date": "2026-02-13T15:06:39Z",
    "url": "https://rocketnews.com/2026/02/coheres-240m-year-sets-stage-for-ipo/",
    "content": "As the top AI labs like Google, Anthropic, and OpenAI chase enterprise adoption, Canadian AI startup Cohere has been quietly cleaning up.\n\nThe startup told investors in a memo that it surpassed its $200 million annual recurring revenue target in 2025, hitting $240 million with quarter-over-quarter growth of more than 50% throughout the year, per CNBC.\n\nCohere was founded in 2019 and has the backing of enterprise tech investors like Nvidia, AMD, and Salesforce. The startup's core tech is its Command family of generative AI models, which Cohere says are efficient enough to be deployed on limited GPUs -- an attractive promise for enterprises looking to get a handle on cost and resource management.\n\nLast summer, Cohere launched North, a higher-level enterprise platform and AI workspace for secure, custom AI agents and workflows built on Cohere's models.\n\nCohere's CEO Aidan Gomez said last October that the startup may IPO \"soon.\" If \"soon\" means in 2026, Cohere may be contending against OpenAI, Anthropic, and SpaceX/xAI, which are all reportedly weighing their own public debuts."
  },
  {
    "source": "Investing News Network",
    "company": "Cohere",
    "title": "SAP and Cohere Expand Partnership to Launch Sovereign AI Solutions Globally, Beginning in Canada",
    "date": "2026-02-10T13:45:28Z",
    "url": "https://investingnews.com/sap-and-cohere-expand-partnership-to-launch-sovereign-ai-solutions-globally-beginning-in-canada/",
    "content": "Integration provides public sector, regulated, and critical industries customers with powerful generative AI capabilities while ensuring data remains sovereign and secure within Canada\n\nToday, SAP and Cohere are expanding our partnership to deliver full-stack sovereign AI solutions worldwide, starting in Canada. This collaboration builds on our previous integration of Cohere's frontier enterprise AI models and agentic platform, North, into SAP's EU AI Cloud and Business Technology Platform. Together, we are advancing our commitment to providing secure, scalable, and region-specific AI solutions for global enterprises and governments.\n\nSAP Canada Inc. , a subsidiary of SAP SE, plans to integrate North's agentic capabilities into our leading Enterprise Resource Planning (ERP) Sovereign Cloud environment in Canada. This creates a complete Sovereign AI Layer, an important solution for Canadian companies, particularly those in the public sector and highly regulated industries, who need to harness the power of AI while maintaining absolute control over their most sensitive data.\n\nAs artificial intelligence transforms every industry, organizations face the dual challenge of innovating rapidly while adhering to strict data security and sovereignty requirements. The SAP and Cohere partnership directly addresses this by embedding Cohere's powerful agentic automation capabilities, powered by its high performing large language models (LLMs), into SAP's secure, Canadian-operated sovereign cloud infrastructure. This allows customers to deploy advanced AI solutions without the complexity of building and managing their own AI engineering environments, while maintaining data residency and operational control.\n\n\"Canadian organizations are at a critical juncture where they must innovate with AI without compromising on security or data sovereignty,\" said Cathy Tough, Country Manager, SAP Canada. \"By integrating Cohere North's powerful enterprise platform into SAP's trusted sovereign cloud, we are removing the operational burden for our customers. This partnership provides sovereignty at global scale, empowering businesses and public sector entities to unlock the full potential of their data securely and drive the next wave of innovation.\"\n\nFor years, organizations have been caught between the innovative potential of the public cloud and the security of on-premise infrastructure. SAP's sovereign cloud offering was designed to resolve this dilemma, providing a complete cloud stack built to elevated security standards and fully operated in Canada. The integration of Cohere's platform enhances this offering, delivering enterprise-ready AI that is secure by design.\n\n\"Partnering with SAP, a global leader in enterprise applications, is a natural fit, said Francois Chadwick, Chief Financial Officer, Cohere. By integrating our state-of-the-art enterprise AI technology into SAP's sovereign cloud, we are providing organizations both in Canada and globally with the tools to build transformative agentic AI solutions on their own terms- backed by the security, accuracy, and privacy that modern enterprises demand.\"\n\nThe need for robust, integrated AI is urgent. A recent SAP AI report revealed that while 71 percent of organizations rely on data for investment decisions, 75 percent report that incomplete data is a significant challenge. By embedding AI directly into the core SAP applications where critical business data resides, this partnership helps customers overcome data fragmentation and build production-ready AI into their essential processes.\n\nThe future of AI is sovereign, secure, and ready to scale.\n\nAbout SAP\n\nAs a global leader in enterprise applications and business AI, SAP (NYSE: SAP) stands at the nexus of business and technology. For over 50 years, organizations have trusted SAP to bring out their best by uniting business-critical operations spanning finance, procurement, HR, supply chain, and customer experience. For more information, visit www.sap.com"
  },
  {
    "source": "NewsX",
    "company": "Cohere",
    "title": "Who Is Sara Hooker? Adaption Labs CEO Sparks Viral Debate, Says US Fruits Lack Taste, Flavour Compared Io India's - Internet Agrees",
    "date": "2026-02-21T09:08:53Z",
    "url": "https://www.newsx.com/uncategorized/who-is-sara-hooker-adaption-labs-ceo-sparks-viral-debate-india-ai-summit-2026-says-us-fruits-lack-taste-flavour-compared-io-indias-internet-agrees-170477/",
    "content": "Sara Hooker, CEO of Adaption Labs and a veteran of Cohere, Google Brain, and DeepMind, was on a visit to India for the AI Impact India Summit 2026. Hooker posted a photo of her hotel breakfast, reflecting on how fruits available in the United States often feel \"sanitized\" compared to the rich flavors she experienced in India.\n\nThousands of users agreed with her post and shared their own experiences. Many pointed to differences in agricultural and supply-chain practices, noting that American produce is often harvested unripe for long-distance shipping, then treated with gases, wax coatings, and refrigeration to preserve shelf life. In contrast, Indian local markets frequently sell tree-ripened heirloom varieties grown closer to where they are consumed.\n\nWhile some US fruits involve genetically modified crops, experts participating in the discussion emphasized that taste differences are more strongly linked to breeding priorities, particularly durability and transport efficiency, rather than flavor. Several users encouraged Hooker to return to India during mango season.\n\nWhen Sara Hooker Spotted Bata in Delhi\n\nBefore her fruit-related post went viral, Hooker had already shared glimpses of her Delhi visit that drew attention online. One such moment was her surprise at seeing a Bata store in the city.\n\nPosting a photo of the outlet, she wrote, \"I grew up with Bata shoes in Eswatini and in Mozambique. Had no idea it was such a universal brand.\"\n\nThe mention of her childhood in Eswatini and Mozambique prompted a wave of responses from Indian users who associated the brand with their own school memories.\n\nOne user replied, \"We grew up with Bata shoes, all my school shoes were Bata.\" Hooker responded, \"All my school shoes were bata.\"\n\nWho Is Sara Hooker?\n\nHooker is the co-founder and CEO of Adaption, an AI startup headquartered in San Francisco. She also leads Cohere Labs, the research arm of Cohere. Her earlier career includes work at Google Brain, where she contributed to machine learning research.\n\nHer work focuses on improving the efficiency, safety, and reliability of large language models, with particular emphasis on fairness, interpretability, robustness, and model efficiency at scale. She also studies algorithmic bias in machine learning systems.\n\nHooker serves on the machine learning advisory research board of Kaggle and is a member of the World Economic Forum council on the Future of Artificial Intelligence.\n\nIn 2025, Hooker co-founded Adaption, a company focused on developing AI systems capable of continuous real-time learning and efficient adaptation. Prior to this, she was Vice President of Research at Cohere, where she led the company's research division, Cohere For AI.\n\nDuring her tenure, she launched the Cohere For AI Scholars Program aimed at supporting emerging researchers in artificial intelligence."
  },
  {
    "source": "Benzinga",
    "company": "Cohere",
    "title": "SAP, Cohere Break Ground On Sovereign AI Solutions Worldwide - SAP (NYSE:SAP)",
    "date": "2026-02-10T19:42:59Z",
    "url": "https://www.benzinga.com/markets/large-cap/26/02/50522894/sap-cohere-break-ground-on-sovereign-ai-solutions-worldwide",
    "content": "SAP SE (NYSE:SAP) shares are up on Tuesday as the company is expanding its partnership with Cohere to launch sovereign AI solutions globally, starting in Canada.\n\nThis positive momentum occurs while major indices like the Dow Jones are experiencing gains, suggesting a favorable backdrop for SAP's initiatives.\n\n* SAP stock is showing downward bias. Where is SAP stock headed?\n\nSAP and Cohere Announce Global AI Solutions\n\nThe expanded partnership includes delivering full-stack sovereign AI solutions worldwide, which will enhance SAP's offerings in the public sector and regulated industries.\n\nThis collaboration builds on their previous integration of Cohere's AI models into SAP's EU AI Cloud, aiming to provide secure and scalable AI solutions tailored for global enterprises.\n\nThe partnership is particularly relevant for Canadian companies needing to harness AI while ensuring data security and sovereignty.\n\nBy embedding Cohere's automation capabilities into SAP's sovereign cloud infrastructure, the companies aim to address the challenges organizations face in managing sensitive data while innovating with AI.\n\nThe recent partnership with Cohere underscores SAP's commitment to innovation in the AI space, particularly in addressing the needs of regulated industries. This strategic move positions SAP as a key player in the growing market for secure and sovereign AI solutions.\n\nCathy Tough, country manager, SAP Canada, added, \"By integrating Cohere North's powerful enterprise platform into SAP's trusted sovereign cloud, we are removing the operational burden for our customers. This partnership provides sovereignty at global scale, empowering businesses and public sector entities to unlock the full potential of their data securely and drive the next wave of innovation.\"\n\nThe broader market is experiencing gains, with the Dow Jones up 0.69% and the S&P 500 rising 0.18%. SAP's stock is moving in line with these trends, indicating that investor sentiment is generally positive today.\n\nSAP Stock Shows Bearish Technical Signals\n\nSAP's stock is currently trading 4.7% below its 20-day simple moving average (SMA) and 10% below its 50-day SMA, indicating a bearish trend in the short term. Over the past 12 months, shares have decreased by 24.64%, and they are positioned closer to their 52-week lows than highs.\n\nThe RSI is at 42.00, which is considered neutral territory, suggesting that the stock is neither overbought nor oversold. Meanwhile, MACD is below its signal line, indicating bearish pressure on the stock.\n\nThe combination of neutral RSI and bearish MACD suggests mixed momentum.\n\nKey Resistance: $240 Key Support: N/A SAP Financial Update and Analyst Ratings\n\nSAP SE is slated to provide its next financial update on April 21, 2026.\n\nEPS Estimate: $1.92 (Up from $1.51 year-over-year) Revenue Estimate: $11.27 billion (Up from $9.48 billion YoY) Valuation: P/E of 28.5x (Indicates premium valuation) Analyst Consensus & Recent Actions:\n\nThe stock carries a Buy Rating with an average price target of $291. Recent analyst moves include:\n\nArgus Research: Buy (Maintains target to $320 on Oct. 24, 2025) Barclays: Overweight (Raises target to $348 on Oct. 24, 2025) JMP Securities: Market Outperform (Maintains target to $375 on Oct. 23, 2025)\n\nValuation Insight: While the stock trades at a premium P/E multiple, the strong consensus and rising estimates suggest analysts view the growth prospects as justification for the 38% upside to analyst targets.\n\nSAP's Benzinga Edge Scorecard Analysis\n\nBelow is the Benzinga Edge scorecard for SAP SE, highlighting its strengths and weaknesses compared to the broader market:\n\nValue: Weak (Score: 30.1) -- Trading at a steep premium relative to peers. Quality: Strong (Score: 90.84) -- Balance sheet remains healthy. Momentum: Weak (Score: 7.91) -- Stock is underperforming the broader market.\n\nThe Verdict: SAP SE's Benzinga Edge signal reveals a mixed outlook. While the strong Quality score indicates a healthy balance sheet, the weak Value and Momentum scores suggest that the stock is currently underperforming relative to its peers.\n\nETFs with Significant SAP Exposure\n\nSignificance: Because SAP carries significant weight in these funds, any significant inflows or outflows for these ETFs will likely force automatic buying or selling of the stock.\n\nSAP Price Action: SAP shares were up 1.05% at $212.61 at the time of publication on Tuesday, according to Benzinga Pro data.\n\nPhoto: Shutterstock\n\nMarket News and Data brought to you by Benzinga APIs\n\nTo add Benzinga News as your preferred source on Google, click here."
  },
  {
    "source": "凤凰网（凤凰新媒体）",
    "company": "Cohere",
    "title": "3.5亿！AI创企获种子轮融资，打造会进化的智能机器",
    "date": "2026-02-05T23:56:04Z",
    "url": "https://tech.ifeng.com/c/8qW0i7THPwq",
    "content": "本轮融资由Emergence Capital Partners领投，Mozilla Ventures、Fifty Years、Threshold Ventures、Alpha Intelligence Capital、E14 Fund和Neo等机构参与投资。该公司在融资后未透露其估值信息。\n\nAdaption Labs由前Cohere高管Sara Hooker与Sudip Roy联合创办。Cohere是一家加拿大的AI公司，专注于大语言模型及自然语言处理技术，为企业和开发者提供生成式AI解决方案。\n\n该公司的联合创始人兼CEO Sara Hooker介绍说，Adaption Labs的核心工作围绕自适应数据、自适应智能、自适应界面三大支柱展开，分别实现AI系统实时生成处理任务数据、按需动态调配算力、从用户交互中自主学习三大能力，以此摆脱对大型静态数据集和昂贵重训练的依赖。\n\n据《财富》杂志昨日报道，Adaption Labs将利用本轮种子融资，招聘更多AI研究人员和工程师，同时引入设计师，为AI系统开发不同于主流模型\"聊天栏\"形式的用户界面。\n\nAdaption Labs成立于2025年，由前Cohere高管Sara Hooker与Sudip Roy联合创办，Sudip Roy曾是Cohere推理计算总监。该公司总部位于旧金山，致力于打造能够实时适应并持续学习的AI系统，以区别于依赖大规模静态训练、计算开销高的主流AI模型。\n\nAdaption Labs的CTO Sudip Roy在提升AI系统效率方面拥有深厚经验。Hooker在采访中说：\"我的联合创始人能让GPU运行得非常快，这对我们至关重要，因为我们的系统需要实时处理数据。\"\n\n两位创始人希望开发计算资源需求更低、运行成本更可控的AI系统，使模型在实际使用中具备更高效率，而不必频繁进行昂贵的重训练。Hooker提到，最昂贵的计算环节是预训练，因为它需要大量计算资源和时间；相比之下，推理计算可以让每一单位算力获得更高回报。\n\n此外，《财富》杂志报道称，Adaption Labs还专注于构建可通过多种技术手段灵活适配特定任务的模型，提升模型对不同应用场景的适应能力，这也是公司名称\"Adaption\"的由来，这一理念与Cohere所强调的模型协同与适应性方向一致。\n\nSara Hooker提到，Adaption Labs围绕三大\"支柱\"展开工作：\n\n1、自适应数据：AI系统可实时生成和处理解决问题所需的数据，而非依赖大型静态数据集；\n\n▲Adaption Labs围绕工作的三大\"支柱\"（图源：Adaption Labs官网）\n\n报道称，Adaption Labs的方法并非通过耗时训练来调整模型全部内部权重，而是在模型响应查询的瞬间，即推理阶段改变其行为。模型核心权重保持不变，但系统仍可根据当前任务动态调整自身行为。无梯度学习（gradient-free learning）由此规避了微调和提示工程中的诸多复杂问题。\n\n截至目前，Adaption Labs已于昨天宣布完成约5000万美元（约合人民币3.47亿元）的种子轮融资，为其技术研发和团队扩张提供资金支持。\n\n在Cohere任职期间，Sara Hooker曾大力推动Aya项目。该项目汇集了来自119个国家的3000名计算机科学家，将先进AI能力应用于数十种主流模型表现不佳的语言，同时使用相对紧凑的模型规模。\n\n《财富》杂志提到，这项工作创造性的数据管理和训练方法可以在一定程度上弥补模型规模的不足，也为Sara Hooker创办新公司奠定了基础。\n\nHooker在接受采访时说，她希望构建能够持续学习的模型，无需昂贵的重新训练或微调，也不必像当前多数企业那样依赖大量提示和上下文工程，就能让AI系统适应特定用例。持续学习是AI领域尚未攻克的核心难题。Hooker直言：\"这或许是我研究过的最重要课题。\"\n\n\"如何在不修改权重的情况下更新模型？\"Sara Hooker提出这一问题。\n\n她认为，AI架构领域正在出现多项创新，使计算资源得以更高效地利用。\"我们正在摆脱仅仅把它视为一个模型的思维方式，\"她说，\"这是一个基于交互的系统，模型应当根据任务实时调整。\"\n\nHooker曾在谷歌DeepMind从事研究工作，自那时起，她就以反对AI领域\"规模至上\"的主流观点而知名。她在2020年发表的论文《硬件彩票（The Hardware Lottery）》提到，AI创新理念能否成功，往往取决于是否适配现有硬件条件，而非其自身价值。\n\n近期，她又发表论文《规模的缓慢消亡（On the Slow Death of Scaling）》，论证采用更优训练方法的小型模型，可能在性能上超越更大规模的模型。\n\n03.\n\n结语：AI行业正处于一个关键时期\n\nAdaption Labs并非唯一一家试图攻克持续学习难题的新型AI实验室。近年来，一批被称为\"新实验室\"的初创公司陆续出现，它们诞生于OpenAI、Anthropic和谷歌DeepMind等老牌机构取得突破之后。\n\nOpenAI高级研究员Jerry Tworek近期离职并创办了初创公司Core Automation，他同样说对构建能够持续学习的AI系统充满兴趣。此外，谷歌DeepMind前顶级研究员David Silver上个月离职，创办Ineffable Intelligence，该公司将专注于强化学习，即AI系统通过自身行动而非静态数据进行学习，在某些条件下也可能实现持续学习能力。\n\nSara Hooker说，AI行业正处于一个关键时期，技术进步不再仅依赖构建更大的模型，而在于打造能够更便捷、更经济地适应具体任务的系统。"
  },
  {
    "source": "The Logic",
    "company": "Cohere",
    "title": "Cohere Labs open sources smaller models that work in many languages - The Logic",
    "date": "2026-02-17T20:28:28Z",
    "url": "https://thelogic.co/briefing/cohere-labs-open-sources-smaller-models-that-work-in-many-languages/",
    "content": "The Tiny Aya family includes a system that's conversant in over 70 languages, as well as one that can power applications that need to switch between vernaculars. Cohere Labs also launched models that specialize in widely used languages from Africa, Europe, West Asia, South Asia and Asia-Pacific. (The Logic)\n\nTalking point: Cohere Labs, the non-profit research arm of Toronto AI firm Cohere, has long made multilingual AI systems a major focus. That includes languages spoken by large populations but that aren't common on the English-dominated Internet. While Cohere has made the new and previous Aya models freely available, the technical advances its staff and research collaborators made while developing the technology have helped improve the firm's commercial products. Tuesday's launch also shows the company can build small. Cohere claims Tiny Aya was trained on a single cluster of 64 Nvidia AI chips, and can run on a phone. The firm has touted its technology as more efficient than competitors to both develop and run, crucial to potential users working in low-connectivity zones like remote regions or the open seas."
  },
  {
    "source": "SaskToday.ca",
    "company": "Cohere",
    "title": "Shelly Palmer: Cohere bets on small, multilingual and offline",
    "date": "2026-02-17T16:57:39Z",
    "url": "https://www.sasktoday.ca/opinion/shelly-palmer-cohere-bets-on-small-multilingual-and-offline-11887577",
    "content": "Think About This: While OpenAI, Google and Anthropic fight over who can build the biggest brain, Cohere went small and wide.\n\nGreetings from New York. Cohere, the $5.5 billion Toronto-based AI company, just launched Tiny Aya: an open-source family of models that supports 70+ languages and runs entirely offline on devices. No cloud required. No API fees. While OpenAI, Google and Anthropic fight over who can build the biggest brain, Cohere went small and wide.\n\nRoughly 1.5 billion people speak English as a first or second language. The other five-ish billion do not, and most lack reliable broadband. Current AI models, including GPT-4, degrade noticeably outside English and a handful of European languages. Cohere is targeting the markets that got skipped: remote clinics in sub-Saharan Africa, agricultural systems in Southeast Asia, local commerce across Latin America. Offline-first means these models can actually reach them.\n\nWhether \"70+ languages\" is a genuine capability or a marketing number remains to be seen. GPT-4 claims multilingual support too, but performance in Yoruba or Bengali tells a different story. Google has Gemini Nano, Microsoft has Phi, and Meta keeps iterating on Llama, but none of them have packaged this many languages in a lightweight offline format. The enterprise AI market is projected to hit $150 billion by 2027, and most of that revenue will come from outside the United States. Cohere just built an interesting on-ramp.\n\nAs always, your thoughts and comments are both welcome and encouraged. -s\n\nP.S. If your organization is planning to deploy AI across multilingual markets, consider booking a custom AI Leadership Workshop. Each session covers model selection, on-device vs. cloud deployment, and multilingual performance trade-offs to help your team turn global AI ambitions into measurable impact. Learn more at shellypalmer.com/ai.\n\nAbout Shelly Palmer\n\nShelly Palmer is the Professor of Advanced Media in Residence at Syracuse University's S.I. Newhouse School of Public Communications and CEO of The Palmer Group, a consulting practice that helps Fortune 500 companies with technology, media and marketing. Named LinkedIn's \"Top Voice in Technology,\" he covers tech and business for Good Day New York, is a regular commentator on CNN and writes a popular daily business blog. He's a bestselling author, and the creator of the popular, free online course, Generative AI for Execs. Follow @shellypalmer or visit shellypalmer.com."
  },
  {
    "source": "Beritaja",
    "company": "Cohere",
    "title": "Cohere Launches A Family Of Open Multilingual Models",
    "date": "2026-02-17T09:48:51Z",
    "url": "https://beritaja.com/cohere-launches-a-family-of-open-multilingual-models-beritaja-405544.html",
    "content": "BERITAJA is a International-focused news website dedicated to reporting current events and trending stories from across the country. We publish news coverage on local and national issues, politics, business, technology, and community developments. Content is curated and edited to ensure clarity and relevance for our readers.\n\nEnterprise AI institution Cohere launched a caller family of multilingual models connected the sidelines of the ongoing India AI Summit. The models, dubbed Tiny Aya, are open-weight -- meaning their underlying codification is publically disposable for anyone to usage and modify -- support complete 70 languages, and could tally connected mundane devices for illustration laptops without requiring an net connection.\n\nThe model, launched by the company's investigation limb Cohere Labs, supports South Asian languages specified arsenic Bengali, Hindi, Punjabi, Urdu, Gujarati, Tamil, Telugu, and Marathi.\n\nThe guidelines exemplary contains 3.35 cardinal parameters -- a measurement of its size and complexity. Cohere has besides launched TinyAya-Global, a type fine-tuned to amended travel personification commands, for apps that require wide connection support. Regional variants information retired the family: TinyAya-Earth for African languages; TinyAya-Fire for South Asian languages; and TinyAya-Water for Asia Pacific, West Asia, and Europe.\n\n\"This attack allows each exemplary to create stronger linguistic grounding and taste nuance, creating systems that consciousness much earthy and reliable for the communities they are meant to serve. At the aforesaid time, each Tiny Aya models clasp wide multilingual coverage, making them elastic starting points for further adjustment and research,\" the institution said successful a statement.\n\nCohere noted that these models, which were trained connected a azygous cluster of 64 H100 GPUs (a type of high-powered spot by Nvidia) utilizing comparatively humble computing sources, are perfect for researchers and developers building apps for audiences that speak autochthonal languages. The models are could of moving straight connected devices, truthful developers could usage them to powerfulness offline translation. The institution noted that it built its underlying package to suit on-device usage, requiring little computing powerfulness than about comparable models.\n\nIn linguistically divers countries for illustration India, this benignant of offline-friendly capacity could unfastened up a divers group of applications and usage cases without the request for changeless net access.\n\nThe models are disposable connected HuggingFace, the celebrated level for sharing and testing AI models, and the Cohere Platform. Developers could download them connected HuggingFace, Kaggle, and Ollama for section deployment. The institution is besides releasing training and information datasets connected HuggingFace and plans to merchandise a method study detailing its training methodology.\n\nThe startup's CEO, Aidan Gomez, said past twelvemonth that the institution plans to spell nationalist \"soon.\" According to CNBC, the institution ended 2025 connected a precocious note, posting $240 cardinal successful yearly recurring revenue, pinch 50% maturation quarter-over-quarter passim the year."
  },
  {
    "source": "TechCrunch",
    "company": "Cohere",
    "title": "Cohere launches a family of open multilingual models",
    "date": "2026-02-17T09:07:03Z",
    "url": "https://techcrunch.com/2026/02/17/cohere-launches-a-family-of-open-multilingual-models/",
    "content": "Enterprise AI company Cohere launched a new family of multilingual models on the sidelines of the ongoing India AI Summit. The models, dubbed Tiny Aya, are open-weight -- meaning their underlying code is publicly available for anyone to use and modify -- support over 70 languages, and can run on everyday devices like laptops without requiring an internet connection.\n\nThe model, launched by the company's research arm Cohere Labs, supports South Asian languages such as Bengali, Hindi, Punjabi, Urdu, Gujarati, Tamil, Telugu, and Marathi.\n\nThe base model contains 3.35 billion parameters -- a measure of its size and complexity. Cohere has also launched TinyAya-Global, a version fine-tuned to better follow user commands, for apps that require broad language support. Regional variants round out the family: TinyAya-Earth for African languages; TinyAya-Fire for South Asian languages; and TinyAya-Water for Asia Pacific, West Asia, and Europe.\n\n\"This approach allows each model to develop stronger linguistic grounding and cultural nuance, creating systems that feel more natural and reliable for the communities they are meant to serve. At the same time, all Tiny Aya models retain broad multilingual coverage, making them flexible starting points for further adaptation and research,\" the company said in a statement.\n\nCohere noted that these models, which were trained on a single cluster of 64 H100 GPUs (a type of high-powered chip by Nvidia) using relatively modest computing sources, are ideal for researchers and developers building apps for audiences that speak native languages. The models are capable of running directly on devices, so developers can use them to power offline translation. The company noted that it built its underlying software to suit on-device usage, requiring less computing power than most comparable models.\n\nIn linguistically diverse countries like India, this kind of offline-friendly capability can open up a diverse set of applications and use cases without the need for constant internet access.\n\nThe models are available on HuggingFace, the popular platform for sharing and testing AI models, and the Cohere Platform. Developers can download them on HuggingFace, Kaggle, and Ollama for local deployment. The company is also releasing training and evaluation datasets on HuggingFace and plans to release a technical report detailing its training methodology.\n\nThe startup's CEO, Aidan Gomez, said last year that the company plans to go public \"soon.\" According to CNBC, the company ended 2025 on a high note, posting $240 million in annual recurring revenue, with 50% growth quarter-over-quarter throughout the year."
  },
  {
    "source": "k.sina.com.cn",
    "company": "Cohere",
    "title": "新浪人工智能热点小时报丨2026年02月14日00时_今日实时人工智能热点速递",
    "date": "2026-02-13T16:53:41Z",
    "url": "https://k.sina.com.cn/article_7857201856_1d45362c001902di3c.html",
    "content": "1、企业AI初创公司Cohere营收超目标，IPO势头渐起\n\nCohere公司首席执行官艾丹・戈麦斯尽管谷歌、Anthropic、OpenAI等竞争对手正在争夺市场份额，人工智能初创公司Cohere仍向投资者表示，其在企业客户领域的发展势头持续向好。2月投资者备忘录显示，Cohere去年年度经常性收入约达2.4亿美元，超出2亿美元的预期目标。备忘录提到，2025年全年该公司季度环比增速均超过50%。\"我们的核心理念在市场中得到了明显认可，\"公司在备忘录中写道，\"随着全球各受监管行业的机构选择Cohere作为其规模化部署安全人工智能的可靠合作伙伴，我们的销售渠道持续拓展。\n\nhttp://finance.sina.com.cn/stock/usstock/c/2026-02-13/doc-inhmsuve5731272.shtml\n\n美国劳工劳工部：将加强对劳动者的人工智能技能培训。\n\n据知情人士透露，法律人工智能公司Legora正洽谈新一轮融资，可能将其估值提高至60亿美元，为四个月前上一轮融资估值的三倍。Legora启动融资的时间距人工智能巨头Anthropic发布法律AI工具不到两周。该工具的推出引发数据和法律服务类股票大范围抛售，并加剧市场对相关技术将颠覆行业的担忧。这波由恐慌情绪驱动的抛售随后蔓延至软件、金融服务和资产管理板块，蒸发数千亿美元市值。\n\n中国文旅农业(00542)发布公告，随着全球人口结构老化、慢性疾病盛行、消费意识提升，以及人工智慧(AI)与数位科技的创新应用等因素，大健康产业正迎来前所未有的发展机遇。特别是在中国，随着老龄化进程加快及数字技术广泛应用，以\"银发经济\"为代表的大健康市场持续扩大。为积极响应国家\"健康中国\"战略导向，在稳固所有现有业务的前提下，集团致力发展大健康、数字医疗与康养服务，以AI科技为核心基础，构建\"智能健康设备+社区健康服务+数字化赋能\"的三维健康服务生态，推动健康管理从\"疾病治疗\"向\"预防调养\"的转型，以期该板块业务为集团收入做出贡献。\n\nhttp://finance.sina.com.cn/stock/hkstock/ggscyd/2026-02-13/doc-inhmsuve5731625.shtml\n\n基于硅芯片的人工智能在短时间内取得了巨大进步，推动了当今生成式AI聊天机器人、图像创建工具和自主智能体核心大语言模型的重大发展。现在，一家名为生物计算公司(The Biological Computing Co.)的初创企业认为，人工智能的未来在于使用真实活体神经元的生物计算系统，这将为新一代模型提供动力，比目前任何技术都更快、更便宜、适应性更强。TBC(该初创公司的简称)今天宣布，已从Primary Ventures获得2500万美元的种子轮融资，同时推出全球首个用于计算机视觉和生成式AI的生物计算平台。\n\n来源：@央视财经微博【#中国独角兽企业抢抓人工智能新机遇##我国独角兽企业冲击科学智能第一股#】2025年，我国独角兽企业达到409家，位居全球第二，占比近30%。黑湖科技与思朗科技，这两家独角兽企业，两位80后掌舵者抢抓\"人工智能+\"的创新机遇，赋能中国智造、探索科学智能。2026年初，在一份由世界经济论坛评出的\"AI应用之星\"名单中，与富士康、西门子等知名跨国公司并列在先进制造业一栏的，有一家来自中国的专精特新中小企业 -- -- 黑湖科技。创立还不足十年，这家企业就从最初的十几人团队，成长为服务三万多家制造业企业数字化转型的行业独角兽。\n\n据知情人士透露，国防初创公司护盾人工智能公司正洽谈融资至多10亿美元，此举可能使其估值在不到一年内翻倍以上。知情人士称，这轮融资将使护盾人工智能在融资前的估值达到约110亿美元，融资后估值最高可达120亿美元。谈判仍在进行中，交易条款可能发生变化。\n\n格隆汇2月13日｜美国联邦贸易委员会（FTC）正加速对微软(MSFT.O)的审查，作为对其是否通过云软件及人工智能产品（包括Copilot）非法垄断企业计算市场大规模份额的持续调查的一部分。知情人士称，该机构近几周已向微软在商业软件和云计算市场的竞争对手发出民事调查要求。这些要求包含一系列关于微软许可及其他商业行为的质询。至少六家公司收到了此类要求。通过这些相当于民事传票的指令，FTC正寻求证据证明微软使客户更难在竞争对手的云服务上使用Windows、Office等产品。部分知情人士称，该机构还要求提供有关微软将人工智能、安全和身份软件捆绑到Windows和Office等产品中的信息。\n\nhttp://finance.sina.com.cn/stock/bxjj/2026-02-13/doc-inhmszca5619241.shtml\n\n韩国IT巨头Kakao于2025年2月12日宣布与谷歌达成战略合作，旨在共同开发基于AndroidXR和Android移动体验的AI眼镜用户体验，并将前沿人工智能技术融入其中。这一合作预示着人工智能技术在移动设备和增强现实领域的深度融合，也为用户体验带来了新的可能性。AndroidXR：移动端AI的新入口？此次合作的核心在于将谷歌的AndroidXR平台与Kakao的AI技术相结合。AndroidXR作为谷歌在增强现实领域的重要布局，为开发者提供了丰富的工具和资源。Kakao的加入，无疑将加速AndroidXR在AI领域的应用落地。\n\n格隆汇2月13日丨中国文旅农业(00542.HK)公告，随着全球人口结构老化、慢性疾病盛行、消费意识提升，以及人工智慧(AI)与数位科技的创新应用等因素，大健康产业正迎来前所未有的发展机遇。特别是在中国，随着老龄化进程加快及数字技术广泛应用，以银发经济为代表的大健康市场持续扩大。为积极响应国家健康中国战略导向，在稳固所有现有业务的前提下，本集团致力发展大健康、数字医疗与康养服务，以AI科技为核心基础，构建智能健康设备+社区健康服务+数字化赋能的三维健康服务生态，推动健康管理从疾病治疗向预防调养的转型，以期该板块业务为集团收入做出贡献。\n\nhttp://finance.sina.com.cn/stock/bxjj/2026-02-13/doc-inhmsuvk0029016.shtml"
  },
  {
    "source": "CNBC",
    "company": "Cohere",
    "title": "Enterprise AI startup Cohere tops revenue target as momentum builds to IPO: Investor memo",
    "date": "2026-02-13T14:12:29Z",
    "url": "https://www.cnbc.com/2026/02/13/ai-startup-cohere-revenue-ipo.html",
    "content": "Cohere hit roughly $240 million in annual recurring revenue last year, surpassing its $200 million target, according to a February investor memo viewed by CNBC. It saw quarter-over-quarter growth of more than 50% throughout 2025, the memo said.\n\n\"Our thesis is clearly resonating in the market,\" the company wrote. \"Our sales pipeline continues to grow as global organizations across regulated sectors choose Cohere as their trusted partner for secure AI adoption at scale.\"\n\nFounded in Toronto in 2019, Cohere develops models and builds software tools for businesses. The company is backed by investors including Nvidia and Salesforce Ventures, and its valuation has swelled to roughly $7 billion.\n\nCohere's investor memo comes after CEO Aidan Gomez said in October that the startup hopes to make its public market debut \"soon.\" He told Bloomberg that he thinks investors would welcome a \"pure play AI investment opportunity.\"\n\nBut Cohere's competitors OpenAI and Anthropic are also weighing potential IPOs, according to people familiar with the companies' thinking. And they have not been shy about their ambitions to win in the enterprise market."
  },
  {
    "source": "The Manila times",
    "company": "Cohere",
    "title": "Colonel Chris Hadfield and Martin Kon to address students at the World Affairs Conference (WAC) on Wednesday, February 25 at Upper Canada College in Toronto",
    "date": "2026-02-20T15:00:17Z",
    "url": "https://www.manilatimes.net/2026/02/20/tmt-newswire/globenewswire/colonel-chris-hadfield-and-martin-kon-to-address-students-at-the-world-affairs-conference-wac-on-wednesday-february-25-at-upper-canada-college-in-toronto/2282040",
    "content": "In partnership with Branksome Hall and Upper Canada College, WAC is the longest-running student-led conference in North America. New this year: Livestream broadcast internationally.\n\nColonel Chris Hadfield, opening keynote speaker\n\nGet the latest news\n\ndelivered to your inbox\n\nSign up for The Manila Times newsletters\n\nBy signing up with an email address, I acknowledge that I have read and agree to the Terms of Service and Privacy Policy.\n\nA Canadian legend, Chris Hadfield's opening keynote speech is not to be missed. Colonel Hadfield is an astronaut, leader, author, and speaker on leadership, technology, and change. As a former Commander of the International Space Station, having flown three space missions and completed two spacewalks, and as NASA's Director of Operations in Russia, he brings years of experience to his work, sharing his expertise with young audiences. A renowned engineer and leader, Hadfield has contributed to the construction of numerous space stations and has received impressive awards from institutions such as NASA and the Order of Canada. Currently, Colonel Hadfield is also involved with the upcoming Artemis II mission to the moon, where CSA Astronaut Jeremy Hansen is part of the crew. With extensive leadership experience, Hadfield will leave WAC attendees with many stories and insights to reflect on and grow from.\n\nAdvertisement\n\nMartin Kon\n\nAdvertisement\n\nMartin Kon, an Upper Canada College alumnus and Branksome Hall parent, is the President Emeritus & Former COO of Cohere, a leading data-security-first AI company valued at over $7 billion, with offices in Toronto, Montreal, San Francisco, New York, London, Paris, and Seoul. Before joining Cohere, Martin served as CFO of YouTube, overseeing global strategy, finance, business operations and analytics. He previously served as Managing Partner at Boston Consulting Group (BCG) and Head of the Global Communications, Media and Technology Practice at Oliver Wyman. With extensive leadership across a variety of endeavours and his insights into the geopolitical implications of technology and AI in our rapidly changing world, Mr. Kon's keynote will leave a lasting impact on WAC attendees.\n\nToronto, Feb. 20, 2026 (GLOBE NEWSWIRE) -- The renowned World Affairs Conference (WAC) is pleased to announce that Colonel Chris Hadfield and Mr. Martin Kon will be keynotes at this year's student-led conference to be held at Upper Canada College in Toronto on Wednesday, February 25. The day-long conference is co-hosted with Branksome Hall and features thirteen speakers who will present on this year's theme,\n\nPower in Perspective. Secondary students from schools across the GTA will participate in person, and a live broadcast will be available for viewing internationally.\n\nAs North America's longest-running student-led conference, the World Affairs Conference aims to provide unforgettable opportunities for dialogue for thousands of inquisitive, curious, and globally minded high school students. Its mission is to motivate high school students to take initiative and proactively engage in world affairs, helping them recognize humanity's most pressing challenges by exposing them to the diverse global perspectives of inspiring role models. In the past, the conference has hosted incredible speakers such as Edward Snowden, Martin Luther King III, Geoffrey Hinton, Dr. Roberta Bondar and others. Notably, the conference will expand its reach this year with a livestream broadcast internationally, furthering WAC's commitment to engaging students in critical discussions and deepening their understanding of global issues.\n\nAdvertisement\n\nRuhani Mainra, WAC Co-Chair and student at Branksome Hall, notes that, \"This year's theme, Power in Perspective, allows students the chance to connect with role models who continue to challenge the status quo in their communities and beyond, exemplifying the importance of recognizing global viewpoints.\" Alyssa Dhanji, the second Co-Chair from Branksome Hall, said, \"We are truly honoured to host an incredible list of speakers ranging from our keynote speakers Colonel Chris Hadfield and Martin Kon, to plenary speakers including humanitarian advocates, environmentalists, and innovators.\" The theme and the diverse, globally-minded speaker lineup further underscore WAC's primary goal of achieving true international impact. In line with our goal of making the conference accessible to all, the conference is free for any interested grades 9-12 students from all school boards. Liyang Yin, WAC Co-Chair and student at Upper Canada College, says he \"hopes that this year's conference will inspire students to view existing challenges through novel lenses, allowing attendees to develop and pursue new, unexplored ideas.\" Similarly, Karan Maheswari, the second Co-Chair from Upper Canada College, said \"we aspire that students will use this unique opportunity to engage directly with role models, world leaders and industry experts to shape the future of global affairs.\"\n\nKeynote Speakers:\n\nColonel Chris Hadfield\n\nA Canadian legend, Chris Hadfield's opening keynote speech is not to be missed. Colonel Hadfield is an astronaut, leader, author, and speaker on leadership, technology, and change. As a former Commander of the International Space Station, having flown three space missions and completed two spacewalks, and as NASA's Director of Operations in Russia, he brings years of experience to his work, sharing his expertise with young audiences. A renowned engineer and leader, Hadfield has contributed to the construction of numerous space stations and has received impressive awards from institutions such as NASA and the Order of Canada. Currently, Colonel Hadfield is also involved with the upcoming Artemis II mission to the moon, where CSA Astronaut Jeremy Hansen is part of the crew. With extensive leadership experience, Hadfield will leave WAC attendees with many stories and insights to reflect on and grow from.\n\nAdvertisement\n\nMartin Kon\n\nMartin Kon, an Upper Canada College alumnus and Branksome Hall parent, is the President Emeritus & Former COO of Cohere, a leading data-security-first AI company valued at over $7 billion, with offices in Toronto, Montreal, San Francisco, New York, London, Paris, and Seoul. Before joining Cohere, Martin served as CFO of YouTube, overseeing global strategy, finance, business operations and analytics. He previously served as Managing Partner at Boston Consulting Group (BCG) and Head of the Global Communications, Media and Technology Practice at Oliver Wyman. With extensive leadership across a variety of endeavours and his insights into the geopolitical implications of technology and AI in our rapidly changing world, Mr. Kon's keynote will leave a lasting impact on WAC attendees.\n\nMedia Opportunities\n\nInterested media and press are invited to attend WAC throughout the day from 9:00 a.m. to 3:30 p.m. at Upper Canada College. Interview opportunities can be coordinated with speakers and other WAC participants. These interviews must be organized in advance and are subject to our speaker's availability and schedule throughout the conference day. Press and media will have access to and can listen to our keynotes and plenary speakers. This includes Colonel Hadfield's keynote from 9:15 to 10:00 a.m. and Mr. Kon's keynote from 2:30 to 3:15 p.m. Interested media should contact Shawna Delgaty or Sarah Baumann (see contacts below) for accreditation and additional details.\n\nAdvertisement\n\nWorld Affairs Conference Background and Resources\n\nSince the 1980s, WAC has reached over 10,000 students from more than 80 schools and 35 countries-and gained formal recognition from former Canadian Prime Minister Justin Trudeau and City of Toronto Mayor Olivia Chow. A detailed list of speakers is available on the WAC website. For more information, please visit the following:\n\n● Website: https://worldaffairscon.org/\n\n● LinkedIn: World Affairs Conference\n\nAdvertisement\n\n● Instagram: @worldaffairscon\n\nAttachments\n\nCONTACT: Shawna Delgaty, Associate Director, Communications"
  },
  {
    "source": "Shelly Palmer",
    "company": "Cohere",
    "title": "Cohere Bets on Small, Multilingual, and Offline",
    "date": "2026-02-17T14:48:41Z",
    "url": "https://shellypalmer.com/2026/02/cohere-bets-on-small-multilingual-and-offline/",
    "content": "Cohere, the $5.5 billion Toronto-based AI company, just launched Tiny Aya: an open-source family of models that supports 70+ languages and runs entirely offline on devices. No cloud required. No API fees. While OpenAI, Google, and Anthropic fight over who can build the biggest brain, Cohere went small and wide.\n\nRoughly 1.5 billion people speak English as a first or second language. The other five-ish billion do not, and most lack reliable broadband. Current AI models, including GPT-4, degrade noticeably outside English and a handful of European languages. Cohere is targeting the markets that got skipped: remote clinics in sub-Saharan Africa, agricultural systems in Southeast Asia, local commerce across Latin America. Offline-first means these models can actually reach them.\n\nWhether \"70+ languages\" is a genuine capability or a marketing number remains to be seen. GPT-4 claims multilingual support too, but performance in Yoruba or Bengali tells a different story. Google has Gemini Nano, Microsoft has Phi, and Meta keeps iterating on Llama, but none of them have packaged this many languages in a lightweight offline format. The enterprise AI market is projected to hit $150 billion by 2027, and most of that revenue will come from outside the United States. Cohere just built an interesting on-ramp."
  },
  {
    "source": "World Byte News",
    "company": "Cohere",
    "title": "AI minister seeks new investments through visits to Germany, Saudi Arabia, India - World Byte News",
    "date": "2026-02-17T00:26:35Z",
    "url": "https://worldbytenews.com/ai-minister-seeks-new-investments-through-visits-to-germany-saudi-arabia-india/",
    "content": "OTTAWA -- Artificial Intelligence Minister Evan Solomon is visiting Saudi Arabia and India after a trip to Germany as part of an effort to drum up new investment in Canadian tech.\n\nThe stops are \"about broadening our trade alliances and looking for investment ... in great Canadian infrastructure, great Canadian technology,\" Solomon said in an interview.\n\nHe was at the Munich Security Conference over the weekend, and will stop in Saudi Arabia before going to the AI Impact Summit in India.\n\nThe global conferences are happening as Prime Minister Mark Carney has drawn global attention for a speech in which he called for middle powers to work together against great-power economic coercion.\n\nSolomon said the main reason he went to Munich was to sign the Sovereign Technology Alliance, which he called a \"really important step\" in deepening alliances with Europe, and particularly with Germany.\n\nA government press release said through the alliance, Canada and Germany would \"deepen co-ordination with trusted partners to strengthen sovereign AI capacity and reduce strategic technology dependencies.\"\n\nCanada also agreed to expand bilateral co-operation on AI with Germany. In December at a G7 ministers' meeting in Montreal, Solomon signed an agreement with Germany, as well as new agreements with the European Union and the United Kingdom.\n\nSolomon said he has been working on establishing relationships with Saudi Arabia and working toward a \"framework to start increasing investment there.\" He visited Qatar and the United Arab Emirates last year, signing a memorandum of understanding with the UAE and issuing a joint statement with Qatar.\n\n\"The world is changing quickly. And we need to engage,\" he said, adding he plans to do so in both Saudi Arabia and India.\n\n\"We've got to open up new markets because some markets are getting harder to access because of tariffs. And our strategy is to build new trade alliances, drive new investment and show off Canadian talent.\"\n\nHe cited the significant contributions Canadians have historically made toward the development of modern AI, as well as the Toronto-based company Cohere. Cohere develops large language models, a type of generative AI focused on language.\n\n\"We're one of four countries in the world that have a functioning large language model,\" Solomon noted.\n\nCarney had initially been scheduled to attend the Munich conference, but cancelled the trip following a mass shooting in Tumbler Ridge, B.C.\n\nSolomon said he heard about Carney's Davos speech in every meeting he had in Munich.\n\n\"Absolutely all the speeches at Munich were essentially confirmations of that world view. And so the prime minister is a very important voice for Canada in the world, and people are looking for Canada in that leadership,\" he said.\n\nAsked whether Canada will also take a leadership role when it comes to AI governance, Solomon pointed to the work of Canadian Yoshua Bengio, one of the so-called godfathers of AI.\n\nThe Montreal-based Bengio, who has been a global advocate for AI safety and transparency, founded a new non-profit called LawZero that develops technical solutions for safe AI.\n\n\"People around the world are very interested in that,\" Solomon said, adding the text of the sovereign technology agreement he signed in Germany specifically mentions LawZero.\n\n\"The Germans are also very keen because they believe that Canada is taking a leadership role in those governance questions,\" he said.\n\nThis report by The Canadian Press was first published Feb. 16, 2026.\n\nAnja Karadeglija, The Canadian Press\n\nOTTAWA -- Artificial Intelligence Minister Evan Solomon is visiting Saudi Arabia and India after a trip to Germany as part of an effort to drum up new investment in Canadian tech. The stops are \"about broadening our trade alliances and looking for investment ... in great Canadian infrastructure, great Canadian technology,\" Solomon said in an Business, Canada\n\nOTTAWA -- Artificial Intelligence Minister Evan Solomon is visiting Saudi Arabia and India after a trip to Germany as part of an effort to drum up new investment in Canadian tech.\n\nThe stops are \"about broadening our trade alliances and looking for investment ... in great Canadian infrastructure, great Canadian technology,\" Solomon said in an interview.\n\nHe was at the Munich Security Conference over the weekend, and will stop in Saudi Arabia before going to the AI Impact Summit in India.\n\nThe global conferences are happening as Prime Minister Mark Carney has drawn global attention for a speech in which he called for middle powers to work together against great-power economic coercion.\n\nSolomon said the main reason he went to Munich was to sign the Sovereign Technology Alliance, which he called a \"really important step\" in deepening alliances with Europe, and particularly with Germany.\n\nA government press release said through the alliance, Canada and Germany would \"deepen co-ordination with trusted partners to strengthen sovereign AI capacity and reduce strategic technology dependencies.\"\n\nCanada also agreed to expand bilateral co-operation on AI with Germany. In December at a G7 ministers' meeting in Montreal, Solomon signed an agreement with Germany, as well as new agreements with the European Union and the United Kingdom.\n\nSolomon said he has been working on establishing relationships with Saudi Arabia and working toward a \"framework to start increasing investment there.\" He visited Qatar and the United Arab Emirates last year, signing a memorandum of understanding with the UAE and issuing a joint statement with Qatar.\n\n\"The world is changing quickly. And we need to engage,\" he said, adding he plans to do so in both Saudi Arabia and India.\n\n\"We've got to open up new markets because some markets are getting harder to access because of tariffs. And our strategy is to build new trade alliances, drive new investment and show off Canadian talent.\"\n\nHe cited the significant contributions Canadians have historically made toward the development of modern AI, as well as the Toronto-based company Cohere. Cohere develops large language models, a type of generative AI focused on language.\n\n\"We're one of four countries in the world that have a functioning large language model,\" Solomon noted.\n\nCarney had initially been scheduled to attend the Munich conference, but cancelled the trip following a mass shooting in Tumbler Ridge, B.C.\n\nSolomon said he heard about Carney's Davos speech in every meeting he had in Munich.\n\n\"Absolutely all the speeches at Munich were essentially confirmations of that world view. And so the prime minister is a very important voice for Canada in the world, and people are looking for Canada in that leadership,\" he said.\n\nAsked whether Canada will also take a leadership role when it comes to AI governance, Solomon pointed to the work of Canadian Yoshua Bengio, one of the so-called godfathers of AI.\n\nThe Montreal-based Bengio, who has been a global advocate for AI safety and transparency, founded a new non-profit called LawZero that develops technical solutions for safe AI.\n\n\"People around the world are very interested in that,\" Solomon said, adding the text of the sovereign technology agreement he signed in Germany specifically mentions LawZero.\n\n\"The Germans are also very keen because they believe that Canada is taking a leadership role in those governance questions,\" he said.\n\nThis report by The Canadian Press was first published Feb. 16, 2026.\n\nAnja Karadeglija, The Canadian Press"
  },
  {
    "source": "k.sina.com.cn",
    "company": "Cohere",
    "title": "新浪AI热点小时报丨2026年02月14日00时_今日实时AI热点速递",
    "date": "2026-02-13T16:56:30Z",
    "url": "https://k.sina.com.cn/article_7857201856_1d45362c001902di3e.html",
    "content": "1、擎天租订单总量预计突破5000单 机器人租赁或迎来\"史上最热春节\"\n\n中经记者 杨让晨 张家振 上海报道（擎天租方面预计，马年春节假期，平台订单规模预计将超过5000单。受访者/图）马年春节假期，或将成为机器人租赁行业有史以来最热闹的一个消费节点。机器人租赁平台擎天租公布的数据显示，截至2月12日，该平台已累计收到覆盖春节假期的订单超过1000单。据擎天租方面测算，综合当前已下单及预约情况，整个马年春节假期，平台订单规模预计将超5000单，平台整体GMV（商品交易总额）预计环比增涨80%。据了解，擎天租此前已推出\"999元全民机器人体验计划\"，该计划相关订单已占平台成交单量的约15%。\n\n（来源：爱范儿）二十年前，周星驰在《功夫》里用一个满脸淤青的阿星告诉我们：真正的绝世高手，往往藏在市井之间，只待任督二脉被打通的那一刻。二十年后，这位万中无一的武学奇才再次出现了。不过，它不是人。就在刚才，智元机器人在视频号上甩出了一段名为「绝世高手，马上下山」的视频。官方还特意标注了全程实景实拍，没用 CG 特效，也不是 AI 创作。视频画面开头，名为智元远征 A3 的人形机器人在练功房里来了个干脆利落的凌空飞踹，不仅踢碎了玻璃，踢开了马年的新春大门，估计也踢醒了不少人对国产机器人「腿脚不利索」的刻板印象。\n\n马斯克的银行家正在讨论解决合并后XAI债务问题的计划。\n\nCohere公司首席执行官艾丹・戈麦斯尽管谷歌、Anthropic、OpenAI等竞争对手正在争夺市场份额，人工智能初创公司Cohere仍向投资者表示，其在企业客户领域的发展势头持续向好。2月投资者备忘录显示，Cohere去年年度经常性收入约达2.4亿美元，超出2亿美元的预期目标。备忘录提到，2025年全年该公司季度环比增速均超过50%。\"我们的核心理念在市场中得到了明显认可，\"公司在备忘录中写道，\"随着全球各受监管行业的机构选择Cohere作为其规模化部署安全人工智能的可靠合作伙伴，我们的销售渠道持续拓展。\n\nhttp://finance.sina.com.cn/stock/usstock/c/2026-02-13/doc-inhmsuve5731272.shtml\n\n开放数据研究所（ODI）的一项研究发现，主流大语言模型无法为医疗、税务和福利等关键公共服务提供可靠信息。研究基于超过22000个大语言模型提示进行，这些提示旨在反映人们向生成式AI聊天机器人可能提出的问题类型，例如我如何申请全民信贷？。研究结果引发了对聊天机器人能否被信任提供准确政府服务信息的担忧。该研究发布时正值英国政府宣布与Meta和Anthropic建立合作关系，开发用于导航公共服务的AI智能体。ODI研究主任埃琳娜·辛珀尔表示：如果大语言模型要在面向公民的服务中安全使用，我们需要了解这项技术在哪些方面可以信任，在哪些方面不能信任。\n\n您好，基于讯飞星火大模型，讯飞智文支持一句话主题、长文本快速生成Word、PPT文档，从大纲构建、内容提炼、排版设计、配图生成到个性化演讲稿撰写一键完成，截至2025年1月，讯飞智文已服务数百万活跃个人用户以及超过150万企业用户；讯飞绘文为数百万创作者提供一站式解决方案，包括精准选题分析、高效内容生成、多平台发布及数据优化支持，达成优质内容的快速产出，欢迎登录讯飞智文（官网：https://zhiwen.xfyun.cn）、讯飞绘文（官网：https://turbodesk.xfyun.cn）体验。\n\n当王羲之的虚拟形象挥毫泼墨为现代家庭书写春联，当敦煌飞天的衣袂在算法驱动下翩然起舞，人工智能技术正以颠覆性姿态叩响文化遗产传承的大门，掀起一场传统与现代的深度对话。一、技术赋能：破解保护与传承的千年困局AI技术为文化遗产的存续提供了革命性解决方案。在保护层面，高精度三维扫描与多光谱成像技术可毫米级还原文物细节，建立不可篡改的数字孪生体。敦煌研究院已完成300个洞窟数字化采集，荆州文保中心通过AI辅助修复万余件脆弱木漆器，显著延长了遗产生命周期。在修复领域，生成对抗网络（GAN）能智能补全壁画残缺部分，山东工艺美术学院的\"天工开物\"模型甚至可模拟古代工艺逻辑，为陶瓷碎片拼接提供最优方案。\n\n快科技2月13日消息，前几天国产AI来了一波爆发，智谱GLM-5、Minimax 2.5及DeepSeek在11日同一天都发布了新的大模型，其中DeepSeek的自然最受关注。此前我们已经报道过了，这次更新主要是提升了上下文能力，达到了1M，而之前的DeepSee V3系列也就是128K，这方面相对前代V3系列大模型提升了7倍。今晚DeepSeek也在官方群中正式确认了这一点，表示网页及APP版正在测试新的长文本模型结构，支持1M上下文。同时DeepSeek还强调API服务还没变，还是V3.2系列大模型，只支持128K上下文。\n\nhttp://finance.sina.com.cn/tech/roll/2026-02-13/doc-inhmsuve5730995.shtml\n\n前言谁能想到，AI+短剧/漫剧，已经成为2026年最火的赛道之一！AIGC技术的爆发，彻底打破了短剧、漫剧的生产瓶颈 -- -- 以前拍一部短剧要几周，现在AI几天就能搞定；以前漫剧制作成本居高不下，现在AI加持下成本直降80%+。赛道风口之下，一批上市公司率先布局，抢占红利。今天就给大家整理了AI+短剧/漫剧领域10家热门公司，每家的核心优势、最新布局一次性说透，不管是关注行业、还是找布局方向，都能直接参考！1. 捷成股份：AI短剧量产王者，出海势头猛核心亮点：主打\"AI+短剧规模化生产\"，手握丰富IP+技术底座，出海布局超前。"
  },
  {
    "source": "The Logic",
    "company": "Cohere",
    "title": "CPP Investments backed both Anthropic and Cohere in AI rivals' megarounds - The Logic",
    "date": "2026-02-13T15:58:13Z",
    "url": "https://thelogic.co/briefing/cpp-investments-backed-both-anthropic-and-cohere-in-ai-rivals-megarounds/",
    "content": "The Canada Pension Plan Investment Board disclosed Friday that it put US$50 million into Toronto-headquartered Cohere via the second close of its US$600-million September 2025 funding round, which valued it at US$7 billion. The asset manager also invested US$50 million in San Francisco-based Anthropic's US$13-billion Series F round the same month, which valued it at US$183 billion. (The Logic)\n\nTalking point: Anthropic and Cohere are two of the handful of firms developing their own foundation AI models. Each company grew rapidly last year in the enterprise market -- Anthropic by selling its own coding tool and powering others, and Cohere by customizing models and agentic tools for large firms. While they're taking different approaches to building their technology and business, the Silicon Valley firm dwarfs the Toronto one in terms of revenue, funds raised and valuation. CPP Investments joins a growing number of investors that have broken an unwritten rule of Silicon Valley by investing in two or more model makers amid the generative AI boom, although the pairing is typically Anthropic and OpenAI. The Canadian pension fund has long backed Toronto's Radical Ventures, Cohere's earliest and one of its larger investors."
  },
  {
    "source": "Seeking Alpha",
    "company": "Cohere",
    "title": "Nvidia-backed Cohere beats revenue target for 2025",
    "date": "2026-02-13T15:20:10Z",
    "url": "https://seekingalpha.com/news/4551925-nvidia-backed-cohere-beats-revenue-target-for-2025",
    "content": "Cohere achieved about $240M in annual recurring revenue in 2025, surpassing its target of $200M, CNBC reported, citing a February investor memo.\n\nThe Canadian AI startup -- which builds large language models, or LLMs, and customizes them for businesses -- saw quarter-over-quarter growth of more than\n\nCohere's capital-efficient model lets customers run models on managed cloud or their own hardware, reducing high infrastructure costs and allowing more investment in customer acquisition and R&D.\n\nCohere exceeded its $200M revenue target with $240M annual recurring revenue and achieved about 70% gross margins, expanding 25 basis points year-over-year.\n\nCohere plans rapid European expansion, AI agent platform development, and is considering a near-term public market debut while competing with OpenAI and Anthropic for enterprise clientele."
  },
  {
    "source": "ABC Money",
    "company": "Cohere",
    "title": "Ottawa's Push For AI Ethics In Public Contracts Puts Startups On Notice | ABC Money",
    "date": "2026-02-10T09:41:01Z",
    "url": "https://www.abcmoney.co.uk/2026/02/ottawas-push-for-ai-ethics-in-public-contracts-puts-startups-on-notice/",
    "content": "The move by Ottawa to raise ethical AI criteria for public contracts has caught the attention of startup owners who are attempting to navigate complicated regulations and prepare for the future. On the surface, the Cohere deal's announcement caused enthusiasm, but it also subtly shook the smaller companies that were observing from the sidelines.\n\nThe most well-known AI candidate in Canada, Cohere, has excellent resources and is in a good position to align. Smaller AI inventors, however, who have more resilient teams and promising technologies, can suddenly be shut out of the very potential they helped to develop. The ethics themselves, which almost all founders concur are crucial, are not the source of the worry. Pace, capacity, and whether the procurement door is closing more quickly than they can run are the key considerations.\n\nThe Canadian government declared its intention to support ethical, domestic AI in federal systems by entering into a non-binding agreement with Cohere. The action was presented as being exceptionally successful in demonstrating national resilience and trust. But it also established a high standard that many fledgling businesses are still struggling to comprehend, let alone reach.\n\nOttawa's requirements place a strong emphasis on privacy safeguards, auditability, system explainability, and data transparency. There is no denying the significance of these ideas. Retrofitting infrastructures to pass ethics inspections is more than a patch; it's a transformation for businesses with small technical teams and little funding. In order to be eligible for compliance, one entrepreneur subtly revealed that they could have to increase their engineering budget.\n\nCanada has made ambitious and visionary investments in its AI ecosystem over the last ten years. Research chairs, collaborations, and grants are all geared toward a flourishing, innovation-based IT sector. However, the shift from cultivation to curation is occurring at this time, and not all players are ready for the new requirements.\n\nThe current situation is eerily reminiscent of past cycles in telecom or banking, where well-meaning regulatory hurdles ultimately favored industry titans over upstarts. In this situation, the price of being ethically prepared could unintentionally turn into the price of being disqualified. For businesses that are already in line with public requirements but are unable to pay for the systems and documentation that Ottawa now demands, that is very upsetting.\n\nCompanies such as Cohere are establishing connections with the government through strategic alliances. Their North platform is being positioned as an AI layer that prioritizes privacy and is prepared for implementation in many ministries. It is exceedingly adaptable and immediately meets almost all regulatory requirements. As a result, it is both an obvious favorite and an unintentional filter. Startups may find themselves pitching into a vacuum if they lack comparable contacts or infrastructure.\n\nThe policy initiative from Ottawa is not without merit. Public sector trust depends on ethical boundaries in light of recent worldwide AI mishaps, including chatbots gone awry, data breaches, and synthetic misinformation. The government is taking proactive management of a fast developing field by incorporating these standards immediately. However, being a leader also entails helping others who are attempting to advance, not simply those who are already above the threshold.\n\nThe budget for November will provide important clues. The expenditure plan might specify which companies are silently excluded and which are permitted to grow, given that $59 million in AI-related contracts are being reviewed. That deadline seems more like a cliff to many.\n\nA number of trade associations have asked for more precise guidelines since the announcement. Instead of a wall, they want a ramp. A few recommendations are advice clinics, technical toolkits, staggered deployment, and short-term carveouts for startups. These actions could raise the bar gradually while maintaining velocity. Without compromising ethics, it would be a noticeably better method of onboarding innovation.\n\nEven big players are keeping a careful eye on things. IBM Canada issued a warning about infrastructural deficiencies that can put domestic companies at a competitive disadvantage when compared to their US or UK counterparts. Visa advised against overregulating, particularly in areas where AI and fraud detection are intertwined. While civil groups want an AI regulator with authority, writers' unions have called for stricter copyright laws. It's a full table, and startups are frequently ignored, especially those without policy teams.\n\n\"We built something that solves real problems, but now I worry the only thing we're missing is a lobbyist,\" said a founder I once sat across from. I still remember that line. It illustrates a conflict between speed and support rather than between ethics and innovation.\n\nThe largest obstacle for early-stage firms is still obtaining capital. Ethical compliance, however, can now come in second. If Ottawa's ethics-first model was built with ladders rather than just barriers, it might be a shining example. Multi-tiered procurement access points, ethics tooling grants, and sandboxes would all indicate an ecosystem that prioritizes both integrity and agility.\n\nAI will be crucial in the upcoming years for everything from climate monitoring to healthcare. Careful leadership is a smart move for Canada. However, in this instance, care is realizing that ethical AI is more than just deciding who creates the best product. It concerns who is given the opportunity to construct at all.\n\nOttawa won't only be setting standards if ethics can empower rather than exclude participation and policy can change from exclusion to enablement. It will be constructing an inclusive, aspirational, and incredibly successful future."
  },
  {
    "source": "Tech News | Startups News",
    "company": "Cohere",
    "title": "Best Tech Companies to Work for in 2026 (Including Top Startups) - Tech Startups",
    "date": "2026-01-30T23:16:01Z",
    "url": "https://techstartups.com/2026/01/30/best-tech-companies-to-work-for-in-2026-including-top-startups/",
    "content": "With hundreds of thousands of companies globally, choosing where to work in tech has become far more complex than it was just a few years ago. After multiple hiring booms and corrections, job seekers are no longer optimizing solely for brand names, perks, or short-term compensation. Stability, leadership quality, and long-term relevance matter more than ever.\n\nThe companies shaping the best places to work in 2026 are not all the same. Some offer depth and predictability at scale; others provide ownership and momentum at growth-stage startups; and a smaller group operates at the frontier of regulation-heavy or infrastructure-driven industries. What makes a company \"best\" depends heavily on where you are in your career and the kind of environment you thrive in.\n\nThis guide highlights tech companies and startups that show strong, forward-looking signals as places to build meaningful careers heading into 2026. Rather than ranking employers, companies are grouped by the types of career experiences they tend to offer, helping candidates make more informed and intentional decisions.\n\nChoosing the \"best\" companies to work for is inherently subjective. Compensation, perks, and personal preferences vary widely. Rather than relying on popularity rankings or paid awards, this list focuses on observable signals that suggest long-term workplace health heading into 2026.\n\nThis is not a ranking. It is a curated list.\n\nTo apply this framework consistently, we evaluated companies across six core signal categories:\n\nEach company was assessed across six signal categories:\n\nWe looked for companies that are:\n\nStrong workplace cultures tend to follow leadership behavior. Signals include:\n\nRemote and hybrid work are no longer differentiators. Execution matters.\n\nRather than isolated reviews, we focused on patterns:\n\nMaintaining workplace quality is challenging without financial discipline.\n\nThis list emphasizes forward momentum rather than past glory.\n\nThis list is not sponsored, ranked, or paid. Inclusion is based on publicly observable signals and long-term workplace indicators, not compensation packages, perks, or brand recognition.\n\nThis guide intentionally avoids using company size or brand recognition as a proxy for career quality. In recent years, many large technology companies have reached a scale at which hiring, growth, and individual impact vary significantly across teams and cycles.\n\nInstead of focusing on legacy tech giants, this list prioritizes organizations that currently show stronger signals around execution clarity, ownership, talent density, and skill compounding -- factors that increasingly matter for long-term career leverage heading into 2026.\n\nWebsite: https://www.homelight.com\n\nHiring Status: Actively Hiring\n\nCompany Stage: Scale Stage\n\nIndustry: Real Estate Technology, FinTech\n\nTeam Size: 500-1,000 employees\n\nHeadquarters: San Francisco\n\nAdditional Locations: Scottsdale\n\nWork Model: Hybrid\n\nHomeLight is the essential technology platform used by hundreds of thousands of homebuyers and sellers to partner with top real estate agents and navigate every step of the real estate journey -- from finding a trusted agent to securing competitive financing and closing on time.\n\nEach year, HomeLight facilitates billions of dollars in residential real estate transactions, operating at the intersection of data, marketplace infrastructure, and financial services. Its platform helps consumers make higher-confidence decisions during one of life's most consequential events: buying or selling a home.\n\nThe company's long-term vision is ambitious but grounded: a world where every real estate transaction is simple, certain, and satisfying.\n\nHomeLight's position in the real estate ecosystem gives it a structural advantage as an employer. Unlike consumer apps driven by trends or discretionary spending, HomeLight focuses on transaction reliability and platform trust, which tend to produce more stable operating environments over time.\n\nRather than chasing rapid expansion, the company has invested in building durable infrastructure that supports agents, lenders, and consumers alike. This discipline shows up internally through clearer priorities, fewer reactive pivots, and a stronger connection between day-to-day work and real customer outcomes.\n\nBased on observable hiring and employee-experience indicators, HomeLight shows several positive workplace signals:\n\nThese signals suggest a culture optimized for steady output and long-term contribution rather than short-term spikes in intensity.\n\nWhile individual experiences vary, recurring themes from employee feedback include:\n\nLooking ahead, HomeLight appears positioned as a durable employer rather than a cyclical bet. As real estate transactions continue to digitize and professionalize, the company's platform-first approach aligns well with long-term industry needs.\n\nFor candidates seeking:\n\nHomeLight represents a compelling place to build a career heading into 2026.\n\nWebsite: https://stripe.com\n\nHiring Status: Selectively Hiring\n\nCompany Stage: Late-Stage Private\n\nIndustry: FinTech, Payments Infrastructure\n\nTeam Size: 5,000+ employees\n\nHeadquarters: San Francisco\n\nGlobal Presence: North America, Europe, Asia-Pacific\n\nWork Model: Hybrid / Distributed\n\nStripe is a global payments and financial infrastructure platform that enables businesses of all sizes to accept payments, manage revenue, prevent fraud, and expand internationally. Its technology underpins millions of online transactions every day, powering everything from early-stage startups to some of the world's largest internet companies.\n\nBeyond payments, Stripe has expanded into billing, treasury, embedded finance, tax compliance, and identity verification -- positioning itself as a foundational layer of the modern internet economy. The company processes hundreds of billions of dollars in transactions annually, operating largely behind the scenes while remaining mission-critical to its customers.\n\nStripe's long-term ambition is clear: to increase the internet's GDP by making it easier for businesses to start, scale, and operate globally.\n\nStripe's culture is shaped by its role as infrastructure. Reliability, correctness, and long-term thinking are not optional -- they are core requirements. This reality has driven an internal environment that favors depth, rigor, and sustained ownership over short-term output theater.\n\nRather than optimizing for speed at all costs, Stripe places a premium on thoughtful execution and durable systems. Teams are expected to build software that can withstand scale, regulation, and global complexity -- which tends to attract engineers, product leaders, and operators who value substance over surface-level wins.\n\nSeveral observable signals suggest Stripe remains a strong long-horizon employer:\n\nThe culture favors individuals who are comfortable thinking deeply and working independently.\n\nAcross employee feedback and public commentary, recurring themes include:\n\nStripe is often described as demanding, but fair -- particularly for those who value craft and long-term impact.\n\nLooking ahead, Stripe's relevance is unlikely to diminish. As online commerce grows more regulated, global, and complex, demand for robust financial infrastructure will continue to rise.\n\nFor candidates seeking:\n\nStripe remains one of the most durable career bets in tech heading into 2026.\n\nWebsite: https://www.databricks.com\n\nHiring Status: Actively Hiring\n\nCompany Stage: Late-Stage Private\n\nIndustry: Data Platforms, AI Infrastructure\n\nTeam Size: 6,000+ employees\n\nHeadquarters: San Francisco\n\nGlobal Presence: North America, Europe, Asia\n\nWork Model: Hybrid\n\nDatabricks is a data and AI platform built around the open-source Apache Spark ecosystem, designed to help organizations unify data engineering, analytics, and machine learning on a single platform.\n\nThe company's lakehouse architecture is used by thousands of enterprises to manage massive data workloads, train AI models, and deploy analytics at scale. Databricks operates deep within customer infrastructure, powering mission-critical systems rather than surface-level applications.\n\nIts long-term vision centers on making data and AI accessible, reliable, and production-ready for organizations operating at a global scale.\n\nDatabricks benefits from being foundational rather than consumer-facing. Its customers rely on the platform to run essential workloads, creating a strong internal bias toward correctness, reliability, and thoughtful system design.\n\nThe company has matured beyond early hypergrowth, placing greater emphasis on sustainable execution, research-to-product translation, and enterprise trust. This shift has reshaped the internal environment to reward patience, collaboration, and technical depth.\n\nDatabricks shows several workplace signals associated with long-term stability:\n\nThese signals point to a company optimized for sustained output rather than volatility.\n\nEmployee feedback frequently emphasizes:\n\nThe environment tends to favor builders who enjoy solving hard problems without obvious answers.\n\nAs AI moves from experimentation into production environments, Databricks' role in the enterprise data stack is likely to become even more central.\n\nFor candidates seeking:\n\nDatabricks offers a strong platform for career growth heading into 2026.\n\nWebsite: https://www.cloudflare.com\n\nHiring Status: Actively Hiring\n\nCompany Stage: Public Company\n\nIndustry: Internet Infrastructure, Cybersecurity, Network Services\n\nTeam Size: 3,000+ employees\n\nHeadquarters: San Francisco\n\nGlobal Presence: Worldwide\n\nWork Model: Hybrid / Remote-Friendly\n\nCloudflare is a global internet infrastructure company that provides security, performance, and reliability services for websites, applications, and networks. Its platform sits between end users and customers' infrastructure, helping protect against cyberattacks, improve performance, and ensure uptime.\n\nThe company operates one of the world's largest networks, with data centers in hundreds of cities. Cloudflare's services are used by organizations ranging from individual developers to large enterprises and public-sector institutions.\n\nAt its core, Cloudflare's mission is to help build a better internet by making online services faster, more secure, and more reliable.\n\nCloudflare's role as internet infrastructure shapes its internal culture. Reliability, security, and correctness are central to both the product and the way teams operate. Unlike consumer-facing companies driven by trends, Cloudflare's work is tied to long-term internet fundamentals.\n\nThe company has steadily expanded its platform, adding products while maintaining a focus on network integrity and operational discipline. This approach tends to create a more stable internal environment, particularly for engineers, security professionals, and infrastructure-focused roles.\n\nObservable workplace indicators suggest:\n\nThese signals point to a culture that values preparation and responsibility over speed alone.\n\nCommon themes from employee feedback include:\n\nThe environment tends to suit individuals who prefer structured problem-solving and long-term impact.\n\nAs cybersecurity risks increase and internet reliability becomes even more critical, Cloudflare's core services are likely to remain essential.\n\nFor candidates seeking:\n\nCloudflare represents a durable and relevant place to build a career heading into 2026.\n\nWebsite: https://www.notion.so\n\nHiring Status: Selectively Hiring\n\nCompany Stage: Late-Stage Private\n\nIndustry: Productivity Software, Collaboration Tools\n\nTeam Size: 600+ employees\n\nHeadquarters: San Francisco\n\nGlobal Presence: Distributed team\n\nWork Model: Remote-First\n\nNotion is a productivity and collaboration platform that combines documents, databases, and project management into a single, flexible workspace. It is used by individuals, startups, and large organizations to organize information, manage workflows, and collaborate across teams.\n\nThe product is known for its adaptability, allowing users to customize workspaces rather than conform to rigid software structures. This flexibility has contributed to strong adoption across a wide range of use cases, from personal knowledge management to company-wide operations.\n\nNotion's long-term focus centers on helping people and teams think, write, and work together more effectively.\n\nNotion's internal culture closely mirrors its product philosophy: clarity, intentionality, and flexibility. Growth has been relatively measured compared to other productivity startups, allowing the company to maintain focus and avoid excessive internal complexity.\n\nThe company operates with a remote-first model, emphasizing written communication and asynchronous collaboration. This structure tends to reward thoughtful planning and independent execution rather than constant real-time coordination.\n\nKey workplace indicators include:\n\nThese signals suggest a culture designed to reduce noise and maximize clarity.\n\nRecurring themes from employee experiences include:\n\nThe culture tends to favor individuals who value depth, clarity, and long-term thinking.\n\nAs teams continue consolidating tools and prioritizing flexibility, Notion's position as a central workspace remains strong.\n\nFor candidates seeking:\n\nNotion offers a compelling place to build a career heading into 2026.\n\nWebsite: https://www.figma.com\n\nHiring Status: Actively Hiring\n\nCompany Stage: Late-Stage Private\n\nIndustry: Design Software, Collaboration Tools\n\nTeam Size: 1,000+ employees\n\nHeadquarters: San Francisco\n\nGlobal Presence: North America, Europe, Remote\n\nWork Model: Hybrid\n\nFigma is a collaborative design platform that enables teams to design, prototype, and iterate on digital products in real time. Its browser-based approach allows designers, engineers, and product managers to work together on shared files without the friction of traditional desktop-based tools.\n\nThe platform is widely used across the software industry, from startups to large enterprises, and has become a core part of modern product development workflows. In addition to design, Figma supports prototyping, developer handoff, and collaborative feedback, helping teams move from idea to implementation more efficiently.\n\nFigma's broader vision centers on making design accessible, collaborative, and central to how digital products are built.\n\nFigma's internal culture reflects the collaborative nature of its product. Cross-functional work among design, engineering, and product teams is a core part of how the company operates, not an afterthought.\n\nAs the company has grown, it has maintained a focus on product quality and user experience, while gradually introducing more structure to support scale. This balance tends to create an environment where expectations are high, but roles and ownership are clearly defined.\n\nObservable workplace signals include:\n\nThese indicators suggest a culture focused on craft and execution rather than speed alone.\n\nCommon themes from employee experiences include:\n\nThe environment tends to appeal to individuals who care deeply about how products are built and experienced.\n\nAs digital collaboration and design remain central to software development, Figma's role in product teams is likely to remain strong.\n\nFor candidates seeking:\n\nFigma represents a solid long-term career option heading into 2026.\n\nWebsite: https://www.snowflake.com\n\nHiring Status: Selectively Hiring\n\nCompany Stage: Public Company\n\nIndustry: Cloud Data Platforms, Enterprise Software\n\nTeam Size: 6,000+ employees\n\nHeadquarters: Bozeman, Montana\n\nGlobal Presence: North America, Europe, Asia\n\nWork Model: Hybrid\n\nSnowflake is a cloud-based data platform that enables organizations to store, analyze, and share large volumes of data across multiple cloud environments. Its architecture separates compute from storage, allowing customers to scale data workloads more efficiently.\n\nThe platform is used primarily by enterprises to support analytics, data science, and business intelligence use cases. Snowflake operates as a core component of many organizations' data infrastructure, supporting mission-critical reporting and decision-making.\n\nThe company's long-term focus is on helping organizations derive value from data while reducing operational complexity.\n\nSnowflake's evolution from rapid growth to operational maturity has shaped a more structured internal environment. As a public company serving large enterprises, reliability, security, and predictability are central to how teams operate.\n\nThe work often involves complex systems, long-term customer relationships, and enterprise-scale challenges. This environment often favors engineers, product leaders, and operators who value stability, clarity, and depth over constant change.\n\nKey workplace indicators include:\n\nThese signals suggest a culture aligned with long-term enterprise needs rather than short-term experimentation.\n\nRecurring themes from employee feedback include:\n\nThe environment is often described as demanding but organized.\n\nAs data infrastructure remains foundational to AI, analytics, and enterprise decision-making, Snowflake's platform is likely to remain relevant.\n\nFor candidates seeking:\n\nSnowflake offers a reliable career path heading into 2026.\n\nWebsite: https://ramp.com\n\nHiring Status: Actively Hiring\n\nCompany Stage: Growth Stage (Private)\n\nIndustry: FinTech, Corporate Spend Management\n\nTeam Size: 1,000+ employees\n\nHeadquarters: New York City\n\nAdditional Locations: Miami, San Francisco\n\nWork Model: Hybrid\n\nCareer Signal: High ownership, execution-driven, efficiency-first environment\n\nRamp is a financial technology company that provides corporate cards, expense management, bill payments, and accounting automation tools designed to help businesses control spending and improve financial visibility.\n\nStartups and mid-sized companies use the platform to centralize financial operations, automate workflows, and reduce unnecessary expenses. Ramp positions its product around cost efficiency, transparency, and automation, rather than rewards-based spending incentives.\n\nRamp's broader mission is to help businesses save time and money by making financial operations more efficient and data-driven.\n\nWhy Ramp Stands Out as a Place to Work in 2026\n\nRamp's internal culture is closely aligned with its product philosophy. Efficiency, clarity, and accountability are emphasized across teams, shaping an environment where output and impact matter more than appearances.\n\nThe company has been vocal about prioritizing capital efficiency and disciplined growth. This mindset tends to translate into clearer internal priorities, tighter execution loops, and less tolerance for unnecessary complexity.\n\nObservable workplace indicators include:\n\nThese signals suggest a culture optimized for execution rather than experimentation for its own sake.\n\nCommon themes from employee experiences include:\n\nThe environment tends to suit individuals who thrive in performance-oriented settings.\n\nAs companies continue to scrutinize spending and operational efficiency, Ramp's value proposition remains relevant.\n\nFor candidates seeking:\n\nRamp represents a solid opportunity heading into 2026.\n\nThis environment is best suited for candidates who value execution, ownership, and operational clarity over experimentation or flexible pace.\n\nWebsite: https://www.brex.com\n\nHiring Status: Selectively Hiring\n\nCompany Stage: Growth Stage (Private)\n\nIndustry: FinTech, Financial Services\n\nTeam Size: 1,000+ employees\n\nHeadquarters: San Francisco\n\nGlobal Presence: North America, International\n\nWork Model: Hybrid\n\nBrex is a financial services company that offers corporate cards, spend management, and cash management tools designed primarily for startups and fast-growing businesses. Its platform helps companies centralize financial operations, manage expenses, and enforce budget controls across teams.\n\nOriginally built to serve venture-backed startups, Brex expanded its product suite over time to support a broader range of companies. The platform became widely used by finance teams seeking integrated tools for managing company spending and cash flow.\n\nOn January 22, Brex was acquired by Capital One for $5.15 billion, marking a turning point for the fintech after years of positioning itself as a standalone public-market contender.\n\nBrex's stated mission is to simplify financial operations for modern businesses.\n\nBrex's internal environment has evolved in parallel with its business strategy. After a period of rapid expansion, the company has shifted toward a more explicit focus and tighter execution, thereby reshaping internal priorities and team structures.\n\nThis transition has resulted in a more defined operating model, with greater emphasis on core products and sustainable growth. For candidates, this can translate into clearer expectations and a more stable working environment than during earlier hypergrowth phases.\n\nKey workplace indicators include:\n\nThese signals suggest that the organization is moving toward maturity.\n\nRecurring themes from employee experiences include:\n\nThe environment may appeal to individuals who value structure and focus within a growth-stage company.\n\nAs financial tools remain central to startup and business operations, Brex continues to play a significant role in the ecosystem.\n\nFor candidates seeking:\n\nBrex offers a relevant, if more measured, career path heading into 2026.\n\nWebsite: https://www.rippling.com\n\nHiring Status: Actively Hiring\n\nCompany Stage: Growth Stage (Private)\n\nIndustry: Workforce Management, HR Software, IT Operations\n\nTeam Size: 2,000+ employees\n\nHeadquarters: San Francisco\n\nGlobal Presence: North America, Europe\n\nWork Model: Hybrid\n\nRippling is a workforce management platform that combines human resources, IT, and finance operations into a single system. The company's software helps businesses manage employee onboarding, payroll, benefits, devices, apps, and access controls from one centralized platform.\n\nBy unifying fragmented systems across multiple vendors, Rippling aims to reduce operational complexity for growing companies. Its platform is used by startups and mid-sized businesses that need tighter integration between people operations and IT infrastructure.\n\nRippling's long-term focus is on simplifying how organizations manage and scale their workforce.\n\nRippling's culture is shaped by its emphasis on execution and system-level thinking. The company builds software that touches critical internal operations for customers, placing a premium on accuracy, reliability, and resolution speed.\n\nInternally, this often translates into high expectations and a fast-paced environment, balanced by clear ownership and well-defined goals. Teams are encouraged to move quickly while remaining accountable for outcomes.\n\nObservable workplace indicators include:\n\nThese signals point to an environment that rewards discipline and follow-through.\n\nRecurring themes from employee experiences include:\n\nThe culture tends to suit individuals who thrive in high-expectation settings.\n\nAs workforce complexity increases and companies seek integrated solutions, Rippling's platform remains well-positioned.\n\nFor candidates seeking:\n\nRippling offers a challenging but potentially rewarding environment heading into 2026.\n\nWebsite: https://scale.com\n\nHiring Status: Actively Hiring\n\nCompany Stage: Late-Stage Private\n\nIndustry: AI Infrastructure, Data Operations\n\nTeam Size: 900+ employees\n\nHeadquarters: San Francisco\n\nGlobal Presence: United States, International\n\nWork Model: Hybrid\n\nScale AI provides data infrastructure services that help organizations build, train, and deploy artificial intelligence models. The company is best known for its data labeling and data management capabilities, which support machine learning systems across autonomous vehicles, natural language processing, and computer vision.\n\nScale works with enterprise customers and government organizations, operating primarily behind the scenes to ensure that AI systems are trained on high-quality, well-structured data. Its services prioritize reliability, accuracy, and scalability over consumer-facing features.\n\nScale AI's core mission is to accelerate the development of artificial intelligence by improving the quality and usability of training data.\n\nScale AI occupies a foundational position within the AI ecosystem. Rather than building end-user applications, the company focuses on infrastructure that enables AI systems to function reliably in production environments.\n\nThis positioning influences internal priorities. Accuracy, process discipline, and customer trust matter as much as speed. Teams often work closely with enterprise and public-sector customers, which adds additional operational and compliance considerations.\n\nKey workplace indicators include:\n\nThese signals suggest a culture grounded in practical AI deployment rather than experimentation alone.\n\nCommon themes from employee feedback include:\n\nThe work appeals to those interested in applied AI infrastructure.\n\nAs AI systems continue moving into regulated and mission-critical environments, demand for reliable data infrastructure is likely to persist.\n\nFor candidates seeking:\n\nScale AI offers a stable and relevant platform heading into 2026.\n\nWebsite: https://gusto.com\n\nHiring Status: Selectively Hiring\n\nCompany Stage: Late-Stage Private\n\nIndustry: Payroll, HR Software, Benefits Administration\n\nTeam Size: 2,000+ employees\n\nHeadquarters: San Francisco\n\nGlobal Presence: United States\n\nWork Model: Hybrid / Remote-Friendly\n\nGusto is a payroll and human resources platform designed primarily for small and mid-sized businesses. The company provides tools for payroll processing, benefits administration, hiring, onboarding, and compliance, helping employers manage core people operations in a centralized system.\n\nGusto's platform is widely used by SMBs across the United States, particularly those seeking an integrated and approachable alternative to legacy payroll providers. The company emphasizes usability, regulatory accuracy, and customer support in a domain where mistakes can be costly.\n\nGusto's stated mission is to create a world where work empowers a better life by simplifying complex employer responsibilities.\n\nGusto operates in a mature, compliance-heavy market where trust and accuracy are critical. This reality tends to shape a more deliberate internal culture, prioritizing correctness and customer impact over rapid experimentation.\n\nOver time, Gusto has built a reputation for people-centered values combined with operational discipline. Growth has been comparatively measured, allowing teams to focus on product stability and long-term customer relationships rather than aggressive expansion.\n\nObservable workplace indicators include:\n\nThese signals suggest a culture oriented toward sustainability rather than spikes in intensity.\n\nRecurring themes from employee experiences include:\n\nThe environment often appeals to those seeking meaningful impact without constant urgency.\n\nPayroll and HR compliance remain durable needs for businesses of all sizes. As regulations evolve, demand for reliable platforms like Gusto is likely to persist.\n\nFor candidates seeking:\n\nGusto offers a steady, credible career path through 2026.\n\nWebsite: https://plaid.com\n\nHiring Status: Selectively Hiring\n\nCompany Stage: Late-Stage Private\n\nIndustry: FinTech Infrastructure, Financial Data Platforms\n\nTeam Size: 1,000+ employees\n\nHeadquarters: San Francisco\n\nGlobal Presence: North America, Europe\n\nWork Model: Hybrid\n\nPlaid is a financial data connectivity platform that securely enables applications to connect with users' bank accounts and financial institutions. Its APIs are widely used across the fintech ecosystem to support payments, personal finance tools, lending, and account verification.\n\nOperating primarily behind the scenes, Plaid provides the infrastructure that enables consumers to securely and standardly share financial data with apps. Its platform plays a foundational role in many modern financial services products.\n\nPlaid's focus centers on making financial data more accessible, secure, and practical while navigating a highly regulated environment.\n\nPlaid's position at the intersection of finance, data security, and regulation shapes a culture that values correctness, trust, and long-term thinking. The company operates in an environment where mistakes can have significant consequences, influencing how teams approach product development and operations.\n\nThis context tends to produce a more methodical internal cadence, with strong emphasis on security reviews, compliance, and cross-functional coordination.\n\nKey workplace indicators include:\n\nThese signals point to a culture optimized for trust and durability.\n\nCommon themes from employee feedback include:\n\nThe environment tends to suit individuals who value precision and long-term impact.\n\nAs financial regulation increases and data-sharing standards evolve, Plaid's infrastructure role is likely to remain central.\n\nFor candidates seeking:\n\nPlaid represents a durable and relevant place to build a career heading into 2026.\n\nWebsite: https://www.anthropic.com\n\nHiring Status: Actively Hiring\n\nCompany Stage: Growth Stage (Private)\n\nIndustry: Artificial Intelligence Research, AI Safety\n\nTeam Size: 700+ employees\n\nHeadquarters: San Francisco\n\nAdditional Locations: London\n\nWork Model: Hybrid\n\nAnthropic is an artificial intelligence research company focused on developing large language models with an emphasis on safety, reliability, and interpretability. The company is best known for Claude, a family of AI models designed for general-purpose reasoning and assistance.\n\nFounded by former OpenAI researchers, Anthropic positions itself on the idea of building AI systems that are helpful, honest, and harmless. Its work spans model research, alignment techniques, and deployment of AI systems for enterprise and consumer use.\n\nAnthropic operates at the frontier of AI research while maintaining a stated commitment to long-term safety and responsible development.\n\nAnthropic's research-driven mission shapes a culture that prioritizes caution, depth, and long-term thinking. Unlike many fast-moving AI startups that prioritize rapid commercialization, Anthropic places significant emphasis on research rigor and safety considerations.\n\nThis emphasis tends to influence internal decision-making, with longer planning horizons and a more deliberate pace relative to consumer-first AI companies. Teams are often composed of researchers, engineers, and policy-focused professionals working closely together.\n\nObservable workplace indicators include:\n\nThese signals suggest a culture optimized for thoughtful progress rather than rapid iteration alone.\n\nCommon themes from employee experiences include:\n\nThe environment appeals to individuals motivated by research depth and societal implications.\n\nAs scrutiny around AI safety, governance, and reliability increases, Anthropic's focus may become increasingly relevant.\n\nFor candidates seeking:\n\nAnthropic offers a distinctive and serious place to build a career heading into 2026.\n\nWebsite: https://www.perplexity.ai\n\nHiring Status: Actively Hiring\n\nCompany Stage: Growth Stage (Private)\n\nIndustry: AI Search, Information Retrieval\n\nTeam Size: 200+ employees\n\nHeadquarters: San Francisco\n\nGlobal Presence: Distributed team\n\nWork Model: Hybrid / Remote-Friendly\n\nPerplexity is an AI-powered search and answer engine designed to provide direct, cited responses to user questions. Rather than returning a list of links, the product synthesizes information from multiple sources into concise answers, often with references.\n\nThe platform is positioned as an alternative interface for accessing web information, blending traditional search with conversational AI. Perplexity has gained attention for its emphasis on transparency through citations and source attribution.\n\nThe company's broader goal is to improve how people discover and understand information online.\n\nPerplexity operates at the intersection of search, AI, and information quality -- an area with both technical and editorial complexity. The company's small size relative to incumbents shapes a builder-centric environment where individuals can have a visible impact.\n\nGrowth has been rapid, but teams remain relatively lean, which tends to require broader ownership and faster feedback loops. This environment often suits engineers and product builders who are comfortable with ambiguity and iteration.\n\nKey workplace indicators include:\n\nThese signals suggest a culture optimized for experimentation and fast learning.\n\nRecurring themes from employee experiences include:\n\nThe environment often appeals to builders who enjoy shipping and learning quickly.\n\nAs search and information access continue to evolve, Perplexity occupies a visible and competitive niche.\n\nFor candidates seeking:\n\nPerplexity offers meaningful upside heading into 2026, though it comes with the trade-offs typical of growth-stage startups.\n\nWebsite: https://cohere.com\n\nHiring Status: Actively Hiring\n\nCompany Stage: Growth Stage (Private)\n\nIndustry: Artificial Intelligence, Enterprise Software\n\nTeam Size: 500+ employees\n\nHeadquarters: Toronto\n\nAdditional Locations: San Francisco, London\n\nWork Model: Hybrid / Remote-Friendly\n\nCohere is an artificial intelligence company that develops large language models and AI tools primarily for enterprise use cases. Unlike consumer-focused AI platforms, Cohere concentrates on helping businesses integrate language models into their own products and workflows through APIs and customized deployments.\n\nThe company works with organizations that require greater control over data handling, privacy, and model behavior. Its offerings are often used for tasks such as text generation, classification, summarization, and semantic search within enterprise environments.\n\nCohere's stated focus is on bringing AI capabilities to businesses in a way that aligns with enterprise requirements and governance standards.\n\nCohere's enterprise-first strategy shapes a more measured internal operating model. Serving business customers with production requirements emphasizes reliability, documentation, and long-term support rather than rapid consumer feature churn.\n\nThis positioning tends to influence hiring and team structure, favoring engineers, researchers, and product leaders who are comfortable working within customer constraints and longer deployment cycles.\n\nWebsite: https://replit.com\n\nHiring Status: Actively Hiring\n\nCompany Stage: Growth Stage (Private)\n\nIndustry: Developer Tools, Education Technology\n\nTeam Size: 300+ employees\n\nHeadquarters: San Francisco\n\nGlobal Presence: Distributed team\n\nWork Model: Remote-First\n\nReplit is a developer platform that lets users write, run, and deploy code directly in a web browser. The platform supports multiple programming languages and is designed to reduce setup friction for coding, collaboration, and learning.\n\nReplit is used by individual developers, students, educators, and teams to prototype applications, collaborate on code, and experiment with new ideas. Its browser-based approach lowers barriers to entry for software development and enables real-time collaboration.\n\nThe company's broader mission is to make software development more accessible and immediate.\n\nReplit's product philosophy strongly influences its internal culture. The company emphasizes rapid iteration, experimentation, and direct user feedback, reflecting the needs of a developer-centric audience.\n\nAs a remote-first organization, Replit relies heavily on written communication and asynchronous collaboration. Teams tend to operate with significant autonomy, which can create both flexibility and high responsibility for individual contributors.\n\nKey workplace indicators include:\n\nThese signals suggest an environment geared toward experimentation and ownership.\n\nRecurring themes from employee experiences include:\n\nThe culture tends to appeal to self-directed builders.\n\nWebsite: https://www.vanta.com\n\nHiring Status: Actively Hiring\n\nCompany Stage: Growth Stage (Private)\n\nIndustry: Security, Compliance Automation\n\nTeam Size: 600+ employees\n\nHeadquarters: San Francisco\n\nGlobal Presence: Distributed\n\nWork Model: Hybrid / Remote-Friendly\n\nVanta is a security compliance automation platform that helps companies achieve and maintain certifications such as SOC 2, ISO 27001, HIPAA, and other industry standards. The platform automates evidence collection, monitoring, and reporting, reducing the manual burden traditionally associated with compliance.\n\nVanta is widely used by startups and growing companies that need to demonstrate security and compliance to customers, partners, and regulators. Its software integrates with common cloud services and internal tools to continuously monitor compliance status.\n\nThe company's focus is on making security and compliance more accessible, continuous, and less disruptive for modern organizations.\n\nVanta operates in a compliance-driven market where consistency, trust, and reliability matter more than speed alone. This shapes an internal culture that emphasizes calm execution, clear processes, and predictable delivery.\n\nBecause compliance requirements tend to grow over time rather than disappear, Vanta benefits from durable demand. That durability often translates into steadier internal planning and fewer abrupt shifts in direction.\n\nObservable workplace indicators include:\n\nThese signals suggest a culture optimized for long-term sustainability.\n\nRecurring themes from employee experiences include:\n\nThe environment often appeals to those who prefer structure over volatility.\n\nWebsite: https://www.openai.com\n\nHiring Status: Actively Hiring\n\nCompany Stage: Late-Stage Private\n\nIndustry: Artificial Intelligence Research & Products\n\nTeam Size: 2,000+ employees\n\nHeadquarters: San Francisco\n\nGlobal Presence: United States, International\n\nWork Model: Hybrid\n\nOpenAI is an artificial intelligence research and deployment company focused on developing advanced AI systems, including large language models and related products. Its technologies are used across a wide range of applications, from research and software development to education and enterprise tools.\n\nThe organization operates at the frontier of AI capability development, balancing research, product deployment, and infrastructure at a global scale. OpenAI's work has had a significant influence on the broader AI ecosystem.\n\nIts stated mission is to ensure that artificial general intelligence benefits all of humanity.\n\nOpenAI's scale and influence create an environment unlike most tech companies. The pace is intense, expectations are high, and the technical challenges are substantial. At the same time, the opportunity to work on frontier systems is unmatched.\n\nInternal priorities can shift as research, safety, infrastructure, and product considerations evolve. This makes the environment demanding, but also uniquely educational for those seeking exposure to cutting-edge AI development.\n\nObservable workplace indicators include:\n\nThese signals suggest a culture that rewards resilience and adaptability.\n\nCommon themes from employee experiences include:\n\nThe environment tends to suit individuals comfortable with ambiguity and intensity.\n\nOpenAI is likely to remain central to AI development and policy discussions.\n\nFor candidates seeking:\n\nOpenAI offers a rare, high-impact -- though demanding -- career path heading into 2026.\n\nWebsite: https://carbonhealth.com\n\nHiring Status: Selectively Hiring\n\nCompany Stage: Growth Stage (Private)\n\nIndustry: Health Technology, Clinical Care\n\nTeam Size: 1,500+ employees\n\nHeadquarters: San Francisco\n\nGlobal Presence: United States\n\nWork Model: Hybrid\n\nCarbon Health is a healthcare technology company that operates a network of primary care clinics supported by proprietary software. The company combines in-person care with digital tools for scheduling, telehealth, and patient management.\n\nIts model aims to modernize healthcare delivery by improving access, coordination, and patient experience. Carbon Health operates within a heavily regulated industry, requiring close alignment between clinical operations and technology systems.\n\nThe company focuses on improving healthcare access and outcomes through technology-enabled care.\n\nCarbon Health sits at the intersection of technology and healthcare delivery, which shapes a culture grounded in real-world constraints and patient impact. Unlike pure software companies, its work is directly tied to clinical outcomes.\n\nOver time, the company has refined its operating model, placing greater emphasis on sustainability and operational discipline. This shift has influenced internal structures and priorities.\n\nKey workplace indicators include:\n\nThese signals point to a culture shaped by responsibility rather than speed.\n\nRecurring themes from employee experiences include:\n\nThe culture often appeals to those motivated by mission and accountability.\n\nAs healthcare continues to adopt digital tools and navigate regulations, companies like Carbon Health remain relevant.\n\nFor candidates seeking:\n\nCarbon Health offers a grounded and meaningful career path heading into 2026.\n\nThere is no single \"best\" company to work for in tech -- only better fits for different stages of a career. As the industry continues to mature and recalibrate, the most important signals are no longer headline valuations or brand prestige, but the environments where skills compound, ownership is real, and leadership decisions are visible.\n\nThe companies highlighted in this guide represent a range of career paths for 2026. Some offer stability and depth at scale, others provide momentum and responsibility in fast-growing organizations, and a smaller set operates in long-horizon sectors where pressure is high but learning curves are unmatched. Each category carries trade-offs, and understanding those trade-offs matters more than chasing any single name.\n\nUltimately, the right choice depends on what you're optimizing for -- learning velocity, impact, stability, or long-term optionality. By focusing on signals rather than hype, candidates can make more intentional decisions about where to invest their time and energy as the next phase of the tech industry takes shape.\n\nAs hiring patterns continue to evolve, clarity -- not consensus -- is the real advantage."
  },
  {
    "source": "The Logic",
    "company": "Cohere",
    "title": "Cohere also joins Hanwha Ocean's bid to sell Canada submarines - The Logic",
    "date": "2026-01-27T20:04:09Z",
    "url": "https://thelogic.co/briefing/cohere-also-joins-hanwha-oceans-bid-to-sell-canada-submarines/",
    "content": "The Toronto-based firm and South Korean defence contractor said they will jointly develop AI models to \"enhance submarine operations and shipyard efficiency.\" If successful, Cohere could become a supplier to Hanwha, said Hanwha Canada CEO Glenn Copeland. (The Logic)\n\nTalking point: Hanwha Ocean is competing with Germany's TKMS to sell the Canadian navy as many as 12 patrol submarines worth a reported $60 billion. Foreign firms that win large Canadian defence contracts typically have domestic spending obligations; Hanwha plans to use its work with Cohere to help meet them if it wins the submarine deal, Copeland said. Cohere has now joined both submarine bids, after signing a deal with TKMS earlier this month to work on AI models for naval vessels. Any work Cohere does with Hanwha would be exclusive, Copeland said. To boost its case to Ottawa, the South Korean firm on Tuesday also announced tie-ups with Algoma Steel, MDA Space, Telesat and PV Labs."
  },
  {
    "source": "The Korea Times",
    "company": "Cohere",
    "title": "Hanwha signs 5 MOUs with Canadian firms to boost submarine project odds - The Korea Times",
    "date": "2026-01-27T06:03:21Z",
    "url": "https://www.koreatimes.co.kr/business/companies/20260127/hanwha-signs-5-mous-with-canadian-firms-to-boost-submarine-project-odds",
    "content": "Hanwha Systems CEO Son Jae-il, second from left, Hanwha Ocean CEO Kim Hee-cheul, fourth from left, and Cohere co-founder Ivan Zhang, third from left, pose after the three firms signed a memorandum of understanding for a strategic partnership on marine artificial intelligence, in Toronto, Monday (local time). From left are Deputy Minister of Innovation, Science and Economic Development Canada Philip Jennings, Son, Zhang, Kim, Korean Minister of Trade, Industry and Resources Kim Jung-kwan and Minister of Economic Development, Job Creation and Trade for Ontario Victor Fedeli. Courtesy of Hanwha Group\n\nHanwha Group has sealed multiple deals with Canadian steel, artificial intelligence (AI), satellite communication and aerospace firms to strengthen bilateral partnerships, as Canada approaches a decision between Korea and Germany for its submarine construction project.\n\nThe signings in Toronto on Monday (local time) were part of a Korea-Canada industrial cooperation forum. A representative of the presidential office and the ministers of industry and defense participated in the event, emphasizing the Korean government's strong commitment to winning the bidding process.\n\nHanwha Group's shipbuilding arm, Hanwha Ocean, and defense systems affiliate, Hanwha Systems, jointly signed five memorandums of understanding (MOUs) with Canadian companies. The agreements outline proposals to provide industrial and technological benefits to Canadian firms, aligning with Ottawa's \"Buy Canadian\" initiative amid rising global trade protectionism.\n\nHanwha Ocean signed an MOU with Canadian steelmaker Algoma Steel. Under the agreement, the two companies will cooperate on the construction of a new steel mill in Canada and the supply of steel for next-generation Canadian submarines.\n\nThe project will also support the development of new maintenance, repair and overhaul infrastructure aimed at sustaining the Canadian maritime fleet. Hanwha Ocean plans to invest up to $250 million in the partnership.\n\nHanwha Ocean CEO Kim Hee-cheul said at the forum that the partnership will enable \"stable and long-term steel-producing infrastructures that will contribute to preparing a submarine force for not just the present but also future generations of Canada.\"\n\nHanwha Ocean and Hanwha Systems reached a tripartite agreement with Canadian AI firm Cohere. Valued at more than $7 billion and backed by investors including Nvidia and Oracle, Cohere agreed to work with the Korean firms to develop a new AI model for submarine production planning, design and manufacturing.\n\nThe new system will be built on Cohere's large language model and large multimodal model technologies.\n\nWith Canadian satellite communication firm Telesat, Hanwha System agreed to jointly develop a competitive low earth orbit (LEO) satellite network for not just Canada but the global market. Telesat, aiming to launch 198 LEO satellites by the end of the year, will also cooperate with the Korean firm to introduce a new military LEO satellite communication system for Korea.\n\nHanwha System also signed a MOU with tech company MDA Space to generate synergy based on the latter's software-defined satellite platform Aurora. Another MOU with imaging solutions company PV Labs targets advancing electro-optical and infrared sensor technologies for future defense programs.\n\nPresidential chief of staff Kang Hoon-sik, who attended the forum, said bilateral ties should now move beyond trade and investment expansion toward more future-oriented areas such as AI transformation and clean energy development.\n\nHe pointed to the automobile industry as a linchpin of the partnership, adding that deeper cooperation in the sector would help both countries strengthen their growth engines in North America.\n\nMinister of Trade, Industry and Energy Kim Jung-kwan, who also attended the forum, said Korea is \"ready to accelerate cooperation with Canada in hydrogen-based production, infrastructure and mobility businesses, leveraging Canada's abundant natural resources.\"\n\n\"We will strengthen parts and supply chains for internal combustion, electric vehicles and hydrogen vehicles in partnership with Canada,\" Kim said.\n\nVictor Fedeli, minister of economic development, job creation and trade for Ontario, recounted during the forum his recent visit to Hanwha Ocean's production hub in Geoje, South Gyeongsang Province.\n\n\"I recently returned from Korea, where one of the highlights of my political career was touring a Hanwha submarine. Once I stepped inside, they practically had to pull me out by my hair. It was truly impressive,\" he said."
  },
  {
    "source": "World Byte News",
    "company": "Cohere",
    "title": "South Korean sub maker Hanwha signs agreement with Algoma Steel - World Byte News",
    "date": "2026-01-26T23:30:07Z",
    "url": "https://worldbytenews.com/south-korean-sub-maker-hanwha-signs-agreement-with-algoma-steel/",
    "content": "OTTAWA -- South Korean submarine maker Hanwha announced Monday it had signed a flurry of partnership agreements with Canadian companies, including Sault Ste. Marie's tariff-battered Algoma Steel, as it vies for a massive Canadian military procurement contract.\n\nHanwha Oceans said it signed a memorandum of understanding with Algoma that pledges $275 million in financial support to stand up a new structural steel beam mill.\n\nThe pact also declares Hanwha's intention to purchase steel products for both the construction of Canada's new submarine fleet and the maintenance infrastructure on both coasts to support it.\n\nBoth commitments come on the condition that Hanwha wins Ottawa's massive, multi-billion-dollar procurement contract to supply the Royal Canadian Navy with up to 12 modern submarines.\n\nThe company said in a news release Monday that the agreement is worth some $345 million. The proposed beam mill would allow Algoma to supply steel for Canadian infrastructure, housing and road projects.\n\nThe MOU also mandates that once Algoma establishes the new facility, the Canadian steel company must make annual payments to Hanwha Ocean worth three per cent of net sales from the mill for a decade.\n\nU.S. President Donald Trump's aggressive steel tariffs have disrupted Algoma's business model and effectively shut it out of the U.S. market, resulting in the company recently announcing layoffs for some 1,000 workers.\n\nThe federal and Ontario governments have also provided Algoma with $500 million in financing to help support the company through this tumultuous period.\n\nHanwha Systems, another arm of the massive South Korean conglomerate, also signed co-operation agreements on Monday with Telesat, MDA Space, Cohere and PV Labs -- all while a South Korean delegation of government and business officials visited Toronto.\n\nRival German bidder TKMS has meanwhile signed teaming agreements with Canadian companies related to its submarine procurement bid, including Quebec manufacturer Marmen and Cohere, an artificial intelligence company.\n\nThe battle for the lucrative submarine contract has heated up over the past year. It's possible the contract could be awarded as early as this year.\n\nThis report by The Canadian Press was first published Jan. 26, 2026.\n\nKyle Duggan, The Canadian Press\n\nOTTAWA -- South Korean submarine maker Hanwha announced Monday it had signed a flurry of partnership agreements with Canadian companies, including Sault Ste. Marie's tariff-battered Algoma Steel, as it vies for a massive Canadian military procurement contract. Hanwha Oceans said it signed a memorandum of understanding with Algoma that pledges $275 million in financial support Business, Canada\n\nOTTAWA -- South Korean submarine maker Hanwha announced Monday it had signed a flurry of partnership agreements with Canadian companies, including Sault Ste. Marie's tariff-battered Algoma Steel, as it vies for a massive Canadian military procurement contract.\n\nHanwha Oceans said it signed a memorandum of understanding with Algoma that pledges $275 million in financial support to stand up a new structural steel beam mill.\n\nThe pact also declares Hanwha's intention to purchase steel products for both the construction of Canada's new submarine fleet and the maintenance infrastructure on both coasts to support it.\n\nBoth commitments come on the condition that Hanwha wins Ottawa's massive, multi-billion-dollar procurement contract to supply the Royal Canadian Navy with up to 12 modern submarines.\n\nThe company said in a news release Monday that the agreement is worth some $345 million. The proposed beam mill would allow Algoma to supply steel for Canadian infrastructure, housing and road projects.\n\nThe MOU also mandates that once Algoma establishes the new facility, the Canadian steel company must make annual payments to Hanwha Ocean worth three per cent of net sales from the mill for a decade.\n\nU.S. President Donald Trump's aggressive steel tariffs have disrupted Algoma's business model and effectively shut it out of the U.S. market, resulting in the company recently announcing layoffs for some 1,000 workers.\n\nThe federal and Ontario governments have also provided Algoma with $500 million in financing to help support the company through this tumultuous period.\n\nHanwha Systems, another arm of the massive South Korean conglomerate, also signed co-operation agreements on Monday with Telesat, MDA Space, Cohere and PV Labs -- all while a South Korean delegation of government and business officials visited Toronto.\n\nRival German bidder TKMS has meanwhile signed teaming agreements with Canadian companies related to its submarine procurement bid, including Quebec manufacturer Marmen and Cohere, an artificial intelligence company.\n\nThe battle for the lucrative submarine contract has heated up over the past year. It's possible the contract could be awarded as early as this year.\n\nThis report by The Canadian Press was first published Jan. 26, 2026.\n\nKyle Duggan, The Canadian Press"
  },
  {
    "source": "Pulse 2.0",
    "company": "Cohere",
    "title": "FlexTecs Raises Strategic Investment From Cohere Capital To Expand Services",
    "date": "2026-01-24T18:37:59Z",
    "url": "https://pulse2.com/flextecs-cohere-capital/",
    "content": "FlexTecs, a provider of recovery audit, contract compliance, and payment accuracy software and services, said it has received a strategic investment from Cohere Capital, a Boston-based private equity firm focused on technology and tech-enabled services businesses.\n\nThe company said its platform and audit services review more than $1 trillion in client transactions and recover or prevent more than $1 billion in incorrect payments each year, positioning FlexTecs as an increasingly prominent player in what it describes as a modernized, technology-first approach to the audit recovery market. FlexTecs said its audit team uses a proprietary platform to conduct in-year audits that pull recoveries forward to near real-time, while also offering preventative solutions designed to stop payment errors before they occur.\n\nFlexTecs said the investment will help it expand its services and software offering, accelerate growth, and continue innovating across both its tech-enabled services business and its proprietary SaaS platform, FlexTrap, while maintaining the company's existing leadership, culture and operating model. The company said its current leadership team will continue to lead the business with added strategic guidance and resources from Cohere Capital and board advisors.\n\nThe announcement follows what FlexTecs characterized as several years of rapid expansion. The company said it has doubled in size over the past three years and grown its global workforce to more than 600 employees. FlexTecs also reported a 25% compound annual revenue growth rate since 2020 and said the new capital will support further investment in its core recovery audit and contract compliance services, alongside continued development of FlexTrap.\n\nFlexTecs positioned FlexTrap as a productized extension of its recovery audit expertise, designed to address \"cash leakage\" in accounts payable by preventing errors before they occur and automating statement reconciliations that often remain manual even in modern AP environments. The company said FlexTrap is undergoing a strategic expansion intended to translate operational insights from audits into software that improves efficiency, reduces time spent on payables work, and shifts organizations from repeated recovery cycles toward prevention.\n\nCohere Capital said it plans to support FlexTecs' strategy through service and technology innovation, geographic expansion, and potential acquisitions. The firm also pointed to the role of advancing AI in widening differentiation for companies with technology embedded at the core of their operating model.\n\nKEY QUOTES\n\n\"This partnership comes at a time of tremendous momentum for FlexTecs,\" said Tom Cook, Co-Founder and CEO of FlexTecs. \"We've doubled the size of the business over the past three years and have grown our global team to more than 600 employees. Our differentiated model has created a strong foundation and continues to drive market share gains and lasting value for our clients.\"\n\n\"Our tech-enabled services business remains foundational to who we are,\" said Cook. \"FlexTrap is in the midst of a strategic expansion, building on the deep expertise we've developed through years of recovery audits, turning real-world insights into software that helps clients optimize accounts payable, reclaim valuable time, and prevent errors rather than just repeatedly recover them.\"\n\nTom Cook, Co-Founder and CEO, FlexTecs\n\n\"We are excited to partner with Tom and the rest of the FlexTecs leadership team to build on the momentum they have generated over the last several years. It is clear that a key reason for their success is the experienced team and culture that they have purpose-built,\" said Daniel Gedney, Co-Founder and Partner at Cohere Capital. \"FlexTecs was founded with technology at its core and that has created tremendous differentiation in the market versus their competitors, which we only expect to accelerate as AI continues to advance.\""
  },
  {
    "source": "WION",
    "company": "Cohere",
    "title": "THIS US tech CEO misses AI Summit gala hosted by PM Modi after four hours in Delhi traffic. Who is she?",
    "date": "2026-02-19T11:42:03Z",
    "url": "https://www.wionews.com/india-news/this-us-tech-ceo-misses-ai-summit-gala-hosted-by-pm-modi-after-four-hours-in-delhi-traffic-who-is-she-1771497626452",
    "content": "US CEO Sara Hooker missed Prime Minister Narendra Modi's AI Summit dinner after four hours in Delhi traffic chaos near Bharat Mandapam\n\nAn American technology executive was unable to attend a high-profile dinner hosted by Prime Minister Narendra Modi after being stuck in severe traffic congestion near the venue in Delhi. Sara Hooker, co-founder and CEO of Adaption Labs, posted on social media that she spent nearly four hours trying to reach Bharat Mandapam for the gala event before ultimately giving up and heading back to her hotel. Instead of attending the dinner with the Prime Minister, she ended her evening with room service.\n\nHooker explained that after participating in multiple sessions at the India AI Impact Summit earlier in the day, she briefly returned to her hotel to change clothes for the formal dinner. She had swapped her casual jeans for gala attire in preparation for the exclusive gathering of global tech leaders and VIP delegates. However, her plans were derailed by the heavy traffic around the venue. In a post on X, she shared that despite receiving an invitation to the prestigious dinner, the gridlock prevented her from making it back in time. After hours of waiting in traffic, she decided to turn around and head back to the hotel.\n\nAlso Read: Ancient wisdom, future tech: Meet youngest keynote speaker Ranvir Sachdeva at India AI Summit\n\nLater that night, she posted a photo of the Indian meal she ordered at her hotel, noting that while she would have appreciated attending the event, she was equally grateful to finally sit down to a late dinner at 11 pm. Hooker was not alone in facing difficulties. Several other attendees reported major disruptions around Bharat Mandapam, where roads were closed for VIP movement. With nearby Metro access suspended and taxis restricted from approaching the venue, many guests were left walking long distances to find transportation.\n\nThe situation triggered significant frustration among participants, many of whom voiced their concerns online. The congestion also caused extended traffic jams in central Delhi, with some motorists resorting to crossing road dividers unlawfully in an attempt to bypass the standstill.\n\nSara Hooker is a computer scientist specialising in artificial intelligence, with a focus on model efficiency at scale, large language models, and research on algorithmic bias and fairness in machine learning. In 2025, she co-founded Adaption, a startup dedicated to building AI systems capable of continuous real-time learning and efficient adaptation.\n\nBefore this, she served as Vice President of Research at Cohere, where she led the company's research division, Cohere For AI. During her tenure, she launched the Cohere For AI Scholars Program to support emerging researchers in the field. In recognition of her impact, she was named one of AI's Top 13 Innovators by Fortune in 2023 and was included in Time's 2024 list of the most influential people in AI."
  },
  {
    "source": "BetaKit",
    "company": "Cohere",
    "title": "Why Canada's defence spending should follow this Cold War blueprint | BetaKit",
    "date": "2026-02-18T16:17:23Z",
    "url": "https://betakit.com/why-canadas-defence-spending-should-follow-this-cold-war-blueprint/",
    "content": "US military spending helped create your laptop. Can Canada make similar leaps?\n\nLiam Gill is a former startup founder, investor, and lawyer who leads the Capital Program at MaRS Discovery District, where he supports Canadian startups in raising the capital they need to grow and scale.\n\nLast week, the federal government unveiled a $6.6-billion Defence Industrial Strategy that will play a key part in its push to rebuild the Canadian Armed Forces (CAF) and the country's sovereign defence capabilities. As Canada ramps up defence spending, the country must look to American spending during the Cold War as its blueprint. The goal should not just be to fund technologies that improve the military's operational readiness, but to fund foundational technologies, such as AI and clean energy systems, that will underpin the next generation of the consumer economy.\n\nThe power of countries like China and the US comes not just from their militaries or economies, but from the world's reliance on their tech stacks.\n\nThis approach shifts the definition of national security. Today, owning foundational technology is a defence strategy. We have long recognized a secure oil supply as necessary for national security; the United States recently initiated another global conflict to secure its oil pipeline. Sovereign foundational technology is as important as oil. The power of countries like China and the US comes not just from their militaries or economies, but from the world's reliance on their tech stacks. Spending billions on traditional defence without prioritizing investments in domestic AI and clean energy capacity is a strategic failure.\n\nHistory proves that defence spending is the most potent industrial policy a nation can deploy. In 1961, the majority of computer semiconductor chips produced in America were purchased by the US government to support the Apollo program. Over the next few years, the government remained the top buyer, providing the critical capital needed for companies like Fairchild Semiconductor to survive and innovate. The US government purchased frequency synthesizers for deep-space exploration from Hewlett-Packard (HP) and funded high-risk research, such as the Gravity Probe B at Stanford University.\n\nThese investments did more than put a man on the moon and help to win the Cold War, they laid the foundation for America's technological dominance. A group of executives left Fairchild to form Intel, which, along with HP and Stanford, formed the bedrock of Silicon Valley. To this day, roughly 78 percent of all laptops use Intel processors, direct successors to the chips created for the Apollo mission. If you are reading this on a laptop, you are benefiting from a Cold War investment made six decades ago.\n\nRELATED: Canada's Defence Industrial Strategy must include procurement pathways for tech\n\nCanada has long needed this type of spark in our innovation ecosystem. For the country's new defence spending to have a comparable impact, it must be ruthlessly targeted, not just at technologies that fit the traditional definition of dual use -- both improving military operational readiness and having civilian applications -- but at foundational technologies that all Canadians, including military and government officials, use on a daily basis.\n\nConsider Chemshift Technologies, a Calgary-based startup using a proprietary process to refine lithium. Canada ships more than 60 percent of our raw lithium to China for refining. Chemshift's process allows this refining to be done domestically. This lithium can be used by the military for batteries in unmanned vehicles or to power essential gear. It is also used daily by civilians in their phones, laptops, and electric cars. While this technology might not fit the traditional definition of dual use, by backing the technology, the government secures a domestic supply chain for critical energy needs, while simultaneously breaking our dependence on foreign competitors.\n\nRELATED: Feds unveil $358-million initiative to bolster Canada's defence supply chain\n\nCohere is another example. Canada's horse in the large language model (LLM) race is reportedly planning an IPO. Given its current cap table, with Canadians owning just over 50 percent of the company, it is unlikely Cohere will be entirely owned and controlled by Canadians following its next funding round. With 90 percent of its revenues coming from outside Canada, its ties to the country are quickly diminishing. By taking an equity position in Cohere, the Canadian government can ensure that we have a sovereign LLM owned by Canadians, accountable to the Canadian government. While there is limited direct operational benefit to owning a sovereign LLM, it does guarantee that, as we move towards an AI-native society, our armed forces, businesses, and citizens won't be at the mercy of foreign technology companies.\n\nA technology stack free of foreign influence is the most vital aspect of our national security.\n\nCanada has committed to spending five percent of its GDP on defence by 2035. For this investment to truly protect Canadians' future, it cannot focus primarily on operational readiness for the armed forces. Instead, it must recognize that, in today's world, a technology stack free of foreign influence is the most vital aspect of our national security. The US won the Cold War not just by sending a man to the moon, but by investing in technologies that were foundational for the next generation of the consumer economy -- providing the seed capital for an innovation ecosystem that is still unmatched.\n\nCanada has a generational opportunity to build a sovereign technology stack and seed our own innovation ecosystem. If we get this right, defence spending will be the catalyst that delivers life-improving products, creates generational wealth, and secures our sovereignty. If we get it wrong, we will remain defenceless in a world run by foreign companies and governments."
  },
  {
    "source": "Fortune",
    "company": "Cohere",
    "title": "Zillow's CTO says AI is reinventing every step of the home buying process | Fortune",
    "date": "2026-02-18T14:11:13Z",
    "url": "https://fortune.com/2026/02/18/zillow-cto-ai-reinventing-every-step-home-buying-process/",
    "content": "This month, Zillow officially celebrated the 20th anniversary of the real estate website's debut. One executive who has served as a senior leader from the ground floor is David Beitel, who has served as CTO for Zillow's entire existence.\n\n\"I really only had two jobs my whole career,\" says Beitel. He initially joined Microsoft as a software design engineer in the 1990s and was slotted into a travel product division that would eventually become travel technology giant Expedia Group. It was there that he initially met two future co-founders of Zillow, Rich Barton and Lloyd Frank.\n\nThroughout the past two decades, Beitel's north star goal as CTO has been to infuse technology throughout all stages of the home buying process, making it easier for shoppers to search for a home, find an agent, schedule tours, make a successful offer, and complete a transaction. But he has also been keenly focused on internal use cases and says there's not one part of the business where artificial intelligence isn't being considered as a tool that can improve workflows.\n\nZillow's engineers have been using AI coding assistant tools like Cursor and Claude for well over two years. Beitel says the secret sauce is that Zillow constantly tests these AI coding tools and models, making constant adjustments to ensure the team is using the best technology for the desired outcome. Zillow measures productivity, as well as the quality of the work these AI coding assistants produce. Overall, \"the tools are getting very good,\" says Beitel. \"The models are improving every month.\"\n\nAcross the broader enterprise, every employee has access to OpenAI's ChatGPT. Zillow also works closely with enterprise search and AI applications startup Glean, which hit $200 million in annual recurring revenue in December. Glean has made it easier for employees across Zillow to explore generative AI in a protected environment. IT has set up the proper guardrails to make sure workers can only tap data that they have permission to access. Using Glean, Zillow's employees have already created more than 4,600 AI agents.\n\nBeitel says employees explore AI at three distinct stages. The entry level is using AI to find information, then automation is one level higher. On the top floor, the most complex cases involve drastic workflow changes that would speed up the development of a new product or uncover a new pricing strategy or competitive analysis research that hadn't yet been explored using AI. Now a few years into the generative AI journey, Beitel says Zillow's workforce is expected to be at that third stage of adoption.\n\n\"Don't just think of AI as an afterthought,\" says Beitel. \"But how do I use AI upfront to make the tasks I'm doing faster, more efficient, more effective, and more easily accomplished?\"\n\nAnd while Zillow's business has been booming, with revenue totaling $2.6 billion in 2025, up 16% year over year, the broader housing market has been quite choppy. Last year, sales of previously owned homes hit their lowest level since 1995, as home buyers face mortgage rates that are double pandemic-era levels, high prices, and low inventory.\n\n\"If you're a new home buyer, it's daunting, and it's a very important purchase -- typically one of the biggest ones you might make in your life,\" says Beitel. \"So it's natural that it would be fraught with some challenges.\"\n\nA key part of Zillow's external AI story has been the company's Zestimate feature, which uses AI and machine learning to analyze public records, market trends, and other data points to estimate the value of a home. Other technology investments have included leveraging computer vision models to create 3D walkthroughs of a home listing, AI-enabled virtual staging to help buyers envision what a home may look like with various design styles, and a newer feature called SkyTour, the latter uses drones to capture exterior images and give a potential buyer a fuller picture of what the neighborhood looks like.\n\nBeitel says LLMs are also making it easier for Zillow shoppers to ask natural language questions, rather than toggle through pre-set search criteria. One feature is called AI Assist, a conversational assistant that can answer renter questions including \"Are utilities included in the rental?\" or \"Is there on-site parking?\"\n\nLast year, the company also debuted functionality within ChatGPT. A ChatGPT user can write, \"ZIllow, show me apartments in the West Village, New York City, for under $5,000 per month.\" The AI chatbot will then pull various listings and link to Zillow's website or app for additional information. Beitel says Zillow and OpenAI were able to ship the feature in about six weeks.\n\nAI is also being used for simple, mundane tasks like scheduling property visits. And in 2023, Zillow paid $400 million (plus $100 million in a potential cash earnout) for Follow Up Boss, a customer relationship management software system that uses AI to summarize calls and can suggest a to-do list for agents and personalized messages that can be sent to a potential client.\n\nIn the future, Zillow would like to infuse more AI into the more complex, tail-end stages of buying a home. Beitel says this work can be tedious and take a lot of time, as mortgage documents are shared between clients, buying and selling agents, loan officers, home inspectors, and more. AI could make this entire experience a bit smoother, Beitel says.\n\n\"We haven't built it all yet,\" he concedes. \"We started at the top many years ago and we're going deeper into the transaction.\"\n\nAnthropic raises $30 billion; Cohere tops revenue target for 2025. Chatbot Claude maker Anthropic and Canadian AI startup Cohere are among the AI startups that have been busy signaling to investors that momentum is building for their businesses as they move towards potential debuts on the public markets later this year. For Anthropic, its valuation grew to $380 billion after it raised $30 billion in its latest round of funding, with the money planned to go toward building \"enterprise-grade products\" and AI models, Fortune reports, citing Anthropic Chief Financial Officer Krishna Rao. Nvidia-backed Cohere, meanwhile, told investors that it hit roughly $240 million in annual recurring revenue in 2025, exceeding its $200 million target, according to an investor memo that was viewed by CNBC.\n\nMeanwhile, Anthropic's Pentagon connection gained fresh attention. Two separate media reports honed in on Anthropic's work with U.S. military operations, including a Wall Street Journal story published over the weekend asserting that Anthropic's Claude was used to capture former Venezuelan President Nicolás Maduro. And yet, one day later, Axios reported that the Pentagon may sever its relationship with Anthropic over the company's insistence that two use cases -- the mass surveillance of Americans and fully autonomous weaponry -- remain off limits. Axios, citing a senior administration official, says that there's considerable gray area around what would and wouldn't fall into those categories.\n\nNew York City gets a new CTO. Less than two months after Mayor Zohran Mamdani was sworn into office, America's largest city has a new CTO, Lisa Gelobter, a computer scientist who served on the launch team for the streaming service Hulu; founded tEQuitable, which uses technology to make workplaces more equitable; and is credited with the creation of GIFs (though Gelobter has said the latter isn't entirely true). As CTO, Gelobter will oversee cybersecurity, technology infrastructure, and data management. She will focus on leveraging technology to expand access to city services and advance \"digital equity across New York City.\" Gelobter will also serve as Commissioner of the Office of Technology and Innovation.\n\nAs AI jitters software, private companies release their earnings early. Bloomberg reports that software companies including McAfee, Rocket Software, and Perforce Software have disclosed their earnings ahead of schedule to quell fears that AI may be upending the business model for these legacy software providers. But the numbers that were disclosed weren't all that stellar. Cybersecurity company McAfee's preliminary fourth-quarter revenue was close to prior-year levels, while IT modernization software company Rocket Software's revenue rose 5.2% in 2025 from the prior year. And at Perforce Software, which provides tech to software developers, last year's annual revenue slightly dipped to $644 million from $654 million in 2024.\n\nCIOs have a stronger voice in shaping business strategy at the top-performing companies. CIOs at the fastest-growing global companies are far more likely to be \"very involved\" in shaping their enterprise business strategy, according to a new study by McKinsey. The consulting giant, which defined top performers as companies with an average growth rate of at least 10% in revenue and earnings over the past three years, says 64% of CIOs at those firms are highly involved in shaping the business agenda. Only 52% of CIOs were so involved at the rest of the companies surveyed (McKinsey surveyed 632 C-suite and IT leaders in total and said 114 qualified as top performers for this study).\n\nMore than a quarter of the top performers also project that they will increase their tech budgets by more than 10% in 2026, while just 3% of the rest of the companies said the same. Aamer Baig, senior partner at McKinsey, tells Fortune that top performers \"realize the value from improving the IT function has a multiplicative effect on the business, whereas all others are still thinking of it in a very traditional way.\"\n\nAnother factor holding some enterprises back from boosting their tech spending is the ongoing conversation around the ROI of AI. Studies have shown that a majority of generative AI pilots fail. And AI productivity use cases may save some time on the margins, but aren't yet proving to be potent enough to translate into business impact.\n\nBaig says most organizations focus too much on the tech, and not enough on everything else. He estimates 20% of the effort should go toward the AI tools, with the remainder focused on changing workflows, training people differently, and setting clear incentives to entice employees to embrace new ways of working. \"Companies that haven't gotten value probably just focused on the tool,\" cautions Baig.\n\n- Scholar Rock is seeking a CIO, vice president of IT, based in Cambridge, Massachusetts. Posted salary range: $300K-$400K/year.\n\n- Goodr is seeking a senior vice president of technology, based in Los Angeles. Posted salary range: $250K-$270K/year.\n\n- The Chicago White Sox is seeking a VP of IT and enterprise architecture, based in Chicago. Posted salary range: $200K-$230K/year.\n\n- Franklin Templeton is seeking a head of digital assets technology, based in New York City. Posted salary range: $198K-$270K/year.\n\nHired:\n\n- Indeed appointed Jim Giles as CTO, effective immediately, where he will steer the technical strategy and global engineering team for the job listings website. Giles joins Indeed after most recently serving as VP of engineering at Google, where he led engineering for products including Docs, Sheets, Slides, and Drives. He was also previously a distinguished engineer at IBM.\n\n- Brown & Brown announced the appointment of Dorothea (\"Dori\") Henderson as chief information technology officer. Prior to joining the insurance brokerage firm, Henderson served as SVP and chief digital information officer at health insurance provider CareFirst BlueCross BlueShield. She also previously spent 18 years in technology leadership at Collins Aerospace.\n\n- BAE Systems has appointed Mona Bates as SVP and chief information and digital officer, effective February 23. Bates joins the defense company after most recently serving as chief digital officer at aerospace and defense systems company Collins Aerospace. She also held technology leadership roles at defense and space manufacturer Raytheon.\n\n- MEMX has promoted Richard Gomez to serve as CTO, responsible for all software development and technological support across the company's exchanges. Gomez joined MEMX in 2019 as a software engineer and later became chief architect in 2021. In his new role, Gomez will report to CIO Dominick Paniscotti.\n\n- Create Music Group appointed Mitchell Shymansky as chief data and technology officer, joining the music distribution and publishing company after nearly two decades at Universal Music Group, where he most recently led the record label's global data and analytics organization.\n\n- OpenMetal announced the appointment of Jamie Tischart as CTO. Prior to joining the private cloud infrastructure provider, Tischart served as chief information technology officer at social and influencer marketing software provider Later. He also previously held the same role at software vendor BetterCloud.\n\n- Carbon promoted Jason Rolland to the role of CTO, after previously serving as SVP of materials for the 3D printing and manufacturing company. Rolland initially joined Carbon in 2014 as VP of materials and previously served as a co-founder of biopharmaceuticals company Liquidia."
  },
  {
    "source": "The Logic",
    "company": "Cohere",
    "title": "Cohere tells investors its revenues have hit US$240M rate: Report - The Logic",
    "date": "2026-02-13T20:29:33Z",
    "url": "https://thelogic.co/briefing/cohere-tells-investors-its-revenues-have-hit-us240m-rate-report/",
    "content": "The Toronto-headquartered AI firm's annual recurring revenue last year topped its US$200 million target, according to a memo obtained by CNBC. According to the document, sales grew more than 50 per cent quarter over quarter, and the firm had a 70 per cent average gross margin. Cohere declined to comment to The Logic. (CNBC, The Logic)\n\nTalking point: Cohere sells AI models and agent-builder tools, mostly to large companies in regulated industries, and increasingly to government departments. Those sales are picking up. The firm has more than doubled its business since last May, when it reportedly crossed the US$100 million annualized revenue -- typically a measure of monthly sales projected over a year-long period. That's still slower growth and a lower total than other developers that sell models and tools to the enterprise market, like Mistral or Anthropic. But Cohere CEO Aidan Gomez has argued his company has a more sustainable business model because it doesn't have a consumer-facing app and its commercial clients pay for the compute costs of running its technology."
  },
  {
    "source": "BetaKit",
    "company": "Cohere",
    "title": "Modem closes $4.4-million USD pre-seed round to help developers ship faster | BetaKit",
    "date": "2026-02-10T14:01:29Z",
    "url": "https://betakit.com/modem-closes-4-4-million-usd-pre-seed-round-to-help-developers-ship-faster/",
    "content": "Co-founders from Sentry, Cohere back Toronto startup's AI product management platform.\n\nBefore AI, software developers already had to contend with a flood of information. As large-language models (LLMs) have sped up the pace of development, keeping all the context in one place and arranging it in order of importance remains a persistent challenge.\n\nBen Vinegar, founder of Toronto-based Modem, knows this firsthand. As a former VP of engineering at Silicon Valley unicorn Sentry, he has a unique insight into the bottlenecks faced by developer teams.\n\nHis new startup, which launched last May, has attracted the attention of former colleagues and other veteran tech angels. Modem closed a $4.4-million USD ($6 million CAD) equity pre-seed round led by Silicon Valley firm Accel, with participation from Montréal's Inovia Capital and several angel investors, including Cohere co-founder Ivan Zhang and Sentry co-founders David Cramer and Chris Jennings. Sentry has also become a client, using Modem in its workflows.\n\nGetting user feedback to developers is like a \"game of telephone,\" Vinegar said in an interview with BetaKit. \"By the time it reaches you, you're already getting a mutated version of what's true.\"\n\nVinegar explained that Modem integrates into software teams' existing platforms to manage its product workflow by proactively flagging bugs, curating support tickets, and even following up with customers after software updates. Like other agentic AI products, Modem can take action on a user's behalf and respond to natural-language questions like: \"What are the biggest issues my customers are facing?\"\n\nInvestors are looking to companies like Modem as startups building with AI eat up a larger share of the software space in Canada.\n\n\"We've seen firsthand how AI-powered coding tools have transformed developer productivity,\" Taha Mubashir, Inovia Capital partner, told BetaKit in an email. \"Ben had a similar insight -- an AI that does for product teams what coding agents do for engineers.\"\n\nRELATED: Amid AI proof-of-concept fatigue, Cohere co-founder urges potential customers to keep the faith and focus on ROI\n\nVinegar said that, in addition to Sentry, Modem has seen success selling to small developer teams building software with AI. It sells through a subscription model inspired by some of the leading AI products, which can fluctuate depending on data usage.\n\nWhile developers are shipping software faster with the help of AI, the productivity gains have been uneven. A January study by the Complexity Science Hub, which looked at the work of more than 160,000 users on GitHub, found that generative AI tools increased programmers' productivity by four percent on average -- but that increase was concentrated among senior developers. A July 2025 study by research nonprofit METR found that experienced developers took 19 percent longer to complete tasks when they used AI tools, despite thinking they were working more quickly.\n\nDespite living in California during his time at Sentry and for a few years after, Vinegar returned to Canada during the COVID-19 pandemic and decided to build Modem from home. Though San Francisco is undoubtedly the \"centre of the universe\" for building in tech, Vinegar said that doesn't mean Toronto, and Canada at large, doesn't have its own benefits. Talent is one of them, Vinegar said, as he remembers Sentry participating in the \"brain drain\" by hiring Canadian talent down south.\n\nLocation has been a point of conversation in Canadian tech this month, after famed accelerator Y Combinator briefly took Canada off its list of investable sites (only to reinstate it a week later).\n\nTo Vinegar, the key takeaway for Canada was how it could better emulate the sense of community created by the accelerator's cohort companies.\n\n\"Even though some are competing, they are kind of on the same team,\" Vinegar said. If Canadian companies get better about buying each other's products, \"that can exist right now in Canada.\""
  },
  {
    "source": "TechBullion",
    "company": "Cohere",
    "title": "AI Agent Development Companies That Prevent Hallucinations",
    "date": "2026-02-06T08:39:43Z",
    "url": "https://techbullion.com/ai-agent-development-companies-that-prevent-hallucinations/",
    "content": "AI agents are becoming deeply embedded in business workflows, customer support, analytics, and decision-making systems. However, as adoption increases, so does one of the most critical risks associated with agent-based AI: hallucinations. When AI agents generate incorrect, fabricated, or misleading information, the consequences can range from minor inefficiencies to serious operational, legal, or reputational damage.\n\nIn response, businesses are now prioritizing AI agent solutions that are designed to prevent hallucinations rather than merely optimize for fluency or speed. This shift has increased demand for development partners that understand how to build grounded, reliable, and verifiable AI agents. Companies such as Tensorway have set early benchmarks in this space by treating hallucination prevention as a system-level responsibility rather than a model-side afterthought.\n\nThis listicle highlights AI agent development companies that focus specifically on reducing hallucinations through architecture, data grounding, monitoring, and control mechanisms, with Tensorway positioned as the reference standard.\n\nAI Agent Development Companies That Prevent Hallucinations 1. Tensorway\n\nTensorway is widely regarded as the leading AI agent development company when it comes to hallucination prevention. The company approaches agent development from a system-first perspective, where reliability, grounding, and control are treated as foundational requirements rather than optional enhancements.\n\nTensorway designs AI agents that operate within clearly defined knowledge boundaries. Instead of relying solely on generative responses, its agents are tightly integrated with structured data sources, retrieval mechanisms, and validation layers. This significantly reduces the likelihood of fabricated outputs and unsupported claims.\n\nA key strength of Tensorway lies in its use of architecture-level safeguards, including retrieval-augmented workflows, response verification, and continuous monitoring. By aligning agent behavior with business logic and trusted data, Tensorway delivers AI agents that are suitable for high-stakes environments where accuracy and trust are non-negotiable.\n\n2. Anthropic Applied AI Services\n\nAnthropic Applied AI Services focuses on building AI systems with an emphasis on safety, interpretability, and controlled behavior. Its agent development work often centers around minimizing unexpected or misleading outputs through constrained reasoning and alignment-focused design.\n\nThe company's approach is particularly relevant for organizations deploying AI agents in sensitive domains such as policy analysis, research assistance, or internal knowledge systems. By emphasizing predictability and grounded responses, Anthropic's applied services help reduce hallucination risks at both the model and system levels.\n\n3. Cohere Enterprise Solutions\n\nCohere Enterprise Solutions develops AI agents that prioritize factual consistency and controlled language generation. Its work often involves integrating language models with enterprise knowledge bases, ensuring responses are derived from verified internal data rather than open-ended generation.\n\nCohere's agent solutions are commonly used for search, summarization, and internal support systems where hallucinations can erode trust quickly. The company emphasizes retrieval-first workflows and response constraints to keep outputs aligned with source material.\n\n4. Vectara\n\nVectara specializes in building AI agents and search-driven systems that are explicitly designed to reduce hallucinations. Its technology focuses on grounding responses in indexed data and returning answers that are traceable to original sources.\n\nVectara's approach is well suited for organizations that need AI agents to answer questions based on documentation, policies, or proprietary content. By limiting generation to retrieved evidence, Vectara helps ensure that agent outputs remain factual and auditable.\n\n5. Snorkel AI\n\nSnorkel AI approaches hallucination prevention through data-centric AI development. Rather than focusing solely on models, the company helps organizations improve the quality, consistency, and supervision of training data used by AI agents.\n\nSnorkel AI's solutions are often applied in environments where labeled data is scarce or noisy. By strengthening data foundations and validation processes, Snorkel AI reduces the risk of agents learning incorrect patterns that lead to hallucinated outputs.\n\n6. Seldon\n\nSeldon develops infrastructure and tooling for deploying and managing machine learning and AI agent systems in production. A major focus of its platform is observability, monitoring, and control.\n\nFor hallucination prevention, Seldon enables organizations to detect anomalous outputs, enforce response policies, and roll back problematic agent behavior quickly. Its tools are especially valuable for companies operating AI agents at scale, where manual oversight is not feasible.\n\n7. Arize AI\n\nArize AI focuses on AI observability and performance monitoring, helping organizations understand how their AI agents behave in real-world conditions. While not an agent builder in isolation, Arize plays a critical role in hallucination prevention by detecting drift, bias, and unexpected output patterns.\n\nOrganizations use Arize AI to monitor when agents begin generating unreliable responses and to trace those issues back to data or system changes. This makes it a strong complement for companies prioritizing long-term reliability.\n\nWhat Sets Hallucination-Resistant AI Agents Apart\n\nAI agents that successfully prevent hallucinations share several defining characteristics. First, they rely on grounded data sources rather than open-ended generation. Second, they incorporate validation layers that check responses against known constraints. Third, they include monitoring systems that detect and correct issues over time.\n\nMost importantly, hallucination-resistant agents are designed as systems, not standalone models. This system-level thinking is what separates providers like Tensorway from teams that focus only on prompt engineering or model tuning.\n\nHow Businesses Should Evaluate AI Agent Providers\n\nWhen selecting an AI agent development company, businesses should assess how hallucination risks are addressed across the entire lifecycle. Key questions include how agents retrieve and verify information, how responses are constrained, how errors are detected, and how systems evolve as data changes.\n\nProviders that cannot clearly explain their hallucination prevention strategy often rely on manual fixes rather than robust design. In high-impact environments, this approach introduces unnecessary risk.\n\nFinal Thoughts\n\nAs AI agents become more autonomous and more influential, hallucination prevention has emerged as one of the most important success factors. Businesses deploying agents without safeguards risk eroding trust and undermining the value of their AI investments.\n\nAmong the companies reviewed, Tensorway stands out as the best option for building hallucination-resistant AI agents. Its system-first architecture, emphasis on grounding and validation, and focus on long-term reliability make it the strongest choice for organizations that require accurate, trustworthy AI agent behavior.\n\nRelated Items:AI Agent Development, Prevent Hallucinations"
  },
  {
    "source": "Observer",
    "company": "Cohere",
    "title": "Sara Hooker Raises $50M to Challenge A.I.'s Conventional Wisdom: Interview",
    "date": "2026-02-05T19:37:01Z",
    "url": "https://observer.com/2026/02/sara-hooker-raises-50m-to-challenge-a-i-s-conventional-wisdom-interview/",
    "content": "The former Google and Cohere researcher is betting on self-learning models over ever-larger A.I. systems.\n\nSara Hooker has spent the past few years doing her \"tour of duty\" across some of the A.I. industry's most competitive frontier labs. Now, she's striking out on her own -- and taking a rebellious stance against the industry's most ingrained principles. Hooker and her co-founder, Sudip Roy, have raised $50 million for their new startup, Adaption Labs. The company is centered around the premise that efficient, self-learning training methods -- not mass amounts of data and energy -- will lead to the best models.\n\nSign Up For Our Daily Newsletter Sign Up\n\nThank you for signing up!\n\nBy clicking submit, you agree to our <a href=\"http://observermedia.com/terms\">terms of service</a> and acknowledge we may use your information to send you emails, product samples, and promotions on this website and other properties. You can opt out anytime.\n\nSee all of our newsletters\n\n\"Most of A.I. progress is: you build the biggest model, and then you ship the same model to billions of people around the world; no matter what language, what industry, what enterprise,\" Hooker told Observer. \"This is a radical departure from that.\"\n\nHer ambitious wager has grabbed the attention of Silicon Valley. The San Francisco-based startup's round was led by Emergence Capital Partners, with participation from Mozilla Ventures, Fifty Years, Threshold Ventures, Alpha Intelligence Capital, e14 Fund and Neo. Adaption Labs declined to share details of its valuation.\n\nConventional wisdom in A.I. suggests that more compute leads to bigger and more capable models. But Adaption Labs believes that approach has hit a wall. \"One of our core beliefs is that no frontier A.I. lab is going to quadruple the size of their model for the next year,\" said Hooker. \"That means all bets are off.\"\n\nInstead, Hooker is interested in experimenting with building models that can learn continuously and adapt to workloads in real time as they interact with different environments. User feedback, for example, should immediately change the behavior of A.I. tools instead of being \"lost in a vacuum.\" An emphasis on efficiency at Adaption Labs will also include exploring ideas like \"gradient-free learning,\" which looks for alternatives to traditional training methods that rely on optimization algorithms to adjust model parameters in an effort to minimize errors.\n\nHooker and Roy are poised to disrupt an industry that they have spent their careers advancing. Both worked for years at Google before moving on to Cohere, where Hooker acted as vice president of research, and Roy served as senior director of inference. Despite departing Cohere last year, Hooker said she is \"proud\" of the work she did, which included a focus on multilingual model development. But Adaption Labs' goals of enabling adaptive data, intelligence and interfaces wouldn't be achievable within a traditional frontier lab, she said, due to those areas being split across different teams. \"It's much easier to start with that urgency of putting them all on the same pillar from the beginning,\" she said.\n\nAdaption Labs isn't alone in going against the grain. A growing chorus of voices in Silicon Valley has begun to question the industry's dominant assumptions. Yann LeCun, who recently left Meta to launch AMI Labs, has raised doubts about traditional scaling law principles; so has David Silver, a former Google DeepMind researcher whose startup, Ineffable Intelligence, focuses on training self-learning models through experience rather than feeding them data.\n\nHooker's fresh funding is primarily earmarked for building out a team. Adaption Labs is currently hiring for 10 roles, some of which can be based in global locations such as Turkey, Mexico and Brazil. Staffers are also offered a distinctive perk: an \"Adaptive Passport\" that allows them to take an annual trip to a country they've never visited before. \"We want to encourage people to not only explore, but we want to represent that we're a global technology company from day one,\" said Hooker. Roy was the first to take advantage of the benefit, using it for a trip to Costa Rica last year.\n\nHooker expects that a reckoning of traditional scaling laws as major A.I. developers confront the reality that ever-greater computing power is yielding diminishing returns. Algorithmic innovation will be the real driver of progress. \"This is the year in which it will really matter,\" she said."
  },
  {
    "source": "Fortune",
    "company": "Cohere",
    "title": "Former Cohere execs Sara Hooker and Sudip Roy secure $50 million seed round for their new startup Adaption Labs | Fortune",
    "date": "2026-02-04T13:08:37Z",
    "url": "https://fortune.com/2026/02/04/adaption-labs-50-million-seed-funding-emergence-captial-sara-hooker-sudip-roy-ai-models-that-learn-on-the-fly/",
    "content": "Sara Hooker, an AI researcher and advocate for cheaper AI systems that use less computing power, is hanging her own shingle.\n\nThe former vice president of research at AI company Cohere and a veteran of Google DeepMind, has raised $50 million in seed funding for her new startup, Adaption Labs\n\nHooker and cofounder Sudip Roy, who was previously director of inference computing at Cohere, are trying to create AI systems that use less computing power and cost less to run than most of the current leading AI models. They are also targeting models that use a variety of techniques to be more \"adaptive\" than most existing models to the individual tasks they are being asked to tackle. (Hence the name of the startup.)\n\nThe funding round is being led by Emergence Capital Partners, with participation from Mozilla Ventures, venture capital firm Fifty Years, Threshold Ventures, Alpha Intelligence Capital, e14 Fund, and Neo. Adaption Labs, which is based in San Francisco, declined to provide any information about its valuation following the fundraise.\n\nHooker told Fortune she wants to create models that could learn continuously without the expensive retraining or fine-tuning and without the extensive prompt and context engineering that most enterprises currently use to adapt AI models to their specific use cases.\n\nCreating models that can learn continuously is considered one of the big outstanding challenges in AI. \"This is probably the most important problem that I've worked on,\" Hooker said.\n\nAdaption Labs represents a significant bet against the prevailing AI industry wisdom that the best way to create more capable AI models is to make the underlying LLMs bigger and train them on more data. While tech giants pour billions into ever-larger training runs, Hooker argues the approach is seeing diminishing returns. \"Most labs won't quadruple the size of their model each year, mainly because we're seeing saturation in the architecture,\" she said.\n\nHooker said the AI industry was at a \"reckoning point\" where improvements would no longer come from simply building larger models, but rather by building systems that can more readily and cheaply adapt to the task at hand.\n\nAdaption Labs is not the only \"neolab\" (so-called because they are a new generation of frontier AI labs following the success that more established companies like OpenAI, Anthropic, and Google DeepMind have had) pursuing new AI architectures aimed at cracking continuous learning. Jerry Tworek, a senior OpenAI researcher, left that company in recent weeks to found his own startup, called Core Automation, and has said he is also interested in using new AI methods to create systems that can learn continually. David Silver, a former Google DeepMind top researcher, left the tech giant last month to launch a startup called Ineffable Intelligence that will focus on using reinforcement learning -- where an AI system learns from actions it takes rather than from static data. This could, in some configurations, also lead to AI models that can learn continuously.\n\nHooker's startup is organizing its work around three \"pillars\" she said: adaptive data (in which AI systems generate and manipulate the data they need to answer a problem on the fly, rather than having to be trained from a large static dataset); adaptive intelligence (automatically adjusting how much compute to spend based on problem difficulty); and adaptive interfaces (learning from how users interact with the system).\n\nSince her days at Google, Hooker has established a reputation within AI circles as an opponent of the \"scale is all you need\" dogma of many of her fellow AI researchers. In a widely-cited 2020 paper called \"The Hardware Lottery,\" she argued that ideas in AI often succeed or fail based on whether they happen to fit existing hardware, rather than their inherent merit. More recently, she authored a research paper called \"On the Slow Death of Scaling,\" that argued smaller models with better training techniques can outperform much larger ones.\n\nAt Cohere, she championed the Aya project, a collaboration with 3,000 computer scientists from 119 countries that brought state-of-the-art AI capabilities to dozens of languages for which leading frontier models did not perform well -- and did so using relatively compact models. The work demonstrated that creative approaches to data curation and training could compensate for raw scale.\n\nOne of the ideas Adaption Labs is investigating is what is called \"gradient-free learning.\" All of today's AI models are extremely large neural networks encompassing billions of digital neurons. Traditional neural network training uses a technique called gradient descent, which works a bit like a blindfolded hiker trying to find the lowest point in a valley by taking baby steps and trying to feel whether they are descending a slope. The model makes small adjustments to billions of internal settings called \"weights\" -- which determine how much a given neuron emphasizes the input from any other neuron it is connected to in its own output -- checking after each step whether it got closer to the right answer. This process requires enormous computing power and can take weeks or months. And once the model has been trained, these weights are locked in place.\n\nTo hone the model for a particular task, users sometimes rely on fine-tuning. This involves further training the model on a smaller, curated data set -- usually still consisting of thousands or tens of thousands of examples -- and making further adjustments to the models' weights. Again, it can be expensive, sometimes running into millions of dollars.\n\nAlternatively, users simply try to give the model highly specific instructions, or prompts, about how it should accomplish the task the user wants the model to undertake. Hooker dismisses this as \"prompt acrobatics\" and notes that the prompts often stop working and need to be rewritten whenever a new version of the model is released.\n\nShe said her goal is \"to eliminate prompt engineering.\"\n\nGradient-free learning sidesteps many of the issues with fine-tuning and prompt engineering. Instead of adjusting all of the model's internal weights through expensive training, Adaption Labs' approach changes how the model behaves at the moment it responds to a query -- what researchers call \"inference time.\" The model's core weights remain untouched, but the system can still adapt its behavior based on the task at hand.\n\n\"How do you update a model without touching the weights?\" Hooker said. \"There's really interesting innovation in the architecture space, and it's leveraging compute in a much more efficient way.\"\n\nShe mentioned several different methods for doing this. One is \"on-the-fly merging,\" in which a system selects from what is essentially a repertoire of adapters -- often small models that are separately trained on small datasets. These adapters then shape the large, primary model's response. The model decides which adapter to use depending on what question the user asks.\n\nAnother method is \"dynamic decoding.\" Decoding refers to how a model selects its output from a range of probable answers. Dynamic decoding changes the probabilities based on the task at hand, without changing the model's underlying weights.\n\n\"We're moving away from it just being a model,\" Hooker said. \"This is part of the profound notion -- it's based on the interaction, and a model should change [in] real time based on what the task is.\"\n\nHooker argues that shifting to these methods radically changes AI's economics. \"The most costly compute is pre-training compute, largely because it is a massive amount of compute, a massive amount of time. With inference compute, you get way more bang for [each unit of computing power],\" she said.\n\nRoy, Adaption's CTO, brings deep expertise in making AI systems run efficiently. \"My co-founder makes GPUs go extremely fast, which is important for us because of the real-time component,\" Hooker said.\n\nHooker said Adaption will use the funding from its seed round to hire more AI researchers and engineers and also to hire designers to work on different user interfaces for AI beyond just the standard \"chat bar\" that most AI models use."
  },
  {
    "source": "Chosun.com",
    "company": "Cohere",
    "title": "Hanwha Group Forms Strategic Alliances for Canada Submarine Project",
    "date": "2026-01-27T00:42:56Z",
    "url": "https://www.chosun.com/english/industry-en/2026/01/27/6AYAXHAV3FEWDJA4SP2WUKLHQ4/",
    "content": "Partnerships with Canadian steel, AI, space firms align with 'Buy Canadian' policy\n\nHanwha Group has formed strategic partnerships with Canadian steel, artificial intelligence (AI), and space sector companies to secure the contract for the up-to 60 trillion Korean won 'Canada Patrol Submarine Project' (CPSP).\n\nThis move specifically presents an industrial cooperation model aligned with the Canadian government's emphasis on expanding local industry participation and offset trade (ITB) under the so-called 'Buy Canadian' policy during the bid evaluation process.\n\nHanwha Group revealed on the 27th that it signed memorandums of understanding (MOUs) with five Canadian steel, AI, and space companies for strategic investment and cooperation at the 'Korea-Canada Industrial Cooperation Forum' held at the Park Hyatt Hotel in Toronto, Canada, on the 26th (local time).\n\nFirst, Hanwha Ocean signed an MOU with Algoma Steel, Canada's largest steelmaker, to support the Canadian submarine project.\n\nThe two companies agreed to collaborate on establishing a stable supply system for steel products to be used in constructing a local steel plant in Canada and submarine construction and maintenance (MRO) infrastructure, contingent on winning the submarine project bid. To this end, Hanwha Ocean will contribute approximately 345 million Canadian dollars (CAD).\n\nKim Hee-chul, representative of Hanwha Ocean, said, \"By establishing a stable and long-term steel production and infrastructure system in Canada, we will contribute to preparing a reliable submarine force not only for today but also for future generations.\"\n\nAdditionally, Hanwha Ocean and Hanwha Systems signed a three-party MOU with Cohere, a Canadian AI unicorn, to collaborate on AI technology.\n\nThrough this, the companies plan to jointly develop specialized AI technologies applicable to production planning, design, manufacturing, and submarine system integration and operation based on Cohere's large language model (LLM) and large multimodal model (LMM).\n\nCohere, a specialist in enterprise AI model development, has attracted investments from NVIDIA and Oracle. Its recent corporate valuation is reported to exceed 7 billion U.S. dollars (10.1 trillion Korean won).\n\nHanwha Systems also signed an MOU with Telesat, a Canadian satellite communications company, for low Earth orbit (LEO) satellite communication cooperation.\n\nHanwha Systems plans to combine its satellite manufacturing and terminal development capabilities with Telesat's satellite network operation and design technologies to launch a 'next-generation LEO satellite communication network' competitive in both domestic and global markets.\n\nThe two companies also plan to materialize technical business cooperation for jointly developing South Korea's 'military LEO satellite communication system.' Telesat aims to launch 198 advanced LEO satellites by this year to provide next-generation satellite communication services.\n\nAdditionally, Hanwha Systems signed an MOU with MDA Space, a local space company, for satellite communication and space technology cooperation for defense and security purposes, and with PV Labs, an electro-optics company, for advancing electro-optical and infrared (EO·IR) sensor technology applicable to security fields.\n\nHanwha Systems particularly plans to enhance synergy by combining its expertise in advanced defense electronics and space systems with MDA Space's next-generation software-defined satellite (SDS) platform, 'Aurora.' This includes secure communication, data resilience, and command and control functions related to submarine operations linked to Canada's next-generation submarine project.\n\nSon Jae-il, representative of Hanwha Systems, stated, \"Hanwha Systems will do its utmost to position South Korea as a key partner in Canada's global economic and security supply chain by leveraging our unparalleled submarine operation technologies in marine, satellite, AI, and security sectors.\"\n\nRecently, global management consulting firm KPMG estimated that if Hanwha's industrial cooperation plan for the Canadian submarine project is executed, it could create cumulative employment for over 200,000 people in Canada from this year to 2040.\n\nThis figure reflects the long-term industrial ripple effects of offset trade across related industries, including steel, satellites, AI, and MRO, beyond mere submarine construction. Employment and economic impacts could grow further with additional industrial cooperation and investment expansion."
  },
  {
    "source": "Trend Hunter",
    "company": "Cohere",
    "title": "Enterprise AI Platforms",
    "date": "2026-01-22T15:07:01Z",
    "url": "https://www.trendhunter.com/trends/cohere1",
    "content": "Cohere is a technology platform that develops enterprise-grade language models for applications in natural language processing. The platform offers customizable AI models that can be deployed on public, private, or hybrid cloud infrastructures, providing flexibility for organizations with varying data security and compliance requirements.\n\nCohere emphasizes performance and scalability, enabling businesses to integrate language AI into products, services, and workflows efficiently. Its focus on secure deployment and enterprise support allows organizations to leverage AI without compromising sensitive information. Use cases range from customer service automation and document analysis to content generation and conversational interfaces. From a business perspective, Cohere reflects the growing trend of AI adoption in enterprises, where robust, secure, and adaptable language models can drive operational efficiency, innovation, and data-driven decision-making while maintaining control over proprietary information."
  },
  {
    "source": "Sky News Australia",
    "company": "Cohere",
    "title": "Linton Besser pillories ASIO staff in 'abusive' defence of ABC's hit job on Bondi attacks",
    "date": "2026-02-20T18:39:59Z",
    "url": "https://www.skynews.com.au/insights-and-analysis/media-watchs-linton-besser-mocks-highlyqualified-asio-staff-as-spooked-spooks-as-he-backs-in-four-corners-probe-on-the-bondi-attacks/news-story/33107ea54669205bf0b4667833ffcb8a",
    "content": "ABC TV's News Breakfast celebrates the holy month of Ramadan but overlooks the Iranian Muslims\n\nThe evening of Wednesday 18 February commenced the Holy Month of Ramadan. Last Wednesday was also Ash Wednesday in the Christian calendar which commences the holy season of Lent. Both are occasions where believers in one or other faiths are expected to fast in the lead up to Eid al-Fitr and Easter Sunday respectively.\n\nABC TV News Breakfast ignored Ash Wednesday this year. It has ignored various Jewish feast days along with notable dates of Hindus and Buddhists. But on Friday 20 Feb, News Breakfast devoted large segments to Ramadan - even to the extent of sending James Glenday and Emma Rebellato to Western Sydney to present the show.\n\nIn the lead up to the 7AM News, the program interviewed Jasmine al-Zoubi of the Sydney Muslim Run Club followed by the owner of the Lebanese Yum Yum bakery. He was asked this question by the program's weather man: \"What should people who aren't Muslim know about Ramadan?\".\n\nThen it was time for the Newspapers segment. Amal Whebe covered the Andrew Mountbatten Windsor scandal plus online interview shows plus a Guardian article about Ramadan.\n\nMedia Watch Dog did not notice any of the News Breakfast presenters wondering about Iranian Muslims in the lead up to Ramadan. Or drawing viewers' attention to the estimated 30,000 Iranians - many of whom were young - who were murdered by the Iranian Revolutionary Guard for protesting the brutalist Iranian Islamist theocracy.\n\nOn 18 February, The Age's deputy state topic editor Katy Hall wrote a column on The Age's Opinion Page titled \"Australian women turned their backs on Ley but they won't forgive Taylor for this assassination\".\n\nIn a contradictory article, Hall blamed new Liberal Party leader Angus Taylor for assassinating Sussan Ley - even though Hall acknowledged that \"Ley may not have been the right person for the times\". Here's how Comrade Hall's column commenced:\n\nSo this is how the Liberal Party's first foray into female leadership ends. Not with a bang, but with an underwhelming sense of inevitability. Sussan Ley is out after nine months, and the architects of her downfall are wasting no time in telling anybody who'll listen that there was no choice, they had to do it. \"We must change, or we will not continue to exist,\" senator James Paterson said last week.\n\nIt's not as if people were betting their life savings on the good ship SS Ley making it to the next federal election. But in appointing Angus Taylor as leader and Jane Hume as deputy, the Liberals have once again shown that their version of generational change bears a striking resemblance to the past. And watching a group of men wheel and deal in the shadows until they got their way doesn't feel like an evolution so much as a return to regular programming.\n\nThe truth is, though, even without the industrious efforts of her colleagues to undermine her, Ley was never going to be the solution to the Liberal Party's women problem because serious women never took her seriously.\n\nConfused? Well, MWD is. According to one of The Age's senior editors, Sussan Ley was due to die of natural cause if she hadn't been assassinated first. How about that?\n\nThen Comrade Hall endorsed Malcolm Turnbull's claim that Taylor is a \"best qualified idiot\". In fact, the Miserable Ghost (re which see the hugely popular Five Paws Award segment) said that \"a lot of people say about Angus, Angus Taylor is he [sic] is the best qualified idiot they've ever seen\". Turnbull didn't say who these (alleged) people are. It would seem that Hall believes what she wants to believe.\n\nAnd here is the penultimate paragraph of the Katy Hall rant:\n\nTaylor says he's a Liberal in the traditional sense, and his supporters say his success meant the party averted the crisis of a more right-wing leader in Andrew Hastie. Maybe, but Taylor's treatment of Ley, unintentionally or otherwise, was a dog whistle to the far right that the party's brief foray into at least pretending they were interested in new-age woke politics like letting women have a go and earning back the trust of women is over.\n\nThat was published on 18 February the very day that the new Liberal Party leader announced his shadow cabinet. The following Liberal Party women are in Taylor's shadow cabinet - Claire Chandler, Melissas McIntosh, Jane Hume, Jacinta Nampijinpa Price, Michaelia Cash, Anne Ruston and Sarah Henderson.\n\nThere never has been so many influential women in senior positions in the Liberal Party cabinet or shadow cabinet. And yet one of The Age's senior editors claimed that Angus Taylor has delivered a \"dog whistle\" to the party's \"far right\" that the party is \"not interested in giving women a fair go\". How ignorant can an Age senior editor get? And here's another question: Can You Bear It?\n\nIt is not long ago that Sydney Morning Herald and Age cartoonist Cathy Wilcox was criticising her employer for apologising for a cartoon she had drawn which was regarded to many as anti-Semitic. In that it implied that Australians who supported a royal commission into the Bondi massacre were controlled by Israel's prime minister Benjamin Netanyahu. Comrade Wilcox put out a post suggesting that the Sydney Morning Herald and The Age lacked integrity due to their apology for the cartoon rant. However, despite Wilcox's recent apparent disgust with her employer she is still taking Nine's moolah.\n\nCathy Wilcox was on ABC TV's Insiders' \"Talking Pictures\" segment last Sunday commenting on the political cartoons - including her own. \"Talking Pictures\" has been presented by the left-of-centre photographer Mike Bowers since Moses was a boy. He - Comrade Bowers that is - once worked for the left-of-centre Sydney Morning Herald, then worked for the leftist The Guardian Australia before settling at the leftist New Daily. You get the picture.\n\nOn 14 February, Bowers chose to discuss with Cathy Wilcox cartoons on the Liberal Party's leadership change, anti-Herzog protestors regarding Israel's president Isaac Herzog's visit to Australia and NSW Police's handling of the protestors. Let's go to the transcript:\n\nMike Bowers: Cathy, amidst all the calls for social cohesion, there was a fair bit of division as the Israeli President, Mr. Herzog, visited Australia. Megan Herbert [The Age and Sydney Morning Herald] has drawn the social cohesion daisy being watered by the tears of Bondi and then trampled on the freedoms of expression here.\n\nCathy Wilcox: [laughing] Black booted riot squad of New South Wales stamping over while laying out the red carpets. It's an extremely poignant cartoon.\n\nMike Bowers: Yep. Lovely cartoon by you [Cathy Wilcox] here, that should socially cohere them nicely. Have you been cohered enough?\n\nCathy Wilcox: What do we want? To be stopped from saying what we feel. When do we want it? Right now.\n\nBelow is Wilcox's so-called pro-free expression cohere. It depicts NSW Police murdering anti-Israel protestors with the support of NSW Labor premier Chris Minns. And Comrade Wilcox reckons that The Age and Sydney Morning Herald lack integrity. Can You Bear It?\n\nMedia Watch Dog's Five Paws Award was inaugurated in Issue Number 26 (4 September 2009) during the time of Nancy (2004-2017). The first winner was ABC TV presenter Emma Alberici. Ms Alberici scored for remembering the Nazi-Soviet Pact of 23 August 1939 whereby Hitler and Stalin divided Eastern Europe between Germany and the Soviet Union. And for stating that the Nazi-Soviet Pact had effectively started the Second World War, since it was immediately followed by Germany's invasion of Poland from the West (at a time when the Soviet Union had become an ally of Germany). Soon after, the Soviet Union invaded Poland from the East. Over the years, the late Nancy's Five Paws Award has become one of the world's most prestigious gongs - rating just below the Nobel Prize and the Academy Awards.\n\nThese days Tom Switzer writes articles for The Australian, presents the podcast Switzerland (covering politics, modern history and international relations) and is the author of the recently published monograph Events, Dear Boy: How Any Government Can Be Derailed (Centre for Independent Studies). When working for Opposition leader Brendan Nelson in 2007, Switzer had dealings with then Liberal Party frontbencher Malcolm Turnbull. On Monday 16 February, The Australian published a Switzer article titled \"The enduring ghastliness of Malcolm Turnbull\".\n\nThe fact is that Malcolm Turnbull would never have become prime minister of Australia without the Liberal Party of Australia. A number of NSW Labor Party operatives have said that Turnbull expressed interest in joining the Labor Party some decades ago - but was advised that he was not wanted.\n\nTurnbull contested Liberal Party pre-selection against the sitting Liberal Party MP Peter King in 2003 and won after what some described as a branch-stack. He became the MP for Wentworth in 2004. Following the Coalition's defeat in the November 2007 election, Brendan Nelson defeated Turnbull to replace John Howard as Liberal Party leader. In time Turnbull successfully defeated Nelson to become Liberal Party leader in September 2008.\n\nTurnbull subsequently lost support of the Liberal Party room in December 2009 and was replaced by Tony Abbott as Liberal Party leader. Turnbull replaced Abbott as prime minister in September 2015 after a party room ballot. Having lost 14 seats to Labor in the August 2016 election, Turnbull was replaced by Scott Morrison at a Liberal Party room ballot in August 2018. And he has been whinging ever since.\n\nTom Switzer reminded readers that, despite stating after he stepped down as prime minister that he would not become a Miserable Ghost - meaning a former political leader who kept commenting on politics - he did precisely that. As Switzer wrote in The Australian:\n\nEver since losing the Liberal leadership in 2018, he [Turnbull] has repeatedly lashed out at his former party, reserving particular fire for the internal opponents he faced while prime minister after succeeding Tony Abbott. Turnbull remains the only federal party leader to have lost the leadership twice in party room ballots, and he is widely known to attribute both downfalls to conservatives who resisted his positions on the emissions trading scheme in 2009 and the National Energy Guarantee in 2018.\n\nTurnbull's interventions are too often cheap and unbecoming of a former leader of a great political party. Instead of reasoned argument, he defaults to abuse and denigration. The judgment may sound severe, but no former prime minister has descended so readily to this level. Hardly a week passes without him airing his grievances in public. From The Guardian to the ABC, he delivers poisonous and deeply personal broadsides against Liberal leaders and the party he once led.\n\nAt times, his bile borders on the comical. Consider his March 2019 interview with the BBC's Andrew Neil, when Turnbull claimed his party defenestrated him in August 2018 because colleagues knew he would win the next election. An incredulous Neil replied: \"You're telling me your own party didn't want you to win the next election? That's not credible.\" He then reminded Turnbull he had lost 40 consecutive polls in the lead-up to the coup.\n\nMalcolm Turnbull's comment to Andrew Neil was delusional. What he said was that fellow Liberals in government were so desperate to depose him that they wanted the Coalition to lose office and were willing to forego the job satisfaction and high salaries that go with being ministers and backbenchers.\n\nIn recent times Turnbull has fanged his successors as Liberal Party leader. In Turnbull's view (i) Scott Morrison is \"deceitful\", \"duplicitous\" and a \"liar\", (ii) Peter Dutton is a \"thug\" and (iii) he said this about Angus Taylor in a response to a question from Patricia Karvelas on the ABC TV News Channel on 13 February:\n\nMalcolm Turnbull: Well, I mean, Angus, that's - that the direction Angus is going to take, that will just condemn the Liberal Party to further irrelevance. I mean, the curious thing a lot of people say about Angus Taylor, is he is the best qualified idiot they've ever met...\n\nMalcolm Turnbull was not asked - and did not say - who are the \"lot of people\" who say that Angus Taylor \"is the best qualified idiot they've ever met\". Or did Turnbull just make this up?\n\nTom Switzer, who used to know Turnbull well, ended his article with this insightful remark:\n\nBefore the 2024 US election, Foreign Affairs magazine ran Malcolm Turnbull's character sketch of a presidential contender: \"Like most people, he is often wrong. Unlike most people, however, he is never in doubt. A powerful narcissistic self-belief has given him the strength to defy not just his many enemies but even reality itself.\" He was describing Donald Trump. But the description lands with the force of irony, for it doubles as a precise study of Turnbull himself.\n\nMedia Watch Dog's \"You Must Remember This\" segment is based on the chorus line in the song As Time Goes By, which was popularised by the film Casablanca. It is devoted to reminding the usual suspects of what they and/or those they supported once wrote or said or did - or, indeed, what they failed to write or say.\n\nAs covered in this issue's Five Paws Award segment, Tom Switzer has won MWD's hugely prestigious Five Paws Award for his article in The Australian on 16 February titled \"The enduring ghastliness of Malcolm Turnbull\". Well done Mr Switzer - and so on.\n\nHowever, Ellie's (male) co-owner has reason to question one aspect of Switzer's piece where he wrote that former Labor minister Gareth Evans had coined the term \"relevance deprivation syndrome\" to describe some former political leaders who found it difficult to live a life away from the attention they received, and the influence they had, when running a country.\n\nNot so, according to Ellie's (male) co-owner. In newspaper columns which Gerard Henderson wrote in 1993 he referred to a condition which he described as \"Limelight Deprivation Syndrome\". He attributed this term to an article written by Christa D'Souza. In The Sunday Times in London on 17 January 1993, D'Souza commented that \"LDS...can strike at any time but the trigger is always the same: a taste of fame\". She added, \"Like every other addiction, fame has its withdrawal symptoms if it is not fulfilled; LDS is that dreadful downward spiral to nowhere that every once-famous person dreads\".\n\nHenderson used the term to describe former Labor prime minister Bob Hawke and a Greens senator. He referred to D'Souza's LDS theory, among other places, in a column he wrote in the Courier Mail on 30 July 1993 and in the Sydney Morning Herald and The Age on 5 October 1993. When The Age/SMH article was published, a staffer working in the Keating Labor government's media unit at the time requested a copy of D'Souza's article - which Henderson provided.\n\nAfter Labor lost office in 1996, Gareth Evans commenced talking about \"Relevance Deprivation Syndrome\" with respect to himself and others - without reference to Christa D'Souza.\n\nMedia Watch Dog believes that avid readers would like to know - and remember - this.\n\nSince the ABC TV's Media Watch program commenced in 1989 it has consisted of presenters laying down the (media behaviour) law as if they were a modern-day\n\nMoses descending from the mountain, commandments in hand. All of them have\n\nbeen left-of-centre types as befits a Conservative Free Zone.\n\nLast Saturday Gerard Henderson criticised the ABC TV Four Corners' program titled \"Bondi: Path to Terror\" (which aired on 9 February) in his Weekend Australian column see here. The program was presented by Sean Rubinsztein-Dunlop and produced by Kylie Taylor with Matthew Carney as executive producer.\n\n\"Bondi: Path to Terror\" presented itself as asking questions about whether Australian intelligence organisations failed to prevent the attack by radical Islamists on the Jewish community at Bondi Beach on 14 December 2025. In fact, Comrade Rubinsztein-Dunlop and his colleagues effectively accused the Australian Security Intelligence Organisation (ASIO), headed by the director-general Mike Burgess, of doing just that. Despite the lack of compelling evidence of reliable sources and in the face of advice from ASIO that the program contained serious errors.\n\nAs avid readers know, the ABC is a Conservative Free Zone without one conservative presenter, producer or editor for any of its TV, radio or online outlets. So it came as no surprise that ABC TV's Media Watch essentially came to the defence of ABC TV's Four Corners and was critical of ASIO.\n\nThis is how Linton Besser commenced the Media Watch segment which the ABC titled \"ASIO vs ABC\":\n\nLinton Besser: And now, an extraordinary broadside against this country's most prestigious public affairs television program launched by Australia's security service.\n\nAustralia's intelligence agency has issued a rare public rebuttal ahead of ABC's Four Corners program ...- The Australian, 8 Feb 2026\n\nLinton Besser: Eight days ago, before the airing of a Four Corners episode about the Bondi attacks, the Australian Security Intelligence Organisation warned it had \"grave concerns about its accuracy\" and said: \"If the ABC chooses to publish claims it cannot substantiate - particularly ones it has been told are untrue - we will reserve our right to take further action.\" - ASIO statement: Four Corners, 8 Feb 2026. What further action precisely was left to our imagination. But what was it that had the spooks so spooked? A program which would go on to pose this critical question:\n\nSean Rubinsztein-Dunlop: ... we ask whether the Bondi massacre could have been prevented.\n\n- Four Corners: Bondi (Part 2) Path to Terror, ABC, 9 Feb 2026\n\nWhat a load of tosh. In fact the Four Corners program did not merely ask whether the Bondi massacre could have been prevented - it implied that it should have been prevented. And effectively blamed the ASIO for its failure to do so.\n\nNote that Comrade Besser referred to Four Corners as Australia's \"most prestigious public affairs television program\". Whereas the ASIO operatives were described by Besser as \"spooks\" who had been \"spooked\". ASIO is led by its highly qualified director-general Mike Burgess and is staffed by intelligent male and female operatives. They are not spooked spooks. That's mere abuse.\n\nThe ABC ran the program in spite of ASIO's view that it was seriously flawed. Besser made a number of criticisms of \"Bondi: Path to Terror\" but concluded as follows:\n\nLinton Besser: It is entirely conceivable that later this year the Royal Commission will identify errors in this Four Corners and errors in 'Marcus's' recollections. But in the face of what was in fact improper hostility from ASIO, Four Corners seems to have done everything possible to avoid them. This program was a vital and extraordinary piece of journalism which was honest about its weaknesses and it also served the highest of public interests, getting to the heart of what is clearly the worst possible failure of Australia's security apparatus.\n\nHow about that? Comrade Besser described Four Corners' \"Bondi: Path to Terror\" as an \"extraordinary piece of journalism\" in \"getting to what was clearly the worst possible failure of Australia's security apparatus\". Note the word \"possible\"- which suggests that Four Corners is not sure whether its fanging of ASIO's (alleged) spooks will stand investigation by the Royal Commission into the Bondi massacre. Moreover, Besser and his comrades seem to hold the view that they can criticise ASIO but if ASIO publicly rejects their position it is engaging in \"improper hostility\". An unpleasant double standard, to be sure.\n\nMedia Watch asked two former senior Four Corners' executive producers - Marian Wilkinson and Sally Neighbour - to comment on the program. Guess what? They both backed the program presided over by current executive producer Matt Carney. Despite the fact that even Besser conceded that could be discredited by the Royal Commission which is due to report in late 2026.\n\nComrade Wilkinson asserted that the Four Corners program \"revealed arguably Australia's greatest ever intelligence failure\". Which means that, arguably, this is not the case. [Good point. The word usage of \"arguably\" in this context reveals the person making the claim is not really sure of their position. - MWD Editor.]\n\nAnd Comrade Neighbour referred to \"the terrible events of December 14 and the failure of intelligence and policing that preceded this\". She provided no evidence to support her assertion that the Bondi terrorist attack was an intelligence failure.\n\nJohn Howard said some years ago that ABC journalists were invariably hostile to institutions such as ASIO and the Australian Defence Force. \"Bondi: Path to Terror\" was an example of this. For its part, MWD will not rush to judgment on this issue and will await the report by the Royal Commission headed by the former High Court judge Virginia Bell. In the meantime MWD understands that there are so many potential terrorists in our midst that it would be all but impossible for all of them to be under constant intelligence surveillance.\n\n[It appears that the ABC is in defensive mode on this issue. On 12 February ABC Radio National devoted its ABC News Daily program to a defence of Four Corners. This was a one-sided case for the (ABC) prosecution. - MWD Editor.]\n\nThe late Sydney radio presenter John Laws was wont to say that he did not make mistakes when broadcasting - only \"Deliberate Mistakes\" to entice corrections, comments and so on. Ellie's (male) co-owner Hendo is something of a disciple of Mr Laws - and has adopted this practice when he needs to.\n\nIn last week's MWD, Karen Middleton was referred to as working for The [Boring] Saturday Paper. Lotsa thanks to the avid reader who advised that this is no longer the case. It would seem that Ellie's (male) co-owner has lost track of Comrade Middleton's brilliant career. She moved from the leftist The Guardian Australia to the left-of-centre The Saturday Paper and then to nothing much so far.\n\nWhen David (\"Please call me Speersy\") Speers introduced his colleagues on ABC TV's Insiders on 8 February, he referred to Ms Middleton as an \"author and journalist\". Speersy got that right. What he got wrong was in not correcting Middleton's false claim that there has been a recent terrorist attack in Australia on a Muslim preacher. Not so. The only terrorist attack on a religious leader in Australia was the knifing of a Christian bishop during a religious service of an Armenian Christian group.\n\n[It's great that, unlike the taxpayer funded broadcaster, you correct mistakes in a prominent place. - MWD Editor.]\n\nABC Radio National's Saturday Extra, presented by former BBC journalist Nick Bryant, is one of the taxpayer public broadcaster's leading programs. Yet Comrade Bryant and his producers seem not to correct errors.\n\nOn 14 February, Bryant interviewed Fiona Scott, the federal Liberal Party vice-president, who had this to say, among other things:\n\nFiona Scott: When Robert Menzies formed the Liberal Party back in 1944, he actually pulled together eight different parties. And, you know, some of it included the suffragette movement. I mean, Dame Enid Lyons was the first member of - female member of parliament. She was a member of the Liberal Party. She was the first woman in cabinet. She was also the wife of a former Labor prime minister.\n\nJoseph Lyons has been a Labor premier of Tasmania. But he was elected to the House of Representatives and split with Labor over economic policy during the Great Depression. He became the United Australia Party prime minister of Australia from early 1932 until dying in office in April 1939. Lyons won three elections for the UAP, the predecessor of the Liberal Party. He is one of six Australian prime ministers to have won three or more elections - along with Billy Hughes, Robert Menzies, Malcolm Fraser, Bob Hawke and John Howard.\n\nNick Bryant, who has written a book on Australia, should have picked up and corrected Ms Scott's misstatement. Likewise his producers. After all, Joseph Lyons was a successful prime minister throughout the 1930s. The only long-form biography of Australia's 10th prime minister is Anne Henderson's Joseph Lyons: The People's Prime Minister (NewSouth, 2011)."
  },
  {
    "source": "凤凰网（凤凰新媒体）",
    "company": "Hugging Face",
    "title": "Hugging Face曾拒绝英伟达5亿美元投资：不想看单一巨头脸色",
    "date": "2026-01-29T12:44:34Z",
    "url": "https://tech.ifeng.com/c/8qJcDKteMv1",
    "content": "IT之家 1 月 29 日消息，据英国《金融时报》报道，去年年末，当芯片巨头英伟达向 Hugging Face 抛出一份 5 亿美元的投资意向书时，这家 AI 初创公司做出了一个令业界意外的决定 -- -- 拒绝。\n\n据知情人士透露，这笔交易本可将 Hugging Face 的估值推至 70 亿美元，超过其过去十年融资总额。然而，这家公司选择了说\"不\"。\n\nHugging Face 方面对此表示，不希望出现单一主导投资者来左右决策，英伟达则对此不予置评。\n\nHugging Face 运营着一个托管 250 万个公开 AI 模型和 70 余万个公开数据集的平台，用户可自由下载。一款模型在 Hugging Face 上的受欢迎程度，已成为衡量其被 AI 开发者采纳程度的重要标尺。\n\n凭借全球 1300 万用户，Hugging Face 致力于推广面向开发者免费开放的\"开源\"模型，这使得中国 DeepSeek、阿里巴巴等企业的 AI 产品得以更便捷地触达全球用户。\n\n这一模式与 OpenAI、谷歌、Anthropic 等硅谷主流玩家背道而驰，后者专注于构建\"闭源\"专有模型，再向消费者和企业出售访问权限。Hugging Face 正处于一场关于人工智能应如何构建、共享与拥有的地缘政治和商业博弈的中心。\n\n\"开源模型有助于推动 AI 民主化，对抗权力集中。在我看来，权力集中是 AI 领域最大的风险，\"Hugging Face 联合创始人兼首席执行官克莱姆 · 德朗格表示。\n\n稳健的财务状况是 Hugging Face 敢于拒绝英伟达的底气所在。公司采用\"免费增值\"商业模式，约 3% 的客户（通常为大型企业）会为额外功能付费，如更大存储空间和私有仓库权限。\n\n德朗格透露，公司 2025 年已实现盈利，但因投资数据集，今年第一季度出现亏损。Hugging Face 累计融资了 4 亿美元，2023 年估值为 45 亿美元，目前账面上仍留存半数资金。德朗格强调，公司并不追求收入最大化，而是鼓励开发者为文本、图像和视觉模型提供开源替代方案。\n\n公司联合创始人兼首席科学官托马斯 · 沃尔夫认为，DeepSeek 等中国企业的崛起正成为 Hugging Face 的利好，它们证明了开源模型的威力。\"开源模型不必是二等公民，实际上可以非常、非常令人印象深刻，\"他说。\n\n2022 年，Hugging Face 曾推出多语言 AI 模型 BLOOM，但此后为控制成本、避免与平台托管的模型竞争，已逐步退出自研模型领域。\n\n沃尔夫介绍，公司转而投资机器人、数据集和科学研究 AI 领域，并于去年以未公开金额收购了机器人公司 Pollen。\n\nHugging Face 独特的企业架构与其去中心化的 AI 开发理念一脉相承。员工可在全球任何地方远程办公，公司在美国、英国、法国和瑞士设有办公室，德朗格本人常驻迈阿密。这种通过 Slack 沟通的全球化布局，却让部分前员工感到在重大战略决策上被边缘化。\n\n知情人士称，近几个月已有员工因感觉自身工作与快速变化的公司优先级不符而离职。\n\n薪酬方面，Hugging Face 给研究人员的年薪通常在 10 万至 20 万美元（IT之家注：现汇率约合 69.6 万至 139.1 万元人民币）之间，远低于科技巨头顶尖 AI 研究员的百万美元级别，但基本与初创公司持平。沃尔夫认为，作为补偿，公司为研究人员提供了明确的使命 -- -- 对抗硅谷霸权，并允许他们追求感兴趣的研究方向。\n\n与通过保密协议严格管控员工对外沟通的大型科技公司不同，Hugging Face 允许员工公开谈论工作。\"我们的员工通常曝光度极高，\"沃尔夫说，\"我认为这对他们很好。但 [因此] 他们不断收到挖角邀请。\"\n\n这种自由对 Hugging Face 首席伦理科学家、谷歌负责任 AI 团队联合创始人玛格丽特 · 米切尔仍具吸引力。这位常驻西雅图的研究员表示，她曾拒绝年薪超百万美元的工作机会，只为坚守 Hugging Face\"反硅谷\"的理念。\"我不想失去自己的声音，\"她说。"
  },
  {
    "source": "Observer",
    "company": "Hugging Face",
    "title": "Hugging Face's Monetization Chief Jeff Boudier Isn't Interested in Chasing Money",
    "date": "2026-02-11T20:28:20Z",
    "url": "https://observer.com/2026/02/hugging-face-monetization-chief-jeff-boudier-business-model/",
    "content": "The head of monetization says Hugging Face would rather build trust and sustainability than chase funding or hyper growth.\n\nHugging Face, the A.I. startup known for its GitHub-like hosting platform used by millions of developers and businesses, is pushing back against Silicon Valley's obsession with relentless fundraising and hyper growth. The New York City-based company, which hasn't raised venture capital in more than two years, is in no rush to seek new funding or pursue lucrative revenue opportunities like introducing ads, said Jeff Boudier, the company's head of monetization.\n\nSign Up For Our Daily Newsletter Sign Up\n\nThank you for signing up!\n\nBy clicking submit, you agree to our <a href=\"http://observermedia.com/terms\">terms of service</a> and acknowledge we may use your information to send you emails, product samples, and promotions on this website and other properties. You can opt out anytime.\n\nSee all of our newsletters\n\n\"In a funny way, we're taking a contrarian approach,\" Boudier, who has led product and growth at Hugging Face since 2020, told Observer. \"We're not famous for making money. We're famous for making A.I. more accessible and easier to use.\"\n\nA former business development executive at GoPro, Boudier joined Hugging Face as its first \"business-focused\" employee, in his words, tasked with building up the company's business model. Five years into his tenure, Hugging Face is now either net profitable or making investments, depending on the quarter, he said.\n\nHugging Face was founded a decade ago by French entrepreneurs Clément Delangue, Julien Chaumond and Thomas Wolf. It started out building a chatbot but later gained prominence as a platform where researchers and developers share models and datasets, with a strong emphasis on open-source code. Its offerings extend beyond large language models to include speech, image and video. Today, the platform hosts 2.5 million models, 700,000 datasets and has 13 million users worldwide.\n\nHugging Face hasn't raised funds in more than two years. Its last round, a $235 million Series D in 2023, valued the company at $4.5 billion -- a figure impressive at the time but modest by today's A.I. standards. Last year, the company reportedly turned down a $500 million investment from Nvidia, according to the Financial Times. The publication reported that Hugging Face rejected the offer to avoid taking on a single, dominant investor. Boudier said the company still has money in the bank from the 2023 round and that funding isn't a \"priority\" right now. Instead, he's focused on building a financially sustainable platform.\n\nHow does Hugging Face make money?\n\nHugging Face operates a \"freemium\" business model, where most users access its services for free and a small portion -- roughly 3 percent to 5 percent of users -- pay for high usage or enterprise collaboration, Boudier said. Paying clients include major tech players like Meta, Google, Microsoft, Apple, OpenAI and Anthropic.\n\nIn recent months, Hugging Face has also been experimenting with a new, albeit small, revenue stream: robots. At the end of last year, it began shipping the Reachy Mini, a small, desktop-sized robot that starts at $299 and allows users to experiment with various A.I. applications. More than 5,000 units have been sold. The robot made an appearance during Nvidia CEO Jensen Huang's CES keynote in January to demonstrate the capabilities of Nvidia models. \"It's a successful experiment, and we're looking to scale it this year,\" said Boudier.\n\nHugging Face is investing heavily in robotics, betting that demand for on-device A.I. will continue soaring, Boudier noted. Production of the Reachy Mini was made possible by Hugging Face's acquisition of Pollen Robotics, a developer of open-source humanoid robots, in 2025. The previous year, Hugging Face launched the LeRobot library, which targets software for robotics systems.\n\n\"No\" to ads in chatbots\n\nIn-app advertising has sparked a debate across the A.I. community in recent weeks. OpenAI recently began introducing ads in ChatGPT. The move that was condemned by rival Anthropic. Hugging Face has faced the same question but decided to stay ad-free. A few years ago, Hugging Face was approached by ad networks interested in integrating ads into its chat app, Hugging Chat.\n\n\"We didn't consider at all these proposals,\" said Boudier. \"In my view, it was just completely off what we're trying to do and our values and business model.\"\n\nBoudier said the introduction of ads will only further complicate the delicate balance of trust between users and A.I. tools. \"It's interesting to see how it plays out.\""
  },
  {
    "source": "WinBuzzer",
    "company": "Hugging Face",
    "title": "Hugging Face Repositories Used to Spread Android RAT Malware",
    "date": "2026-02-03T12:47:46Z",
    "url": "https://winbuzzer.com/2026/02/03/hugging-face-repositories-used-spread-android-rat-malware-xcxwbn/",
    "content": "Platform Response: Hugging Face removed the malicious repositories after notification, but attackers quickly relaunched the campaign under a new identity.\n\nAttackers exploited Hugging Face repositories to distribute Android malware disguised as a security app, hosting thousands of malicious files on the trusted AI platform before researchers discovered the campaign.\n\nBitdefender researchers discovered the malware operation using Hugging Face platform, an open-source platform used by users and researchers worldwide to host machine learning models, datasets and other tools. Malicious APK files were hosted directly inside a dataset that was set as public, bypassing traditional security scrutiny.\n\nForensic analysis revealed an industrial-scale campaign leveraging server-side polymorphism to evade detection. Attackers use server-side polymorphism by producing new payloads roughly every 15 minutes, which alters file hashes while preserving malicious behavior to frustrate signature-based detection tools.\n\n\"At the time of investigation, the repository was approximately 29 days old and had accumulated more than 6,000 commits\"\n\nThe 15-minute generation cycle enables attackers to maintain persistent presence even on platforms with active scanning. Each new variant appears clean until security tools can analyze and catalog it.\n\nThis sophisticated infrastructure supported an equally deceptive distribution strategy. The campaign distributed a fake security app named TrustBastion malware, which promised comprehensive security features for free but served as a dropper for an Android RAT (remote access trojan) with no genuine security functionality.\n\nTrustBastion malware was distributed through scareware-style advertisements warning that devices are infected. Once installed, the dropper immediately displays a prompt telling users an update is required to continue using the application.\n\nThe malware presents visuals made to look like Google Play and Android system update dialogs, exploiting user trust in familiar Android interfaces.\n\nOnce the user accepts the fake update prompt, TrustBastion contacts a remote server linked to trustbastion[.]com, which redirects the app to a Hugging Face infrastructure dataset repository where the final malicious APK is downloaded using Hugging Face's CDN.\n\nThe multi-stage infection chain exploits both interface familiarity and infrastructure reputation to bypass user skepticism that might otherwise prevent installation.\n\nAfter deployment, the malware requests extensive permissions. Android RAT requests Accessibility Services under the guise of 'Phone Security,' gaining permissions for screen recording, overlays, and casting.\n\nThe malware itself can grab screenshots, display fake login interfaces for popular payment services, and steal the lock screen code. The data is then exfiltrated to a third-party server.\n\nAttackers use fake system and financial interfaces to steal credentials, including Alipay and WeChat login details. Android RAT can harvest sensitive credentials including the lockscreen password. The payload exfiltrates data to a Command and Control server at 154.198.48.57:5000, tied to trustbastion[.]com.\n\nThis combination of accessibility permissions, screen recording, and overlay capabilities enables attackers to capture payment credentials, authentication tokens, private communications, and behavioral patterns. Account takeovers across multiple services beyond the initially targeted payment platforms become possible.\n\nThe attackers selected Hugging Face strategically. Hugging Face infrastructure attracts threat actors because it offers a way to host various file types, not just those needed to develop AI solutions.\n\nMalicious APK files downloads occur from Hugging Face's CDN, bypassing scrutiny of suspicious domains.\n\nThe exploitation of Hugging Face's trusted infrastructure creates a security blind spot. Organizations and users whitelist traffic from legitimate developer platforms while maintaining strict controls over unknown domains.\n\nBitdefender notified Hugging Face about the malicious repositories, prompting swift dataset removal. However, after takedown, attackers relaunched the campaign as 'Premium Club' with identical code but new icons.\n\nMalicious code found in AI models shared on Hugging Face has been deployed in the past, indicating ongoing security challenges for the platform.\n\nThis immediate resurrection under a new identity indicates that reactive takedowns alone cannot eliminate platform abuse threats. Hugging Face and similar open platforms must implement proactive detection mechanisms rather than relying solely on researcher notifications.\n\nThe platform has also faced other security breaches including the Hugging Face Spaces platform incident that compromised authentication secrets in 2024.\n\nThe platform doesn't seem to have meaningful filters that govern what people can upload. While Hugging Face states that all uploads are scanned with ClamAV, an open-source antivirus engine, attackers circumvent this protection with polymorphic variants. The platform's history also includes exposed API tokens affecting major companies in 2023.\n\nBitdefender Mobile Security detects the malware via behavioral analysis including permission abuse and C2 patterns. For affected users, the malware's comprehensive surveillance capabilities mean attackers can drain bank accounts, hijack social media profiles, and maintain persistent access to private communications even after the initial infection is removed.\n\nThe failure of signature-based scanning against 15-minute polymorphic generation cycles demonstrates that platforms must adopt behavioral detection systems analyzing permission requests and runtime behavior to protect users from credential theft and device compromise."
  },
  {
    "source": "Medium",
    "company": "Hugging Face",
    "title": "Scaling LLM Post-Training at Netflix",
    "date": "2026-02-13T08:33:31Z",
    "url": "https://netflixtechblog.com/scaling-llm-post-training-at-netflix-0046f8790194",
    "content": "Pre-training gives Large Language Models (LLMs) broad linguistic ability and general world knowledge, but post-training is the phase that actually aligns them to concrete intents, domain constraints, and the reliability requirements of production environments. At Netflix, we are exploring how LLMs can enable new member experiences across recommendation, personalization, and search, which requires adapting generic foundation models so they can better reflect our catalog and the nuances of member interaction histories. At Netflix scale, post-training quickly becomes an engineering problem as much as a modeling one: building and operating complex data pipelines, coordinating distributed state across multi-node GPU clusters, and orchestrating workflows that interleave training and inference. This blog describes the architecture and engineering philosophy of our internal Post-Training Framework, built by the AI Platform team to hide infrastructure complexity so researchers and model developers can focus on model innovation -- not distributed systems plumbing.\n\nA Model Developer's Post-Training Journey\n\nPost-training often starts deceptively simply: curate proprietary domain data, load an open-weight model from Hugging Face, and iterate batches through it. At the experimentation scale, that's a few lines of code. But when fine-tuning production-grade LLMs at scale, the gap between \"running a script\" and \"robust post-training\" becomes an abyss of engineering edge cases.\n\nGetting the data right\n\nOn paper, post-training is straightforward: choose a tokenizer, preprocess the dataset, and build a dataloader. In practice, data preparation is where things break. High-quality post-training -- instruction following, multi-turn dialogue, Chain-of-Thought -- depends on precisely controlling which tokens contribute to the loss. Hugging Face chat templates serialize conversations, but don't specify what to train on versus ignore. The pipeline must apply explicit loss masking so only assistant tokens are optimized; otherwise the model learns from prompts and other non-target text, degrading quality.\n\nVariable sequence length is another pitfall. Padding within a batch can waste compute, and uneven shapes across FSDP workers can cause GPU synchronization overhead. A more GPU-efficient approach is to pack multiple samples into fixed-length sequences and use a \"document mask\" to prevent cross-attention across samples, reducing padding and keeping shapes consistent.\n\nSetting up the model\n\nLoading an open-source checkpoint sounds simple until the model no longer fits on one GPU. At that point you need a sharding strategy (e.g., FSDP, TP) and must load partial weights directly onto the device mesh to avoid ever materializing the full model on a single device.\n\nAfter loading, you still need to make the model trainable: choose full fine-tuning vs. LoRA, and apply optimizations like activation checkpointing, compilation, and correct precision settings (often subtle for RL, where rollout and policy precision must align). Large vocabularies (>128k) add a further memory trap: logits are [batch, seq_len, vocab] and can spike peak memory. Common mitigations include dropping ignored tokens before projection and computing logits/loss in chunks along the sequence dimension.\n\nStarting the training\n\nEven with data and models ready, production training is not a simple \"for loop\". The system must support everything from SFT's forward/backward pass to on-policy RL workflows that interleave rollout generation, reward/reference inference, and policy updates.\n\nAt Netflix scale, training runs as a distributed job. We use Ray to orchestrate workflows via actors, decoupling modeling logic from hardware. Robust runs also require experiment tracking (model quality metrics like loss and efficiency metrics like MFU) and fault tolerance via standardized checkpoints to resume cleanly after failures.\n\nThese challenges motivate a post-training framework that lets developers focus on modeling rather than distributed systems and operational details.\n\nThe Netflix Post-Training Framework\n\nWe built Netflix's LLM post-training framework so Netflix model developers can turn ideas like those in Figure 1 into scalable, robust training jobs. It addresses the engineering hurdles described above, and also constraints that are specific to the Netflix ecosystem. Existing tools (e.g., Thinking Machines' Tinker) work well for standard chat and instruction-tuning, but their structure can limit deeper experimentation. In contrast, our internal use cases often require architectural variation (for example, customizing output projection heads for task-specific objectives), expanded or nonstandard vocabularies driven by semantic IDs or special tokens, and even transformer models pre-trained from scratch on domain-specific, non-natural-language sequences. Supporting this range requires a framework that prioritizes flexibility and extensibility over a fixed fine-tuning paradigm.\n\nFigure 2 shows the end-to-end stack from infrastructure to trained models. At the base is Mako, Netflix's internal ML compute platform, which provisions GPUs on AWS. On top of Mako, we run robust open-source components -- PyTorch, Ray, and vLLM -- largely out of the box. Our post-training framework sits above these foundations as a library: it provides reusable utilities and standardized training recipes for common workflows such as Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), Reinforcement Learning (RL), and Knowledge Distillation. Users typically express jobs as configuration files that select a recipe and plug in task-specific components.\n\nFigure 3 summarizes the modular components we built to reduce complexity across four dimensions. As with most ML systems, training success hinges on three pillars -- Data, Model, and Compute -- and the rise of RL fine-tuning adds a fourth pillar: Workflow, to support multi-stage execution patterns that don't fit a simple training loop. Below, we detail the specific abstractions and features the framework provides for each of these dimensions:\n\n* Data: Dataset abstractions for SFT, reward modeling, and RL; high-throughput streaming from cloud and disk for datasets that exceed local storage; and asynchronous, on-the-fly sequence packing to overlap CPU-heavy packing with GPU execution and reduce idle time.\n\n* Model: Support for modern architectures (e.g., Qwen3, Gemma3) and Mixture-of-Experts variants (e.g., Qwen3 MoE, GPT-OSS); LoRA integrated into model definitions; and high-level sharding APIs so developers can distribute large models across device meshes without writing low-level distributed code.\n\n* Compute: A unified job submission interface that scales from a single node to hundreds of GPUs; MFU (Model FLOPS Utilization) monitoring that remains accurate under custom architectures and LoRA; and comprehensive checkpointing (states of trained parameters, optimizer, dataloader, data mixer, etc.) to enable exact resumption after interruptions.\n\n* Workflow: Support for training paradigms beyond SFT, including complex online RL. In particular, we extend Single Program, Multiple Data (SPMD) style SFT workloads to run online RL with a hybrid single-controller + SPMD execution model, which we'll describe next.\n\nToday, this framework supports research use cases ranging from post-training large-scale foundation models to fine-tuning specialized expert models. By standardizing these workflows, we've lowered the barrier for teams to experiment with advanced techniques and iterate more quickly.\n\nLearnings from Building the Post-Training Framework\n\nBuilding a system of this scope wasn't a linear implementation exercise. It meant tracking a fast-moving open-source ecosystem, chasing down failure modes that only appear under distributed load, and repeatedly revisiting architectural decisions as the post-training frontier shifted. Below are three engineering learnings and best practices that shaped the framework.\n\nScaling from SFT to RL\n\nWe initially designed the library around Supervised Fine-Tuning (SFT): relatively static data flow, a single training loop, and a Single Program, Multiple Data (SPMD) execution model. That assumption stopped holding in 2025. With DeepSeek-R1 and the broader adoption of efficient on-policy RL methods like GRPO, SFT became table stakes rather than the finish line. Staying close to the frontier required infrastructure that could move from \"offline training loop\" to \"multi-stage, on-policy orchestration.\"\n\nSFT's learning signal is dense and immediate: for each token position we compute logits over the full vocabulary and backpropagate a differentiable loss. Infrastructure-wise, this looks a lot like pre-training and maps cleanly to SPMD -- every GPU worker runs the same step function over a different shard of data, synchronizing through Pytorch distributed primitives.\n\nOn-policy RL changes the shape of the system. The learning signal is typically sparse and delayed (e.g., a scalar reward at the end of an episode), and the training step depends on data generated by the current policy. Individual sub-stages -- policy updates, rollout generation, reference model inference, reward model scoring -- can each be implemented as SPMD workloads, but the end-to-end algorithm needs explicit coordination: you're constantly handing off artifacts (prompts, sampled trajectories, rewards, advantages) across stages and synchronizing their lifecycle.\n\nIn our original SFT architecture, the driver node was intentionally \"thin\": it launched N identical Ray actors, each encapsulating the full training loop, and scaling meant launching more identical workers. That model breaks down for RL. RL required us to decompose the system into distinct roles -- Policy, Rollout Workers, Reward Model, Reference Model, etc. -- and evolve the driver into an active controller that encodes the control plane: when to generate rollouts, how to batch and score them, when to trigger optimization, and how to manage cluster resources across phases.\n\nFigure 4 highlights this shift. To add RL support without reinventing distributed orchestration from scratch, we integrated the core infrastructure from the open-source Verl library to manage Ray actor lifecycle and GPU resource allocation. Leveraging Verl's backend let us focus on the \"modeling surface area\" -- our Data/Model/Compute abstractions and internal optimizations -- while keeping orchestration concerns decoupled. The result is a hybrid design: a unified user interface where developers can move between SFT and RL workflows without adopting an entirely different mental model or API set.\n\nHugging Face-Centric Experience\n\nThe Hugging Face Hub has effectively become the default distribution channel for open-weight LLMs, tokenizers, and configs. We designed the framework to stay close to that ecosystem rather than creating an isolated internal standard. Even when we use optimized internal model representations for speed, we load and save checkpoints in standard Hugging Face formats. This avoids \"walled garden\" friction and lets teams pull in new architectures, weights, and tokenizers quickly.\n\nThis philosophy also shaped our tokenizer story. Early on, we bound directly to low-level tokenization libraries (e.g., SentencePiece, tiktoken) to maximize control. In practice, that created a costly failure mode: silent training-serving skew. Our inference stack (vLLM) defaults to Hugging Face AutoTokenizer, and tiny differences in normalization, special token handling, or chat templating can yield different token boundaries -- exactly the kind of mismatch that shows up later as inexplicable quality regressions. We fixed this by making Hugging Face AutoTokenizer the single source of truth. We then built a thin compatibility layer (BaseHFModelTokenizer) to handle post-training needs -- setting padding tokens, injecting generation markers to support loss masking, and managing special tokens / semantic IDs -- while ensuring the byte-level tokenization path matches production.\n\nWe do take a different approach for model implementations. Rather than training directly on transformers model classes, we maintain our own optimized, unified model definitions that can still load/save Hugging Face checkpoints. This layer is what enables framework-level optimizations -- e.g., FlexAttention, memory-efficient chunked cross-entropy, consistent MFU accounting, and uniform LoRA extensibility -- without re-implementing them separately for every model family. A unified module naming convention also makes it feasible to programmatically locate and swap components (Attention, MLP, output heads) across architectures, and provides a consistent surface for Tensor Parallelism and FSDP wrapping policies.\n\nThe trade-off is clear: supporting a new model family requires building a bridge between the Hugging Face reference implementation and our internal definition. To reduce that overhead, we use AI coding agents to automate much of the conversion work, with a strict logit verifier as the gate: given random inputs, our internal model must match the Hugging Face logits within tolerance. Because the acceptance criterion is mechanically checkable, agents can iterate autonomously until the implementation is correct, dramatically shortening the time-to-support for new architectures.\n\nToday, this design means we can only train architectures we explicitly support -- an intentional constraint shared by other high-performance systems like vLLM, SGLang, and torchtitan. To broaden coverage, we plan to add a fallback Hugging Face backend, similar to the compatibility patterns these projects use: users will be able to run training directly on native transformers models for rapid exploration of novel architectures, with the understanding that some framework optimizations and features may not apply in that mode.\n\nProviding Differential Value\n\nA post-training framework is only worth owning if it delivers clear value beyond assembling OSS components. We build on open source for velocity, but we invest heavily where off-the-shelf tools tend to be weakest: performance tuned to our workload characteristics, and integration with Netflix-specific model and business requirements. Here are some concrete examples:\n\nFirst, we optimize training efficiency for our real use cases. A representative example is extreme variance in sequence length. In FSDP-style training, long-tail sequences create stragglers: faster workers end up waiting at synchronization points for the slowest batch, lowering utilization. Standard bin-packing approaches help, but doing them offline at our data scale can add substantial preprocessing latency and make it harder to keep datasets fresh. Instead, we built on-the-fly sequence packing that streams samples from storage and dynamically packs them in memory. Packing runs asynchronously, overlapping CPU work with GPU compute. Figure 5 shows the impact: for our most skewed dataset, on-the-fly packing improved the effective token throughput by up to 4.7x.\n\nWe also encountered subtler performance cliffs around vocabulary expansion. Our workloads frequently add custom tokens and semantic IDs. We found that certain vocabulary sizes could cause the language model head to fall back from a highly optimized cuBLAS kernel to a much slower CUTLASS path, tripling that layer's execution time. The framework now automatically pads vocabulary sizes to multiples of 64 so the compiler selects the fast kernel, preserving throughput without requiring developers to know these low-level constraints.\n\nSecond, owning the framework lets us support \"non-standard\" transformer use cases that generic LLM tooling rarely targets. For example, some internal models are trained on member interaction event sequences rather than natural language, and may require bespoke RL loops that integrate with highly-customized inference engines and optimize business-defined metrics. These workflows demand custom environments, reward computation, and orchestration patterns -- while still needing the same underlying guarantees around performance, tracking, and fault tolerance. The framework is built to accommodate these specialized requirements without fragmenting into one-off pipelines, enabling rapid iteration.\n\nWrap up\n\nBuilding the Netflix Post-Training Framework has been a continual exercise in balancing standardization with specialization. By staying anchored to the open-source ecosystem, we've avoided drifting into a proprietary stack that diverges from where the community is moving. At the same time, by owning the core abstractions around Data, Model, Compute, and Workflow, we've preserved the freedom to optimize for Netflix-scale training and Netflix-specific requirements.\n\nIn the process, we've moved post-training from a loose collection of scripts into a managed, scalable system. Whether the goal is maximizing SFT throughput, orchestrating multi-stage on-policy RL, or training transformers over member interaction sequences, the framework provides a consistent set of primitives to do so reliably and efficiently. As the field shifts toward more agentic, reasoning-heavy, and multimodal architectures, this foundation will help us translate new ideas into scalable GenAI prototypes -- so experimentation is constrained by our imagination, not by operational complexity.\n\nAcknowledgements\n\nThis work builds on the momentum of the broader open-source ML community. We're especially grateful to the teams and contributors behind Torchtune, Torchtitan, and Verl, whose reference implementations and design patterns informed many of our training framework choices -- particularly around scalable training recipes, distributed execution, and RL-oriented orchestration. We also thank our partner teams in Netflix AI for Member Systems for close collaboration, feedback, and shared problem-solving throughout the development and rollout of the Post-Training Framework, and the Training Platform team for providing the robust infrastructure and operational foundation that makes large-scale post-training possible."
  },
  {
    "source": "Notimérica",
    "company": "Hugging Face",
    "title": "Una campaña de 'malware' utiliza la plataforma Hugging Face como...",
    "date": "2026-01-30T12:05:19Z",
    "url": "https://www.notimerica.com/ciencia-tecnologia/noticia-campana-malware-utiliza-plataforma-hugging-face-repositorio-distribuir-troyanos-android-20260130122127.html",
    "content": "Archivo - December 17, 2024, India: In this photo illustration, an Android logo is seen displayed on a smartphone with a Malware logo in the background. - Europa Press/Contacto/Avishek Das - Archivo\n\nMADRID, 30 Ene. (Portaltic/EP) -\n\nInvestigadores han advertido sobre una nueva campaña de ciberataques que utilizan la plataforma Hugging Face como repositorio para distribuir miles de variaciones de cargas maliciosas de troyanos de acceso remoto (RAT) para Android, con lo que se roban datos de credenciales para servicios financieros.\n\nHugging Face es una plataforma de alojamiento 'online' de código abierto utilizada habitualmente para albergar modelos de aprendizaje automático e inteligencia artificial (IA), de manera que permite a desarrolladores compartir o entrenar modelos predeterminados, conjuntos de datos o aplicaciones.\n\nSin embargo, esta plataforma ideada como espacio abierto al que cualquier usuario puede acceder, está siendo utilizada por ciberdelincuentes para distribuir cargas de 'malware' con fines maliciosos, eludiendo los filtros que regulan el contenido que se puede subir a Hugging Face, que habitualmente se analiza mediante el antivirus ClamAV.\n\nAsí lo han dado a conocer investigadores de la compañía de ciberseguridad Bitdefender, quienes han matizado que las cargas maliciosas son una campaña de distribución de RAT para dispositivos Android, que combina métodos de ingeniería social y recursos de Hugging Face para comprometer los dispositivos y, forzando los Servicios de Accesibilidad de Android, robar sus credenciales de servicios financieros.\n\nINGENIERÍA SOCIAL Y APP MALICIOSA QUE CONECTA CON HUGGING FACE\n\nConcretamente, el servicio de Hugging Face ha utilizado de forma abusiva para alojar y distribuir miles de variantes APK peligrosos, como han detallado la firma de ciberseguridad en un comunicado.\n\nEl modus operandi comienza utilizando métodos de ingeniería social con los que los actores maliciosos intentan convencer a los usuarios de que se descarguen una aplicación legítima llamada TrustBastion. Para ello, muestran anuncios de tipo 'scareware', es decir, diseñados para asustar a los usuarios y engañarlos haciendo creer que sus dispositivos están infectados o presentan fallos graves.\n\nEn este marco, la aplicación TrustBastion se presenta como una solución de ciberseguridad gratuita, con capacidades para detectar estafas, mensajes fraudulentos o intentos de 'phishing'. A pesar de que no contiene ninguna funcionalidad peligrosa, una vez instalada, solicita a los usuarios descargar una actualización obligatoria para continuar utilizando la aplicación, a través de una página falsa que simula ser la tienda de aplicaciones Google Play.\n\nLa actualización solicitada no descarga directamente la carga maliciosa, sino que, según han explicado los expertos en ciberseguridad, es en este momento donde entra en juego Hugging Face. Esto se debe a que la actualización conecta con un servidor (trustbastion.com) vinculado a la aplicación TrustBastion, que redirige al repositorio de datos de Hugging Face, donde se aloja el contenido APK malicioso.\n\nPor tanto, la carga útil que contiene el troyano se descarga desde la infraestructura del repositorio y se distribuye a través de su red CDN. Asimismo, para evitar que sea detectada por los sistemas de Hugging Face, los ciberdelincuentes producen nuevas cargas útiles aproximadamente cada 15 minutos.\n\nEsto se lleva a cabo utilizando un método conocido como polimorfismo del lado del servidor, que se basa en un concepto de programación aplicado al 'backend' que permite la reutilización de código sin cambiar la lógica principal, por lo que pasa desapercibido.\n\nEXPLOTAN LOS SERVICIOS DE ACCESIBILIDAD DE ANDROID PARA ROBAR DATOS\n\nUna vez efectuada la descarga maliciosa en el dispositivo Android, se entra en la segunda fase del ataque. Concretamente, el troyano se utiliza para explotar los permisos de los Servicios de Accesibilidad de Android, que posibilitan acceder a funciones de captura de pantalla o bloquear intentos de desinstalación.\n\nPara ello, el 'malware' se hace pasar por una función de 'Seguridad del Teléfono' y guía a los usuarios a través del proceso de activación de los Servicios de Accesibilidad. De esta forma, consigue monitorizar la actividad del usuario en su 'smartphone', realizando capturas de pantalla y, con ello, filtrando la información de sus servicios financieros. Incluso, el propio troyano suplanta servicios financieros como Alipay y WeChat para que, al iniciar sesión, acceda al código de bloqueo de pantalla.\n\nUna vez se consiguen los datos, el troyano envía la información robada a los actores maliciosos mediante el servidor de comando y control centralizado (C2), desde donde se coordina la entrega de carga útil y la exfiltración de datos.\n\nDesde Bitdefender han matizado que, en el momento de la investigación, el repositorio ya contaba con aproximadamente 29 días y había acumulado \"más de 6.000 confirmaciones\". Sin embargo, durante su análisis, el repositorio se eliminó a finales de diciembre y resurgió bajo un nuevo repositorio, esta vez vinculado a una nueva aplicación para Android con nombre de Premium Club.\n\nCon todo ello, tras la investigación, Bitdefender informó a Hugging Face sobre la existencia de este repositorio, que fue eliminado por la plataforma."
  },
  {
    "source": "Diario Siglo XXI",
    "company": "Hugging Face",
    "title": "Una campaña de 'malware' utiliza la plataforma Hugging Face como repositorio para distribuir troyanos para Android",
    "date": "2026-01-30T12:04:54Z",
    "url": "https://www.diariosigloxxi.com/texto-ep/mostrar/20260130122047/campana-malware-utiliza-plataforma-hugging-face-como-repositorio-distribuir-troyanos-android",
    "content": "MADRID, 30 (Portaltic/EP)\n\nInvestigadores han advertido sobre una nueva campaña de ciberataques que utilizan la plataforma Hugging Face como repositorio para distribuir miles de variaciones de cargas maliciosas de troyanos de acceso remoto (RAT) para Android, con lo que se roban datos de credenciales para servicios financieros.\n\nHugging Face es una plataforma de alojamiento 'online' de código abierto utilizada habitualmente para albergar modelos de aprendizaje automático e inteligencia artificial (IA), de manera que permite a desarrolladores compartir o entrenar modelos predeterminados, conjuntos de datos o aplicaciones.\n\nSin embargo, esta plataforma ideada como espacio abierto al que cualquier usuario puede acceder, está siendo utilizada por ciberdelincuentes para distribuir cargas de 'malware' con fines maliciosos, eludiendo los filtros que regulan el contenido que se puede subir a Hugging Face, que habitualmente se analiza mediante el antivirus ClamAV.\n\nAsí lo han dado a conocer investigadores de la compañía de ciberseguridad Bitdefender, quienes han matizado que las cargas maliciosas son una campaña de distribución de RAT para dispositivos Android, que combina métodos de ingeniería social y recursos de Hugging Face para comprometer los dispositivos y, forzando los Servicios de Accesibilidad de Android, robar sus credenciales de servicios financieros.\n\nINGENIERÍA SOCIAL Y APP MALICIOSA QUE CONECTA CON HUGGING FACE\n\nConcretamente, el servicio de Hugging Face ha utilizado de forma abusiva para alojar y distribuir miles de variantes APK peligrosos, como han detallado la firma de ciberseguridad en un comunicado.\n\nEl modus operandi comienza utilizando métodos de ingeniería social con los que los actores maliciosos intentan convencer a los usuarios de que se descarguen una aplicación legítima llamada TrustBastion. Para ello, muestran anuncios de tipo 'scareware', es decir, diseñados para asustar a los usuarios y engañarlos haciendo creer que sus dispositivos están infectados o presentan fallos graves.\n\nEn este marco, la aplicación TrustBastion se presenta como una solución de ciberseguridad gratuita, con capacidades para detectar estafas, mensajes fraudulentos o intentos de 'phishing'. A pesar de que no contiene ninguna funcionalidad peligrosa, una vez instalada, solicita a los usuarios descargar una actualización obligatoria para continuar utilizando la aplicación, a través de una página falsa que simula ser la tienda de aplicaciones Google Play.\n\nLa actualización solicitada no descarga directamente la carga maliciosa, sino que, según han explicado los expertos en ciberseguridad, es en este momento donde entra en juego Hugging Face. Esto se debe a que la actualización conecta con un servidor (trustbastion.com) vinculado a la aplicación TrustBastion, que redirige al repositorio de datos de Hugging Face, donde se aloja el contenido APK malicioso.\n\nPor tanto, la carga útil que contiene el troyano se descarga desde la infraestructura del repositorio y se distribuye a través de su red CDN. Asimismo, para evitar que sea detectada por los sistemas de Hugging Face, los ciberdelincuentes producen nuevas cargas útiles aproximadamente cada 15 minutos.\n\nEsto se lleva a cabo utilizando un método conocido como polimorfismo del lado del servidor, que se basa en un concepto de programación aplicado al 'backend' que permite la reutilización de código sin cambiar la lógica principal, por lo que pasa desapercibido.\n\nEXPLOTAN LOS SERVICIOS DE ACCESIBILIDAD DE ANDROID PARA ROBAR DATOS\n\nUna vez efectuada la descarga maliciosa en el dispositivo Android, se entra en la segunda fase del ataque. Concretamente, el troyano se utiliza para explotar los permisos de los Servicios de Accesibilidad de Android, que posibilitan acceder a funciones de captura de pantalla o bloquear intentos de desinstalación.\n\nPara ello, el 'malware' se hace pasar por una función de 'Seguridad del Teléfono' y guía a los usuarios a través del proceso de activación de los Servicios de Accesibilidad. De esta forma, consigue monitorizar la actividad del usuario en su 'smartphone', realizando capturas de pantalla y, con ello, filtrando la información de sus servicios financieros. Incluso, el propio troyano suplanta servicios financieros como Alipay y WeChat para que, al iniciar sesión, acceda al código de bloqueo de pantalla.\n\nUna vez se consiguen los datos, el troyano envía la información robada a los actores maliciosos mediante el servidor de comando y control centralizado (C2), desde donde se coordina la entrega de carga útil y la exfiltración de datos.\n\nDesde Bitdefender han matizado que, en el momento de la investigación, el repositorio ya contaba con aproximadamente 29 días y había acumulado \"más de 6.000 confirmaciones\". Sin embargo, durante su análisis, el repositorio se eliminó a finales de diciembre y resurgió bajo un nuevo repositorio, esta vez vinculado a una nueva aplicación para Android con nombre de Premium Club.\n\nCon todo ello, tras la investigación, Bitdefender informó a Hugging Face sobre la existencia de este repositorio, que fue eliminado por la plataforma."
  },
  {
    "source": "europa press",
    "company": "Hugging Face",
    "title": "Una campaña de 'malware' utiliza la plataforma Hugging Face como...",
    "date": "2026-01-30T11:31:03Z",
    "url": "https://www.europapress.es/portaltic/ciberseguridad/noticia-campana-malware-utiliza-plataforma-hugging-face-repositorio-distribuir-troyanos-android-20260130122044.html",
    "content": "Archivo - December 17, 2024, India: In this photo illustration, an Android logo is seen displayed on a smartphone with a Malware logo in the background. - Europa Press/Contacto/Avishek Das - Archivo\n\nEuropa Press PortalTIC\n\nPublicado: viernes, 30 enero 2026 12:20\n\nAbrir opciones para compartir\n\nIA\n\nSeguir en\n\nMADRID, 30 Ene. (Portaltic/EP) -\n\nInvestigadores han advertido sobre una nueva campaña de ciberataques que utilizan la plataforma Hugging Face como repositorio para distribuir miles de variaciones de cargas maliciosas de troyanos de acceso remoto (RAT) para Android, con lo que se roban datos de credenciales para servicios financieros.\n\nHugging Face es una plataforma de alojamiento 'online' de código abierto utilizada habitualmente para albergar modelos de aprendizaje automático e inteligencia artificial (IA), de manera que permite a desarrolladores compartir o entrenar modelos predeterminados, conjuntos de datos o aplicaciones.\n\nSin embargo, esta plataforma ideada como espacio abierto al que cualquier usuario puede acceder, está siendo utilizada por ciberdelincuentes para distribuir cargas de 'malware' con fines maliciosos, eludiendo los filtros que regulan el contenido que se puede subir a Hugging Face, que habitualmente se analiza mediante el antivirus ClamAV.\n\nAsí lo han dado a conocer investigadores de la compañía de ciberseguridad Bitdefender, quienes han matizado que las cargas maliciosas son una campaña de distribución de RAT para dispositivos Android, que combina métodos de ingeniería social y recursos de Hugging Face para comprometer los dispositivos y, forzando los Servicios de Accesibilidad de Android, robar sus credenciales de servicios financieros.\n\nINGENIERÍA SOCIAL Y APP MALICIOSA QUE CONECTA CON HUGGING FACE\n\nConcretamente, el servicio de Hugging Face ha utilizado de forma abusiva para alojar y distribuir miles de variantes APK peligrosos, como han detallado la firma de ciberseguridad en un comunicado.\n\nEl modus operandi comienza utilizando métodos de ingeniería social con los que los actores maliciosos intentan convencer a los usuarios de que se descarguen una aplicación legítima llamada TrustBastion. Para ello, muestran anuncios de tipo 'scareware', es decir, diseñados para asustar a los usuarios y engañarlos haciendo creer que sus dispositivos están infectados o presentan fallos graves.\n\nEn este marco, la aplicación TrustBastion se presenta como una solución de ciberseguridad gratuita, con capacidades para detectar estafas, mensajes fraudulentos o intentos de 'phishing'. A pesar de que no contiene ninguna funcionalidad peligrosa, una vez instalada, solicita a los usuarios descargar una actualización obligatoria para continuar utilizando la aplicación, a través de una página falsa que simula ser la tienda de aplicaciones Google Play.\n\nLa actualización solicitada no descarga directamente la carga maliciosa, sino que, según han explicado los expertos en ciberseguridad, es en este momento donde entra en juego Hugging Face. Esto se debe a que la actualización conecta con un servidor (trustbastion.com) vinculado a la aplicación TrustBastion, que redirige al repositorio de datos de Hugging Face, donde se aloja el contenido APK malicioso.\n\nPor tanto, la carga útil que contiene el troyano se descarga desde la infraestructura del repositorio y se distribuye a través de su red CDN. Asimismo, para evitar que sea detectada por los sistemas de Hugging Face, los ciberdelincuentes producen nuevas cargas útiles aproximadamente cada 15 minutos.\n\nEsto se lleva a cabo utilizando un método conocido como polimorfismo del lado del servidor, que se basa en un concepto de programación aplicado al 'backend' que permite la reutilización de código sin cambiar la lógica principal, por lo que pasa desapercibido.\n\nEXPLOTAN LOS SERVICIOS DE ACCESIBILIDAD DE ANDROID PARA ROBAR DATOS\n\nUna vez efectuada la descarga maliciosa en el dispositivo Android, se entra en la segunda fase del ataque. Concretamente, el troyano se utiliza para explotar los permisos de los Servicios de Accesibilidad de Android, que posibilitan acceder a funciones de captura de pantalla o bloquear intentos de desinstalación.\n\nPara ello, el 'malware' se hace pasar por una función de 'Seguridad del Teléfono' y guía a los usuarios a través del proceso de activación de los Servicios de Accesibilidad. De esta forma, consigue monitorizar la actividad del usuario en su 'smartphone', realizando capturas de pantalla y, con ello, filtrando la información de sus servicios financieros. Incluso, el propio troyano suplanta servicios financieros como Alipay y WeChat para que, al iniciar sesión, acceda al código de bloqueo de pantalla.\n\nUna vez se consiguen los datos, el troyano envía la información robada a los actores maliciosos mediante el servidor de comando y control centralizado (C2), desde donde se coordina la entrega de carga útil y la exfiltración de datos.\n\nDesde Bitdefender han matizado que, en el momento de la investigación, el repositorio ya contaba con aproximadamente 29 días y había acumulado \"más de 6.000 confirmaciones\". Sin embargo, durante su análisis, el repositorio se eliminó a finales de diciembre y resurgió bajo un nuevo repositorio, esta vez vinculado a una nueva aplicación para Android con nombre de Premium Club.\n\nCon todo ello, tras la investigación, Bitdefender informó a Hugging Face sobre la existencia de este repositorio, que fue eliminado por la plataforma."
  },
  {
    "source": "DiarioBitcoin",
    "company": "Hugging Face",
    "title": "Meta publica Llama 3.3 70B Instruct en Hugging Face y reaviva la carrera por modelos abiertos",
    "date": "2026-02-17T16:10:11Z",
    "url": "https://www.diariobitcoin.com/ia/meta-publica-llama-3-3-70b-instruct-en-hugging-face-y-reaviva-la-carrera-por-modelos-abiertos/",
    "content": "La colección oficial de Meta para Llama 3.3 70B Instruct aparece en Hugging Face como un punto de acceso a transformadores y repositorios originales. Aunque el anuncio es breve, la actualización registrada el 6 de diciembre de 2024 reabre el debate sobre la velocidad con la que evoluciona la IA abierta y cómo se organiza su distribución en plataformas de referencia.\n\n***\n\n* La página de Hugging Face identifica la colección como el hogar de los transformadores y repositorios originales de Llama 3.3.\n\n* El listado indica una actualización con fecha 6 de diciembre de 2024 y menciona que incluye 1 artículo.\n\n* La presencia de \"70B Instruct\" refuerza el foco en modelos orientados a instrucciones dentro del ecosistema Llama.\n\nHugging Face muestra un listado para meta-llama/Llama-3.3-70B-Instruct, descrito como una colección que \"alberga los transformadores y repositorios originales del Llama 3.3\". El repositorio aparece bajo el título \"meta-llama/Llama-3.3-70B-Instruct · Hugging Face\" y funciona como punto de encuentro para piezas clave del modelo, según indica la propia ficha.\n\nEl registro, según la plataforma, incluye \"1 artículo\" y señala que fue \"actualizado el 6 de diciembre de 2024\". En el mismo resumen se observa el número \"196\", presentado dentro del bloque informativo del listado, aunque el fragmento disponible no especifica a qué métrica corresponde.\n\nLa referencia a \"70B Instruct\" sugiere que se trata de una variante entrenada o afinada para seguir instrucciones, un formato habitual en modelos de lenguaje modernos orientados a uso conversacional y tareas de asistencia. En la práctica, estos modelos suelen priorizar utilidad para usuarios finales y desarrolladores, al responder con más estructura a solicitudes y reglas de interacción.\n\nQué es lo que realmente dice el listado y por qué importa\n\nEl elemento más relevante del texto es la frase que atribuye a la colección el alojamiento de \"transformadores y repositorios originales\" vinculados a Llama 3.3. En el contexto de IA, \"transformadores\" suele referirse a la arquitectura dominante de modelos de lenguaje, pero en plataformas como Hugging Face también se usa para nombrar librerías, artefactos y variantes listas para integrarse en flujos de trabajo.\n\nLa mención de \"repositorios originales\" aporta una señal de procedencia. Para el ecosistema, no es lo mismo un espejo comunitario que una publicación identificada como parte de la familia \"meta-llama\", ya que esto suele asociarse con una fuente primaria y con materiales alineados a lo que el editor considera oficial.\n\nEl texto disponible no lista capacidades, evaluaciones, licencias, ni requisitos de uso. Por esa razón, cualquier lectura sobre rendimiento, costo de inferencia o compatibilidad específica quedaría fuera de lo que la fuente citada afirma explícitamente en su resumen.\n\nAun así, la disponibilidad organizada en una plataforma como Hugging Face tiene un peso práctico. Para investigadores y equipos de producto, la estandarización del acceso suele acelerar pruebas, documentación y reutilización de componentes, especialmente cuando se agrupan recursos bajo una colección reconocible.\n\nEl contexto: por qué los \"repositorios\" son tan importantes como el modelo\n\nEn IA generativa, el modelo no llega solo. Generalmente va acompañado de ficheros de configuración, documentación, pesos, scripts, y, a veces, variantes con ajustes distintos. Por eso, cuando una ficha habla de \"repositorios originales\", el foco se desplaza desde un único artefacto hacia un conjunto de recursos que permiten reproducir o integrar el sistema con menos fricción.\n\nPara el público universitario y técnico, esta distinción es clave al comparar avances. Dos equipos pueden \"usar el mismo modelo\", pero diferir radicalmente en resultados si cambian tokenizadores, configuraciones, plantillas de chat o parámetros de generación. En términos de adopción, el repositorio se convierte en la unidad operativa, no solo en la vitrina.\n\nAdemás, el adjetivo \"Instruct\" suele estar asociado a formatos de uso más directos. En aplicaciones, estos modelos se implementan para asistentes, agentes de soporte, análisis de texto y automatización de tareas, donde la capacidad de seguir instrucciones coherentes y seguras se vuelve tan valiosa como la creatividad.\n\nDesde la perspectiva del mercado, la estandarización de acceso también impacta competencia. Cuando un actor publica recursos de forma organizada, se reducen barreras para prototipar y medir alternativas. Eso obliga a proveedores y competidores a diferenciarse por calidad, soporte, tooling o condiciones de distribución, no solo por el \"nombre\" del modelo.\n\nLa fecha de actualización y lo que sugiere sobre el ritmo del ecosistema\n\nEl listado indica que la colección fue actualizada el 6 de diciembre de 2024. Aunque el fragmento no detalla qué cambió en esa actualización, la sola presencia de una marca temporal aporta una pista: el repositorio se mantiene activo y sujeto a ajustes, algo frecuente en proyectos de IA que van refinando documentación y empaquetado.\n\nEn proyectos de gran escala, estas actualizaciones pueden incluir desde reorganización de archivos hasta correcciones de metadatos. También pueden reflejar la publicación de notas, tarjetas de modelo o recursos auxiliares. Sin embargo, la fuente citada, tal como fue proporcionada, no precisa el alcance.\n\nEl resumen también menciona \"1 artículo\". En el ecosistema de Hugging Face, ese tipo de indicador suele asociarse a contenidos que acompañan la colección, como una entrada explicativa o un documento de referencia. Nuevamente, el texto disponible no describe el tema del artículo ni su contenido.\n\nFinalmente, aparece el número \"196\" en el bloque de la ficha. Dado que no se explica su significado en el fragmento, no es posible concluir si se trata de descargas, likes, colecciones relacionadas u otra métrica interna de la plataforma. Lo prudente es tratarlo como un dato mostrado por la interfaz, sin atribución funcional específica.\n\nImplicaciones para desarrolladores, investigación y adopción empresarial\n\nPara desarrolladores, la existencia de un punto centralizado de recursos reduce el tiempo de arranque. En la práctica, un repositorio claro facilita pruebas de inferencia, integración con frameworks comunes, y la evaluación inicial para decidir si un modelo \"encaja\" con un caso de uso.\n\nEn investigación, el orden y procedencia de los artefactos ayuda a comparar resultados. Cuando los recursos se describen como originales, se minimiza la ambigüedad sobre qué versión se está evaluando, lo que mejora trazabilidad en experimentos y discusiones académicas.\n\nPara empresas, un repositorio bien presentado suele funcionar como un termómetro de madurez. Aunque la fuente no enumera soporte ni garantías, el hecho de que exista una colección identificada puede ser relevante para auditorías internas y para equipos legales y de cumplimiento que revisan la procedencia de los materiales.\n\nEn conjunto, la publicación o consolidación de recursos de Llama 3.3 70B Instruct en Hugging Face actúa como señal de continuidad del ecosistema. El listado no ofrece una narrativa extensa, pero sí confirma la existencia de una colección dedicada, con una fecha de actualización concreta y un enfoque explícito en recursos originales.\n\nADVERTENCIA: DiarioBitcoin ofrece contenido informativo y educativo sobre diversos temas, incluyendo criptomonedas, IA, tecnología y regulaciones. No brindamos asesoramiento financiero. Las inversiones en criptoactivos son de alto riesgo y pueden no ser adecuadas para todos. Investigue, consulte a un experto y verifique la legislación aplicable antes de invertir. Podría perder todo su capital."
  },
  {
    "source": "WebProNews",
    "company": "Hugging Face",
    "title": "How Cybercriminals Weaponized Hugging Face's AI Platform to Deploy Android Banking Trojans at Scale",
    "date": "2026-02-02T18:21:13Z",
    "url": "https://www.webpronews.com/how-cybercriminals-weaponized-hugging-faces-ai-platform-to-deploy-android-banking-trojans-at-scale/",
    "content": "In a sophisticated campaign that highlights the dark side of open-source artificial intelligence platforms, cybersecurity researchers have uncovered a sprawling malware operation that exploited Hugging Face's infrastructure to distribute Android banking trojans. The attack, which targeted users across multiple continents, represents a troubling evolution in how threat actors are leveraging legitimate AI and machine learning platforms to bypass traditional security measures and reach victims at unprecedented scale.\n\nAccording to TechRepublic, security researchers identified a campaign where attackers uploaded malicious Android applications disguised as legitimate software to Hugging Face's model repository. The platform, widely known as a collaborative hub for machine learning models and datasets, became an unwitting accomplice in distributing remote access trojans (RATs) capable of stealing banking credentials, intercepting two-factor authentication codes, and maintaining persistent access to compromised devices. The campaign's sophistication lay not in revolutionary malware techniques, but in its strategic abuse of a trusted platform that security tools rarely flag as suspicious.\n\nThe malware operation specifically targeted Android users through a multi-stage infection process that began with seemingly innocuous applications. These trojanized apps were crafted to appear as productivity tools, system utilities, or entertainment applications, leveraging social engineering tactics to convince users to grant extensive permissions. Once installed, the malware established command-and-control communications and began harvesting sensitive financial data, including banking credentials, cryptocurrency wallet information, and payment card details.\n\nThe Mechanics of Platform Abuse and Trust Exploitation\n\nThe attackers' choice of Hugging Face as a distribution mechanism was far from random. As one of the most prominent platforms in the artificial intelligence community, Hugging Face enjoys widespread trust among developers, researchers, and technology enthusiasts. This reputation created a perfect cover for malicious activities, as security solutions and users alike tend to view content hosted on the platform as legitimate. The threat actors created multiple accounts and uploaded their malicious payloads disguised as AI models or datasets, complete with professional-looking documentation and readme files that mimicked legitimate projects.\n\nThe technical implementation revealed a concerning level of operational security awareness among the attackers. Rather than hosting the entire malware payload on Hugging Face, the threat actors used the platform primarily as a content delivery network for initial stage loaders and configuration files. These components would then download additional malicious modules from secondary infrastructure, creating a layered approach that complicated attribution efforts and made takedown operations more challenging. The malware itself incorporated advanced evasion techniques, including environment checks to detect analysis tools, delayed execution to avoid sandbox detection, and encrypted communications to hide command-and-control traffic.\n\nBanking trojans deployed in this campaign demonstrated capabilities consistent with established malware families, though researchers noted several custom modifications suggesting active development. The malware could overlay fake login screens atop legitimate banking applications, intercept SMS messages containing authentication codes, and even manipulate screen content to hide fraudulent transactions from victims. These features enabled attackers to bypass multi-factor authentication systems and conduct unauthorized transactions while maintaining the appearance of normal device operation.\n\nIndustry Response and Platform Security Challenges\n\nHugging Face's response to the discovery underscored the challenges facing platforms that host user-generated content at scale. The company moved quickly to remove identified malicious content and suspend associated accounts, but the incident raised fundamental questions about content moderation and security verification for AI platforms. Unlike traditional software repositories that can scan for known malware signatures, AI model repositories face unique challenges in distinguishing between legitimate models, benign but poorly documented projects, and deliberately malicious uploads.\n\nThe incident has prompted broader discussions within the cybersecurity community about the security implications of increasingly democratized AI infrastructure. As machine learning platforms lower barriers to entry for AI development and deployment, they simultaneously create new attack vectors that traditional security paradigms struggle to address. The trust mechanisms that make these platforms valuable for collaboration -- open access, minimal barriers to contribution, and community-driven validation -- can be exploited by sophisticated threat actors who understand how to mimic legitimate behavior patterns.\n\nSecurity researchers emphasize that this campaign represents a proof of concept for a potentially much larger problem. If attackers can successfully abuse Hugging Face's infrastructure, similar techniques could be applied to other AI platforms, code repositories, and collaborative development environments. The economics of such attacks are particularly favorable for cybercriminals: by leveraging trusted platforms' infrastructure and reputation, they reduce their own operational costs while increasing the likelihood of successful infections.\n\nFinancial Impact and Victim Targeting Patterns\n\nAnalysis of the malware's targeting patterns revealed a focus on users in regions with high smartphone banking adoption but potentially less mature mobile security awareness. The attackers appeared to prioritize markets where Android devices dominate market share and where banking applications have become primary interfaces for financial services. This strategic targeting maximized the potential return on investment for the campaign, as successful compromises in these markets could yield access to active banking accounts with substantial balances.\n\nThe financial impact of such campaigns extends beyond direct theft from compromised accounts. Victims often face extended periods of financial disruption as they work to secure accounts, dispute fraudulent transactions, and restore their digital identities. Financial institutions bear costs associated with fraud investigation, customer support, and implementing additional security measures. The broader ecosystem suffers reputational damage as incidents erode consumer confidence in mobile banking security and digital financial services.\n\nForensic analysis of the command-and-control infrastructure revealed connections to previously known cybercriminal operations, suggesting the campaign was conducted by an established threat actor group rather than opportunistic amateurs. The infrastructure showed signs of professional operational security, including the use of bulletproof hosting providers, layered proxy networks, and cryptocurrency-based payment systems for monetizing stolen credentials on underground markets.\n\nTechnical Detection and Prevention Strategies\n\nFor security professionals and organizations, the campaign highlights several critical detection and prevention considerations. Traditional perimeter security and signature-based detection prove insufficient against malware distributed through trusted platforms. Instead, organizations must implement behavioral analysis systems that can identify anomalous application activities regardless of their origin. This includes monitoring for unusual permission requests, unexpected network communications, and behaviors inconsistent with an application's stated purpose.\n\nMobile device management solutions and enterprise mobility management platforms require updates to account for these evolving threat vectors. Security policies should enforce application vetting processes that extend beyond simple reputation checks of distribution sources. Organizations should implement network-level monitoring to detect command-and-control communications, even when they originate from applications that passed initial security screenings.\n\nFor individual users, the campaign underscores the importance of maintaining healthy skepticism even toward applications from seemingly legitimate sources. Security experts recommend verifying application authenticity through multiple channels, carefully reviewing permission requests before granting access, and maintaining up-to-date security software on mobile devices. Users should be particularly cautious of applications requesting permissions that seem excessive for their stated functionality, such as SMS access for a calculator app or overlay permissions for a simple game.\n\nRegulatory and Policy Implications for AI Platforms\n\nThe incident has attracted attention from regulatory bodies concerned with both cybersecurity and artificial intelligence governance. Policymakers face the challenge of developing frameworks that protect users from malicious activities while preserving the open, collaborative nature that makes AI platforms valuable for innovation. Overly restrictive regulations could stifle legitimate research and development, while insufficient oversight leaves users vulnerable to sophisticated attacks.\n\nIndustry experts suggest that AI platforms may need to implement tiered trust systems similar to those used by established software repositories. Such systems could require additional verification for accounts seeking to host certain types of content, implement automated scanning for known malicious patterns, and establish community reporting mechanisms that enable rapid response to suspicious activities. However, the technical challenges of implementing such systems for AI models and datasets differ significantly from traditional software security approaches.\n\nThe campaign also raises questions about liability and responsibility when platforms are abused for malicious purposes. Legal frameworks must balance the need for platform accountability with recognition that platforms cannot reasonably inspect every piece of user-generated content in real-time. The debate mirrors similar discussions in other technology sectors about the responsibilities of platforms versus the accountability of individual bad actors.\n\nFuture Threat Evolution and Defensive Adaptation\n\nLooking ahead, security researchers anticipate that abuse of AI platforms will become increasingly sophisticated as threat actors recognize the strategic advantages these platforms offer. Future campaigns may incorporate actual functional AI models that perform legitimate tasks while simultaneously conducting malicious activities, making detection even more challenging. The integration of AI-generated content in social engineering attacks could further enhance the effectiveness of distribution campaigns.\n\nThe cybersecurity industry must evolve its defensive strategies to address these emerging threats. This includes developing specialized security tools capable of analyzing AI models and datasets for malicious components, creating industry-wide information sharing mechanisms to rapidly disseminate threat intelligence about platform abuse, and fostering collaboration between AI platform operators and security researchers. The goal is to maintain the open, innovative character of AI development platforms while implementing sufficient safeguards to prevent their exploitation by malicious actors.\n\nAs artificial intelligence continues its rapid integration into everyday technology, the security implications of AI infrastructure abuse will only grow more significant. The Hugging Face campaign serves as an important case study in how trusted platforms can be weaponized, and how the security community must adapt to protect users in an increasingly complex digital ecosystem. Organizations and individuals alike must recognize that trust in a platform's reputation, while valuable, cannot substitute for comprehensive security practices and vigilant threat awareness in an era where cybercriminals continuously innovate their attack methodologies."
  },
  {
    "source": "Infosecurity Magazine",
    "company": "Hugging Face",
    "title": "Android RAT Uses Hugging Face to Host Malware",
    "date": "2026-02-02T10:50:35Z",
    "url": "https://www.infosecurity-magazine.com/news/android-rat-hugging-face-host/",
    "content": "A new Android remote access trojan (RAT) uses popular AI platform Hugging Face to host and distribute malicious payloads, Bitdefender has revealed.\n\nThe security vendor claimed that Hugging Face - which is designed to host AI tools, models datasets and other assets - did not conduct sufficient checks to vet the content that users upload.\n\nAll uploads are meant to be scanned with open source antivirus engine ClamAV.\n\nAccording to Bitdefender, the infection chain begins when users download malicious Android app dubbed TrustBastion. This appears to be scareware which is forced on users via popups claiming their device is infected with malware.\n\nIn reality, the app is a dropper which, on installation, immediately prompts the user to run an update in order to use it. This update is designed to look like legitimate Google Play and Android system update dialog boxes, increasing the chances of victims following the instructions.\n\nRead more on Hugging Face threats: Malicious AI Models on Hugging Face Exploit Novel Attack Technique.\n\nThe dropper then contacts an encrypted endpoint hosted at trustbastion[.]com, which returns not a malicious APK file but an HTML file. This contains a redirect link which points to the Hugging Face repository hosting the malware.\n\nThis in turn downloads the malicious APK to the victim's device. Using Hugging Face in this way helps those behind the malware campaign avoid setting off any alarms on the victim's device.\n\n\"Typically, traffic from low-trust domains gets flagged immediately, which is why attackers often will try to use well-established domains that don't raise suspicions,\" Bitdefender said.\n\nBitdefender said it contacted Hugging Face before publishing the research and they quickly took down the datasets containing malware. However, the campaign itself already appears to have infected thousands of victims.\n\n\"Analysis of the Hugging Face repository revealed a high volume of commits over a short period of time,\" said Bitfdefender. \"New payloads were generated roughly every 15 minutes. At the time of investigation, the repository was approximately 29 days old and had accumulated more than 6000 commits.\"\n\nIt also appears to be persistent: although one repository went offline, the whole operation simply moved to another redirect link, \"with the project using different icons and some minor adjustments,\" but the same code.\n\nTo increase their chances of success further, the threat actors behind the campaign are using polymorphic techniques.\n\n\"Each new file upload is actually a newly built APK that has the same malicious functionality while introducing minor variations,\" Bitdefender explained. \"They are intended to evade hash-based detection.\"\n\nHowever, the fact that the various payloads share common behavioral traits, permission requests and communication patterns, makes them easier to detect using behavioral analysis techniques, the report noted.\n\nOnce the payload is installed, the malware masquerades as a \"Phone Security\" feature and guides users through the process of enabling Accessibility Services, which in fact gives the RAT \"broad visibility into user interactions across the device,\" said Bitdefender.\n\nIt also requests permissions enabling screen recording, screen casting and overlay display - monitoring all user activity, capturing screen content and sending it to a command-and-control server.\n\nThe malware also impersonates popular financial and payment services like Alipay and WeChat, in order to harvest sensitive credentials.\n\nIt can even capture lockscreen information for these apps' security verification."
  },
  {
    "source": "elEconomista.es",
    "company": "Hugging Face",
    "title": "Alerta en Android: todos los dispositivos están en riesgo por un nuevo virus que roba tus datos bancarios",
    "date": "2026-01-30T12:23:43Z",
    "url": "https://www.eleconomista.es/tecnologia/noticias/13752708/01/26/alerta-en-android-todos-los-dispositivos-estan-en-riesgo-por-un-nuevo-virus-que-roba-tus-datos-bancarios.html",
    "content": "La viuda de Steve Jobs ya ha gastado la mitad de la herencia del antiguo CEO de Apple: su objetivo es cumplir su legado\n\nInvestigadores han advertido sobre una nueva campaña de ciberataques que utilizan la plataforma Hugging Face como repositorio para distribuir miles de variaciones de cargas maliciosas de troyanos de acceso remoto (RAT) para Android, con lo que se roban datos de credenciales para servicios financieros.\n\nHugging Face es una plataforma de alojamiento 'online' de código abierto utilizada habitualmente para albergar modelos de aprendizaje automático e inteligencia artificial (IA), de manera que permite a desarrolladores compartir o entrenar modelos predeterminados, conjuntos de datos o aplicaciones.\n\nSin embargo, esta plataforma ideada como espacio abierto al que cualquier usuario puede acceder, está siendo utilizada por ciberdelincuentes para distribuir cargas de 'malware' con fines maliciosos, eludiendo los filtros que regulan el contenido que se puede subir a Hugging Face, que habitualmente se analiza mediante el antivirus ClamAV.\n\nAsí lo han dado a conocer investigadores de la compañía de ciberseguridad Bitdefender, quienes han matizado que las cargas maliciosas son una campaña de distribución de RAT para dispositivos Android, que combina métodos de ingeniería social y recursos de Hugging Face para comprometer los dispositivos y, forzando los Servicios de Accesibilidad de Android, robar sus credenciales de servicios financieros.\n\nConcretamente, el servicio de Hugging Face ha utilizado de forma abusiva para alojar y distribuir miles de variantes APK peligrosos, como han detallado la firma de ciberseguridad en un comunicado.\n\nEl modus operandi comienza utilizando métodos de ingeniería social con los que los actores maliciosos intentan convencer a los usuarios de que se descarguen una aplicación legítima llamada TrustBastion. Para ello, muestran anuncios de tipo 'scareware', es decir, diseñados para asustar a los usuarios y engañarlos haciendo creer que sus dispositivos están infectados o presentan fallos graves.\n\nEn este marco, la aplicación TrustBastion se presenta como una solución de ciberseguridad gratuita, con capacidades para detectar estafas, mensajes fraudulentos o intentos de 'phishing'. A pesar de que no contiene ninguna funcionalidad peligrosa, una vez instalada, solicita a los usuarios descargar una actualización obligatoria para continuar utilizando la aplicación, a través de una página falsa que simula ser la tienda de aplicaciones Google Play.\n\nLa actualización solicitada no descarga directamente la carga maliciosa, sino que, según han explicado los expertos en ciberseguridad, es en este momento donde entra en juego Hugging Face. Esto se debe a que la actualización conecta con un servidor (trustbastion.com) vinculado a la aplicación TrustBastion, que redirige al repositorio de datos de Hugging Face, donde se aloja el contenido APK malicioso.\n\nPor tanto, la carga útil que contiene el troyano se descarga desde la infraestructura del repositorio y se distribuye a través de su red CDN. Asimismo, para evitar que sea detectada por los sistemas de Hugging Face, los ciberdelincuentes producen nuevas cargas útiles aproximadamente cada 15 minutos.\n\nEsto se lleva a cabo utilizando un método conocido como polimorfismo del lado del servidor, que se basa en un concepto de programación aplicado al 'backend' que permite la reutilización de código sin cambiar la lógica principal, por lo que pasa desapercibido.\n\nUna vez efectuada la descarga maliciosa en el dispositivo Android, se entra en la segunda fase del ataque. Concretamente, el troyano se utiliza para explotar los permisos de los Servicios de Accesibilidad de Android, que posibilitan acceder a funciones de captura de pantalla o bloquear intentos de desinstalación.\n\nPara ello, el 'malware' se hace pasar por una función de 'Seguridad del Teléfono' y guía a los usuarios a través del proceso de activación de los Servicios de Accesibilidad. De esta forma, consigue monitorizar la actividad del usuario en su 'smartphone', realizando capturas de pantalla y, con ello, filtrando la información de sus servicios financieros. Incluso, el propio troyano suplanta servicios financieros como Alipay y WeChat para que, al iniciar sesión, acceda al código de bloqueo de pantalla.\n\nUna vez se consiguen los datos, el troyano envía la información robada a los actores maliciosos mediante el servidor de comando y control centralizado (C2), desde donde se coordina la entrega de carga útil y la exfiltración de datos.\n\nDesde Bitdefender han matizado que, en el momento de la investigación, el repositorio ya contaba con aproximadamente 29 días y había acumulado \"más de 6.000 confirmaciones\". Sin embargo, durante su análisis, el repositorio se eliminó a finales de diciembre y resurgió bajo un nuevo repositorio, esta vez vinculado a una nueva aplicación para Android con nombre de Premium Club."
  },
  {
    "source": "Computer Hoy",
    "company": "Hugging Face",
    "title": "Hugging Face secuestrada por hackers: más de 6.000 variantes de malware para Android se esconden en su plataforma",
    "date": "2026-01-30T10:21:53Z",
    "url": "https://computerhoy.20minutos.es/ciberseguridad/hugging-face-secuestrada-por-hackers-mas-6-000-variantes-malware-para-android-se-esconden-su-plataforma_6927786_0.html",
    "content": "Un peligroso malware empieza a extenderse entre los dispositivos con Android a través de Hugging Face, hay más de 6.000 variantes en menos de un mes.\n\nApple, Salesforce, Nvidia y otros gigantes de la industria se llevaron un buen susto un par de semanas atrás. La plataforma Hugging Face se había convertido en una puerta de entrada de accesos no autorizados.\n\nUna de las bibliotecas de inteligencia artificial más famosas del mundo estaba en peligro. Los expertos en ciberseguridad detectaron que el código de algunos modelos de IA podía esconder metadatos maliciosos.\n\nEl malware estaba listo para activarse cuando un usuario lo descargara y abriese el archivo en su dispositivo. Los afectados se cuentan por millones, estas enormes bibliotecas de inteligencia artificial se utilizan en empresas y universidades de todo el mundo.\n\nHugging Face se extiende en Android\n\nLa plataforma Hugging Face ha vuelto a sufrir otra oleada de archivos infectados con un malware. El nuevo objetivo son las aplicaciones APK que recopilan las credenciales de los usuarios cuando introducen sus claves bancarias al realizar pagos.\n\nHugging Face aloja y distribuye modelos, datos y aplicaciones de inteligencia artificial. Los ciberdelincuentes han encontrado la forma perfecta de llegar a millones de usuarios con una simple descarga de un modelo de IA malicioso.\n\nLos investigadores de la empresa de ciberseguridad Bitdefender han detectado que los malware se distribuyen en Android a través de aplicaciones APK. El ataque comienza cuando instalas un archivo de descarga conocido como TrustBastion.\n\nLa aplicación maliciosa simula ser una herramienta de seguridad. TrustBastion promete detectar amenazas como estafas online, mensajes SMS fraudulentos, phishing y malware, pero es todo lo contrario.\n\nTrustBastion copia a Google Play\n\nTrustBastion se cuela silenciosamente en el dispositivo, luego muestra una alerta de actualización obligatoria con una estética que imita a Google Play. El malware contacta con un servidor vinculado que redirige a los usuarios a un repositorio de Hugging Face que alberga el APK malicioso.\n\nHugging Face termina difundiendo el malware a través de su red de distribución de contenido (CDN). TrustBastion pasa desapercibido ya que genera nuevas variantes de carga útil cada 15 minutos, según confirma Bitdefender.\n\nLa empresa de ciberseguridad confirma que, con 29 días de antigüedad, TrustBastion había acumulado más de 6.000 variantes. El malware sigue activo en Hugging Face, ahora bajo el nuevo nombre Premium Club.\n\nTrustBastion y Premium Club muestran interfaces de inicio de sesión falsas que suplantan herramientas como Alipay y WeChat para robar los datos de millones de usuarios. El malware incluso intenta acceder al código de bloqueo de pantalla de los móviles con Android."
  },
  {
    "source": "Presse-citron",
    "company": "Hugging Face",
    "title": "Pourquoi la startup Hugging Face a refusé 500 millions $ à NVIDIA",
    "date": "2026-01-28T10:16:47Z",
    "url": "https://www.presse-citron.net/pourquoi-startup-hugging-face-refuse-500-millions-nvidia/",
    "content": "Hugging Face, pépite de l'intelligence artificielle (IA) ouverte, vient de décliner une offre de rachat partielle de 500 millions de dollars de la part du géant NVIDIA. Dans un secteur où les milliards pleuvent et les valorisations s'envolent, cette décision fait clairement figure d'exception.\n\nFondée en 2016 par Clément Delangue, Julien Chaumond et Thomas Wolf, Hugging Face s'est imposée comme un point de passage obligé de l'IA mondiale. Basée aux États-Unis mais profondément marquée par ses racines françaises, l'entreprise fonctionne comme une immense bibliothèque collaborative.\n\nEt contrairement à OpenAI ou Google qui gardent leurs technologies secrètes, Hugging Face mise tout sur l'open source. Sa plateforme héberge aujourd'hui plus de 2,5 millions de modèles et 700 000 jeux de données, devenant le standard incontournable pour les développeurs du monde entier. Et le succès de modèles comme DeepSeek R1 lui donnent raison.\n\nUne position centrale qui explique évidemment l'intérêt de NVIDIA. À la fin de l'année dernière, le fabricant de puces lui aurait proposé 500 millions de dollars, une transaction qui aurait valorisé la startup à 7 milliards de dollars, rapporte le Financial Times. Mais Hugging Face a dit non.\n\nUne culture \" anti-Silicon Valley \" assumée\n\nSes finances le lui permettent. Avec son modèle freemium où les grandes entreprises paient pour des fonctionnalités avancées, la société a été rentable en 2025, et dispose encore de 200 millions de dollars de cash en réserve. Mais c'est aussi une question de principe : la direction refuse qu'un investisseur unique et dominant puisse influencer ses décisions ou compromettre son rôle d'arbitre neutre dans l'industrie.\n\nCar chez Hugging Face, on cultive une approche pragmatique et décentralisée. Loin de l'idée de bâtir une superintelligence comme OpenAI, Meta et autre consorts, la startup préfère démocratiser l'accès aux outils existants. Son organisation interne reflète également cette volonté d'indépendance. Les employés travaillent en distanciel à travers le monde, communiquant principalement par messagerie instantanée. Si cette structure très souple peut parfois créer des décalages, elle garantit une diversité de points de vue essentielle à leur mission.\n\nEn restant ouverte et accessible, Hugging Face espère empêcher une trop grande concentration du pouvoir technologique. Ses dirigeants en sont convaincus : la véritable innovation ne doit pas rester verrouillée derrière des systèmes propriétaires, mais appartenir à la communauté mondiale des chercheurs et des développeurs.\n\n* Hugging Face a refusé une offre de rachat partielle de NVIDIA à 500 millions de dollars, malgré une valorisation potentielle à 7 milliards.\n\n* La startup défend une ligne claire : rester indépendante, rentable et neutre dans l'écosystème de l'IA ouverte.\n\n* Pour Hugging Face, l'innovation doit rester open source et collective, pas capturée par un acteur dominant.\n\n📍 Pour ne manquer aucune actualité de Presse-citron, suivez-nous sur Google Actualités et WhatsApp."
  },
  {
    "source": "k.sina.com.cn",
    "company": "Hugging Face",
    "title": "范式CEO戴文渊：AI两极分化严重 绝大多数开发者不需要万卡集群",
    "date": "2026-01-28T06:03:57Z",
    "url": "https://k.sina.com.cn/article_5953190046_162d6789e06702mafg.html",
    "content": "（来源：雷递）\n\n雷递网 乐天 1月28日\n\n范式智能CEO戴文渊日前表示，当下几千个不同的应用的落地都是在帮助英伟达的生态，99%以上的AI的工作都是做在英伟达的体系下。\n\n\"中美之间的AI竞争是99%都是在中国的中国人和在美国的中国人（华人）之间的竞争。但非常遗憾的一点，我们现在99%的，哪怕是在中国的中国人做的AI应用，也是英伟达体系下的，不是在国产的信创体系下。\"\n\n戴文渊指出，当前，大厂都说要搭万卡集群，搭10万卡集群。实际上，AI的两极分化是非常严重，可能头部的几家大厂，他们觉得万卡集群都小了。但对于绝大多数的商业开发者来说，他们需要的不是万卡，不是用1万张卡去跑一个模型。他们需要的是能用一张卡去跑1万个模型，否则，它的利用率就会非常低。\n\n\"当下，很多应用开发者用不掉一张卡，只能可能用1‱张卡。这不是万卡，是1‱张卡。\"戴文渊说，范式智能希望能够和曦望携手一起去打造百万token一分钱的算力云。\n\n以下是范式智能CEO戴文渊演讲实录：\n\n雷递网创始人雷建平与范式智能CEO戴文渊合影\n\n戴文渊：从去年开始，我看到国产算力的崛起。我从2009年开始在产业界做AI，一直以来做了很多的应用。我们范式到上市的那一天，我们统计了一下，我们大概做了22个不同的行业，做了几千个不同的应用的落地。自豪之余，又有一些遗憾，这所有的应用都是在帮助英伟达的生态。\n\n所以去年我和徐冰在聊到这件事情的时候，我说我们作为一个中国的企业，我们应该要帮助信创产业能够把生态做好。我们确实也有这方面的能力。所以我们应该干这件事情。当然，在干这件事情之前，我们也要看到这方面的差距，因为没有差距就没有我们的价值，也不需要我们做什么。\n\n很多人可能都是GPU公司的股东，买过寒武纪的股票，买过天数的股票，买过沐曦的股票。但是我不知道在座有多少人，除了曦望的同学，就是有多少人用过国产的GPU？\n\n我们做个类比，想象一下，就假设你打开苹果手机，你的app store里面有200万个APP，你打开了另外一个华为的手机里面大概有50个APP， 那你是觉得这个手机是性能慢呢，还是觉得这个手机完全都没法用，可能你都不会考虑这个手机的性能，也不会考虑这个手机的价格，你就直接就不用了。这是我们现在面临的一个很大挑战。\n\n第二个就是90%，我觉得都是保守了。可能过去的大概有99%以上的AI的工作都是做在英伟达的体系下，我们现在说这个中国人挺自豪的，就是在中美之间的AI竞争是99%，都是在中国的中国人和在美国的中国人之间的竞争。甚至我们认为在中国，我们的AI人才多，我们做了很多AI的应用。未来我们在AI应用领域，我们应该跟美国比是有优势的。\n\n但是非常遗憾的一点，我们现在99%的，哪怕是在中国的中国人做的AI应用，也是英伟达体系下的，不是在国产的信创体系下。\n\n第三个，我分享一个数字，大概20倍。英伟达的CUDA体系下的这个核心数是什么核心数呢？什么叫核心数？大家就可以认为算子的数量大概在18000个，国产的平均在几百个。大概来说大概差20倍。\n\n徐立总也讲到GPU的利用率问题。我们现在很多的GPU集群的利用率都是很低的。我前一段时间我和一个我们顶尖的央企在交流，他们搭了一个万卡集群，完了以后他们问我一个问题，他们说这个卡的性能都太强了。我们的应用开发者用不掉一张卡，甚至就用不掉一张卡，我们只能可能用1‱张卡。这不是万卡，是1‱张卡。\n\n这个可能有点反常识，因为我们看到大厂都说我们要搭万卡集群，我们搭10万卡集群。实际上，AI的两极分化是非常严重，可能头部的几家大厂，他们觉得万卡集群都小了。但是对于绝大多数的商业开发者来说，他们需要的不是万卡，不是用1万张卡去跑一个模型。他们需要的是能用一张卡去跑1万个模型，否则，它的利用率就会非常低。\n\n最后，就是国产的性价比，这是我们绕不开的一个话题。我们虽然说去年国产的卡的销量在迅速地提升，但是我们也必须承认，在去年销量提升的原因不是因为国产卡的性价比高，而是因为国产的卡能够解决安全性的问题。\n\n很多企业买国产卡，是因为怕将来买不到英伟达的卡，所以我需要买一些国产卡作为plan b。实际上，绝大多数的国产卡现在宣称的是英伟达的H100的百分之多少的性能，或者A100的百分之多少的性能。而不是说我们的性价比比英伟达的B300要高多少。所有的问题加起来是我们现在要做国产的算力所必须面对的问题。\n\n当然，我们看到这些数字差距是比较大的，但是我们一定要相信中国人是能解决问题的。因为美国的这些很多工作也都是中国人做的，没有道理在中国的中国人就做不了。\n\n范式在过去几年，我们在这些领域，我们一直致力于去帮助国产卡解决一些问题。为此我们也做了一些工作。比方说，我们在去年我们发布了一个叫信创魔盒model hub XC，model hub信创。\n\n我们致力于要打造的是叫作业界最大的信创的大模型社区，可以认为就叫作信创板的hugging face. 这个定义和中国的hugging face是有区别的。我们知道我们有很多社区，中国的社区定位叫中国的hugging face，在我看来中国的hugging face的价值不大。\n\n因为中国的hugging face还是给英伟达用的，能做模型的人都知道hugging face在哪儿，不需要在中国再做一个镜像了。中国真正需要的hugging face是信创的hugging face，这些模型是能跑在国产的GPU上面。\n\n在去年9月份，我们发布model hub x1的时候，我们的社区模型数量是0，到上周我们的模型数量大概是1万多，现在我们比英伟达差两个数量级。\n\n但是我们为什么对此非常有信心？因为我们的速度爬坡非常快。我们截至上周一共是一万多个，但是上周我们适配了2500个，也就是说，我们上周适配了我们从九月份到现在的四分之一，我们能看到爬坡的速度，根据这个速度我们非常有信心能够在今年能做到10万以上的量级，做到比英伟达差一个数量级，到差一个数量级的时候。实际上我们可以认为国产的卡可能比英伟达卡稍微不好用一点。就好像苹果手机假设有200万个应用，如果华为手机有20万个应用，你会觉得华为手机还是能用的，稍微有一点不好用。\n\n我们今年的目标达到这个，到明年，我们希望我们用中国的开发者的力量，把我们的生态应用的丰富度能够追上英伟达的社区。同时我们连续七年在打造市场份额第一的AI开发平台。这是我们一直以来给到我们的市场，我们的客户的核心价值。\n\n在过去的十几年，我们也帮助了二十多个行业，去打造了近万个垂直的模型。实际上垂直的模型也是非常重要的。我们知道去年十月份，OpenAI关闭了财务、法务、医疗的纯专业问题的回答，这是为什么呢？这是因为通用模型它在专业领域可能答得不够好。对于这些垂直的问题，我们需要垂直的模型来进行更专业地回答。\n\n最后，我们一直以来在打造一个叫什么呢？叫honey社区VGPU社区。这个社区是全球最大的异构VGPU的社区。什么叫异构？就是我们能支持我们的VGPU, 下面部署不同种类的卡，包括像英伟达，包括各种各样的信创的卡，因为在GPU领域，大家是一个非常大的两极分化，头部的厂商需要搭建万卡集群，除了头部的厂商，大家需要的是1‱张卡。这个时候GPU的虚拟机就是非常重要的技术。如果做不到GPU的虚拟机的技术，你是不可能去帮助广大的客户去把GPU的利用率做上。\n\n近期，我们也推出范式版的云服务，叫作fancy cloud. 在这个fancy cloud我们提供的是基于国产的GPU上面打造一个国产的云服务算力的云服务，上面集成了我们的VGPU的技术，以及我们的开发平台，我们的信创魔盒，在信创魔盒上面，此时此刻已经能够给我们的云服务的客户提供超过上万个各种各样的模型。\n\n在今年我们力争把这个数量提升到10万量级以上，到明年我们要追上Huggingface，我们也提供了一个叫作fancy one one神殿模型。在这个fancy里面，我们要提供我们在各个领域积累的是垂直模型的能力。使用范式的fancy cloud. 实际上我们就能够享用到源源不断的国产算力的供给。我们能够享用到范式过去积累的VGPU的能力，能够帮助客户能更好地去提升GPU的利用率。同时我们也能够基于fancy cloud能够提供开发的能力，能够提供市面上所有能够去获取到的大模型的能力，在国产的信创的算力上的调用。\n\n最后也能够享用到范式。在过去十几年在二十多个行业的垂直模型的积累。这是我们希望我们能够给市场提供核心能力。但是刚才也看到，我们有一项我们没有提到就是性价比。性价比不是一家软件公司独自能够提供的。我们也非常高兴能够看到曦望的出现，曦望的出现，让我们看到了国产算力和英伟达相比的性价比的机会。我们也非常高兴今天能够看到启望S3的发布，能够看到百万token一分钱的这样的一个计划。\n\n我们也非常高兴能够参与到这个计划，能够和曦望携手一起去打造百万token一分钱的算力云。我们曦望在中国，我们推出国产的算力，我们不仅仅是给我们各个市场的客户去解决他们的安全性问题。当然，安全性问题也很重要。除此之外，我们也希望中国的算力是全世界最便宜的。\n\n今天我们希望和曦望一起借着启望S3的发布，也希望未来S4S5的发布，曦望一起去打造fancy cloud. 基于希望的fancy cloud, 一起为全世界提供最便宜的算力。\n\n雷递由媒体人雷建平创办，若转载请写明来源。"
  },
  {
    "source": "Ad Hoc News",
    "company": "Hugging Face",
    "title": "Hugging Face: KI-Plattform verteilt Schadsoftware an Android-Nutzer",
    "date": "2026-02-04T20:28:18Z",
    "url": "https://www.ad-hoc-news.de/boerse/ueberblick/hugging-face-ki-plattform-verteilt-schadsoftware-an-android-nutzer/68552122",
    "content": "Eine groß angelegte Malware-Kampagne missbraucht die Infrastruktur der KI-Plattform Hugging Face, um Android-Geräte zu infizieren. Die Angreifer nutzen den vertrauenswürdigen Ruf der Entwicklerplattform, um Schadcode zu verbreiten - und umgehen so traditionelle Sicherheitsmaßnahmen.\n\nDie Sicherheitslücke betrifft eine zentrale Drehscheibe der globalen KI-Entwicklung und zeigt die wachsende Gefahr durch Angriffe auf offene Ökosysteme. Für deutsche Unternehmen und Entwickler, die auf Plattformen wie Hugging Face setzen, wird das Risiko durch derartige Supply-Chain-Angriffe konkret.\n\nDer Angriff beginnt mit der Android-App \"TrustBastion\". Sie wird über betrügerische Werbung verbreitet, die Nutzern vortäuscht, ihr Gerät sei infiziert. Die App gibt sich als legitimes Sicherheitstool aus. Nach der Installation fordert sie ein kritisches Update an - und leitet den Nutzer dabei direkt zu einem Repository auf Hugging Face um.\n\nDort wird die eigentliche Schadsoftware, ein Remote Access Trojan (RAT), heruntergeladen. Dieses Schadprogramm erschleicht sich umfangreiche Berechtigungen, oft unter dem Vorwand eines Sicherheitschecks. Hat es erst einmal Zugriff auf die Barrierefreiheits-Dienste (Accessibility Services) von Android, kann es das Gerät komplett übernehmen, Nachrichten lesen, Tastatureingaben protokollieren und Banking-Apps ausspähen.\n\nDie EU-KI-Verordnung stellt Unternehmen und Entwickler vor neue, verbindliche Pflichten - viele Teams unterschätzen die Anforderungen an Kennzeichnung, Risikobewertung und Dokumentation. Gerade nach Vorfällen wie TrustBastion ist schnelles Handeln nötig: Der kostenlose Umsetzungsleitfaden erklärt praxisnah, wie Sie KI-Modelle richtig klassifizieren, welche Nachweise nötig sind und welche Fristen jetzt gelten. Mit Checklisten und Vorlagen für Entwickler- und Compliance-Teams. Jetzt kostenlosen KI-Umsetzungsleitfaden herunterladen\n\nDie Angreifer wählten die KI-Plattform mit Bedacht. Hugging Face wird von Millionen Entwicklern weltweit genutzt, um AI-Modelle wie etwa Sprach-KIs zu teilen. Der Datenverkehr zu dieser vertrauenswürdigen Domain wird von vielen Sicherheitssystemen kaum hinterfragt. Für die Cyberkriminellen war sie damit das perfekte Versteck.\n\nDas Ausmaß der Kampagne ist beachtlich. In einem bösartigen Repository wurden alle 15 Minuten neue, leicht veränderte Varianten des Schadcodes hochgeladen. So entstanden über 6.000 einzigartige Malware-Varianten binnen eines Monats. Diese polymorphe Taktik macht eine Erkennung durch klassische Virenscanner, die auf Signaturen setzen, nahezu unmöglich. Die Standard-Scans der Plattform mit der Open-Source-Engine ClamAV wurden so systematisch umgangen.\n\nHugging Face reagierte auf die Entdeckung und löschte die betroffenen Repositories. Doch die Betreiber der Kampagne zeigten sich hartnäckig. Kurz nach der ersten Säuberungsaktion tauchten neue Repositories mit anderem Namen, aber gleichem Inhalt auf der Plattform auf. Ein klassisches Katz-und-Maus-Spiel begann.\n\nDer Vorfall wirft grundsätzliche Fragen zur Sicherheit von Plattformen mit nutzergenerierten Inhalten auf, besonders im KI-Bereich. Die Herausforderung: Wie unterscheidet man zwischen einem legitimen KI-Modell, einem experimentellen Projekt und bösartigem Code? Herkömmliche Malware-Erkennung reicht hier oft nicht aus.\n\nDer Missbrauch von Hugging Face ist Teil eines größeren Trends. Cyberkriminelle nutzen zunehmend legitime Cloud-Dienste und Entwicklerplattformen für ihre Angriffe. Die Infrastruktur großer, vertrauenswürdiger Marken hilft ihnen, Blocklisten zu umgehen und ihre Kommunikation mit den infizierten Geräten zu tarnen.\n\nDie Kampagne zeigt auch die fortschreitende \"Weaponization\" von KI - also deren Nutzung für Angriffe. Je stärker KI-Tools in Wirtschaft und Alltag integriert werden, desto attraktiver werden ihre Plattformen als Angriffsziele. Für die Sicherheit bedeutet das eine notwendige Verschiebung: Statt sich auf die Reputation einer Quelle zu verlassen, muss das Verhalten von Apps auf dem Gerät selbst analysiert werden. Ungewöhnliche Update-Prozesse oder der Missbrauch von Barrierefreiheits-Diensten sind hier entscheidende Alarmzeichen.\n\nDer Fall \"TrustBastion\" ist eine deutliche Warnung für das gesamte KI-Ökosystem. Für Plattformen wie Hugging Face bedeutet dies, dass herkömmliche Sicherheitsprotokolle nicht ausreichen. Nötig sind ausgefeiltere Verhaltensanalysen hochgeladener Dateien und eine strengere Überwachung der Repository-Aktivität.\n\nFür Android-Nutzer bleibt die wichtigste Regel: Apps nur aus offiziellen Quellen wie dem Google Play Store installieren. Jede Aufforderung, Apps von Drittseiten zu installieren (\"Sideloading\"), ist höchst verdächtig. Besondere Skepsis ist angebracht, wenn eine App ungewöhnlich viele Berechtigungen, insbesondere für Barrierefreiheits-Dienste, verlangt. In einer Zeit, in der Angreifer immer besser darin werden, sich als vertrauenswürdiger Verkehr zu tarnen, ist die Wachsamkeit der Nutzer der letzte entscheidende Schutz.\n\nPS: Sie betreuen Entwickler-Repositories oder verantworten KI-Projekte? Dann sollten Sie die Übergangsfristen und Dokumentationspflichten der EU-KI-Verordnung kennen - sonst drohen Bußgelder und Compliance-Risiken. Dieses kompakte E-Book fasst die wichtigsten Pflichten zusammen, zeigt konkrete Maßnahmen zur Risikominimierung und liefert Vorlagen für Kennzeichnung und Nachweise. Kostenlosen KI-Leitfaden anfordern"
  },
  {
    "source": "it-markt.ch",
    "company": "Hugging Face",
    "title": "Cyberkriminelle verteilen Android-Malware über Hugging Face",
    "date": "2026-02-03T15:09:33Z",
    "url": "https://www.it-markt.ch/news/2026-02-03/cyberkriminelle-verteilen-android-malware-ueber-hugging-face",
    "content": "Sie bezeichnet sich selber als \"Home of Machine Learning\" und begrüsst Besucher der Website mit dem slogan \"The AI community building the future\" - die Plattform Hugging Face. Tatsächlich findet sich dort so ziemlich alles, was das KI-entwicklerherz begehrt: von Trainingsdaten über trainierte Sprachmodelle bis hin zu einer Vielzahl an KI-Anwendungen hält Hugging Face alles mögliche bereit - zum herunterladen, aber auch zum gemeinsamen Weiterentwickeln.\n\nDoch nicht nur KI-Enthusiasten haben Hugging Face entdeckt, sondern auch Cyberkriminelle. Sie missbrauchten die Plattform unlängst, um Malware zu verteilen. Das berichtet \"Bleeping Computer\" unter Berufung auf eine Analyse des Cybersecurity-Anbieters Bitdefender.\n\nUm ihr bösartiges Tun möglichst lange zu verschleiern, setzen die Cybergauner auf eine mehrstufige Taktik: Zunächst jubeln sie ihren künftigen Opfern ein angebliches Sicherheits-Tool unter. Um ihre Opfer vom Download zu überzeugen, arbeiten sie mit gefälschten Warnhinweisen zu (erfundenen) Sicherheitslücken und behaupten, ihre Software könne die Probleme beheben. Die angebotene App enthält noch keine bekannte Malware. Somit wird die App von seriösen Security-Tools auch nicht als gefährlich eingestuft.\n\nDoch das ändert sich bald, wie der Analyse weiter zu entnehmen ist. Noch immer unter dem Deckmantel eines Security-Tools, erschleicht sich die heruntergeladene App bei ihrem Opfer zunächst weitgehende Zugriffsrechte auf das infizierte Android-Gerät und behauptet dann, es sei ein Upgrade erforderlich.\n\nUnd sobald der User diesen Updatevorgang startet, kommt Hugging Face ins Spiel. Von dort lädt die App nun die schädlichen Programmteile nach, die im ursprünglichen Download gefehlt hatten. Damit auch dieses Nachladen möglichst lange nicht von Security-Tools als bösartig erkannt wird, sorgen die Hacker dafür, dass sich die nachgeladene Payload alle 15 Minuten leicht ändert, wie Bitdefender anmerkt. Auch den Umstand, dass der schädliche Code auf Hugging Face gehostet wird, versuchen die Hacker zu verschleiern. In der App, die sie dem Opfer im ersten Schritt unterjubeln, kommt kein Hinweis auf Hugging Face vor. Stattdessen kontaktiert die App einen Zwischenserver, von dem die Anfrage dann erst an die KI-Plattform weitergeleitet wird.\n\nBitdefender merkt an, dass die zunächst beobachtete Malware-Kampagne noch während der Analyse verschwand. Wenig später aber tauchte sie wieder auf: mit neuem Namen und neuem Erscheinungsbild, aber auch mit dem alten, bekannten, bösartigen Schadcode.\n\n\"Bleeping Computer\" fügt hinzu, Hugging Face sei im Allgemeinen eine vertrauenswürdige Plattform. Dennoch sei sie in der Vergangenheit schon einmal für bösartige Zwecke missbraucht worden: Damals schafften es Cyberkriminelle, mit Malware verseuchte KI-Modelle einzuschleusen.\n\nLesen Sie auch: Cyberkriminelle nutzen laut einer Analyse von Kaspersky immer öfter vermeintlich vertrauenswürdige Open-Source-Datenpakete, um Schadsoftware zu verbreiten. Besonders Entwickler sollten daher vorsichtig sein."
  },
  {
    "source": "Netzwoche",
    "company": "Hugging Face",
    "title": "Cyberkriminelle verteilen Android-Malware über Hugging Face",
    "date": "2026-02-03T15:04:29Z",
    "url": "https://www.netzwoche.ch/news/2026-02-03/cyberkriminelle-verteilen-android-malware-ueber-hugging-face",
    "content": "Sie bezeichnet sich selber als \"Home of Machine Learning\" und begrüsst Besucher der Website mit dem slogan \"The AI community building the future\" - die Plattform Hugging Face. Tatsächlich findet sich dort so ziemlich alles, was das KI-entwicklerherz begehrt: von Trainingsdaten über trainierte Sprachmodelle bis hin zu einer Vielzahl an KI-Anwendungen hält Hugging Face alles mögliche bereit - zum herunterladen, aber auch zum gemeinsamen Weiterentwickeln.\n\nDoch nicht nur KI-Enthusiasten haben Hugging Face entdeckt, sondern auch Cyberkriminelle. Sie missbrauchten die Plattform unlängst, um Malware zu verteilen. Das berichtet \"Bleeping Computer\" unter Berufung auf eine Analyse des Cybersecurity-Anbieters Bitdefender.\n\nUm ihr bösartiges Tun möglichst lange zu verschleiern, setzen die Cybergauner auf eine mehrstufige Taktik: Zunächst jubeln sie ihren künftigen Opfern ein angebliches Sicherheits-Tool unter. Um ihre Opfer vom Download zu überzeugen, arbeiten sie mit gefälschten Warnhinweisen zu (erfundenen) Sicherheitslücken und behaupten, ihre Software könne die Probleme beheben. Die angebotene App enthält noch keine bekannte Malware. Somit wird die App von seriösen Security-Tools auch nicht als gefährlich eingestuft.\n\nDoch das ändert sich bald, wie der Analyse weiter zu entnehmen ist. Noch immer unter dem Deckmantel eines Security-Tools, erschleicht sich die heruntergeladene App bei ihrem Opfer zunächst weitgehende Zugriffsrechte auf das infizierte Android-Gerät und behauptet dann, es sei ein Upgrade erforderlich.\n\nUnd sobald der User diesen Updatevorgang startet, kommt Hugging Face ins Spiel. Von dort lädt die App nun die schädlichen Programmteile nach, die im ursprünglichen Download gefehlt hatten. Damit auch dieses Nachladen möglichst lange nicht von Security-Tools als bösartig erkannt wird, sorgen die Hacker dafür, dass sich die nachgeladene Payload alle 15 Minuten leicht ändert, wie Bitdefender anmerkt. Auch den Umstand, dass der schädliche Code auf Hugging Face gehostet wird, versuchen die Hacker zu verschleiern. In der App, die sie dem Opfer im ersten Schritt unterjubeln, kommt kein Hinweis auf Hugging Face vor. Stattdessen kontaktiert die App einen Zwischenserver, von dem die Anfrage dann erst an die KI-Plattform weitergeleitet wird.\n\nBitdefender merkt an, dass die zunächst beobachtete Malware-Kampagne noch während der Analyse verschwand. Wenig später aber tauchte sie wieder auf: mit neuem Namen und neuem Erscheinungsbild, aber auch mit dem alten, bekannten, bösartigen Schadcode.\n\n\"Bleeping Computer\" fügt hinzu, Hugging Face sei im Allgemeinen eine vertrauenswürdige Plattform. Dennoch sei sie in der Vergangenheit schon einmal für bösartige Zwecke missbraucht worden: Damals schafften es Cyberkriminelle, mit Malware verseuchte KI-Modelle einzuschleusen.\n\nLesen Sie auch: Cyberkriminelle nutzen laut einer Analyse von Kaspersky immer öfter vermeintlich vertrauenswürdige Open-Source-Datenpakete, um Schadsoftware zu verbreiten. Besonders Entwickler sollten daher vorsichtig sein."
  },
  {
    "source": "TechRepublic",
    "company": "Hugging Face",
    "title": "Hugging Face Repositories Abused in New Android Malware Campaign",
    "date": "2026-02-02T15:53:59Z",
    "url": "https://www.techrepublic.com/article/news-hugging-face-android-rat-malware-campaign/",
    "content": "Attackers exploited Hugging Face's trusted infrastructure to spread an Android RAT, using fake security apps and thousands of malware variants.\n\nHugging Face is widely used by researchers and developers to host machine learning models, datasets, and tools. But researchers say attackers have found a way to exploit that trust.\n\nCybersecurity researchers at Bitdefender have uncovered a massive campaign in which attackers are using Hugging Face's trusted infrastructure to host and spread a malicious Android Remote Access Trojan (RAT). By hiding their malicious code on a platform used by millions of developers, the attackers managed to fly under the radar of traditional security filters.\n\nThe attack doesn't start with a shady link from a dark corner of the web. Instead, it begins with TrustBastion, an app that markets itself as a top-tier security tool.\n\nAccording to Bitdefender, \"In the most likely scenario, a user encounters an advertisement or similar prompt claiming the phone is infected and urging the installation of a security platform, often presented as free and packed with 'useful' features.\"\n\nOnce a user sideloads this \"security\" app, the trap is sprung. The app immediately prompts an update, using visuals that closely mimic official Google Play and Android system dialogs. When the user clicks \"update,\" the app doesn't open the Play Store; instead, it contacts Hugging Face to retrieve the update.\n\nThousands of versions to dodge detection\n\nOne of the most alarming parts of this discovery is the sheer speed of the operation.\n\nThe hackers used a technique called \"server-side polymorphism,\" which means they constantly churned out slightly different versions of the malware to confuse antivirus software.\n\nBitdefender's analysis of the Hugging Face repository revealed a staggering level of activity: \"New payloads were generated roughly every 15 minutes. At the time of investigation, the repository was approximately 29 days old and had accumulated more than 6,000 commits.\"\n\nWhile Hugging Face does use ClamAV to scan uploads, Bitdefender notes that the \"platform doesn't seem to have meaningful filters that govern what people can upload,\" allowing these thousands of variations to sit on legitimate servers.\n\nTotal control over your phone\n\nOnce the second-stage payload is on the device, it asks for permission to use \"Accessibility Services.\" In the hands of a hacker, this is the \"skeleton key\" to your phone. Bitdefender reports that \"Once granted, this permission gives the RAT broad visibility into user interactions across the device.\"\n\nWith this access, the malware can:\n\n* Record your screen in real time\n\n* Capture your lock screen password\n\n* Display \"fraudulent authentication interfaces\" to steal credentials for apps like Alipay and WeChat\n\nA game of digital whack-a-mole\n\nEven when one part of the operation gets shut down, the hackers simply pivot.\n\nAfter the TrustBastion repository disappeared in late December 2025, a new one called \"Premium Club\" popped up almost immediately. Bitdefender researchers confirmed that \"While it may appear to be a different application, it uses the same underlying code.\"\n\nHugging Face has since removed the malicious datasets after being notified by the security firm."
  },
  {
    "source": "Ad Hoc News",
    "company": "Hugging Face",
    "title": "TrustBastion: Gefälschte Antivirus-App missbraucht KI-Plattformen",
    "date": "2026-02-18T16:50:27Z",
    "url": "https://www.ad-hoc-news.de/boerse/ueberblick/trustbastion-gefaelschte-antivirus-app-missbraucht-ki-plattformen/68591285",
    "content": "Eine gefälschte Sicherheits-App schleust über Updates Trojaner ein und nutzt die KI-Plattform Hugging Face, um Sicherheitsfilter zu umgehen. Millionen Downloads zeigen die Gefahr.\n\nEine gefälschte Android-Sicherheitsapp schleust selbst Schadsoftware ein. Die Hintermänner nutzen dafür die legitime KI-Plattform Hugging Face als Verteilungsweg und umgehen so Sicherheitsfilter.\n\nDer perfide Trick mit dem Update\n\nDie App \"TrustBastion\" tarnt sich als seriöser Virenschutz. Sie lockt Nutzer über aggressive Werbebanner auf Webseiten, die eine angebliche Infektion vortäuschen. Nach der Installation wirkt die App zunächst harmlos.\n\nDer Angriff beginnt erst, wenn die App ein \"kritisches Update\" anfordert. Der Hinweis darauf sieht täuschend echt aus. Stimmt der Nutzer zu, lädt die App die eigentliche Schadsoftware nach. Diese Taktik hilft, automatische Prüfungen in App-Stores zu umgehen.\n\nHugging Face als unfreiwillige Malware-Schleuder\n\nDas Besondere an dieser Kampagne ist der Verteilungsweg. Die Angreifer verstecken ihre Schadcode-Pakete in öffentlichen Datensätzen auf Hugging Face. Diese Plattform ist eigentlich für den Austausch von KI-Modellen gedacht.\n\nDa der Datenverkehr zu Hugging Face oft als vertrauenswürdig eingestuft wird, umgeht die Malware so traditionelle Blocklisten. Zusätzlich nutzen die Kriminellen \"Server-Side Polymorphismus\": Der Schadcode wird alle 15 Minuten leicht verändert, was es signaturbasierten Scannern schwer macht, ihn zu erkennen.\n\nMillionen Downloads, ein gefährlicher Trend\n\n\"TrustBastion\" ist kein Einzelfall. Ein aktueller Bericht von Zscaler zeigt das Ausmaß des Problems:\n\n* Im Google Play Store wurden im vergangenen Jahr über 200 bösartige Apps identifiziert.\n\n* Diese wurden insgesamt mehr als 42 Millionen Mal heruntergeladen.\n\n* Die Transaktionen mit Android-Malware stiegen um 67 Prozent.\n\nBesonders betroffen sind Apps aus der Kategorie \"Tools\", wie PDF-Reader, QR-Scanner oder angebliche Bereinigungs-Apps. Auch andere Schadsoftware wie der Banking-Trojaner \"Crocodilus\" bleibt hochaktiv.\n\nSo klaut die Malware Ihre Daten\n\nHat die Schadsoftware Zugriff, zielt sie auf die Android-Bedienungshilfen ab. Diese Berechtigung ist ein Generalschlüssel. Sie erlaubt der App:\n\n* Bildschirminhalte wie Passwörter in Echtzeit auszulesen.\n\n* Jeden Tastendruck zu protokollieren.\n\n* Gefälschte Login-Masken über echte Banking-Apps zu legen.\n\n\"TrustBastion\" versuchte zunächst, Zugangsdaten für asiatische Zahlungsdienste abzugreifen. Die Architektur ist jedoch modular - die Angreifer können den Trojaner binnen Stunden auf deutsche Bank-Apps oder Krypto-Wallets anpassen.\n\nSo schützen Sie sich\n\nExperten raten zu erhöhter Vorsicht, selbst bei Apps, die Schutz versprechen:\n\n* Ignorieren Sie Panikmache: Webseiten-Popups, die eine sofortige \"Bereinigung\" fordern, sind immer Betrug. Schließen Sie den Tab.\n\n* Prüfen Sie die Quelle: Laden Sie Apps nur aus dem offiziellen Google Play Store. Vermeiden Sie direkte Downloads von Werbebannern.\n\n* Hinterfragen Sie Berechtigungen: Eine Taschenlampen-App braucht keinen Zugriff auf Bedienungshilfen. Verweigern Sie diese Berechtigung, wenn sie nicht plausibel ist.\n\n* Aktivieren Sie Play Protect: Stellen Sie sicher, dass der integrierte Schutzmechanismus auf Ihrem Android-Gerät eingeschaltet ist.\n\nWer sich vor solchen mobilen Angriffen schützen möchte, findet praktische Hilfestellung in einem kostenlosen Cyber-Security-E‑Book. Es erklärt aktuelle Angriffsarten (auch KI-gestützte und mobile Malware), gibt sofort umsetzbare Schutzmaßnahmen für Android-Geräte und zeigt, wie Sie Konto‑ und Bankdaten besser absichern können. Jetzt das kostenlose Cyber-Security E‑Book herunterladen\n\nDer Missbrauch vertrauenswürdiger Plattformen wie Hugging Face markiert eine neue Eskalationsstufe im Kampf gegen Mobile Malware. Die Aufmerksamkeit des Nutzers bleibt die wichtigste Verteidigungslinie.\n\nHol dir den Wissensvorsprung der Profis. Seit 2005 liefert der Börsenbrief trading-notes verlässliche Trading-Empfehlungen - dreimal die Woche, direkt in dein Postfach. 100% kostenlos. 100% Expertenwissen. Trage einfach deine E-Mail Adresse ein und verpasse ab heute keine Top-Chance mehr.\n\nJetzt anmelden."
  },
  {
    "source": "SitePoint",
    "company": "Hugging Face",
    "title": "Ollama vs vLLM: When to Scale Your Local AI Stack",
    "date": "2026-02-15T00:46:43Z",
    "url": "https://www.sitepoint.com/ollama-vs-vllm-scaling-local-ai-stack/",
    "content": "The question of ollama vs vllm is one that thousands of developers are quietly asking themselves right now, usually at 2 AM when their prototype starts buckling under real user traffic. This article is a decision framework backed by benchmark data, designed to answer one specific question: at what concurrency level does Ollama's simplicity become a liability, and when does the engineering overhead of vLLM become the rational investment?\n\nTable of Contents\n\nOllama has become the default on-ramp for local LLM development. Pull a model, hit an endpoint, start building. It is fast to set up and genuinely pleasant to use. But there is a ceiling, and most developers do not see it until they slam into it. vLLM sits on the other side of that ceiling, purpose-built for the throughput demands of production inference serving.\n\nThis article is not a tutorial for either tool. It is a decision framework backed by benchmark data, designed to answer one specific question: at what concurrency level does Ollama's simplicity become a liability, and when does the engineering overhead of vLLM become the rational investment? If you are a startup engineer or technical founder running local models and planning to scale, this is the comparison you need before you commit to either path.\n\nThe methodology is straightforward. Same model, same hardware, same prompts. One user, then fifty concurrent users. The numbers tell a clear story.\n\nWhat Ollama Actually Does (and Does Well)\n\nArchitecture at a Glance\n\nOllama is a local model runner that wraps a model management layer, a CLI, and an HTTP API server (listening by default on port 11434) into a single cohesive tool. Under the hood, it leverages llama.cpp for inference, which means it operates primarily on GGUF-format quantized models. This architecture keeps things lean. You get CPU inference on a MacBook or GPU-accelerated inference on a workstation without manually wiring together backends, quantization toolkits, and serving layers.\n\nThe request processing model is sequential by default, though recent versions of Ollama have added basic parallel request handling. When multiple requests arrive without parallel configuration, they are handled one at a time. This is perfectly fine for a single developer testing prompts or building a personal assistant. It becomes a problem when your project stops being personal, as even with parallel request support enabled, Ollama lacks the sophisticated batching and memory management needed for high-concurrency production workloads.\n\nThe Developer Experience Advantage\n\nNothing in the local LLM ecosystem matches Ollama's setup speed. One command installs it. One command pulls a model. One command runs it. The entire workflow from zero to a working API takes under two minutes on most machines.\n\nThe Modelfile system lets you customize models with system prompts, parameter overrides, and template definitions, stored as reproducible configuration. This is powerful for prototyping because you can version and share model configurations the same way you would a Dockerfile.\n\nOllama dominates the prototyping phase because it eliminates every friction point between \"I want to try a model\" and \"I have a working endpoint.\"\n\nThat is the entire setup. No Python virtual environments, no CUDA toolkit debugging, no dependency resolution. For solo developers and small teams iterating on ideas, this is exactly right.\n\nWhat vLLM Actually Does (and Why It Exists)\n\nArchitecture at a Glance\n\nvLLM is a Python-native inference engine built on PyTorch, designed from the ground up for high-throughput LLM serving. Its defining technical contribution is PagedAttention, a memory management system for the key-value cache that draws directly from operating system virtual memory concepts. Instead of pre-allocating contiguous GPU memory blocks for each request's KV cache, PagedAttention allocates memory in non-contiguous blocks (analogous to pages), dramatically reducing fragmentation and waste.\n\nOn top of this memory layer sits a continuous batching engine that can dynamically add and remove requests from an active batch as they arrive and complete. The system also supports tensor parallelism for distributing a single model across multiple GPUs, as well as pipeline parallelism for multi-node deployments.\n\nThe Throughput Engineering Advantage\n\nThe PagedAttention paper from UC Berkeley (Kwon et al., 2023) demonstrated that naive KV cache allocation wastes up to 60-80% of memory due to fragmentation and over-reservation. PagedAttention nearly eliminates this waste (achieving close to optimal memory utilization), which translates directly into the ability to serve more concurrent requests on the same hardware.\n\nContinuous batching is the other half of the equation. Traditional static batching waits for a full batch to form, processes it, then waits again. Continuous batching inserts new requests into an in-progress batch and releases completed requests immediately. Under concurrent load, this can deliver 2-4x or higher throughput compared to static batching approaches.\n\nvLLM exposes an OpenAI-compatible API server out of the box, making it a practical drop-in for applications already built against the OpenAI SDK.\n\nThe setup is more involved. You need a compatible NVIDIA GPU (or AMD ROCm GPU, which vLLM also supports), the right CUDA or ROCm drivers, a Python environment, and patience for dependency resolution. This is the trade-off: operational complexity in exchange for serving architecture that actually scales.\n\nThe Benchmark: Single User vs. 50 Concurrent Users\n\nThis is where the ollama vs vllm comparison stops being theoretical and starts being measurable.\n\nTest Methodology\n\nTo produce a meaningful comparison, the benchmark should hold everything constant except the serving engine. Here is a representative test configuration:\n\n* Hardware: Single NVIDIA A10G (24GB VRAM) or RTX 4090 for consumer-grade reproducibility\n\n* Model: Llama 3 8B, run at the same effective precision on both systems\n\n* Prompt: Fixed 100-token input prompt, requesting 200-token completions\n\n* Metrics: Aggregate tokens per second (TPS), per-user TPS, time to first token (TTFT), and p99 latency\n\n* Client: Async Python benchmark script using , firing all concurrent requests simultaneously\n\nSingle-User Results\n\nAt single-user concurrency, Ollama and vLLM perform comparably. Both saturate the GPU with a single inference stream, and the raw generation speed is bounded by the same underlying hardware. Ollama may even feel slightly snappier due to lower server startup overhead and the optimized llama.cpp inference path for quantized models.\n\n50 Concurrent Users: Where Everything Changes\n\nThis is the inflection point. When fifty users hit Ollama simultaneously, the lack of efficient batching and memory management means requests are largely serialized, so user number fifty waits for most preceding requests to complete. The aggregate throughput does not meaningfully increase. The per-user experience collapses.\n\nvLLM's continuous batching processes all fifty requests with shared GPU compute cycles, dynamically scheduling them through the PagedAttention memory system. Aggregate throughput scales dramatically.\n\nNote: These figures represent directional magnitudes based on architectural behavior and published community benchmarks. Exact numbers will vary significantly with hardware, model quantization, prompt length, and server configuration. The benchmark script below lets you reproduce this on your own setup.\n\nWhat the Numbers Actually Mean\n\nOllama does not perform efficient concurrent batching. It largely serializes requests. This is not a bug; it reflects the tool's design purpose. A developer workstation running one request at a time does not need a batching scheduler. But when you point user traffic at it, every additional concurrent user adds their full wait time to the queue.\n\nvLLM's continuous batching shares GPU compute across all active requests, and PagedAttention ensures the KV cache memory is allocated efficiently enough to actually fit them all. The gap widens further at 100+ concurrent users, where Ollama becomes entirely unusable for anything resembling real-time interaction.\n\nHere is a simple benchmark script you can run against either backend:\n\nRun this against both backends on identical hardware. The numbers will speak for themselves.\n\nFeature-by-Feature Comparison\n\nSetup Complexity\n\nOllama installs in under a minute on Linux, macOS, or Windows. No Python runtime required, no CUDA configuration, no dependency management. vLLM requires Python 3.9+, a compatible CUDA toolkit (typically CUDA 12.x), GPU drivers, and often 10-30 minutes of environment setup. Dependency conflicts with other Python packages are common. This is a real cost, especially for teams without dedicated MLOps experience.\n\nModel Support and Flexibility\n\nOllama offers a curated model library with tagged versions (e.g., , ) and focuses on GGUF quantized models. The Modelfile system provides straightforward customization. vLLM is Hugging Face-native, supporting FP16, BF16, AWQ, GPTQ, and other quantization formats across a broad range of model architectures. It also supports LoRA adapter serving, which enables multi-tenant deployments where different users or tasks use different fine-tuned adapters on a shared base model.\n\nAPI Compatibility\n\nBoth tools offer OpenAI-compatible API endpoints, which means applications built with the OpenAI Python SDK can target either backend. vLLM's implementation is generally more complete, supporting features like logprobs, structured output via guided decoding, and more granular control over generation parameters. When building frontend applications that consume these APIs, the architecture of your client layer matters significantly. Structuring your React application with clean API abstraction layers makes backend swaps considerably less painful.\n\nGPU Memory Efficiency\n\nOllama relies on llama.cpp's quantization to reduce memory footprint. This works well for fitting models onto consumer GPUs but does not address memory fragmentation under concurrent serving (which is not Ollama's primary use case anyway). vLLM's PagedAttention dynamically manages KV cache memory, nearly eliminating fragmentation and waste according to the original paper. For multi-GPU deployments, vLLM supports tensor parallelism natively; Ollama does not offer built-in multi-GPU distribution.\n\nMonitoring and Observability\n\nOllama provides basic logging. vLLM offers a Prometheus-compatible metrics endpoint, request-level logging, and integration paths for Grafana dashboards. If your production SLA requires latency monitoring, throughput alerting, and capacity planning, vLLM gives you the instrumentation. Ollama does not.\n\nThe Transition Point: A Decision Framework for Startups\n\nStay on Ollama When...\n\nYou are a solo developer or small team in the prototyping and iteration phase. Your use case involves single-user or low-concurrency workloads like internal tools, personal coding assistants, or batch jobs where latency is not critical. You are running on consumer hardware such as a MacBook or a single GPU workstation. You need CPU-only inference, which Ollama handles well and vLLM does not support (vLLM requires a GPU). Your priority is speed of iteration and developer experience, not serving throughput.\n\nMove to vLLM When...\n\nYou are consistently serving 10 or more concurrent users and your p99 latency under load exceeds your application's requirements. You need multi-GPU model serving or plan to scale horizontally across nodes. Production observability is a requirement, not a nice-to-have. You are deploying LoRA adapters for multi-tenant serving scenarios. Your application has a latency SLA that Ollama cannot meet under your actual traffic patterns.\n\nThe Gray Zone (5-15 Concurrent Users)\n\nThis range is genuinely ambiguous and depends heavily on your specific model size, prompt lengths, hardware, and latency requirements. You might extend Ollama's useful life by placing a request queue or rate limiter in front of it, or by running multiple Ollama instances behind a load balancer. If your workload is latency-tolerant (async batch processing, background summarization), Ollama can stretch further than the benchmarks suggest for interactive use cases.\n\nThe One-Line Decision Rule\n\nIf your median concurrent users exceed what Ollama can serve within your latency budget, it is time to migrate. Benchmark on your own hardware with your own prompts. Do not guess.\n\nMigration Path: Ollama to vLLM Without Rewriting Your App\n\nAPI Compatibility Layer\n\nIf you built your application against an OpenAI-compatible endpoint, the migration from Ollama to vLLM can be nearly a drop-in swap. Both support and . The primary differences are model name format (Ollama uses tags like ; vLLM uses Hugging Face repo IDs like ) and minor streaming behavior variations.\n\nThe only code change is the and string. If you externalize those into environment variables from the start, migration is a configuration change, not a code change.\n\nInfrastructure Considerations\n\nvLLM in production typically runs in a Docker container with GPU passthrough, pinned CUDA versions, and health check endpoints for orchestration. Plan for explicit GPU allocation (especially in multi-service environments), restart policies for OOM recovery, and log aggregation from the metrics endpoint. These are standard production concerns, but they represent a step change from Ollama's \"run the binary\" deployment model.\n\nThe Hybrid Approach\n\nA practical pattern for startups: use Ollama in development and staging environments where setup speed and developer experience matter most, and deploy vLLM in production where throughput and observability are required. Drive backend selection through environment variables in your application configuration. This gives you the best of both tools without coupling your application code to either one.\n\nWhat About the Alternatives?\n\nTGI (Text Generation Inference by Hugging Face)\n\nHugging Face's TGI occupies similar territory to vLLM, offering high-throughput serving with continuous batching. It integrates tightly with the Hugging Face ecosystem, which can be an advantage if your model pipeline is already Hugging Face-native. In most published benchmarks, vLLM holds a slight throughput edge, but TGI is a legitimate option worth evaluating.\n\nllama.cpp Server Mode\n\nSince Ollama uses llama.cpp under the hood, you can also run llama.cpp's built-in server directly. This gives you more granular control over inference parameters and server configuration at the cost of Ollama's model management and developer experience layer. It is a middle ground for users who want llama.cpp performance with slightly more server control. Notably, llama.cpp's server mode does support concurrent requests and basic continuous batching, making it more capable under load than Ollama's default behavior.\n\nLocalAI\n\nLocalAI targets a broader scope, supporting audio, image generation, and embeddings alongside text generation. It is less optimized for pure LLM throughput but useful if you need a unified local API for multiple modality types.\n\nThis article focuses on Ollama and vLLM because they represent the clearest points on the developer-to-production spectrum for text generation serving. Other tools like TensorRT-LLM and SGLang are also worth evaluating for specific use cases, particularly if you are operating at very large scale or need compiler-level optimization.\n\nScale When the Numbers Tell You To\n\nOllama and vLLM are not competitors. They are stages in the lifecycle of a local AI stack.\n\nOllama is where you start, because it removes every barrier to getting a model running and an API responding. vLLM is where you graduate, because its architecture is fundamentally designed for the concurrent, latency-sensitive workloads that production traffic creates.\n\nThe benchmark data makes the transition point objective. At single-user concurrency, both tools perform comparably. At fifty concurrent users, vLLM delivers roughly 20x the aggregate throughput with a fraction of the tail latency. The crossover happens somewhere in the 5-15 concurrent user range depending on your specific constraints.\n\nDo not over-engineer early. Deploying vLLM on day one for a prototype is premature optimization that adds complexity you do not yet need. Do not under-engineer late. Running Ollama in production with fifty concurrent users is a fire you will spend weekends extinguishing.\n\nThe benchmark script is above. Run it on your hardware, with your model, at your expected concurrency. Let the numbers make the decision."
  },
  {
    "source": "The Motley Fool",
    "company": "Hugging Face",
    "title": "JFrog (FROG) Q4 2025 Earnings Call Transcript | The Motley Fool",
    "date": "2026-02-13T01:15:25Z",
    "url": "https://www.fool.com/earnings/call-transcripts/2026/02/12/jfrog-frog-q4-2025-earnings-call-transcript/",
    "content": "Product Usage Trends -- MCP server and JFrog Fly integrations support a shift toward business-to-agent market workflows, enabling code agents to interact directly with the platform.\n\nManagement reported a 24% increase in total annual revenue, underscored by a 45% expansion in cloud revenue and 50% growth in large enterprise customers. The security core, led by JFrog Advanced Security and Curation, contributed over 10% to ARR and secured 16% of year-end RPO, reflecting heightened demand amid rising software supply chain threats. Product innovation centered on AI workflows and model governance, with new partnerships deepening JFrog (FROG +2.69%)'s strategic role in AI infrastructure. The company guided to 17.5% revenue growth in 2026, sustained by baseline cloud growth expectations of 30%-32% and continued adoption of bundled security solutions.\n\nShlomi Ben Haim, and Eduard Grabscheid, JFrog Ltd.'s CFO. During this call, we may make statements related to our business that are forward-looking under federal securities laws and are made pursuant to the safe harbor provisions of the Private Securities Litigation Reform Act of 1995, including statements related to our future financial performance and including our outlook for the first quarter and full year of 2026. The words anticipate, believe, continue, estimate, expect, intend, will, and similar expressions are intended to identify forward-looking statements or similar indications of future expectations. You are cautioned not to place undue reliance on these forward-looking statements. They reflect our views only as of today and not as of any subsequent date.\n\nPlease keep in mind that we are not obligating ourselves to revise or publicly release the results of any revision to these forward-looking statements in light of new information or future events. These statements are subject to a variety of risks and uncertainty that could cause actual results to differ materially from expectations. For discussion of material risks and other important factors that could affect our actual results, please refer to our Form 10-Q for the quarter ended 09/30/2025 which is available on the Investor Relations section of our website, and the earnings press release issued earlier today.\n\nAdditional information will be made available in our Form 10-K for the year ended 12/31/2025 to be filed with the SEC on 02/13/2026 and other filings and reports that we may file from time to time with the SEC. Additionally, non-GAAP financial measures will be discussed on this conference call. These non-GAAP financial measures, which are used as a measure of JFrog Ltd.'s performance, should be considered in addition to, not as a substitute for, or in isolation from GAAP measures. Please refer to the table in our earnings release for a reconciliation of those measures to the most directly comparable GAAP financial measures. A replay of this call will be available on the JFrog Ltd.\n\nInvestor Relations website for a limited time. I will now turn the call over to JFrog Ltd. CEO, Shlomi Ben Haim. Shlomi?\n\nShlomi Ben Haim: Thank you, Jeff. Good afternoon, and thank you for joining our call. 2025 was a remarkable year for the frogs. We did not just fire on all cylinders, we set the pace. JFrog Ltd. paved the way for securing and managing the software supply chain in the era of AI, expanded our product portfolio, innovated at a faster pace than ever before, and built deep partnerships with the world's leading companies to strengthen our value to enterprise customers. We delivered quarter after quarter, exceeding our commitments and pairing solid growth and expansion with strong business and operational efficiency. This is what strategy, focused execution, and shared belief look like, and it is what makes this journey special for us.\n\nOn today's call, Ed and I will walk through our fiscal year 2025 results in more detail, share our fourth quarter performance, and discuss our outlook and guidance for 2026. In fiscal year 2025, JFrog Ltd.'s total revenue was $531,800,000, up 24% year over year. Cloud revenue for 2025 was $243,300,000, representing 45% year over year growth. In Q4, greater than $1,000,000 customers grew to 74 compared to 52 in the year-ago period, equaling 42% year over year growth. Customers spending more than $100,000 annually grew to 1,168 compared to 1,018 in the year-ago period, equaling 50% year over year growth.\n\nThese strong, consistent results reflect our alignment with modern enterprise market needs as software continues to transform how the world operates. JFrog Ltd.'s 2025 performance highlights a clear trend: as human developers and AI agents generate software at a massive scale, the resulting surge in binaries demands a trusted platform to manage, secure, and govern them end to end. This reinforces JFrog Ltd.'s leadership as the foundational infrastructure for software delivery in an agent-driven world. Now, I will discuss our success in Q4 in security, cloud, and AI. Security first. In 2023, we set out to support our customers by giving them a complete chain of cyber trust from code creation to production.\n\nWhen we launched JFrog Advanced Security and JFrog Curation, we evolved rapidly from securing artifacts within JFrog Artifactory with JFrog Xray to offering end-to-end trust and governance. JFrog Ltd. has now established itself as a complete system of record for software supply chain security that protects companies' binaries, software packages, and AI models even before software enters the organization. We strategically challenged the security point solution market by unifying security and DevOps on a single platform built on one source of truth. We believe that is the only scalable way to protect our customers' software supply chain. The pain was real, the threat was growing, and our results in 2025 reaffirm that this approach was the right one.\n\nIn today's AI-driven software environment, that source of truth becomes mission critical. Organizations need a secure center of gravity and a foundational platform that stores, manages, and governs all artifacts, whether created by humans or machines. Against this backdrop, we will continue to build JFrog Ltd. as the system of record, single source of truth all developers and code agents rely on to securely manage and govern every binary and AI artifact in the software supply chain. Just as we reported strong early results for JFrog Ltd. security in 2024, our 2025 annual reporting reflects another year of significant momentum and success of our security solutions.\n\nIn 2025, JFrog Ltd. security core products, excluding contributions from JFrog Xray, became an even more meaningful growth engine for the company. As of 12/31/2025, JFrog Advanced Security and JFrog Curation comprised over 10% of our total ARR. This past year, we built on strong RPO growth momentum driven by increased cloud usage, DevOps adoption, and our security core products. We are pleased to report that in 2025, security core comprised 16% of our ending RPO compared to 12% in the prior year, and nearly doubled long-term revenue commitments.\n\nThis growth highlights not only the rapid adoption of JFrog Advanced Security, JFrog Curation, or both by hundreds of our customers, but also the broad opportunity ahead to provide value to the thousands of existing enterprise customers over time. JFrog Ltd. security solutions, tightly coupled with JFrog Artifactory, enable end-to-end software supply chain protection and are now offered as a bundled add-on with our enterprise subscriptions, increasing ASP, expansion in cloud usage, and triggering multiyear commitments. This approach resulted in tangible growth for JFrog Ltd. Cohort data indicates that customers adding JFrog Ltd. security drove strong account expansion with growth extending beyond security and into broad product portfolio usage.\n\nWe are pleased with our financial performance and the execution of our enterprise go-to-market teams. Our security RPO and pipeline numbers show strong momentum carrying into 2026. Our success in security also reflects the deep alignment with the evolving threat landscape. Attackers are increasingly targeting the software supply chain through software packages. Our customers and community described the recent npm Shaykhulud incident as a mega attack on the software supply chain, exposing millions of JavaScript developers, echoing the impact of Log4j, PyPI, and other recent security events. JFrog Ltd. customers using JFrog Curation as their firewall remained protected.\n\nFrom the world's leading enterprises with tens of thousands of developers to a small development shop, JFrog Curation enforced policies, secured the software supply chain, and prevented business impact. Expanding our security offerings in 2025, we also announced the availability of AI Catalog and agentic remediation capabilities to address emerging challenges created by the introduction of AI models and agent-generated code into the software supply chain. JFrog Ltd. is the mission critical infrastructure of a company's primary software assets, their binaries. As more code is generated by human developers and AI coding agents, a tsunami of binaries is being created.\n\nMore binaries, artifacts, and models lead to a greater need for trusted infrastructure that manages, secures, and governs the software supply chain at scale. We anticipate these growing needs will drive sustained customer adoption of our holistic security solutions through 2026. Second, cloud. As we noted earlier on the call, 2025 was a year of high-quality, sustained growth in our cloud business. We delivered 45% year over year growth while remaining highly disciplined in usage management and continuing to migrate customers toward annual commitments. In addition, we drove a higher volume of security deals in the cloud, strong new logo wins, and deeper marketplace partnerships.\n\nDuring the year, we strengthened our partnerships with all major cloud providers, improved our commercial terms, and established a stronger long-term gross margin strategy. Our cloud momentum was also supported by focused investments in service performance and availability, the continued build-out of our global SRE organization, enhanced sales incentives, and the consumption-based pricing model aligned with the market standards. Customer purchasing decisions changed in 2025, as CIOs were less focused on mega cloud migration initiatives and instead increasingly emphasized building fit-to-purpose hybrid and multi-cloud architectures as they adopted new AI solutions. Looking ahead, we see trends that we believe will continue to mature. First, clarity.\n\nWhile cloud remains the preferred deployment environment, CIOs are seeking greater clarity, cost predictability, and ROI as they assess the full impact of AI adoption at scale, along with the evolving security and regulatory requirements that come with it. As AI adoption matures and clarity improves, we anticipate CIOs will increase investments in cloud infrastructures. Second, AI-driven software package ecosystems such as PyPI, Hugging Face, npm, Conda, or Docker are driving consumption spikes from both developers and AI agents, in some cases going beyond customers' commitments. This has the potential to be a tailwind for JFrog Ltd., and we expect customers to align commitments once usage patterns stabilize.\n\nWe will continue to provide guidance based on customers' annual commitment and proactively de-risk exposures to volatile user-driven and sizable deals. Our go-to-market strategy will remain focused on converting usage overages into annual commitments. Looking ahead, we believe that in 2026, we will continue to deliver durable growth in the cloud. Finally, I will address AI and MLOps. In 2025, we strategically built the foundation of JFrog ML as part of our platform to deliver the capabilities enterprises will increasingly require to automate speed, trust, and control across the model life cycle. At the same time, we introduced advanced functionality and integrations that extend beyond traditional B2B workflows into the emerging business-to-agent market.\n\nWe released JFrog's MCP server as a core integration layer, enabling AI agents and LLMs to securely interact with the JFrog Ltd. platform. We introduced the JFrog AI Catalog for model discovery and governance, extending the platform to manage AI and ML models like other software packages. We also announced agent-based security and remediation, leveraging agentic capabilities to drive detection and automated recovery. To strengthen our position as a system of record for all AI artifacts, we partnered with NVIDIA Enterprise AI Factory to serve as its secure model and artifact registry, while also partnering with Hugging Face to secure the world's leading open-source hub for models, supporting trusted enterprise consumption of AI.\n\nLooking into 2026, our roadmap includes capabilities designed to expand our growth and to support JFrog Ltd. users' needs, whether human, machines, or hybrid engineering teams of developers and AI agents. This expanding use case represents a significant opportunity for JFrog Ltd. The market is experiencing a tsunami of binaries accelerated by AI. JFrog Ltd. was built from day one to handle exactly this asset at this scale, and we are fully committed to providing the infrastructure modern software organizations need to confidently embrace the AI-driven revolution.\n\nI will now turn the call over to our CFO, Eduard Grabscheid, for an in-depth recap of Q4 and fiscal year 2025 financial results, as well as our updated outlook for Q1 and the fiscal year of 2026. Ed, thank you. Shlomi, and good afternoon, everyone. We are very pleased by the results of the fourth quarter, exceeding the high end of the range\n\nEduard Grabscheid: on every metric we guided for the quarter. It was a strong finish to an outstanding year, highlighting our consistent execution, operational discipline, and durable business model. During 2025, total revenues equaled $145,300,000, up 25% year over year. For fiscal year 2025, total revenues were $531,800,000, up 24% year over year. Fourth quarter cloud revenues grew to $70,200,000, up 42% year over year, and represented 48% of total revenues versus 43% in the prior year. Our growth in the cloud was driven by customers expanding annual commitments and increasing demand for JFrog Ltd. security core as ongoing npm incidents during the quarter accelerated customer adoption. For the full year 2025, cloud revenues equaled $243,300,000, up 45% year over year.\n\nFull year cloud revenues equaled 46% of total revenues, versus 39% in the prior year. During the fourth quarter, our self-managed, or on-prem, revenues were $75,100,000, with full year 2025 equaling $288,500,000, up 11% year over year. Aligned with our strategy, we continue to proactively engage our on-prem customers to migrate DevSecOps workloads to our cloud or explore solutions better aligned with their specific use cases, including hybrid and fit-for-purpose deployments. We experienced another year of strong customer adoption of the complete JFrog Ltd. platform, driven by customers looking to consolidate their technology stack and secure their software supply chain.\n\nIn Q4, 57% of total revenues came from Enterprise+ subscriptions, up from 54% in the prior year, while delivering year-over-year revenue growth of 33%. Driven by the ongoing execution of our enterprise go-to-market strategy and broader customer adoption of the JFrog Ltd. platform, revenue contributions from Enterprise+ subscriptions grew 36% year over year in 2025. Our security core products, which exclude any benefit from JFrog Xray, continue to gain momentum as customers actively consolidate point solutions. For the full year of 2025, security core revenue was 7% of total revenues, with our security core products now comprising more than 10% of our ending total ARR.\n\nDriven by an increasing number of large, multiyear commitments to JFrog Ltd., our security core represented 16% of remaining performance obligation, or RPO, as of 12/31/2025, compared to 12% in the prior year. Net dollar retention for the four trailing quarters was 119%, an increase of one percentage point from the prior quarter, highlighting the continued adoption of our security core products and increased cloud data consumption resulting in higher customer commitments. We continue to demonstrate that our customers view JFrog Ltd. solutions as mission critical to their software supply chain, with gross retention that equaled 97% as of 2025. Our customer count in fiscal year 2025 equaled approximately 6,600.\n\nDuring the year, we continue to execute on our strategy focusing on our go-to-market initiatives around the enterprise. We further matured our approach by refining our customer logo methodology to eliminate friction for our customers and sales teams. This resulted in the consolidation of approximately 300 lower ASP subsidiaries into their parent entity. Our team also prioritized new customer acquisition, specifically targeting opportunities that land with higher value and greater expansion durability. Now I will review the income statement in more detail. Gross profit in the quarter was $121,600,000, representing a gross margin of 83.7%, versus 83.2% in the year-ago period.\n\nWe remain focused on cloud hosting cost optimization as we anticipate a larger share of our revenues being generated from the cloud. Given our expected increase in cloud revenue contribution to total, we estimate annual gross margins to be in the range of 82% to 83% in 2026. Operating expenses in the fourth quarter were $95,800,000, equaling 66% of revenues. This compares to $75,600,000, or 65% of revenues, in the year-ago period. We remain focused on expense discipline while we continue to invest in strategic initiatives. Our operating profit in Q4 was $25,700,000, or an operating margin of 17.7%, compared to $20,900,000 and an 18% operating margin in 2024.\n\nFor the full year 2025, we delivered non-GAAP earnings per share of $0.82, a 26% increase year over year, assuming approximately 122,000,000 weighted average diluted shares. This compares to $0.65 in the prior year and 115,000,000 weighted average diluted shares. Cash flow from operations equaled $50,700,000 in the fourth quarter. After taking into consideration CapEx requirements, our free cash flow reached $49,900,000, or a 34% margin, compared to $48,400,000 and a 42% margin in the year-ago period. For the full year 2025, we generated $145,700,000 in operating cash flow and $142,200,000 in free cash flow, a 27% margin.\n\nNow turning to the balance sheet, we ended 2025 with $704,000,000 in cash and short-term investments compared to $522,000,000 at the end of 2024. As of 12/31/2025, our RPO totaled $566,000,000, a 40% increase year over year. This performance highlights the successful execution of our go-to-market strategy as customers continue to make larger multiyear commitments to our DevSecOps offering. And now let's turn to our outlook and guidance for the first quarter and full year of 2026. As we enter 2026, we are encouraged by the strength in our pipeline and a stabilized purchasing environment.\n\nWhile we are monitoring the increased cloud usage and early AI workload trends that could result in a tailwind for JFrog Ltd., our guidance philosophy will remain unchanged as we continue to de-risk our largest deals due to timing uncertainties and any benefit from cloud usage above contractual commitments. Our outlook reflects growing contributions from our JFrog Ltd. security core products, ongoing adoption of our full platform, and cloud growth driven from higher annual customer commitments. We estimate full year 2026 baseline cloud growth to be in the range of 30% to 32%. Given the anticipated contribution from our security core and baseline cloud growth assumptions, we expect our net dollar retention rate to be 117% for 2026.\n\nTurning to operating expenses, we will remain focused on investing in innovation across our platform, reinforcing JFrog Ltd.'s role as a system of record for all binaries and AI models. The weakening U.S. dollar against global currencies has created a year-over-year headwind for our operating expenses and is reflected in our operating profit guidance. We remain committed to a disciplined spending philosophy and are confident in our ability to manage expenses in line with prior execution. For Q1, we anticipate revenues to be in the range of $146,000,000 and $148,000,000, with non-GAAP operating profit anticipated to be between $25,000,000 and $26,000,000 and non-GAAP earnings per diluted share of $0.20 to $0.22, assuming a share count of approximately 127,000,000 shares.\n\nFor the full year of 2026, we anticipate a revenue range of $623,000,000 to $628,000,000, representing 17.5% year over year growth at the midpoint. Non-GAAP operating income is expected to be between $106,000,000 and $108,000,000, and non-GAAP diluted earnings per share of $0.88 to $0.92, assuming a share count of approximately 128,000,000 shares. I will now turn the call back to Shlomi for some closing remarks before we take your questions.\n\nShlomi Ben Haim: Thank you, Ed. In today's market, nearly every company is looking to capitalize on AI. But in 2025, when the world's leading AI-native companies select JFrog Ltd. as the core infrastructure for the software supply chain, it was clear validation of our strategy. They became JFrog Ltd. customers not only for our industry-leading platform, but because we have become the trusted system of record for binaries, the foundational asset powering modern software delivery. The adoption of our platform indicates that our roadmap is strongly aligned with where the industry is headed.\n\nAs VIBE engineering and coding agents accelerate software creation, enterprises are facing a massive surge of binaries that must be managed, automated, secured, and governed across the software supply chain. This dynamic clearly differentiates JFrog Ltd. as the trusted enterprise platform in the age of AI. 2025 is now in the rearview, and I am proud to say what we committed to our partners, customers, and shareholders was consistently delivered. Despite significant macroeconomic and geopolitical challenges, the JFrog Ltd. team rose to new heights. This period has also carried personal weight for many in our Israeli team. After years of pain, finally, all hostages are back at home.\n\nWe move forward with renewed hope for peace, stability, and a better future for the region, and the world. To my frogs, thank you. 2025 was a challenging year of uncertainty, yet a remarkable one for us. You did not just live through it, you won it, and for that, I am proud and grateful. As we step into 2026, we remain committed to quality growth, responsible investments, expanding our solutions to meet emerging needs, and bringing us all one leap closer to realizing our liquid software vision: a world where every software flow is effortless. And with that, thank you for joining our call, and may the frog be with you. Operator, we will now open for questions.\n\nOperator: We will now begin the question and answer session. Please limit yourself to one question. If you would like to ask a question, please raise your hand now. If you have dialed into today's call, please press 9 to raise your hand and 6 to unmute. Your first question comes from Sanjit Kumar Singh with Morgan Stanley. Your line is open. Please go ahead. Sanjit, a reminder to please unmute yourself by pressing 6.\n\nSanjit Kumar Singh: Hi. Can you hear me now? Yes. We can, Sanjit. Sorry about that. This is Oscar Savedra on for Sanjit. Thank you for taking my question, and congrats on the great results. Really nice to see the continued growth on the cloud. I wanted to touch maybe on the, you know, the customer count. I understand the intentionality of, you know, you are focusing more on the higher propensity to spend, larger customers, and, you know, we see that on the $100,000 customer adds. But, you know, how should we think about that going forward? And how far along we are in that transition? So should we continue to expect that count to come down while larger customers go up?\n\nAny additional color there would be helpful. Thank you.\n\nShlomi Ben Haim: Yes, thank you. This is Rami. I will take this first question. So we are focused on our strategy, building for the enterprise as we said three years ago, and you can see the results on the over $1,000,000, the significant growth there. You can see the results on the customers that are spending over $100,000 and the increasing spending over there. This is our strategy. This is where our go-to-market is focused, and sometimes it means that we will have to let go of low ASP customers and be focused on our enterprise.\n\nWe also noted that we have made our internal consolidation to avoid friction in our go-to-market team and to focus our team on the big enterprise opportunities. And that, by itself, was approximately 300 logos. And also, I should note that this includes the geographies regulation like churning China, Russia, and other regulatory decisions. So we are very pleased with the stability and the growth. More important than that, the fact that the customers that we brought in, hundreds of new customers that we brought into our portfolio, are spending much more on ASP, growing faster, and adopting not just DevOps, but also DevSecOps and other services in the cloud.\n\nSo, in the bottom line, it is very much aligned with our strategy and reflects our commitment as we enter 2026.\n\nOperator: Your next question comes from the line of Ravi Shankar with UBS. Your line is open. Please go ahead.\n\nEduard Grabscheid: Awesome. Thanks for taking my questions.\n\nRavi Shankar: My question is for Shlomi. It seems like every quarter we are seeing another security incident. You had the npm attacks in Q3 and Q4. You mentioned another one. Are these becoming less like one-time events and more just structural growth drivers? Are we just permanently in a higher threat environment and maybe we should be penciling in a more consistent contribution from these types of incidents going forward? Or how should we think about the long-term contribution from the higher threat environment we are in today?\n\nShlomi Ben Haim: Ravi, this is a great call out of what is happening today. There are more and more attacks over the software supply chain. Attacking the software supply chain by hackers means that they are going after the software packages, the binaries. It started three years ago, this trend, with Log4j, then Python, then MCP, and now npm and Chai Hulud in three different incidents. Every customer, every enterprise, every software organization understands now that the software supply chain is not protected if the software packages, if the binaries, are not protected.\n\nPutting aside this threat that is kind of floating over every company now, code agents become far faster in how they create code and therefore create more binaries using more open-source packages. So this threat and this trend is something that we anticipate will only grow, and therefore, we built our JFrog Ltd. security suite to tackle this threat.\n\nOperator: Your next question comes from the line of Michael Cikos with Needham. Your line is open. Please go ahead.\n\nJeffrey Schreiner: Hey, team. Thank you for taking the questions here.\n\nMichael Cikos: Some very encouraging data points, and I appreciate especially the RPO growth and ARR contribution when thinking about security as far as demonstrating that this truly is a platform and the attach you are seeing. One of the things I wanted to come back to and we have been fielding on our side: with the heightened threat environment and some of these hacks, which unfortunately feel a little bit more commonplace now, is there a general rule of thumb where we would be able to draw parallels between a hack of the npm size and what that would equate to in revenue?\n\nIs there any broad parallels we could draw from, or is it really just happenstance depending on what part of the ecosystem is being hacked? Anything there would be beneficial. And thank you so much.\n\nShlomi Ben Haim: Thank you, Michael. I will take this one. The only responsible way to look at the potential revenue growth is to look at the amount of enterprise customers in our portfolio that still did not adopt JFrog Ltd. security. And while we are growing by hundreds every year, and the ASP is growing significantly, there is still a lot of room to grow within our portfolio. JFrog Curation in the last two quarters is exploding, mainly due to the fact that the threat is real, the scale is required, and you have to move fast. Big companies are not taking any risks, especially in today's changing environment.\n\nSecond thing that I should note, Michael, is that most of our new enterprise landers -- what I referred to before when we spoke about new logos -- are already adopting the JFrog Ltd. platform with security. So there are three avenues of growth. Number one, customers that are still not using security and will hopefully adopt. Number two is with new logos that are joined enterprise logos. Again, we are not buying logos; we are going after the enterprise logos that are subscribing from the get-go with security. And number three is the number of projects within the companies that already adopted JFrog Ltd. security, and we still have room to grow there.\n\nOperator: Your next question comes from the line of William Miller Jump with Truist. Your line is open. Please go ahead.\n\nMichael Cikos: Awesome. Alright. Thank you for taking the question, and I will echo my congrats for the acceleration this year.\n\nWilliam Miller Jump: So, Shlomi, I wanted to come back to your comment about the tsunami of binaries, accelerated by AI. For the customers that have increased their commitments, are you still seeing them grow and consume beyond these re-up commitment levels? And can you give any more color around the consumption change for customers that have leaned into coding agent deployments versus the broader customer base? Thanks.\n\nShlomi Ben Haim: Great question, Miller. This is going to grow, and we call it a tsunami because this is what we see. When we are reviewing the data on a weekly basis, we see how many more software artifacts are being created, how many binaries are being compiled, and that is mainly because of the fact every developer now became a super developer, and we hear how well the code agents are doing: Anthropic, OpenAI, Gemini. These agents are faster than developers. They are powering developers, and therefore, creating more binaries.\n\nYou create more binaries, you need a single source of truth to go to, whether you are an agent or developer, and this is where JFrog Ltd. comes into the picture: to secure, to govern, to build a trusted software supply chain for you and for your agents. So think about the number of developers -- call it millions of developers in the world today -- powered by millions of agents, and therefore, the result would be many more binaries, and then, hopefully, we would see it also in the scale of JFrog Ltd.\n\nOperator: Your next question comes from the line of Mark Charles Cash with Raymond James. Your line is open. Please go ahead.\n\nSanjit Kumar Singh: Yes. Thank you. I think, Shlomi, if I could ask, with all the co-gen tools we were just talking about there creating more software and you have new bottlenecks emerging in the software pipeline, I am just curious if you are thinking about AppTrust in 2026 being maybe the next catalyst to alleviate governance and regulatory pressure in adopting AI and specific tools in that organization they are going through right now. Thank you.\n\nShlomi Ben Haim: Most of the very important highlights in our roadmap throughout the years since we established JFrog Ltd. came from a real enterprise need. Our customers told us last year, listen, we are faster now with DevOps and we are more secured with DevSecOps, but there is a new bottleneck, which is governance, and this is going to be even more painful when it is not only humans that are building code and compiling it to binaries, but also agents. So AppTrust is tackling that exactly. We are addressing this pain of automated governance with all the evidence again stored in JFrog Artifactory as a system of record.\n\nWhether this code was generated by a developer or by a code agent, the governance and the regulation before the binary goes to production will come out from a single source of truth, and this is what AppTrust is addressing to solve.\n\nOperator: Your next question comes from the line of Zachary I Schneider with KeyBanc Capital Markets. Your line is open. Please go ahead.\n\nRavi Shankar: Great. Thanks for taking my question, guys. I am on for Jason Celino today. I wanted to ask about your AI-native customers. In the past, you have talked about having three of the top five AI natives, which is obviously great. But for the other two, or maybe any of the adjacent players that are not using JFrog Ltd., what would they be using? And how confident are you that you can eventually win them as well? Thanks.\n\nEduard Grabscheid: This is --\n\nShlomi Ben Haim: Every company now wants to call itself a native AI company. When I am speaking about the big players, I am speaking about those that are moving the market and calling the trend and setting the standard. And they chose JFrog Ltd. to be their power grid, to be the solution that will run their software supply chain. They build around Artifactory as the system of record, and they are using JFrog Xray to scan that and to collaborate and integrate with the ecosystem. The other two, I do not know if it is two or more, I do not know what they are using.\n\nBut from what we hear, some of them are evaluating tools, and some of them are building their own tools. Slowly, as we are going over a list that our go-to-market team built together throughout the year, we are bringing them one by one by one. So I hope that every quarter I will report another one that chose JFrog Ltd. not only as a partner or an ecosystem player, also as a customer.\n\nOperator: Your next question comes from the line of Kingsley Crane with Canaccord. Your line is open. Please go ahead.\n\nMichael Cikos: Hey. Thanks for taking my question. Shlomi, you have had JFrog Fly in beta a bit now, and you have had MCP server integrations out for a bit now. I am just curious what kinds of usage trends you are seeing with either Fly or the MCP server, and just what that tells you about how customers' workflows are changing, and then how you are positioning around that. Thank you.\n\nShlomi Ben Haim: This is, Kingsley, a very important move that we have done during the year. We understand -- and I called it out on the script -- we understand that the business-to-business is how we built JFrog Ltd. so far. In the future, it would be a business-to-agent market. And if I want agents to become my persona, my customers, then I need to give them access to interact with my platform. This is where the MCP server comes in. But it is even greater than that. MCP itself is also a binary. So we start to see more and more of our customers using JFrog Artifactory as the single source of tools for all MCPs in the market.\n\nSo it is not only that agents will use JFrog Ltd. as the blessed area to pick up binaries from, but also other players will use JFrog Artifactory to place their MCP servers in. Now, if you do that, if people work with one system of record, then consistency happens, a secured solution, a governed software supply chain before the release to production is important. And with the MCP server, agents can now interact with the JFrog Ltd. platform as well.\n\nOperator: Your next question comes from the line of Brian Essex with JPMorgan. Your line is open. Please go ahead.\n\nRavi Shankar: Hi. Good afternoon, and thank you for taking the question. Maybe one for Ed. Bit of an acceleration in both sales and marketing and R&D in the quarter. I would love to get a framework of how you are thinking about investment on both sides of those and how you expect that to materialize through fiscal 2026. And maybe as part of that, if we can get the FX impact both for the quarter as well as what you are thinking about contemplating in your guide. Thank you.\n\nEduard Grabscheid: Sure. Let me first start with Q4 and the sales and marketing. What you are seeing is end-of-the-year bonuses that flow through in our expenses, and that is what is actually happening there in Q4. But as we step forward into 2026, the reason we called out the weakening U.S. dollar is, as you know, more than 50% of our headcount is distributed globally, so we wanted to call that out. We have a very strong hedging program, so most of that risk is already covered, and it is captured in our operating margin guide already. We also have very disciplined operational execution in terms of maintaining expenses.\n\nSo we continue to invest, but we continue to make sure we are doing it smart. Anything that we see in terms of the potential outperformance on the top line, we would still consider a meaningful portion of that to flow into operating margins.\n\nOperator: Your next question comes from the line of Shrenik Kothari with Baird. Your line is open. Please go ahead.\n\nSanjit Kumar Singh: Great. Thanks a lot for taking my question.\n\nShrenik Kothari: Apologies for my bad voice. But just beyond the foundational models that you touched upon, you are now a secure model registry for NVIDIA AI Factory and for Hugging Face, arguably two of the most strategic on-ramps to model consumption. Just curious, Shlomi, how are these top-down partnerships driving new logos for you? Number one. And two, when it comes to monetizing this unique position, where are they on the adoption curve for the additive logos that you have described in terms of AI Catalog adoption and AppTrust and agentic remediation? Thanks a lot.\n\nShlomi Ben Haim: Yes, Shrenik. We managed to expand our platform so successfully with security and with MLOps and with DevOps and distribution that every time that we have another partnership powering the community, it spreads across all of these elements of the platform. Specifically with NVIDIA, beyond the fact that it validates JFrog Ltd. as the single source of truth for all AI tools and for models, it is also aiming to the enterprise, as we mentioned before. This is our strategy. NVIDIA is being used by the enterprise. They are selling GPUs. We are providing them with a secured area for models interaction, and this supports the enterprise. Hugging Face, however, is a different scenario.\n\nHugging Face can be a top of funnel for JFrog Ltd. because it is the open-source hub. It also supports most of our customers that are using Hugging Face as a local repository for the models that they bring from outside, so they will have a secured environment. Now when we are looking forward into 2026, the too integrated to fail philosophy is something that we are not only collecting feedback from the market on what they would like to see, but also how we can serve them better -- not just the human developers, but also the agents. How can we serve Anthropic Opus 4.6 better? How can we serve OpenAI agents better?\n\nBecause these are the new tools that are now using JFrog Ltd. as their system of record.\n\nOperator: Your next question comes from the line of Ittai Kidron with Oppenheimer. Your line is open. Please go ahead.\n\nSanjit Kumar Singh: Thanks, and congrats, guys.\n\nIttai Kidron: Real nice solid finish for the year. I have a couple of questions, one for each of you. For you, Ed, nice to see the progress there on the security and appreciate the disclosure there. If one would do a back of the envelope and strip out your security out of your business, what would that imply that the core business excluding security is growing? Is there acceleration there, leveling off, deceleration? Any color there would be great. And then for you, Shlomi, I would love to think about 2026. What from your perspective are the greatest risks to your business right now?\n\nAnd maybe, clearly, with what is going on in the world right now in the context of AI and how it could potentially do everything, including our laundry, my question is to you: in what way could AI be a risk to your business? Thank you.\n\nEduard Grabscheid: Thank you for the questions, Ittai. I will start here, and then Shlomi can finish. We gave you the results at the end of the year for the revenue contribution that is coming from security, and it is still a relatively small piece in terms of that contribution to the top line. We go to the market as a platform, selling together both Artifactory and security. So we do not want to carve that out. The results that we show you on the top line were impressive. That included security. We are very happy with the direction that it is going.\n\nThe sales team are instructed to sell together the solution and the offering, which we believe, when you go to the market together with Artifactory and security, you secure your software supply chain at scale -- what is needed for today's customer.\n\nShlomi Ben Haim: I will answer the risk question, Ittai. We are all standing at the edge of a cliff. Some people will tell you that we are going into a world of productivity -- you mentioned laundry; some people gave us other examples. And some people will tell you that we are all going to be replaced by robots. I think that for the business, and this is my job as the CEO, to make sure that we are focused, that JFrog Ltd. is differentiated and brings value to our enterprise customers, and what we see now is that we keep building for the future with them. Just a few days ago, Elon Musk tweeted that code is going to be replaced.\n\nThe only outcome will be binary. It reinforces again what we keep saying for the last fifteen years. Binary is the primary asset. This is where we should be focused. And my biggest risk is that I will get confused and start to follow trends and not serve my enterprise with what we do best: being the system of record.\n\nOperator: Your next question comes from the line of Jason Noah Ader with William Blair. Your line is open. Please go ahead.\n\nSanjit Kumar Singh: Yeah. Hi, guys. I hope I could squeeze in as well. One quick one for Ed, one for Shlomi. For Ed, just trying to understand that $800,000,000 FY 2027 revenue target. Is that just kind of aspirational at this point because it would require pretty massive revenue acceleration in 2027? And then for Shlomi, I know you guys price Artifactory on capacity. So it should, logically, follow that the tsunami of binaries that you called out should accelerate growth in your core solution. Are we seeing that at all yet, or is that still to come? And when do you think that could come?\n\nEduard Grabscheid: I will start with the question regarding the $800,000,000 on the long-term model. At the end of the day, we are focused on delivering and executing in 2026. That is what we are guiding to at this point. When we look at the results over the last three years, we are on track with that number. It does not reflect our conservative or responsible guidance philosophy, but the focus right now is on delivering in 2026.\n\nShlomi Ben Haim: I will take it from here, speaking about conservatism. We see -- as we mentioned in the call -- a rise in the AI-related binaries being consumed by our customers. We manage PyPI, npm, Docker, Conda, and others. But, you know, Jason, we are not promising magic. We are being very disciplined with our guidance. And we commit when the customers commit, then we guide the market. We guide you by commitments and not by usage. So yes, we see spikes and, yes, it is an uplift to our performance, but we will not guide based on this data transfer until it will follow a customer commitment. And customer commitment usually comes when usage is stabilized and not based on trends.\n\nOperator: Your next question comes from the line of Andrew Michael Sherman with TD Cowen. Your line is open. Please go ahead.\n\nMark Charles Cash: Oh, great. Thanks. Hey, guys. Shlomi, is demand for Curation to accelerate post the second npm attack in late November? Did this pull any deals forward into Q4, and how much is the pipeline for that up year over year?\n\nShlomi Ben Haim: Curation -- listen, we are not celebrating these attackers coming after our customers, but obviously, we benefit from it because there was very clear value that Curation brought. Curation is a tool that is out in the market for almost two years. It is mature and it was ready, and it was ready not only in terms of the risk but also in terms of the scale. So not only did we perform amazingly in Q4 because of the npm incident, but we also built the pipeline moving forward, as I mentioned in the call.\n\nJason Noah Ader: Great. Thanks.\n\nOperator: Your next question comes from the line of Jonathan Blake Ruykhaver with Cantor. Your line is open. Please go ahead.\n\nShlomi Ben Haim: So, Shlomi, I would love to hear more details around what you are seeing in terms of the potential convergence of the DevOps toolchain to address MLOps. And, specifically, are they seeing the benefits of, or do they understand the benefits of, a potential unified pipeline, particularly around security and governance? And I guess lastly, as a part of that, what are you seeing in terms of adoption trends for JFrog ML and your expectations for that in 2026?\n\nShlomi Ben Haim: JFrog ML was included in -- I think it was Q2 or Q3 this year -- in our platform. Some of our customers are already using JFrog ML to manage their full model life cycle. We treat the model as a package. A model is yet another binary. So by providing them with these capabilities, we are reinforcing the fact that JFrog Ltd. is the central, trusted source of truth, not only for the legacy artifact but also for the new artifacts, which are models. I think that this will evolve as the market evolves. MLOps will not stay as it used to be before the days of LLMs.\n\nWhat we see now is that code agents are also starting to interact with the JFrog Ltd. platform for any push and pull of binaries. So overall, it is growing, it is evolving. This entire landscape is changing, and we are tracking it and we will keep you posted on it.\n\nOperator: Your next question comes from the line of Eamon Robert Coughlin with Barclays. Your line is open. Please go ahead.\n\nKingsley Crane: Hey, guys. Thanks for taking the question, and congrats on the continued execution. I just wanted to go back to the MLOps motion and how to think about that opportunity. Can you help us understand that consumption profile -- what that would look like for an LLM maybe compared to a traditional binary? And then what makes JFrog Ltd. well positioned to be the default LLM repository for these enterprises? Thanks.\n\nShlomi Ben Haim: The MLOps solution is actually going after providing the CI/CD experience for models. It started with small models, and then the world in the last year evolved, and so did our tools and our platform. The idea around it in terms of consumption is that if you treat a model as a binary -- which it is -- then we should see more data transfer. We should see more storage. I think that there is a better potential for monetizing on storage because models, by definition, are bigger binaries than the others.\n\nSince our pricing model is a consumption-based model, if we drive models to use the MLOps capabilities of JFrog Ltd. together with the security and the storage, you should see our consumption going higher, and therefore, the commitment of the customers will go higher. This is something that we are tracking closely and looking forward to see the results.\n\nOperator: Your last question comes from the line of Jeffrey Allan Schreiner with D.A. Davidson. Your line is open. Please go ahead.\n\nEamon Robert Coughlin: Perfect. Thanks for taking my question, and congrats on the strong results here.\n\nJeffrey Allan Schreiner: If I look at cloud revenues, they seemed a little bit more stable quarter to quarter this year compared to historically. Maybe just on the seasonality side, as cloud revenue becomes a bigger portion of revenue, should we expect a little bit more stable seasonality, or is it still the same as you target large enterprises in these larger deals? No change on that front?\n\nEduard Grabscheid: Hi, Jeffrey. When you think about the cloud, you need to think about three different things. First is our guidance philosophy, which we de-risk for those largest deals, as well as usage over minimum commitments, including emerging AI trends. That is what creates variability on a quarter-over-quarter basis, and that is excluded from there. As you mentioned, it is also now a greater portion of our revenue. But when you think about sequential growth today, it would be linear until we start to layer in any usage if that potential continues or these large deal wins.\n\nOperator: There are no further questions at this time. I will now turn the call back to Shlomi for closing remarks.\n\nShlomi Ben Haim: Thank you, everyone. 2025 was a great year. We are looking forward to 2026, and we are very excited about the changes in the market and the new players in the market that we are looking forward to collaborate with. May the frog be with you, and may you have a wonderful Valentine's Day.\n\nOperator: This concludes today's call. Thank you for attending. You may now disconnect."
  },
  {
    "source": "dzone.com",
    "company": "Hugging Face",
    "title": "Running Granite 4.0-1B Locally on Android",
    "date": "2026-02-04T17:18:02Z",
    "url": "https://dzone.com/articles/running-granite-40-1b-locally-on-android",
    "content": "Join the DZone community and get the full member experience.\n\nJoin For Free\n\nThis started the way these things usually do -- watching a podcast instead of doing something productive (I ended up writing this blog, so maybe it was productive after all).\n\nI was listening to a Neuron AI episode about IBM's new Granite 4 model family, with IBM Research's David Cox as the guest. During the discussion on model sizes and deployment targets, they talked about Granite 4 Nano, models designed specifically for edge and on-device use cases. At some point, the discussion turned to running these models on your phone.\n\nNot as a hypothetical. Not as a demo. Just as a thing you could do.\n\nThat was enough.\n\nBecause once someone says, \"You can run this on your phone,\" in that context, the only reasonable response is to stop listening and try it yourself.\n\nGranite 4 Nano isn't pitched as a toy model. What makes it interesting is that it's been designed to be small on purpose. That constraint shows up in how it behaves: more direct answers, less wandering, and a general sense that it's meant to be used as a tool rather than a conversational novelty.\n\nSo that's what this is. Granite 4.0-1B. Fully offline. Running locally on an Android phone. No cloud. No GPU. No vendor magic. Just a slightly unhealthy level of curiosity.\n\nThe result was surprisingly boring. Which is exactly what you want.\n\nI've kept this intentionally step-based so it's easy to reproduce without guessing or filling in gaps.\n\nWhat This Setup Gives You\n\nYou get two primary ways to interact with Granite locally:\n\n* An interactive CLI for quick prompts and experimentation.\n\n* A local web interface backed by an HTTP server.\n\nBoth run fully offline. No accounts, no telemetry, no background calls to anything you didn't ask for.\n\nThe CLI is exactly what you'd expect. It's fast, direct, and good for testing prompts or sanity-checking behavior. Type a question, get an answer, move on.\n\nThe web interface is where things start to get more interesting. By exposing the model through a local HTTP server, you're no longer tied to a terminal. You get streaming responses, a browser-based chat UI, and the ability to interact with the model over simple HTTP requests.\n\nOnce it's reachable this way, Granite stops being \"a chatbot on your phone\" and starts behaving like a local service. Anything that can speak REST and send JSON can interact with it, including scripts, other apps, and automation tools like Tasker.\n\nThis is where the \"tool, not a conversational novelty\" idea actually comes into play. You're not limited to typing prompts into a UI. You can wire the model into workflows, triggers, and background tasks, all without leaving the device or relying on a network connection.\n\nThe setup stays intentionally minimal. No UI frameworks, no wrappers, no attempt to make this look like a consumer app. Just a local model, a simple server, and interfaces that stay out of the way. A tool.\n\nBy the end of the guide, Granite isn't running as a demo. It's running as a local service.\n\nArchitecture (The Short Version)\n\nAt its core, this setup is very simple:\n\n* A transformer-based Granite 4.0-1B model.\n\n* Executed locally using llama.cpp.\n\n* Running on an ARM64 Android device via Termux.\n\nThere's no acceleration layer hiding in the background. No GPU, no Vulkan, no NNAPI. Everything runs on the CPU.\n\nThe model itself is the standard transformer variant of Granite 4.0-1B. IBM also ships Granite 4.0-H models that use a hybrid architecture with state space layers. Those are designed for different runtimes and aren't compatible with llama.cpp.\n\nOn top of the runtime, there are two execution paths:\n\n* for direct, interactive use.\n\n* for exposing the model over HTTP.\n\nBoth binaries use the same model file and the same execution backend. One model, two interfaces.\n\nQuantization is where most practical trade-offs lie. In short, quantization reduces model size by storing weights at lower precision. This setup uses a Q5_K_M quantized model that balances memory usage, speed, and reasoning quality.\n\nPrerequisites\n\nThere are a few things you need in place before this works. None of them is unusual, but missing any of them will show up later in less obvious ways.\n\nAndroid\n\n* An ARM64 Android device (I'm using a Galaxy S25 Ultra)\n\n* At least 8 GB of RAM recommended\n\n* Termux installed from F-Droid\n\nThe Play Store version of Termux is outdated and missing features required to build native code reliably. Download and install F-Droid, then search for Termux and install it.\n\nPC (Model Download Only)\n\n* Python 3.10 or newer.\n\n* A Hugging Face account with a read token.\n\nIf you don't want to use Python, you can also download the model directly from Hugging Face and skip token setup entirely.\n\nStep 1: Install Termux\n\nWith the prerequisites out of the way, it's time to set up the environment on the phone.\n\nOnce Termux is installed from F-Droid, open it and run:\n\nThis updates the base packages and sets up access to shared storage, which you'll need later to place the model file somewhere outside Termux's private directory.\n\nYou'll be prompted to grant storage permissions. Accept them. There's no workaround here that's worth the effort.\n\nAfter this completes, you should have a clean, up-to-date Termux environment ready to build native code.\n\nStep 2: Install Build Tools\n\nWith Termux set up, the next step is installing the tools needed to build llama.cpp locally.\n\nOnce installation finishes, it's worth checking that the basics are actually available:\n\nIf any of these commands fail, stop here and fix that first. The build step won't succeed otherwise.\n\nStep 3: Build Llama.cpp\n\nWith the build tools installed, it's time to compile llama.cpp on the device.\n\nStart by cloning the repository and moving into it:\n\nThen configure the build using CMake and Ninja:\n\nThis builds llama.cpp using all available CPU cores. On a modern phone, this takes no more than a few minutes.\n\nOnce the build completes, verify that the binaries were produced:\n\nYou should see and in the output. If you don't see them, check the build output and see if you can fix whatever is missing.\n\nThis build uses the CPU backend only. No GPU, no Vulkan, no NNAPI. Nothing else is required for this setup.\n\nStep 4: Select and Download the Granite Model\n\nIBM provides multiple pre-quantized versions of Granite 4.0-1B on Hugging Face. They all share the same base model but differ in how they store weights, which directly affects size, speed, and behavior.\n\nThe models live in this repository:\n\nWhy GGUF\n\nllama.cpp does not run models in their original training format. It expects weights in the GGUF format, a runtime-friendly format designed for efficient local inference.\n\nGGUF bundles the model weights together with the metadata llama.cpp needs at runtime: tensor layouts, tokenizer information, and model parameters. That's why these files can be loaded directly without extra configuration.\n\nIBM provides Granite 4 Nano models that are already converted to GGUF, eliminating an entire preparation step. There's no need to export, quantize, or otherwise preprocess the model just to get it running.\n\nIf you want to, you still can.\n\nThe original Granite models can be converted to GGUF manually using llama.cpp's conversion tools, and you can choose your own quantization settings in the process. That's useful if you're experimenting or targeting very specific constraints.\n\nFor this setup, there's no real upside. The provided GGUF files have already been tested and are ready to run. Using them keeps the focus on running the model rather than preparing it.\n\nQuantization Choice\n\nYou'll see a long list of files with names like Q2, Q4, Q5, Q8, and F16. These refer to different quantization levels.\n\nAt a high level:\n\n* Lower quantization means smaller files and faster inference, but weaker reasoning.\n\n* Higher quantization results in better output quality but higher memory usage and slower performance.\n\nOn mobile, this is a balancing act. Very small models respond quickly but fall apart when faced with anything beyond simple prompts. Very large ones work, but offer diminishing returns and unnecessary memory pressure.\n\nFor this setup, Q5_K_M is a good middle ground. It's small enough to run comfortably on a modern phone, but consistent enough to handle longer prompts and multi-step instructions without drifting.\n\nThat's the version used throughout the rest of this guide.\n\nAuthentication and Download\n\nGranite models require authentication to download.\n\nIn this setup, authentication is handled using a Hugging Face read token provided via an environment variable. This avoids interactive logins and keeps the process scriptable and reproducible.\n\nCreate a read token via the Hugging Face web UI, then export it on your PC:\n\nWith the token set, download the model using Python:\n\nIf you don't want to use Python or don't want to switch devices, you can also download the model directly from the Hugging Face website and skip the token setup entirely (you will need an account): https://huggingface.co/ibm-granite/granite-4.0-1b-GGUF.\n\nOnce the file is downloaded, you're done with the PC. The next step is moving the model onto the phone.\n\nStep 5: Copy the Model to Android\n\nOnce the model file is downloaded, it needs to be copied onto the phone.\n\nPlace the file at the following location:\n\nOn Android, is the base directory you see when opening your file manager. It's typically labelled as internal storage or phone storage. Creating a folder there keeps things simple and easy to find.\n\nThe exact directory name doesn't matter much, but keeping models outside Termux's home directory makes them easier to manage and reuse later.\n\nAfter copying the file, verify it from within Termux:\n\nYou should see the file listed at roughly 1.2 GB. If it's there, Termux can access it, and you're ready to move on.\n\nStep 6: Manual Validation Run\n\nBefore wiring anything up or automating it, it's worth making sure the model actually runs.\n\nFrom inside the directory, run the following command:\n\nOn a Galaxy S25 Ultra, you should see something in the ballpark of:\n\n* prompt processing around ~45-50 tokens/sec\n\n* generation speed around ~20-22 tokens/sec\n\nAt around 20 tokens per second, generation is already faster than most people can read.\n\nThe context size is set to 2048 tokens as a stable default for mobile. Larger values increase memory usage and don't buy you much for this kind of setup.\n\nIf you run into out-of-memory errors, sudden process termination, or aggressive thermal throttling, reduce the thread count.\n\nReasonable fallbacks are:\n\nor, if needed:\n\nIf this works, the hard part is over (not that hard, really).\n\nStep 7: Startup Script (Server + CLI)\n\nNow that the model runs manually, it's time to make it slightly more useful. A web browser tends to be more user-friendly than a terminal session anyway.\n\nThe goal here is simple:\n\n* Start the HTTP server in the background.\n\n* Drop straight into an interactive CLI session (for the real techies amongst you).\n\nCreate a startup script in your home directory:\n\nAdd the following:\n\nMake the script executable:\n\nRun it:\n\nWhen you exit the CLI, the HTTP server keeps running.\n\nStep 8: Web UI\n\nWith the server running, open a browser on the phone and navigate to:\n\nThat's it.\n\nYou'll get a web-based chat interface backed by the local HTTP server. Prompts are sent to the model, responses stream back in real time, and everything stays on-device. It is a bit slower than the CLI, but still very useful.\n\nThe interface keeps things simple, but it's not bare bones. You get proper chat behavior: conversation history is preserved, responses can be edited and regenerated, and you can work with multiple chats in parallel. In practice, it behaves much like the web interfaces people are already used to, just backed by a model running locally on the device.\n\nBecause the server binds to, it's only accessible locally.\n\nAt this point, you can close the terminal if you like. If the server process is still running, the web UI will continue to work.\n\nStep 9: Auto-Start on Termux Launch\n\nAt this point, everything works. The last step is making it stick.\n\nThe goal here is simple: when you open Termux, Granite starts automatically. No manual commands, no remembering which script to run. Ready to use, every time.\n\nEdit your shell startup file:\n\nAppend the following:\n\nThis ensures the startup script runs once per Termux session. The guard variable prevents accidental double starts, and closing Termux cleanly shuts everything down.\n\nIf Termux crashes or is force-stopped, the guard resets, and Granite will start again the next time you open it.\n\nStopping the Server\n\nIf you want to stop the HTTP server without closing Termux:\n\nThat's it. From here on out, opening Termux is enough to bring Granite back online.\n\nNotes\n\nA few practical things worth keeping in mind after setting this up:\n\n* Granite 4.0-H models use a hybrid architecture with state space layers and are not compatible with llama.cpp. This setup only applies to the transformer-based Granite 4 Nano models.\n\n* Q5_K_M works well on modern phones. If you run into stability issues, lowering the thread count is usually the first step.\n\n* The CLI and HTTP server can run at the same time. Exiting the CLI does not affect the server as long as the Termux session stays open.\n\n* Once the model is downloaded, everything runs fully offline. No network access is required for inference.\n\n* The HTTP server is bound to localhost by default. Exposing it to the network is possible, but intentionally not covered here.\n\n* Performance, thermals, and battery impact vary by device. Newer phones handle this comfortably; older ones may need more conservative settings.\n\n* This setup is not optimized for background execution or for long battery life. It's meant to be practical, not invisible.\n\nClosing\n\nAt this point, Granite is running locally on the device, starts automatically with Termux, and is accessible both interactively and over HTTP.\n\nI've said this already, but that's what a closing is for, right?\n\nThere's no cloud dependency, no account setup, and no special runtime beyond what's shown above. Once the model is in place, everything else is just process management.\n\nIt's not particularly impressive to look at. It's just useful.\n\nWhich is exactly what you want from a local model."
  },
  {
    "source": "Le Monde Informatique",
    "company": "Hugging Face",
    "title": "Hugging Face enrôlé pour diffuser un malware Android - Le Monde Informatique",
    "date": "2026-02-04T09:03:08Z",
    "url": "https://www.lemondeinformatique.fr/actualites/lire-hugging-face-enrole-pour-diffuser-un-malware-android-99233.html",
    "content": "Une fausse application de sécurité est utilisée pour déployer un malware pour Android hébergé sur Hugging Face. Les attaquants génèrent des milliers de variantes de packages malveillants pour échapper à la détection.\n\nDes chercheurs de Bitdefender Labs ont découvert une campagne de malware sur le site Hugging Face, hébergeant des outils IA ouverts (LLM, frameworks, jeux de données). A travers ce site, les cybercriminels distribuent un cheval de troie d'accès à distance (RAT) pour Android. Les chercheurs ont signalé cette campagne, non seulement pour utiliser une plateforme de développement IA réputée, mais aussi pour son ampleur et son automatisation. Celle-ci comprend des milliers de packages Android uniques, avec de nouvelles variantes générées fréquemment pour contourner les défenses basées sur les signatures.\n\nL'infection commence par inciter les utilisateurs Android à installer l'application de sécurité en apparence officielle nommée \" TrustBastion \". Pour cela, \" le scénario le plus probable est que les pirates envoient un message ou une publicité sur le téléphone de la personne prétendant qu'il est infecté \", souligne les experts de Bitdefender Labs. \" Ils l'incitent ensuite à installer un outil de sécurité présenté comme gratuit et doté de fonctionnalités utiles \", ajoutent-ils. TrustBastion promettait de détecter les arnaques et les SMS frauduleux, le phishing, les logiciels malveillants et bien plus encore.\n\nUne fois lancée l'application affiche immédiatement un message imitant une notification de mise à jour du système Android ou de Google Play, une interface à laquelle de nombreux utilisateurs font confiance. Accepter la \" mise à jour \" déclenche une requête réseau vers un point de terminaison chiffré sur l'infrastructure de l'attaquant, qui redirige la victime vers un jeu de données Hugging Face hébergeant un fichier APK malveillant. Une fois téléchargé, le malware demande à l'utilisateur de lui accorder des autorisations sur plusieurs services comme la capacité d'enregistrer l'écran. L'ensemble de ces autorisations confèrent au malware une visibilité étendue sur les interactions de l'utilisateur et la capacité de capturer le contenu affiché à l'écran dans toutes les applications.\n\nSelon les chercheurs, Hugging Face est désormais exploité pour masquer des téléchargements malveillants au sein d'activités officielles. Par ailleurs, ils observent que l'analyse via ClamAV des fichiers déposés sur la plateforme est insuffisante pour filtrer les malwares habilement dissimulés. \" L'analyse du dépôt Hugging Face a révélé un volume élevé de modifications sur une courte période \", ont déclaré les spécialistes. \" De nouvelles charges utiles étaient générées environ toutes les 15 minutes. Au moment de l'enquête, le dépôt avait environ 29 jours et avait accumulé plus de 6 000 modifications. \" Le dépôt a finalement été mis hors ligne, mais l'opération a refait surface ailleurs avec des modifications superficielles mineures, tandis que le code sous-jacent est resté inchangé.\n\nBitdefender a contacté Hugging Face avant de publier les résultats de ses recherches. La plateforme a rapidement supprimé les dépôts contenant le malware. Interrogée par CSO, elle n'a pas donné de commentaires supplémentaires. Pour obtenir une assistance supplémentaire, Bitdefender a partagé une liste d'indicateurs de compromission, notamment le hachage des droppers, les adresses IP, les domaines et les noms des paquets."
  },
  {
    "source": "Ad Hoc News",
    "company": "Hugging Face",
    "title": "TrustBastion: Android-Malware nutzt KI-Plattform Hugging Face",
    "date": "2026-01-30T03:51:44Z",
    "url": "https://www.ad-hoc-news.de/boerse/ueberblick/trustbastion-android-malware-nutzt-ki-plattform-hugging-face/68533043",
    "content": "Eine hochgefährliche Android-Schadsoftware tarnt sich als Sicherheits-App und verteilt sich über die vertrauenswürdige KI-Plattform Hugging Face. Der als TrustBastion bekannte Remote-Access-Trojaner zielt auf Finanzdaten ab und zeigt, wie Cyberkriminelle zunehmend legitime Cloud-Dienste missbrauchen.\n\nDer Angriff beginnt mit klassischem Social Engineering. Opfer werden durch aggressive Werbung in die Falle gelockt, die vor einem angeblichen Virus warnt. Als Lösung wird die angebliche Sicherheits-App TrustBastion angeboten. Nach der Installation simuliert die App eine Systemaktualisierung von Google Play. Bestätigt der Nutzer diese, startet der eigentliche Schadcode.\n\nStatt eines Updates lädt die App den finalen Trojaner von einem Datensatz-Repository auf der Infrastruktur von Hugging Face herunter. Dieser Trick ist tückisch: Der bösartige Datenverkehr verschmilzt mit dem legitimen Traffic einer angesehenen KI- und Entwicklerplattform. Herkömmliche Netzwerksicherheitstools haben so kaum eine Chance, die Bedrohung zu erkennen.\n\nPassend zum Thema Cyber-Bedrohungen: 73 % der deutschen Unternehmen sind laut Experten schlecht auf Angriffe vorbereitet -- gerade wenn Schadcode über vermeintlich harmlose Plattformen verteilt wird. Unser kostenloses E‑Book erklärt aktuelle Cyber‑Security‑Trends, zeigt einfache Schutzmaßnahmen und welche Kontrollen sofort wirken, auch ohne großes Budget. Enthalten sind Checklisten für Sofortmaßnahmen und Vorlagen zur Risikoanalyse. Ideal für IT‑Verantwortliche und Geschäftsführer, die schnell Risiken mindern wollen. Gratis Cyber-Security-E-Book herunterladen\n\nDas Besondere an dieser Kampagne ist die aggressive server-seitige Polymorphie. Die Angreifer haben die Erstellung neuer Schadsoftware-Varianten automatisiert und auf Hugging Face hochgeladen - in atemberaubendem Tempo.\n\nLaut Analysen von Bitdefender generierten die Kriminellen alle 15 Minuten eine neue Variante des Trojaners. In einem Fall sammelte sich in einem bösartigen Repository binnen eines Monats über 6.000 einzigartige APK-Dateien an. Jede Variante weist minimale Code-Änderungen auf, die ihren digitalen Fingerabdruck verändern. Für signaturbasierte Antivirenprogramme erscheint jede neue Version als unbekannte Bedrohung, obwohl die schädliche Kernfunktionalität identisch bleibt.\n\nDer finale Schadcode ist ein mächtiger Remote-Access-Trojaner. Nach der Installation erschleicht er sich umfangreiche Berechtigungen, indem er den Android-Barrierefreiheitsdienst (Accessibility Services) missbraucht - eine gängige Taktik moderner Mobil-Malware.\n\nMit diesen Rechten erlangen die Angreifer fast vollständige Kontrolle. Der Trojaner kann den Bildschirm abfangen, Aktivitäten aufzeichnen und Overlay-Angriffe starten. Dabei legt er gefälschte Login-Masken über legitime Apps, um Zugangsdaten abzugreifen. Die Kampagne zielt besonders auf Nutzer im asiatisch-pazifischen Raum ab, mit speziellen Overlays für Finanzdienste wie Alipay und WeChat. Alles Erbeutete wird an einen zentralen Command-and-Control-Server gesendet.\n\nDer Missbrauch von Plattformen wie Hugging Face markiert eine gefährliche Entwicklung. Cyberkriminelle nutzen zunehmend seriöse Cloud- und Entwicklerdienste, um ihre Schadsoftware zu hosten. Diese \"Living-off-the-Land\"‑Taktik erschwert die Abwehr enorm. Ein generelles Blockieren des Datenverkehrs zu Hugging Face wäre keine Lösung, da dies legitime Forschungs- und Entwicklungsarbeit lahmlegen würde.\n\nBerichten zufolge versagten die Standard-Sicherheitsscans von Hugging Face, die auf der Open-Source-Antiviren-Engine ClamAV basieren. Das offenbart eine potenzielle Lücke in der Inhaltsmoderation von Plattformen, die nutzergenerierten Code in großem Maßstab hosten. Je zentraler KI-Plattformen für die Softwareentwicklung werden, desto attraktiver werden sie auch für Supply-Chain-Angriffe.\n\nDie TrustBastion-Kampagne zeigt den Trend zu immer raffinierterer Mobil-Malware. Experten erwarten, dass Angreifer diese Techniken weiter verfeinern werden. Plattformen müssen möglicherweise strengere, verhaltensbasierte Sicherheitschecks für hochgeladene Inhalte einführen.\n\nFür Android-Nutzer bleibt Wachsamkeit der beste Schutz. Apps sollten ausschließlich aus dem offiziellen Google Play Store bezogen werden. Unerwünschte Werbung oder Pop-ups, die eine Infektion behaupten, sind mit größter Skepsis zu betrachten. Besonders kritisch sind Berechtigungsanfragen für Barrierefreiheitsdienste, die einer App nahezu vollständige Kontrolle geben. Regelmäßige Sicherheitsupdates des Betriebssystems schließen zudem bekannte Schwachstellen.\n\nPS: IT‑Sicherheit lässt sich oft ohne teure Neueinstellungen deutlich verbessern. Dieser kostenlose Leitfaden zeigt praxisnahe Maßnahmen, wie Sie Ihre Android‑Nutzer schützen, Phishing‑Angriffe erkennen und Cloud‑Services sicher konfigurieren - mit konkreten Checklisten für sofortiges Handeln. Plus: Praxisbeispiele, Priorisierungsempfehlungen und einfache Schritte, die auch kleine IT‑Teams sofort umsetzen können. Perfekt für Unternehmen, die ressourcenschonend ihre Abwehr stärken wollen. Jetzt kostenlosen IT-Security-Report sichern"
  },
  {
    "source": "About Chromebooks",
    "company": "Hugging Face",
    "title": "BLIP-2 Statistics 2026",
    "date": "2026-01-23T17:19:28Z",
    "url": "https://www.aboutchromebooks.com/blip-2-statistics/",
    "content": "BLIP-2 reached 536,142 monthly downloads on Hugging Face as of 2024, establishing itself as a foundational vision-language model since its January 2023 release by Salesforce Research. The model achieved 65.0% accuracy on zero-shot VQAv2 benchmarks while training only 188 million parameters, representing 54 times fewer trainable parameters than competing models. This analysis examines verified performance metrics, adoption statistics, and technical specifications that demonstrate BLIP-2's continued relevance in multimodal AI development.\n\nBLIP-2 Key Statistics\n\n* BLIP-2 records 536,142 monthly downloads on Hugging Face with the blip2-opt-2.7b variant as the most popular model checkpoint.\n\n* The model accumulated 3,099 academic citations by September 2024, ranking among the top 10 most cited AI papers published in 2023.\n\n* BLIP-2 achieved 8.7% higher accuracy than Flamingo80B on zero-shot VQAv2 tasks despite using 54 times fewer trainable parameters.\n\n* The Q-Former architecture requires only 188 million trainable parameters while connecting to language models containing up to 11 billion parameters.\n\n* Memory requirements drop to 1.8 GB with Int4 quantization, enabling deployment on consumer-grade hardware for inference tasks.\n\nBLIP-2 Model Architecture and Parameter Efficiency\n\nThe BLIP-2 framework introduced a parameter-efficient approach through its Querying Transformer architecture. The Q-Former component contains 188 million trainable parameters across 12 transformer layers, generating query embeddings with dimensions of 32 × 768.\n\nThis lightweight module connects frozen image encoders to large language models without requiring end-to-end training. The architecture supports OPT models ranging from 2.7 billion to 6.7 billion parameters and FlanT5 variants from XL to XXL sizes.\n\nThe two-stage pre-training strategy first bootstraps vision-language representation learning using the Q-Former and frozen image encoder. The second stage connects the Q-Former to frozen language models for generative capabilities, transforming an 11-billion parameter LLM into a multimodal system while training less than 2% of total parameters.\n\nBLIP-2 Benchmark Performance Across Tasks\n\nBLIP-2 established state-of-the-art results across multiple vision-language benchmarks upon release. The model scored 65.0% on zero-shot VQAv2, surpassing Flamingo80B's 56.3% performance by 8.7 percentage points.\n\nOn the GQA benchmark, BLIP-2 achieved 52.3% accuracy in zero-shot evaluation. The model recorded a CIDEr score of 121.6 on NoCaps zero-shot captioning tasks, improving 8.4 points over the previous state-of-the-art score of 113.2.\n\nFine-tuned BLIP-2 variants reached 145.8 CIDEr on COCO Caption benchmarks, marking a 9.1-point improvement over the original BLIP model's 136.7 score. Image-to-text retrieval on Flickr30K achieved 92.9% R@1 accuracy, setting new performance standards for the task.\n\nZero-Shot Capabilities\n\nThe zero-shot performance improvements demonstrate BLIP-2's generalization capabilities. The 8.7% VQAv2 accuracy gain over Flamingo80B occurred despite BLIP-2 using 54 times fewer trainable parameters, establishing a new efficiency-performance benchmark for vision-language models.\n\nBLIP-2 Adoption and Community Impact\n\nHugging Face statistics reveal sustained adoption of BLIP-2 in production workflows. The blip2-opt-2.7b checkpoint maintains consistent monthly downloads exceeding 536,000 as of 2024, with 425 likes on the platform.\n\nThe Salesforce organization accumulated 1,990 followers on Hugging Face. Five official BLIP-2 model variants support different language model backends and use cases. The community developed 38 adapter models and 13 fine-tuned derivatives based on BLIP-2's architecture.\n\nOver 100 Hugging Face Spaces applications integrate BLIP-2 for image captioning, visual question answering, and multimodal search functionality. This deployment scale demonstrates BLIP-2's production viability across diverse application domains.\n\nAcademic Recognition\n\nBLIP-2 accumulated 3,099 citations by September 2024 according to research tracking platforms. This citation rate positioned the paper among the top 10 most cited AI research papers published in 2023.\n\nThe paper appeared at ICML 2023 following its January 30, 2023 release. Hugging Face integration followed within 10 days on February 9, 2023, accelerating community access and experimentation.\n\nBLIP-2 Computational Requirements and Memory Efficiency\n\nBLIP-2 supports multiple precision modes that enable deployment across different hardware configurations. Float32 precision requires 14.43 GB for inference and 57.72 GB for training with Adam optimizer.\n\nFloat16 and BFloat16 precision reduce memory requirements to 7.21 GB for inference and 28.86 GB for training. Int8 quantization further decreases inference memory to 3.61 GB with 14.43 GB needed for training operations.\n\nInt4 quantization achieves the lowest memory footprint at 1.8 GB for inference and 7.21 GB for training. This configuration enables BLIP-2 deployment on consumer GPUs and edge devices without specialized hardware requirements.\n\nInference speed reaches approximately 1 second per image on single GPU setups. This processing rate compares favorably with larger multimodal models that require 40 seconds or more for equivalent tasks.\n\nBLIP-2 Training Data and Pre-training Methodology\n\nBLIP-2 pre-training utilized 129 million image-text pairs aggregated from multiple datasets. The training corpus included 115 million pairs from LAION processed with Caption Filtering techniques, plus additional data from COCO, Visual Genome, and Conceptual Captions datasets.\n\nThe pre-training methodology employs three complementary objectives. Image-Text Contrastive learning aligns image and text representations in a shared embedding space. Image-Text Matching captures fine-grained correspondence between visual and linguistic elements.\n\nImage-grounded Text Generation enables text production conditioned on visual inputs. These three objectives work synergistically across two distinct pre-training stages, with the recommended configuration requiring 16 A100 GPUs for optimal training efficiency.\n\nMulti-Objective Learning Strategy\n\nThe ITC objective trains the model to associate semantically related image-text pairs while separating unrelated pairs. ITM learning enables the model to determine whether specific text descriptions accurately match given images.\n\nITG training develops generative capabilities by conditioning language model outputs on visual features extracted through the Q-Former. This multi-objective approach contributes to BLIP-2's strong zero-shot transfer performance across diverse downstream tasks.\n\nBLIP-2 Performance vs Competing Vision-Language Models\n\nComparative analysis reveals BLIP-2's positioning within the vision-language ecosystem. The model achieved 65.0% zero-shot accuracy on VQAv2, while LLaVA-1.5-13B reached 80.0% through end-to-end fine-tuning approaches.\n\nFlamingo-80B recorded 56.3% zero-shot VQAv2 accuracy with 113.2 CIDEr on NoCaps captioning tasks. InstructBLIP, a BLIP-2 derivative incorporating instruction-tuning, improved performance over the base BLIP-2 model across multiple benchmarks.\n\nBLIP-2 maintains advantages in zero-shot captioning and image-text retrieval tasks compared to models requiring extensive fine-tuning. The Q-Former architecture influenced subsequent developments including InstructBLIP and multiple vision-language models adopting adapter-based approaches.\n\nBLIP-2 Derivative Models and Applications\n\nThe BLIP-2 architecture spawned multiple specialized derivatives extending its capabilities. InstructBLIP emerged as the most significant extension, incorporating instruction-tuning across 26 datasets covering 11 task categories.\n\nBLIP-Diffusion adapted the architecture for subject-driven image generation, training on 292,000 OpenImage-V6 subjects. Video-LLaMA extended BLIP-2 to video understanding tasks through audio-visual instruction tuning mechanisms.\n\nDomain-specific adaptations include BLIP-2 Japanese for Japanese language captioning trained on STAIR captions, and PointBLIP enabling zero-shot 3D point cloud classification. These derivatives demonstrate the architecture's flexibility across modalities and languages.\n\nInstructBLIP Advancements\n\nInstructBLIP achieved state-of-the-art zero-shot performance across 13 held-out datasets by incorporating instruction-aware query embeddings. The model maintains BLIP-2's parameter efficiency while significantly improving task-specific performance through instruction tuning."
  },
  {
    "source": "Ad Hoc News",
    "company": "Hugging Face",
    "title": "Android-Malware nutzt Hugging Face als Hintertür",
    "date": "2026-02-06T00:10:45Z",
    "url": "https://www.ad-hoc-news.de/boerse/ueberblick/android-malware-nutzt-hugging-face-als-hintertuer/68555906",
    "content": "Eine neue Schadsoftware tarnt sich als Systemupdate und nutzt die vertrauenswürdige Plattform Hugging Face zur Verbreitung. Nach der Installation erlangt sie über Barrierefreiheitsdienste die vollständige Kontrolle über das Smartphone.\n\nEine neue Android-Schadsoftware kapert Smartphones über die KI-Plattform Hugging Face. Der Trojaner tarnt sich als Sicherheits-App und greift nach voller Kontrolle.\n\nDie Infektion beginnt mit der App \"TrustBastion\". Nutzer werden über Werbung dazu gebracht, sie außerhalb des Play Stores zu installieren. Direkt nach der Installation fordert die App ein dringendes System-Update an - und imitiert dabei täuschend echt die Dialoge von Android oder dem Google Play Store.\n\nStimmt der Nutzer zu, lädt die App die eigentliche Schadsoftware nicht von einem offensichtlich kriminellen Server, sondern von einem verschlüsselten Repository auf Hugging Face herunter. Diese Plattform für KI-Modelle genießt hohes Vertrauen, was die Erkennung der Malware enorm erschwert.\n\nHacker nutzen zunehmend vertrauenswürdige Plattformen, um manipulierte Apps und fertige Schadsoftware zu verteilen -- klassische Erkennungsmechanismen schlagen dabei oft fehl. Der kostenlose Anti-Phishing-Report erklärt in vier klaren Schritten, wie Sie manipulierte Downloads, Social-Engineering-Tricks und fertige Angriffs-Kits identifizieren, betroffene Geräte sichern und einfache Schutzmaßnahmen sofort umsetzen. Enthalten sind eine praxisnahe Checkliste und Hinweise für Nutzer sowie IT-Verantwortliche. Jetzt Anti-Phishing-Paket herunterladen\n\nSobald der Remote-Access-Trojaner installiert ist, drängt er den Nutzer, die Barrierefreiheitsdienste (Accessibility Services) zu aktivieren. Diese Funktion ist für Menschen mit Behinderungen gedacht, gewährt einer App aber auch uneingeschränkten Zugriff.\n\nMit dieser Berechtigung kann die Malware:\n\n* Den Bildschirm in Echtzeit aufzeichnen\n\n* Tastatureingaben und Passwörter mitlesen\n\n* Overlays über Banking-Apps legen\n\n* Das Gerät fernsteuern und Daten abgreifen\n\nSensible Informationen wie Bankdaten, Nachrichten und Passwörter landen so direkt bei den Cyberkriminellen.\n\nDer Missbrauch von Diensten wie Hugging Face ist ein wachsender Trend. Die Sicherheitsforscher von Bitdefender, die den Angriff aufdeckten, beobachteten eine raffinierte Taktik: Die Angreifer generierten im genutzten Repository etwa alle 15 Minuten eine neue Variante der Schadsoftware.\n\nZiel dieser Flut an kleinen Änderungen: Herkömmliche, signaturbasierte Virenscanner sollten umgangen werden. Obwohl Hugging Face die schädlichen Inhalte nach der Meldung löschte, hatten die Kriminellen bereits ein neues Repository vorbereitet und setzten ihren Angriff fort.\n\nDiese Kampagne zeigt, wie professionell Android-Malware heute vorgeht. Sie kombiniert geschicktes Social Engineering mit der Tarnung durch vertrauenswürdige Dienste. Die Methode erinnert an andere hochgefährliche Trojaner wie Vultur, der ebenfalls Google-Warnungen fälscht, um Nutzer zu täuschen.\n\nDie größte Schwachstelle bleibt der Zugriff auf die Barrierefreiheitsdienste. Eine einmal erteilte Erlaubnis gibt den Angreifern praktisch die volle Kontrolle über das Smartphone - eine Gefahr, die viele Nutzer unterschätzen.\n\nDie Bedrohung macht grundlegende Vorsichtsmaßnahmen wichtiger denn je. Die wichtigsten Regeln:\n\n* Apps nur aus dem Google Play Store installieren.\n\n* Aufforderungen zur Aktivierung der Barrierefreiheitsdienste extrem kritisch hinterfragen. Legitime Apps benötigen diese nur in seltenen Ausnahmefällen.\n\n* Android und alle Apps stets aktuell halten, um bekannte Sicherheitslücken zu schließen.\n\nDie Kampagne beweist: Cyberkriminelle werden immer kreativer, wenn es darum geht, das Vertrauen der Nutzer in bekannte Dienste auszunutzen. Wachsamkeit ist der beste Schutz.\n\nPS: Wenn Sie sich vor genau solchen Angriffen schützen wollen, hilft der kompakte Gratis-Guide mit konkreten Gegenmaßnahmen: Er erklärt die gängigen Täuschungsmuster (inklusive Social Engineering), zeigt, wie Kriminelle fertige Schadprogramme aus vermeintlich harmlosen Repositories nutzen, und listet sofort umsetzbare Schutzschritte für Smartphones und Mitarbeiter auf. Ideal für Privatnutzer und kleine IT-Teams. Jetzt kostenlosen Anti‑Phishing-Guide sichern"
  },
  {
    "source": "Ad Hoc News",
    "company": "Hugging Face",
    "title": "Android-Banking-Trojaner nutzen Google Play und KI-Plattformen",
    "date": "2026-02-04T22:53:02Z",
    "url": "https://www.ad-hoc-news.de/boerse/ueberblick/android-banking-trojaner-nutzen-google-play-und-ki-plattformen/68552368",
    "content": "Neue Schadsoftware infiltriert offizielle App-Stores und nutzt Plattformen wie Hugging Face zur Verbreitung. Zehntausende Nutzer sind durch tückische Overlay-Angriffe gefährdet.\n\nNeue Schadsoftware wie Anatsa und TrustBastion infiltriert offizielle Stores und nutzt KI-Plattformen zur Verbreitung. Zehntausende Nutzer sind gefährdet.\n\nEine neue Woche, eine neue Bedrohung: Cyberkriminelle haben ihre Angriffe auf Android-Nutzer massiv ausgeweitet. Aktuelle Berichte aus den letzten 72 Stunden zeigen eine besorgniserregende Entwicklung. Die neuen Banking-Trojaner schleusen sich über scheinbar legitime Apps ein und nutzen ausgeklügelte Overlay-Angriffe, um Bankdaten abzugreifen. Besonders alarmierend: Die Schädlinge schaffen es sogar in den offiziellen Google Play Store und kapern renommierte Plattformen wie die KI-Community Hugging Face für ihre Zwecke.\n\nDer unmittelbarste Gefahrenherd kommt aus einer Quelle, der viele Nutzer vertrauen: dem Google Play Store. Sicherheitsforscher entdeckten dort eine App namens \"StellarGrid\", die sich als harmloser Dokumentenleser ausgab. Über 50.000 Mal wurde sie heruntergeladen, bevor sie als bösartig erkannt wurde. Im Kern verbarg sich ein sogenannter Dropper für den Banking-Trojaner Anatsa, auch als TeaBot bekannt.\n\nDie Masche ist tückisch: Nach der Installation fordert die App unter einem Vorwand umfassende Barrierefreiheits-Berechtigungen (Accessibility Services) an. Stimmt der Nutzer zu, erhält der Trojaner uneingeschränkte Kontrolle. Er kann dann:\n\n* Gefälschte Login-Fenster über echte Banking-Apps legen.\n\n* SMS-Nachrichten abfangen, um Zwei-Faktor-Authentifizierungen zu umgehen.\n\n* Alle Aktivitäten des Nutzers überwachen.\n\nAnatsa zielt auf Hunderte Bank-Apps in Europa und den USA ab. Die Tatsache, dass er den Sicherheitscheck des Play Stores passierte, zeigt ein grundlegendes Problem: Selbst offizielle Märkte bieten keinen absoluten Schutz.\n\nBanking-Trojaner verändern sich rasant: Overlay-Angriffe und polymorphe Malware umgehen klassische Signatur-Scanner und treffen sowohl Privatnutzer als auch Unternehmen. Ein kompakter Gratis-Leitfaden erklärt aktuelle Angriffsformen, konkrete Schutzmaßnahmen für Android-Geräte und wie Sie Mitarbeiter sicherheitsbewusster machen. Ideal für IT-Verantwortliche und Entscheider, die sofort wirkende Schritte umsetzen wollen. Kostenloses Cyber-Security-E-Book herunterladen\n\nNoch raffinierter agiert eine zweite Kampagne. Sie nutzt das Vertrauen in Hugging Face, eine angesehene Plattform für KI-Modelle und Open-Source-Code. Laut einem Report von Bitdefender wird hier der neue Trojaner TrustBastion verbreitet.\n\nDas Vorgehen ist mehrstufig: Zuerst lockt eine App Nutzer mit Scareware-Taktiken - sie simuliert Sicherheitswarnungen und drängt zum Download eines \"Notfall-Updates\". Dieses Update wird jedoch nicht aus dem Store, sondern direkt von einem Repository auf Hugging Face geladen. Die Reputation der Plattform soll Misstrauen zerstreuen.\n\nTrustBastion nutzt ebenfalls die Accessibility Services für seine Angriffe. Seine Besonderheit: Server-seitige Polymorphie. Die Angreifer können den Schadcode alle 15 Minuten leicht verändern und neue Varianten generieren. Für traditionelle Virenscanner, die nach festen Signaturen suchen, ist das wie der Kampf gegen ein Gespenst - fast unmöglich zu erfassen.\n\nDas Herzstück dieser Trojaner ist die Overlay-Technik. Sie erlaubt einer bösartigen App, ein Fenster über jede andere Anwendung zu legen. Startet der Nutzer seine Banking-App, erkennt der Trojaner dies sofort. Sekundenschnell blendet er einen täuschend echten, gefälschten Login-Bildschirm ein.\n\nWas der Nutzer für seine Bank hält, ist in Wirklichkeit die Falle. Jegliche eingegebene Daten - Kontonummern, Passwörter, PINs - landen direkt auf den Servern der Cyberkriminellen. Die Trojaner missbrauchen damit eine Funktion, die eigentlich Menschen mit Behinderungen helfen soll.\n\nDie Strategie der Angreifer hat sich fundamental gewandelt. Sie setzen nicht mehr auf zwielichtige Seiten, sondern infiltrieren vertrauenswürdige Ökosysteme. Der Play Store und Entwicklerplattformen wie Hugging Face werden zur Trojanischen Pferden. Das überwindet sowohl technische Hürden als auch die natürliche Skepsis der Nutzer.\n\nHinzu kommt ein strukturelles Problem bei Android: Viele Millionen Geräte weltweit laufen mit veralteten Betriebssystemen, die keine Sicherheitsupdates mehr erhalten. Diese Geräte sind ein leichtes Ziel für neue Exploits. Einige Bankenverbände diskutieren bereits, den Zugang zu Banking-Apps an aktuelle Android-Versionen zu knüpfen - ein drastischer, aber verständlicher Schritt.\n\nDie Lage erfordert erhöhte Wachsamkeit. Nutzer sollten:\n\n1. Jede Berechtigungsanforderung kritisch hinterfragen, besonders für Accessibility Services. Eine simple Taschenlampen-App benötigt diesen tiefen Systemzugriff nicht.\n\n2. Apps regelmäßig auf Verdächtiges prüfen und unbekannte oder nicht mehr genutzte Anwendungen deinstallieren.\n\n3. Ein seriöses Sicherheitsprogramm für Mobilgeräte in Betracht ziehen, das auch verhaltensbasierte Erkennung bietet.\n\n4. Das Betriebssystem und alle Apps stets aktuell halten.\n\nDer Kampf zwischen Sicherheitsexperten und Cyberkriminellen ist ein Wettlauf. Während Google und Sicherheitsfirmen ihre Erkennungsmethoden anpassen müssen, liegt der letzte Schutz auch in den Händen der Nutzer. Im mobilen Banking ist gesunde Skepsis der beste Begleiter.\n\nPS: Sie möchten konkrete Schutzmaßnahmen gegen Overlay-Angriffe und polymorphe Malware? Der kostenlose Cyber-Guide fasst die wichtigsten Trends zusammen, nennt sofort umsetzbare Schritte für Android-Geräte und zeigt, welche Schulungsmaßnahmen Ihre Mitarbeiter wirklich schützen. Ideal für Geschäftsführer, IT-Verantwortliche und Sicherheitsbeauftragte, die schnell Risiken minimieren wollen. Jetzt kostenlosen Cyber-Security-Guide sichern"
  },
  {
    "source": "Android Headlines",
    "company": "Hugging Face",
    "title": "Cybercriminals are abusing AI platforms to spread mobile malware",
    "date": "2026-01-30T20:31:43Z",
    "url": "https://www.androidheadlines.com/2026/01/cybercriminals-are-abusing-ai-platforms-to-spread-mobile-malware.html",
    "content": "Cybercriminals are exploiting Hugging Face's AI platform to distribute TrustBastion, an Android trojan disguised as a security app that steals banking credentials through fake login overlays. The malware bypasses traditional security by using a trusted platform for distribution and highlights why Google has pushed to restrict sideloading on Android devices.\n\nWe've said it before, and we'll say it again. Whenever you download apps for your phone, it's usually best if you download them from trusted sources. We're talking about platforms such as Google Play, which comes with various security features designed to protect users as much as realistically possible. Unfortunately, if you have turned to third-party sources, it seems that a recently discovered Android Trojan has been using the Hugging Face platform to deliver malware onto smartphones.\n\nAccording to security firm Bitdefender, hackers are using Hugging Face's infrastructure as a distribution platform for an Android trojan called TrustBastion. The malware disguises itself as a security app and uses scareware tactics to scare users into installing it. Once you install it, the app immediately prompts you to download an \"urgent update.\"\n\nHowever, instead of getting a legitimate update, the app redirects users to a Hugging Face repository where it downloads the actual malware. For those unfamiliar, Hugging Face is a popular platform that hosts AI models and datasets. It is used by developers and researchers worldwide. It is also why it's considered a trusted platform.\n\nUnfortunately, this trust has led cybercriminals to take advantage of it to distribute malware. TrustBastion isn't the first instance, and we doubt it will be the last. The attackers also use server-side polymorphism, creating new malware variants roughly every 15 minutes.\n\nOnce the malware has been installed, it takes advantage of Android's Accessibility Services to gain control over your device. It can record your screen, log everything you type, and overlay fake login windows on top of real banking apps. This means when you enter your credentials, you're actually handing them directly to the attackers.\n\nSo how do you avoid falling victim to this kind of attack? The answer is pretty straightforward. Stick to official app stores like Google Play. We get that there is no such thing as a perfect system. In fact, more than once, Google Play has accidentally hosted apps laden with malware. However, Google Play has multiple layers of protection, including Google Play Protect, which scans apps for malicious behavior. Third-party app stores and APK files from random websites typically don't have these safeguards.\n\nThis is actually one of the reasons Google wanted to restrict sideloading on Android in the first place. Sideloading means installing apps from outside the official store, which bypasses all of Google's security measures. While advanced users appreciate the flexibility, it opens the door for exactly this kind of attack. If you absolutely must sideload an app, make sure it's from a developer you completely trust."
  },
  {
    "source": "Global Security Mag Online",
    "company": "Hugging Face",
    "title": "Android Trojan Campaign Uses Hugging Face Hosting for RAT Payload Delivery - Global Security Mag Online",
    "date": "2026-01-30T08:55:08Z",
    "url": "https://www.globalsecuritymag.com/android-trojan-campaign-uses-hugging-face-hosting-for-rat-payload-delivery.html",
    "content": "Bitdefender researchers have discovered an Android RAT (remote access trojan) campaign that combines social engineering, the resources of the Hugging Face online platform as staging, and extensive use of Accessibility Services to compromise devices.\n\nWhat makes this campaign particularly interesting is the attackers' use of Hugging Face to host malicious payloads, and the scale at which new samples are deployed.\n\nHugging Face is a widely used online hosting service that provides a home to machine learning models and gives users a place to host their open-source models, datasets, and other development tools that researchers and developers usually need.\n\nUnfortunately, the space Hugging Face offers can also be used by cybercriminals for malicious purposes as the platform doesn't seem to have meaningful filters that govern what people can upload. They say all uploads are scanned with ClamAV, which is an open-source antivirus engine\n\nKey Findings\n\nThe RAT uses a two-step infection chain that starts with a dropper and is followed by a malicious payload.\n\nThe Hugging Face online service is abused to host and distribute dangerous APKs.\n\nThe attackers use server-side polymorphism by producing new payloads roughly every 15 minutes.\n\nThe Trojan abuses Accessibility Services to obtain persistent visibility and control.\n\nAttackers use fake system and financial interfaces to steal credentials and lock screen information.\n\nA centralized command-and-control server (C2) coordinates payload delivery and data exfiltration.\n\nInitial infection: dropper distribution and deceptive update prompts\n\nThe infection chain begins when users download a malicious Android application called TrustBastion. In the most likely scenario, a user encounters an advertisement or similar prompt claiming the phone is infected and urging the installation of a security platform, often presented as free and packed with \"useful\" features.\n\nWhen its website was online (trustbastion[.]com), it promised to detect scam and fraudulent SMSes, phishing, malware and much more.\n\nThe app is actually a dropper and contains no dangerous functionality at first glance.Once the user manually installs the app, the dropper immediately displays a prompt warning users that an update is required to continue using the application.\n\nThe visual elements resemble legitimate Google Play and Android system update dialogs, which increases the chances that victims will comply."
  },
  {
    "source": "IT News zu den Themen Künstliche Intelligenz, Roboter und Maschinelles Lernen - IT BOLTWISE® x Artificial Intelligence",
    "company": "Hugging Face",
    "title": "Cyberkriminelle nutzen KI-Plattformen zur Verbreitung von Android-Trojanern",
    "date": "2026-01-30T03:07:01Z",
    "url": "https://www.it-boltwise.de/cyberkriminelle-nutzen-ki-plattformen-zur-verbreitung-von-android-trojanern.html",
    "content": "LONDON (IT BOLTWISE) - Eine neue Cyber-Attacke zeigt, wie Cyberkriminelle die KI-Plattform Hugging Face missbrauchen, um den Android-Trojaner TrustBastion zu verbreiten. Diese raffinierte Kampagne zielt darauf ab, Finanzdaten zu stehlen, indem sie die Infrastruktur der Plattform nutzt, um schädliche Software zu verbreiten.\n\nIn einer alarmierenden Entwicklung haben Sicherheitsforscher eine Cyber-Attacke aufgedeckt, bei der die beliebte KI-Plattform Hugging Face zur Verbreitung des Android-Trojaners TrustBastion missbraucht wird. Diese raffinierte Kampagne zielt darauf ab, Finanzdaten zu stehlen, indem sie die Infrastruktur der Plattform nutzt, um schädliche Software zu verbreiten. Der Trojaner tarnt sich als Sicherheits-App und lockt Nutzer mit falschen Warnungen vor angeblichen Infektionen.\n\nDer Angriff erfolgt in zwei Stufen: Zunächst wird die App TrustBastion installiert, die sich als Sicherheitswerkzeug ausgibt. Nach der Installation fordert sie ein dringendes Update an, das den eigentlichen Trojaner enthält. Statt das Update direkt zu liefern, leitet die App auf ein Repository der Hugging-Face-Plattform weiter, von wo aus die schädliche APK-Datei nachgeladen wird. Diese Tarnung hinter legitimen Datenverkehr erschwert die Erkennung durch Sicherheitssoftware erheblich.\n\nEin zentrales Element der Tarnung ist der Missbrauch von Hugging Face. Die Plattform genießt hohes Vertrauen, und indem die Angreifer ihr Content Delivery Network nutzen, mischen sie ihre Malware unter Millionen legitime Datei-Downloads. Um eine Entdeckung zu verhindern, setzen die Kriminellen auf serverseitigen Polymorphismus, bei dem alle 15 Minuten eine leicht veränderte Variante der Schadsoftware generiert wird. Ein einziges Repository produzierte so in 29 Tagen über 6.000 einzigartige Versionen, was herkömmliche, signaturbasierte Virenscanner nahezu wirkungslos macht.\n\nEinmal installiert, entfaltet TrustBastion seine volle Schadwirkung als Remote Access Trojaner (RAT). Die Malware fordert aggressive Berechtigungen und kapert besonders die Android-Bedienungshilfen. Diese eigentlich für Barrierefreiheit gedachten Dienste nutzt der Trojaner für die Aufzeichnung des Bildschirminhalts, das Protokollieren aller Tastatureingaben und Overlay-Angriffe auf Banking-Apps wie Alipay oder WeChat. Bei einem Overlay-Angriff legt der Trojaner ein gefälschtes Login-Fenster über die echte App. Gibt der Nutzer dort seine Daten ein, landen sie direkt auf den Servern der Angreifer.\n\nDer Fall zeigt einen klaren Trend: Cyberkriminelle missbrauchen zunehmend vertrauenswürdige Cloud- und Entwicklerdienste. Hugging Face scannt Uploads zwar mit dem Open-Source-Tool ClamAV, doch das reichte hier nicht aus. Experten warnen seit längerem vor tausenden potenziell bösartigen Dateien auf der Plattform. Für Nutzer bleibt die wichtigste Regel: Apps nur aus dem offiziellen Google Play Store laden. Skepsis ist angebracht, wenn eine App sofort ein Update außerhalb des Stores fordert oder ungewöhnlich weitreichende Berechtigungen verlangt."
  },
  {
    "source": "DataBreachToday",
    "company": "Hugging Face",
    "title": "Breach Roundup: Android RAT Hides Behind Hugging Face",
    "date": "2026-01-29T22:22:47Z",
    "url": "https://www.databreachtoday.com/breach-roundup-android-rat-hides-behind-hugging-face-a-30628",
    "content": "Also, SmarterMail Flaw, Nike Breach Probe, Empire Market Co-Creator Pleads Guilty\n\nEvery week, ISMG rounds up cybersecurity incidents and breaches around the world. This week, researchers exposed an Android RAT abusing Hugging Face infrastructure. Attackers exploited a critical SmarterMail authentication bypass after reverse-engineering a patch. Automakers boosted cyber spending as supply chain risks linger. The U.S. Cybersecurity and Infrastructure Security Agency warned of active exploitation of a VMware vCenter flaw. Microsoft patched an Office security bypass under attack. An Empire Market co-creator pleaded guilty to U.S. federal drug charges. Nike probed a breach tied to a 1.4 terabyte data theft.\n\nSee Also: AI Arms Cybercriminals, and Defenders Must Match Pace\n\nCybercriminals are abusing Hugging Face's machine learning hosting infrastructure to deliver Android remote access Trojans, using trusted cloud services and aggressive polymorphism to sidestep mobile security controls, new research from Bitdefender found.\n\nResearchers at Bitdefender said Thursday the campaign centers on a two-stage infection chain that begins with a malicious dropper app masquerading as a mobile security tool called TrustBastion, likely downloaded after advertising telling victims that their devices have been compromised.\n\nTrustBastion is actually a dropper. Once installed it displays a fake system update alert styled to resemble legitimate Android and Google Play dialogs. Accepting the update triggers the second-stage payload delivery - not from an obvious malicious domain, but from Hugging Face-hosted repositories.\n\nThe attackers use an encrypted endpoint tied to to redirect victims to Hugging Face datasets hosting the final file. Bitdefender said the approach allows attackers to blend malicious traffic into legitimate cloud activity, reducing the likelihood of detection or blocking.\n\nResearchers observed a high volume of commits over a short period of time on the malicious Hugging Face repository. Hackers generated new payloads roughly every 15 minutes, uploading more than 6,000 unique Android files in less than a month. Each variant introduced minor changes to evade hash-based detection while preserving identical malicious behavior.\n\nOnce installed, the RAT abuses Android's accessibility service - just about every Android Trojan seems to do so - to gain persistent control and full visibility over device activity. It requests screen capture, overlay and recording permissions, enabling real-time surveillance and credential theft.\n\nThe malware displays fake login interfaces impersonating financial services such as Alipay and WeChat, harvesting credentials and lock screen information. Stolen data is exfiltrated to a centralized command-and-control server, which also handles payload rotation and configuration updates.\n\nAttackers are actively exploiting a critical authentication bypass vulnerability in SmarterTool's SmarterMail, using reverse-engineered patches to compromise exposed email servers shortly after a fix was released.\n\nThe flaw, numbered WT-2026-0001 by WatchTowr Labs and tracked as CVE-2026-23760, allows an unauthenticated attacker to reset the system administrator password without providing valid credentials. Successful exploitation gives full administrative access to the mail server.\n\nWatchTowr researchers said the issue stems from a logic flaw in SmarterMail's API endpoint. By submitting a crafted request and setting an parameter, an attacker can overwrite the administrator password without authentication checks being enforced.\n\nSmarterTools disclosed the vulnerability on Jan. 8 and released a patched version on Jan. 15. WatchTowr said it observed exploitation attempts within days of the patch release. In at least one confirmed case, attackers reset an administrator password shortly after the update became publicly available, suggesting they reverse-engineered the patch to identify the underlying flaw.\n\nOnce attackers gain administrative access, they can abuse legitimate SmarterMail features to execute operating system commands with system level privileges, resulting in full server compromise.\n\nSmarterMail is commonly used by managed service providers and small to midsized organizations as an on-premises mail server, making exposed instances attractive targets.\n\nThe automotive industry faces escalating cyberthreats as connected vehicles and artificial intelligence-driven systems create new vulnerabilities, according a proprietary assessment by debt rating agency Moody's.\n\nAutomakers are increasing cybersecurity investment and elevating security leadership. More than one-third spent over 10% of their technology budgets on cybersecurity, up from previous years. Nearly 80% have appointed CISO roles, with most reporting directly to senior management rather than through IT channels, Moody's research found. The firm received nearly 2,000 responses from global automotive sector executives, roughly 80% of who were CISOs.\n\nRecent incidents highlight the urgency. A June 2024 ransomware attack on CDK Global disrupted dealership operations across the United States. Jaguar Land Rover underwent a full systems shutdown in fall 2025 following a ransomware attack. Customer data breaches at Stellantis and Renault UK exposed weaknesses across automotive supply chains.\n\nThe survey found defensive maturity is uneven. While 92% of respondents said they carry standalone cyber insurance and conduct quarterly incident response testing, critical gaps persist. Only 64% use identity management service providers, compared with a global 80% average. Automotive suppliers lag behind manufacturers in reviewing vendor cybersecurity practices.\n\nSupply chain exposure is the sector's primary concern. Attackers increasingly bypass stronger corporate defenses by targeting smaller vendors with weaker controls.\n\nNew regulations are adding pressure. The EU's Cyber Resilience Act, whose main mandates take effect in 2027, requires rigorous security for connected vehicle components, with fines up to 15 million euros or 2.5% of global turnover for non-compliance.\n\nRansomware groups are increasingly eyeing software-defined vehicles for extortion, exploiting complex supply chains with multiple entry points like web apps, mobile integrations and constant cloud connectivity (see: Your New Car Could Be the Next Ransomware Target).\n\nA critical VMware vCenter flaw now under active exploitation allows remote attackers to take control of virtualized infrastructure without authentication, U.S. Cybersecurity and Infrastructure Agency said Friday.\n\nThe vulnerability, tracked as CVE-2024-37079, is a heap-overflow flaw in the distributed computing environment/remote procedure call implementation of VMware vCenter Server. An attacker with network access can send a specially crafted request to trigger unauthenticated remote code execution, potentially leading to full system compromise.\n\nBroadcom disclosed and patched the vulnerability in June 2024 as part of a security advisory that addressed multiple flaws in vCenter.\n\nCISA added the bug to its Known Exploited Vulnerabilities Catalog.\n\nMicrosoft released Monday an emergency security patch to address a high-severity security feature bypass vulnerability affecting Microsoft Office and Microsoft 365 that is being exploited in the wild.\n\nThe flaw, tracked as CVE-2026-21509 with a CVSS score of 7.8, stems from Office's reliance on untrusted inputs in a security decision, allowing an unauthenticated attacker to bypass key protections designed to block unsafe Component Object Model, Object Linking and Embedding controls.\n\nExploitation requires only social engineering. An attacker can trick a user into opening a crafted Office document, making everyday channels like phishing a likely delivery vector.\n\nA co-creator of the now-defunct darkweb marketplace pleaded guilty to federal drug conspiracy charges in Chicago, the U.S. Department of Justice announced Tuesday.\n\nRaheim Hamilton, also known online as \"Sydney\" and \"ZeroAngel,\" admitted he co-founded and co-administered Empire Market, a darkweb marketplace that enabled the sale of drugs, stolen data, counterfeit currency and hacking tools, according to his plea agreement. Under the deal, he must forfeit 1,230 bitcoin, worth about $103 million and 24.4 ether coins, or $68,000, as well as three residential properties in Virginia.\n\nProsecutors said Empire Market operated from February 2018 through August 2020, during which it processed more than 4 million transactions with total sales exceeding $432 million. Drug sales accounted for the majority of activity, generating approximately $374 million.\n\nHamilton ran the marketplace alongside Thomas Pavey, known online as \"Dopenugget.\" Pavey pleaded guilty last year to related federal charges and is still awaiting sentencing.\n\nThe two jointly designed, owned and operated Empire Market, overseeing site development, vendor onboarding, dispute resolution and moderation. The pair modeled the platform on the earlier darkweb marketplace AlphaBay, which Pavey and Hamilton used as a blueprint for Empire Market's structure and features.\n\nHamilton pleaded guilty to conspiring to distribute large quantities of heroin, cocaine and methamphetamine, including shipments sent into the United States from overseas vendors. The platform hosted more than 166,000 drug listings and facilitated nearly 2.8 million drug transactions, investigators said.\n\nAuthorities traced cryptocurrency flows linked to the marketplace, identifying wallets tied to marketplace administrators and vendors that facilitated laundering of proceeds through mixers and tumblers.\n\nSportswear brand Nike is investigating its network after threat actors claimed to have exfiltrated 1.4 terabytes of internal data - about 190,000 unique files.\n\nWorldLeaks, likely a rebrand of the Hunters International ransomware operation, said it stole 188,347 files from Nike's systems, publishing design and manufacturing workflows on its darkweb leak site.\n\nAccording to Nike, sample data also included garment measurements for products, details on materials, item retail prices, product lifecycles, clothing testing reports, factory audits, corporate slides and other materials.\n\n\"We always take consumer privacy and data security very seriously. We are investigating a potential cybersecurity incident and are actively assessing the situation,\" Nike said.\n\nNike has not said whether WorldLeaks posed a ransom demand."
  },
  {
    "source": "Springer",
    "company": "Hugging Face",
    "title": "Reducing labeled data requirements in text classification with active learning and BERT-based transformers - Neural Computing and Applications",
    "date": "2026-01-29T15:00:44Z",
    "url": "https://link.springer.com/article/10.1007/s00521-025-11756-8",
    "content": "Although the research fields of AL and transformer-based models are both extensive, there are relatively few studies that combine these two approaches specifically for text classification tasks. The main focus in the existing works that do exploit transformer-based models in AL scenarios, is often to improve model performance by developing new sampling strategies or fine-tuning techniques. In contrast, our study adopts a different perspective on the issue: we focus in the investigation of whether comparable results-as reflected in standard evaluation metrics-can be maintained while significantly reducing the amount of labeled data required, rather than aiming to boost model effectiveness. For this purpose, we designed a two-phase framework: first establishing baseline model performance using the full training set without AL techniques, and then applying AL to determine whether substantial data reduction can be achieved without a notable loss in performance. The presented approach, which emphasizes on data efficiency rather than performance enhancement, is the main difference of our work from prior studies and addresses an important gap in the current literature. Moreover, while - as already mentioned - previous studies integrating AL with transformers have primarily emphasized maximizing predictive performance through specialized sampling heuristics or fine-tuning schemes, few have explicitly quantified the potential reduction in labeled data achievable without compromising accuracy. Our framework directly targets this aspect by systematically measuring the trade-off between annotation effort and model performance across multiple BERT-based variants, thereby offering a complementary perspective to existing AL-transformer research. To further support this comparison, we incorporated the Efficiency-Adjusted Performance Index (EAPI) as a unified meta-metric that quantifies model performance relative to the proportion of labeled data used, enabling a more objective evaluation of data efficiency across different models and sampling strategies. Achieving comparable performance with significantly less labeled data is particularly valuable in scenarios where data annotation is expensive or scarce, as it maximizes the utility of available resources while maintaining-or even enhancing-model effectiveness.\n\nIn this section, the experimental procedure is described to provide a clear understanding of how the experiments were designed and conducted. It is important to emphasize that the procedure was specifically tailored to address the core question of this study: whether AL can achieve comparable results to traditional training scheme that uses the full training set, in terms of evaluation metrics. This means that the ideal scenario is to achieve similar metric scores to those obtained using the full training set, with only a small reduction in data-potentially as little as 1%. While the study also explores some secondary questions, the primary focus of the experimental design was to gain insights into this central inquiry.\n\n3.1 Experimental procedure\n\nBefore providing details about the specific methods employed during the experimental phase, and presenting the datasets utilized, it is essential to provide information about the workflow of the conducted experiments.\n\nTo create a meaningful baseline for evaluating the effectiveness of active learning, each dataset was initially tested in the scenario where the whole dataset is available and fully labeled, ie. i.e., a standard supervised classification task. This step allowed us to compute benchmark performance scores-using the metrics described in evaluation section-under the assumption that all training data is labeled. These scores serve as reference points for investigating how closely an active learning approach can approximate the trivial scenario where no active learning techniques are utilized, using a significantly smaller set of labeled instances. For this step, each dataset was split into training and test set, using an 80%/20% ratio. The training set was used to train the models, while the test set was used for evaluation, as is typical in supervised learning tasks.\n\nFollowing the initial benchmark step, the active learning scenario takes place. Once again, we use an 80%/20% split for training and test sets. In this scenario, we simulate a realistic labeling process by initially treating the entire training set as unlabeled, by hiding the existing labels. From this unlabeled pool, a small random subset of the training data is selected and provided with their true labels, simulating an oracle providing annotations. This small labeled subset is used to fine-tune each model, and then the performance is evaluated on the test set. Although the initial labeled pool is selected randomly, we acknowledge that this approach may introduce a risk of unintentional sampling bias, particularly in the case of imbalanced datasets. However, this choice aligns with realistic AL scenarios, where the labeling process typically begins with a small, minimally annotated dataset. Furthermore, the use of an uncertainty sampling strategy in subsequent iterations helps to progressively mitigate any initial imbalance, by prioritizing the selection of instances where the model exhibits the highest uncertainty, which often correspond to underrepresented or more challenging examples.\n\nAfter the previously described step to initialize the active learning framework, the iterative active learning procedure begins. In each iteration, an additional portion of the remaining unlabeled pool of training data is selected using uncertainty sampling. This portion is equal to the 1% of the training set. These instances are labeled and added to the existing labeled set and then the model is fine-tuned incrementally at each step using the progressively expanding labeled dataset. At this point, we should highlight that at each iteration of the AL process, the model continues training from its previously fine-tuned state using the progressively expanded labeled pool, rather than being reinitialized from its pretrained weights, ensuring stable knowledge retention and more efficient convergence across iterations. After each iteration, the evaluation metrics are calculated and recorded. This process continues until 50% of the original training set has been labeled.\n\nAt this point, it is important to explain why the labeling process should be terminated at 50% of the initial dataset and moreover justify the selection of a 1% increment at each iteration. The main goal of this study is to significantly reduce labeling effort while maintaining competitive model performance, which leads to the decision to cap the labeling at 50%. Employing larger proportions would diminish the relevance of our investigation into minimizing the dependence on labeled data without compromising accuracy. With respect to the 1% increment, this value was selected to achieve an appropriate balance between computational efficiency and the capacity to capture fine-grained variations in model performance. While larger steps could mask important dynamics in the learning process, smaller increments (e.g., per instance) would provide very little additional insight.\n\nConcering the query strategy, in this study, the AL framework relies on an uncertainty-driven query strategy [21], Settles [1] in order to determine which instances from the pool of unlabeled instances, are most valuable for annotation. Using this method, the model ranks the unlabeled samples in the pool according to prediction confidence after estimating their class probabilities. Labeling priority is given to instances for which the model is least certain, i.e., those with the lowest maximum class probability. By focusing the annotation process on examples that are anticipated to yield the most information, this mechanism helps the model improve the quality of its decision boundaries with each iteration. Especially for text classification tasks, uncertainty sampling has long been considered a fundamental and highly interpretable AL technique. Recent studies have demonstrated that this idea still holds true for contemporary transformer-based architectures, such as BERT and its variations [14]. For this reason, the least-confidence variant of uncertainty sampling was adopted here, providing a transparent and consistent baseline for assessing data efficiency and performance stability across multiple transformer models.\n\nTo ensure reproducibility and account for stochastic variability inherent in the AL process, each experiment was conducted over three independent trials using different random seeds. The use of distinct seeds influences factors such as the random initialization of model parameters and the selection of the initial labeled pool, both of which can substantially affect early model performance and learning dynamics. Then, the results from the three independent trials were averaged to provide a more reliable and stable estimate of model behavior, mitigating the impact of random fluctuations and offering a fairer basis for comparison among AL strategies and model variants.\n\nRegarding implementations and hyperparameters, all experiments in this study were implemented using the Hugging Face transformers and datasets libraries. The training setup was standardized across all experiments, with key hyperparameters specified as follows: the learning rate was set to the default value from the Hugging Face trainer; the batch size was 8 for both training and evaluation; each active learning iteration ran for a single epoch; warmup steps were set to 500, and weight decay was applied at a rate of 0.01. We employed gradient accumulation steps of one step and enabled mixed-precision (FP16) training to optimize computational efficiency. Experiments were run on a machine equipped with an 11GB NVIDIA GeForce RTX 2080 Ti, utilizing CUDA acceleration to speed up training. All models used in the study are presented in the tables of the manuscript and are also available for download on the Hugging Face model hub.\n\nIt is essential to highlight once again that this experimental procedure is designed solely for research purposes. In real-life scenarios, it would be unrealistic to use this approach, as the complete information about metric scores from the entire training set would not be available. However, this strategy allows us to investigate how well the models perform relative to their best possible performance, providing valuable insights into the potential of Active Learning techniques.\n\n3.2 Datasets\n\nDuring the experimental procedure, ten datasets obtained from Hugging Face were employed. This selection was made to ensure that the datasets are publicly accessible, thereby enhancing the reproducibility of our results. The datasets were chosen to be diverse in terms of content, number of instances, and the number of classes they represent. Specifically, half of the datasets are designed for binary classification, while the remaining datasets include data with class labels ranging from three to six. The dataset with the fewest instances contains fewer than 1,000 examples, whereas the largest dataset comprises approximately 67,000 instances. This diversity is crucial to guarantee that the experimental outcomes are not biased by any specific characteristics of the datasets. Further details on the nature of each dataset are provided below:\n\n* poem_sentiment [22]: derived from Project Gutenberg, this dataset contains poem verses labeled with four sentiments: positive, negative, mixed, and neutral, supporting sentiment classification in poetry.\n\n* tweet_eval__emotion [23]: a TweetEval subset for emotion analysis, classifying tweets into positive, negative, and neutral categories to assess social media sentiment.\n\n* rotten_tomatoes [24]: also known as the MR dataset, this balanced collection of movie reviews from Cornell University (positive/negative) is widely used for binary sentiment classification.\n\n* tweet_eval__hate [25]: a TweetEval subset for hate speech identification, distinguishing between hateful and non-hateful content to support studies on harmful language detection.\n\n* tweet_sentiment_airlines [26]: consists of airline-related tweets annotated as positive, negative, or neutral, providing data for sentiment and customer feedback analysis in the airline domain.\n\n* emotion(dair-ai) [27]: English tweets labeled with six basic emotions (e.g., joy, anger, sadness), serving as a benchmark for multi-class emotion classification.\n\n* ade_corpus_v2 [28]: a biomedical dataset for identifying Adverse Drug Events (ADEs) and extracting relations between drugs, effects, and dosages.\n\n* Imdb [29]: a standard benchmark for binary sentiment classification, containing positive and negative movie reviews.\n\n* nyu_mll_glue_sst2 [30]: part of the GLUE benchmark, derived from the Stanford Sentiment Treebank, containing sentence-level movie reviews labeled as positive or negative.\n\n* tweet_eval__sentiment [31]: a TweetEval subset for sentiment classification, assigning tweets to positive, negative, or neutral categories.\n\nA summary of the datasets' key characteristics, including data volume and number of classes, is presented in Table 1.\n\n3.3 BERT-based transformers\n\nTransformers, first introduced by Vaswani et al. [4] are sequence transduction models that rely entirely on attention mechanisms, replacing the recurrent layers commonly used in encoder-decoder architectures with multi-headed self-attention. This architecture makes them particularly well-suited for handling sequential data, which has led to their widespread adoption in the NLP field.\n\nTransformer-based ML models, such as BERT, are pre-trained on large datasets and can be fine-tuned for specific tasks, facilitating the concept of transfer learning in NLP. This approach allows models to achieve significant performance gains even with limited task-specific data. Transformers have consistently outperformed previous state-of-the-art models on various NLP tasks, including language translation, sentiment analysis, and text generation. The attention mechanism enables transformers to manage long-range dependencies in data more effectively than previous models like recurrent neural networks (RNNs) or long short-term memory networks (LSTMs) [32]. Additionally, unlike RNNs and LSTMs, which process data sequentially, transformers process all elements of an input sequence simultaneously. This parallelization significantly reduces training time, especially on large datasets, making transformers highly efficient and scalable.\n\nIn this study, seven variations of BERT [33] were utilized, all sourced from the Hugging Face repository. BERT, which stands for Bidirectional Encoder Representations from Transformers, is a language representation model that pre-trains deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. It is pre-trained on a large corpus of English text using two unsupervised tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). MLM involves randomly masking words in a sentence and training the model to predict these masked words, enabling it to learn bidirectional context. NSP involves predicting whether a given sentence pair is sequentially coherent. BERT can be fine-tuned with just one additional output layer for a variety of downstream tasks, achieving state-of-the-art performance in many NLP applications, such as question answering, language inference, and sentiment analysis. Following, brief information about BERT variants that used in our study are provided:\n\n* bert-base-model-cased [33]: an implementation of the original bert model. This implamatation of BERT is case sensitive, meaning it keeps the distinction between uppercase and lowercase letters during its training and inference processes. This can be particularly useful in contexts where the meaning of words can change with capitalization, such as proper nouns and the beginning of sentences.\n\n* bert-base-model-uncased [33]: an uncased variant of the previous model, which means that this variant does not distinguish between uppercase and lowercase letters. In an uncased model, all text is converted to lowercase before being processed by the model, so \"Cat\" and \"cat\" would be treated as the same token.\n\n* distilbert-base-cased [34]: DistilBERT, is a smaller, faster, and cheaper version of BERT base model designed for efficient on-device computation. By reducing the original BERT model's size by 40% and increasing inference speed by 60%, DistilBERT retains 97% of BERT's language understanding performance. It achieves this through a pre-training process that combines masked language modeling, distillation loss, and cosine embedding loss. DistilBERT a highly effective model for scenarios with constrained computational resources.\n\n* distilbert-base-uncased [34]: an uncased variant of the previous model.\n\n* bert-base-multilingual-uncased-sentiment [35]: this BERT-based transformer is fine-tuned for sentiment analysis on product reviews across six languages: English, Dutch, German, French, Spanish, and Italian. It predicts the sentiment of reviews as a star rating from 1 to 5. Trained on substantial datasets for each language, the model achieves an exact accuracy of up to 67% for English and around 57-61% for other languages, with an off-by-1 accuracy exceeding 93% across all languages. This model is suitable for multilingual sentiment analysis tasks.\n\n* distilbert-base-multilingual-cased-sentiments-student [36]: this model is a distilled version of the multilingual BERT, fine-tuned for sentiment analysis across multiple languages and is designed for efficient and accurate sentiment classification. It has 6 layers, 768 hidden dimensions, and 12 attention heads, totaling 134 million parameters. Fine-tuned on a multilingual dataset covering 12 languages, it offers robust performance for sentiment analysis tasks in diverse linguistic contexts.\n\n* bertweet-base-sentiment-analysis [37]: this model is a transformer-based model fine-tuned for sentiment analysis on English tweets. Built on the BERTweet model, which is a RoBERTa variant trained on extensive Twitter data, this model uses the SemEval 2017 corpus ( 40,000 tweets) for training. It categorizes sentiments into Positive , Negative , and Neutral labels and for this reason is ideal for social media monitoring and customer feedback analysis.\n\n3.4 Evaluation metrics\n\nDuring the evaluation stage, three traditional metrics commonly used in classification tasks-accuracy, F1 score, and Matthews Correlation Coefficient (MCC) -- were employed. These metrics provide a comprehensive assessment of model performance. However, since the core objective of this work is to investigate whether the use of AL can reduce the training set size in text classification scenarios without compromising model efficiency, a more specialized index was required to gain deeper insights into the results. For this purpose, we utilized the Efficiency-Adjusted Performance Index, designed specifically to evaluate the trade-off between dataset size and model performance in Active Learning settings. This index offers a better understanding of the efficiency gains achieved through AL during the extended experimental procedure.\n\n3.4.1 Accuracy\n\nAccuracy is one of the most common evaluation metrics used in classification problems, measuring the proportion of correctly predicted instances (both true positives and true negatives) among the total instances, as can be seen in (1) [38]:\n\nor considering that the number of correct classified examples is the sum of true positive and true negative:\n\nAccuracy is a straightforward and easily interpretable metric, ranging from 0 to 1, where higher values indicate better model performance. However, in the context of highly imbalanced datasets, accuracy can be misleading. This occurs because a high accuracy might still reflect poor model performance if the model predominantly predicts the majority class, thus neglecting the minority class. In such situations, despite achieving a high accuracy, the model may fail to accurately classify instances of the minority class, making the accuracy metric less informative regarding the model's overall effectiveness.\n\n3.4.2 F1 score\n\nThe F1 Score is the harmonic mean of Precision and Recall [39]. Precision measures the proportion of true positive predictions among all positive predictions, while Recall measures the proportion of true positives among all actual positive cases. F1 Score integrates precision and recall into a single metric to provide a better insight of model performance. F1 Score formula is the following:\n\nThe F1 Score provides a balance between Precision and Recall, making it useful when you want to find a balance between false positives and false negatives. It's particularly valuable in imbalanced datasets where focusing on the minority class's performance is crucial. F1 scores ranges betwen 0 and 1, with values from 0.8 to 1 indicating very good performance, while values from 0 to 0.2 that the model is performing only slightly better than random guessing or may be completely ineffective in identifying positive cases. Finally, should be noted that F1 Score does not take into account true negatives, which might be important in some problems.\n\n3.4.3 Matthews Correlation Coefficient -- MCC\n\nThe Matthews Correlation Coefficient (MCC) [40] is a classification metric that ranges from \\(-1\\) to 1, where 1 indicates perfect prediction, 0 represents random guessing, and \\(-1\\) signifies total disagreement between predictions and actual outcomes -- the worst case scenario, where all the results are inverted. It is defined by the formula:\n\nUnlike simpler metrics such as accuracy or F1 score, MCC takes into consideration all four elements of the confusion matrix: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). Because of this, it provides a more comprehensive and reliable evaluation, especially in scenarios with class imbalance [41], as it returns good results only if all the four aforementioned values are actually good. Although its formula is more mathematically involved, modern computing makes its calculation straightforward. The benefits of using MCC on binary problems have been justified , but it can also be generalized to handle multiclass classification [42].\n\n3.4.4 Efficiency-adjusted performance index -- EAPI\n\nThe Efficiency-Adjusted Performance Index (EAPI) [43] is a novel evaluation meta metric that we proposed in our previous work to address the specific goal of minimizing training data requirements in machine learning models through AL techniques. Conventional classification metrics evaluate model accuracy or performance but overlook the efficiency benefits gained from using less data. EAPI addresses this limitation by integrating both performance improvement and data utilization efficiency into a single comprehensive measure.\n\nThe metric is defined as follows:\n\nwhich can be simplified as:\n\nThe first term quantifies the relative improvement in model performance, where values greater than 1 denote positive gains. The second term introduces a penalty proportional to the size of the training subset used, thereby rewarding methods that achieve similar or improved results with less data. Combined, these components produce an index that balances model accuracy and data efficiency. A higher EAPI indicates that an AL strategy achieves strong performance with minimal data, while lower values suggest limited efficiency gains or overreliance on larger datasets.\n\nTheoretically, EAPI values may range from negative to positive infinity. In the context of this study, where AL experiments employ between 1 and 5% of the training data, the first component generally remains close to 1, while the second varies between 0.5 and 0.99. Consequently, the resulting EAPI scores are expected to fall within the range of 0 to 1."
  },
  {
    "source": "华商网",
    "company": "Hugging Face",
    "title": "中国开源大模型下载量全球第一 AI应用出海的新机遇在哪",
    "date": "2026-01-23T03:04:46Z",
    "url": "https://news.hsw.cn/system/2026/0123/1903664.shtml",
    "content": "在近日举行的瑞士达沃斯世界经济论坛上，中国企业在AI应用上的全球影响力获得国际权威机构认可。\n\n世界经济论坛在对来自30多个国家和地区、覆盖医疗、能源、基础设施等20多个行业的数百个案例进行分析后，评选出新一批\"AI应用之星\"案例。在入选案例中，有近一半来自中国企业，包括宁德时代、蚂蚁集团、中国工商银行等，涵盖电池制造、卫生医疗、金融服务等垂类AI应用。\n\n澎湃新闻美数课工作室发现，去年1月DeepSeek R1发布以来，中国开源大模型凭借\"高智能，低成本\"的特性迅速打开全球市场，Token使用量全球占比大幅上涨，在开源平台Hugging Face的开源模型下载量跃居全球首位。与此同时，在部分垂类AI应用赛道，中国企业的出海AI应用月活已经超过其他国家和地区的应用。\n\n中国开源大模型下载量全球第一，用量暴涨421%\n\n过去一年，中国开源模型成为全球人工智能领域的新兴力量。据新华网报道，凭借在技术创新、落地应用、生态搭建等多维度突破，中国大模型正跑步进入全球第一梯队。\n\n美国麻省理工学院与开源平台Hugging Face（抱抱脸）的联合报告显示，过去一年，中国研发的开源模型全球下载量占比达到17.1%，反超美国的15.8%，位居全球第一。\n\n在新建模型（<1年）中，中国模型的下载量已超过包括美国在内的任何其他国家。图片来源：Hugging Face\n\n全球最大AI开源社区Hugging Face发表的《\"DeepSeek时刻\"一周年》分析文章指出， DeepSeek R1发布后见证了中国AI格局形成新态势：大型科技公司领跑，初创公司紧随其后，垂直行业的公司也越来越多地进入该领域。中国开源模型生态呈现出三个特点：开源不再是短期战术，而是长期竞争战略的一部分；发布最先进模型和代码库的具有竞争力的中国组织数量激增；发布变得更强且更频繁，高性能模型每周都在发布。\n\n2025年2月至7月期间，中国公司的开源发布明显更加活跃。主要变化是百度和月之暗面从闭源路径转向开源发布，智谱、阿里巴巴从发布模型权重扩展到构建工程系统和生态系统接口。这种战略变化成效显著，在Hugging Face平台的新建模型（<1年）中，中国模型的下载量已超过包括美国在内的任何其他国家。\n\n除此之外，全球头部大模型API聚合平台OpenRouter的数据显示，过去一年，中国大模型在全球市场的Token消耗占比增长了421%。Token是大模型计算开销的量化单位，更能体现大模型的实际使用情况。不同于C端用户日常与AI聊天产生的Token消耗，API形式的Token消耗体现的是MaaS（Model As Service）场景的使用情况，即模型被集成到工具、产品中，以服务形式对外交付。\n\n下载量和使用量大幅增长的一大原因是中国大模型\"高智能、低成本\"的市场优势。Artificial Analysis数据显示，按照每单位美元的大模型智力（intelligence）计算，DeepSeek、MiniMax等中国开源模型的性价比显著优于Gemini、Claude、ChatGPT等模型。\n\n以DeepSeek、MiniMax为代表的中国开源大模型，每单位美元的智能化水平更高，比国外模型更有性价比。图片来源：Artificial Analysis\n\n对于企业级AI应用来说，并非大模型越智能就一定越好，使用成本同样是关键考量因素。中国大模型的高性价比优势，为企业AI应用落地奠定了良好基础，也显著提升了中国AI应用出海的核心竞争力。\n\n中国AI应用出海在哪些赛道更有机会\n\nSensorTower发布的《2025年中国应用出海报告》（下简称报告）指出，AI目前已成为移动生态的\"标配\"。2025年上半年，包含AI功能的应用下载量达到75亿次，约占全球安装量的10%，年同比增长52%。\n\n在AI应用领域，中国开发者是全球最有竞争力的玩家之一。去年8月由硅谷风投机构a16z评选的全球AI产品50强中，中国开发的AI应用总体占比近三分之一。报告指出，中国AI应用之所以能取得如此长足的进步，不仅源于国内海量数据集训练模型，更得益于针对全球受众的用户界面和内容的本地化。\n\nXsignal《全球AI应用行业年度报告》数据显示，中国出海AI应用在部分赛道取得了全球领先的优势，但距离全面超越全球头部应用仍有不小的距离。\n\n字节跳动旗下CapCut（剪映海外版）和Gauth（豆包爱学海外版），已分别成为AI视频编辑、AI教育学习赛道全球月活第一应用。在视频创作领域，爱诗科技的PixVerse（拍我AI海外版）与OpenAI旗下Sora的月活差距已经缩小到100万以内。但在虚拟角色、图像生成、设计工具及竞争最激烈的AI聊天机器人赛道，中国出海AI应用与全球头部应用仍存在千万甚至过亿级别的月活差距。\n\n近年来，中国企业AI应用出海经历了从技术探索到市场拓展，从单一应用到生态创新的过程。InfoQ发布的《AI应用出海指南》（下简称指南）显示，2024年至今，中国AI出海应用进入全面拓展期，在出海策略上呈现出几方面变化：一是中国AI出海企业更加注重本地化和定制化，针对海外市场、文化和政策法规，对AI应用进行了优化，取得了良好的市场反响；二是除大厂和传统科技企业外，越来越多的中小企业、创业团队以及超级个体参与到AI应用出海中，形成了多元化的出海主体格局。三是涌现出一些创新的商业模式和应用模式，如AI驱动的销售触达工具、AI与SaaS结合的服务等。\n\n全球数据合规差异成为AI应用出海一大挑战\n\n全球AI技术不断跃进的同时，欧盟、美国、日本、新加坡等地区陆续出台了数据合规相关法规，监管要求进一步收紧。各国监管机构在个人隐私数据保护、明确企业责任等核心问题上达成了一定的共识，但在具体的适用范畴、合规要求和处罚力度上仍存在明显差异。\n\n指南显示，对于出海企业而言，这既意味着需承担多法域合规叠加产生的技术改造成本。从不同国家的监管重点来看，欧盟更看重风险分级管理，东南亚更看重数据本地化与内容审查，北美更看重出口管制与知识产权保护。对比来看，欧盟和北美的合规成本要高于东南亚地区。\n\n北源律师事务所在发表的《中国出海企业域外数据合规执法案例解析与合规启示》中，梳理了2020年以来中国企业出海在海外市场因个人信息合规、数据安全问题面临的监管挑战和执法案例。案例覆盖欧盟、英国、美国和韩国等4个国家和地区，其中欧盟对中国企业的执法案例数量最多。\n\n撰文律师对执法案例的观察分析指出，欧盟的执法重点主要集中在儿童隐私保护、数据收集处理的最小必要原则的遵守以及数据跨境传输等方面。美国政府重点关注儿童隐私和健康等高敏感数据。亚洲隐私保护相关立法的共同趋势是罚款金额显著上升，且多以企业营业额的一定比例计算。近年来，日韩与新加坡在个人数据保护的执法上紧跟欧美步伐，着手对非本土的外国企业开展执法。\n\n据新华网报道，1月8日商务部新闻发言人何亚东在回应有关审查Meta收购人工智能平台Manus的提问时说，中国政府一贯支持企业依法依规开展互利共赢的跨国经营与国际技术合作。需要说明的是，企业从事对外投资、技术出口、数据出境、跨境并购等活动，须符合中国法律法规，履行法定程序。商务部将会同相关部门对此项收购与出口管制、技术进出口、对外投资等相关法律法规的一致性开展评估调查。\n\n企业在做出海合规时，不能只关注境外的法律法规，更应重点关注是否符合中国出口管制合规要求。锦天城律师事务所合伙人邱梦赟在接受《21世纪经济报道》记者采访时表示，\"很多企业平时在合规工作中容易忽略这一点，认为自己的产品、技术不在清单目录里，出口就不会有问题。其实不是的，要看接收方是谁、用于什么用途，要从中国出口管制合规角度出发来评估合规性问题。\"\n\n德禾翰通律师事务所张晓欣在分析文章中，以合规律师的视角给出了四条出海\"铁律\"：一是合规先行，而非事后补票。技术出口、数据出境、投融资备案的合规成本，应作为核心商业成本计入模型。二是架构设计，在设计红筹架构时，必须充分考虑境内法律对\"实质境内企业\"的认定标准，为未来的融资、并购预留合规接口和应对方案。三是厘清数据资产的边界与路径，企业需建立清晰的数据资产地图，区分境内存储与出境数据。四是对应地缘风险准备预案管理，为可能出现的不同情境（如技术检查、收购审查）制定备选方案，避免在危机来临时陷入被动和仓促决策。\n\n参考资料：\n\n1、21财经,《Manus被审查，为AI 初创公司照见哪些合规考题？》,https://m.21jingji.com/article/20260108/herald/c141d33622ba04e3d2d6177867dde0e7.html\n\n2、安全内参,《中国出海企业域外数据合规执法案例解析与合规启示》,https://www.secrss.com/articles/77554\n\n3、德禾翰通律师事务所,《Manus海上历险记 -- -- 从合规律师视角看科技企业出海的短趋势》,https://www.dehehantong.com/news/articles/358\n\n相关热词搜索： 新机遇"
  },
  {
    "source": "k.sina.com.cn",
    "company": "Hugging Face",
    "title": "UnifoLM-VLA-0:宇树科技开源的通用人形机器人VLA大模型，实现空间语义增强与多任务操作泛化 | AI铺子",
    "date": "2026-02-15T06:00:22Z",
    "url": "https://k.sina.com.cn/article_7857201856_1d45362c001902eqog.html",
    "content": "UnifoLM-VLA-0是宇树科技UnifoLM模型家族中，面向通用人形机器人操作的视觉-语言-动作（Vision-Language-Action，VLA）大模型，也是当前具身智能领域聚焦物理交互的核心开源框架。该模型打破了传统视觉-语言模型（VLM）仅能完成图文理解、无法落地物理操作的局限，通过在专业机器人操作数据集上开展持续预训练，完成了从普通图文理解模型到具备物理常识、可直接驱动机器人执行动作的\"具身大脑\"的转型。\n\n模型以Qwen2.5-VL-7B为基础主干网络，先迭代构建出UnifoLM-VLM-Base视觉语言基础模型，再集成专用动作预测头，最终形成可直接接收图像、文本指令并输出机器人可执行动作序列的UnifoLM-VLA-0。它的核心定位是为人形机器人提供统一的感知-理解-执行链路，用单一模型策略覆盖多类复杂操作任务，降低具身智能研发的技术门槛与数据成本，让人形机器人从单一功能设备向通用操作设备迈进。\n\n二、核心功能特色\n\nUnifoLM-VLA-0围绕人形机器人实际操作需求设计核心能力，两大核心特性构成模型的核心竞争力，同时配套轻量化使用、高数据效率等实用功能，具体如下：\n\n1. 空间语义深度增强\n\n针对机器人操作中指令理解、空间定位、几何推理的高要求，模型通过持续预训练，将文本自然语言指令与2D图像、3D空间几何信息深度融合，大幅提升空间感知与物体几何理解能力。对比基础VLM模型，其空间感知核心指标提升47%，可在零样本场景下完成目标位置推理、运动轨迹生成、可抓取点判断、物体检测分割与精准定位，满足精细操作的空间认知需求。\n\n2. 多任务通用操作泛化\n\n模型基于全链路动力学预测数据训练，具备极强的跨任务泛化能力。在宇树G1人形机器人真机验证中，仅用单一策略模型即可高质量完成12类复杂物理操作任务，涵盖叠积木、插袋子、擦黑板、清洁桌面、折叠毛巾、双机协作整理等日常与工业高频场景。同时在LIBERO仿真基准测试中，多任务处理性能接近理论最优值，无思考模式下的操作精度可对标Gemini-Robotics-ER 1.5国际主流模型。\n\n3. 高数据效率与鲁棒性\n\n模型训练仅需约340小时真机操作数据，特定任务仅需80组演示数据即可快速适配，数据使用效率远高于同类开源VLA模型。面对外部环境扰动时，执行鲁棒性较传统方案提升3.2倍，具备工业级抗干扰能力，可适应非标准化、有动态干扰的真实场景。\n\n4. 全流程开源与易用适配\n\n项目完整开源训练代码、推理代码、三类预训练权重与12组专用数据集，兼容Hugging Face LeRobot标准数据格式，支持自定义数据集快速转换，可无缝对接仿真环境与宇树G1真机部署，降低二次开发与移植成本。\n\n模型采用\"基础VLM预训练→空间语义增强微调→动作头集成→动力学数据训练\"的四层架构。首先以Qwen2.5-VL-7B为基座，结合通用图文VQA数据与开源机器人数据训练得到UnifoLM-VLM-Base；随后融入2D/3D空间检测、轨迹预测、任务分解数据，强化空间语义能力；最后集成专用动作预测头，接入全链路动力学预测数据，完成从语义理解到动作输出的闭环。\n\n2. 数据体系与格式规范\n\n项目官方开源12组适配Unitree G1机器人的专用数据集，覆盖物体操作、整理收纳、精细装配、双机协作等场景，数据包含视觉帧、本体姿态、动作标签、空间标注等全维度信息。自定义数据集需遵循Hugging Face LeRobot V2.1格式，项目提供LeRobot转HDF5、HDF5转RLDS的自动化脚本，适配模型训练的数据输入要求。\n\n3. 核心依赖与环境技术栈\n\n项目基于CUDA 12.4构建，Python版本限定为3.10.18，依赖Hugging Face LeRobot框架与FlashAttention2加速模块，采用分布式训练架构，支持多GPU并行加速。动作输出支持长时序块预测，可配置动作维度、自感知维度与数据归一化方式，适配不同机器人本体的执行接口。\n\n4. 推理部署架构\n\n采用服务端-客户端分离部署架构：服务端加载模型权重完成动作推理，客户端采集机器人视觉与本体数据并上传，通过SSH隧道完成通信，既保证模型推理的算力需求，又适配机器人端的轻量化部署，兼容仿真环境与真机硬件。\n\n适配家庭场景的高频操作任务，包括物品抓取放置、桌面清洁、毛巾折叠、文具收纳、水果整理、瓶盖拧动等，支持自然语言指令控制，满足家庭服务机器人的通用操作需求。\n\n2. 工业辅助与人机协作\n\n可用于工业场景的轻型装配、零件整理、桌面清洁、双机协同作业，在有外部扰动的产线环境中稳定执行精细操作，替代重复性人工工作，提升产线柔性。\n\n3. 具身智能算法研发\n\n作为通用VLA基座模型，供高校、科研机构与企业开展具身智能、机器人学习、空间语义理解、长时序动作规划等方向的算法研究，降低从零构建模型的研发成本。\n\n4. 仿真与真机验证平台\n\n兼容LIBERO专业仿真环境与Unitree G1真机，可用于机器人操作算法的快速验证、性能基准测试、泛化能力评估，搭建从仿真到真机的全流程验证链路。\n\n5. 二次开发与定制化适配\n\n支持开发者基于模型权重微调自有场景数据，适配特种机器人、服务机器人、教育机器人等不同硬件平台，快速打造垂直领域的专用操作模型。\n\nconda create -n unifolm-vla python==3.10.18conda activate unifolm-vlagit clone https://github.com/unitreerobotics/unifolm-vla.gitcd unifolm-vlapip install --no-deps \"lerobot @ git+https://github.com/huggingface/lerobot.git@0878c68\"pip install -e .pip install \"flash-attn==2.5.6\" --no-build-isolation\n\nassets存放演示素材，experiments存放仿真配置，prepare_data存放格式转换脚本，scripts包含训练与评估脚本，src/unifolm_vla为模型核心代码包，覆盖配置、架构、数据加载、训练全模块。\n\n六、常见问题解答\n\n运行环境报错，提示CUDA版本不兼容怎么办？\n\n项目基于CUDA 12.4开发，建议优先使用该版本CUDA；若使用其他版本，需重新编译FlashAttention2模块，或移除该模块后以非加速模式运行，同时检查PyTorch与CUDA的版本匹配关系。\n\n自定义数据集无法加载，提示格式错误如何处理？\n\n需严格遵循LeRobot V2.1的目录结构与数据字段规范，先使用convert_lerobot_to_hdf5.py验证格式有效性，转换失败则核对视觉帧、动作数据、本体状态的字段名称与维度，再执行RLDS转换操作。\n\n真机部署时客户端无法连接服务端是什么原因？\n\n首先检查服务器端口是否开放、防火墙规则是否放行；其次确认SSH隧道命令的IP、端口与用户名正确；最后核对服务端与客户端的端口配置一致，且机器人端的unitree_deploy环境依赖完整安装。\n\n训练过程中显存不足如何优化？\n\n可减小批次大小、降低输入图像分辨率、启用梯度累积、减少动作块长度，也可使用多GPU分布式训练，分散单卡显存压力；同时确保关闭不必要的日志与可视化模块，释放显存资源。\n\n模型在仿真中效果良好，真机执行偏差大怎么调整？\n\n优先校准机器人相机内参、本体姿态传感器，保证视觉输入与仿真一致；其次用少量真机数据对模型做微调，适配真实物理环境的动力学差异；同时调整动作执行的滤波参数，降低噪声干扰。\n\n能否适配非G1系列的人形机器人？\n\n可以，需修改constants.py中的动作维度、自感知维度配置，重新标注适配目标机器人的数据集并完成微调，同时对接目标机器人的控制SDK，完成动作输出的接口适配。\n\n模型权重下载缓慢或无法访问如何解决？\n\n可使用Hugging Face镜像站点下载权重，或通过官方提供的国内镜像链接获取，也可使用git lfs工具完整克隆模型仓库，保证权重文件完整下载。\n\nUnifoLM-VLA-0是宇树科技面向通用人形机器人操作场景推出的开源视觉-语言-动作大模型，以空间语义增强与多任务泛化为核心能力，依托轻量化训练数据、全流程开源代码、标准化部署流程，填补了从视觉语言理解到机器人物理动作执行的技术缺口，单一模型可覆盖12类真机操作任务，同时兼容仿真测试与真机部署，既可为具身智能研究提供通用基座，也能直接支撑家用、工业场景的机器人操作开发，降低了行业研发门槛，是当前国产具身智能领域具备实用价值与代表性的开源项目。"
  },
  {
    "source": "SitePoint",
    "company": "Hugging Face",
    "title": "WebGPU Browser AI: Client-Side Inference in JavaScript",
    "date": "2026-02-15T02:26:20Z",
    "url": "https://www.sitepoint.com/webgpu-browser-ai-javascript-inference/",
    "content": "What if your AI-powered app's server GPU bill dropped to zero? This article covers exactly how WebGPU makes this possible, why it's fundamentally different from WebGL for AI workloads, and how to run a real model client-side with working code you can ship today.\n\nTable of Contents\n\nThe $0 Inference Revolution Is Already Here\n\nWhat if your AI-powered app's server GPU bill dropped to zero? For most teams shipping AI features today, inference costs are the line item that kills margins. Rate-limited API calls, expensive cloud GPU instances, cold starts on serverless inference endpoints. It adds up fast. WebGPU browser AI changes this equation entirely by shifting inference to the user's own hardware, running GPU-accelerated machine learning workloads directly in JavaScript.\n\nThe trade-offs are real: users pay in download bandwidth, client hardware requirements, and initial load time. You still need a CDN for model assets. But the server-side GPU cost, the part that scales linearly with every user and every request, drops to nothing. No Python runtime. No backend inference server. No per-query billing from an API provider.\n\nWebGPU browser AI changes this equation entirely by shifting inference to the user's own hardware, running GPU-accelerated machine learning workloads directly in JavaScript.\n\nThis article covers exactly how WebGPU makes this possible, why it's fundamentally different from WebGL for AI workloads, and how to run a real model client-side with working code you can ship today.\n\nWhat WebGPU Actually Is (And Why It's Not Just \"Better WebGL\")\n\nA Modern GPU API for the Web\n\nWebGPU is a W3C specification that exposes a modern, low-level GPU programming interface through . It is not a drop-in replacement for WebGL. The abstraction model is fundamentally different, drawing from Vulkan, Metal, and Direct3D 12: explicit resource management, command buffers, bind groups, and pipeline state objects replace WebGL's implicit state machine.\n\nChrome shipped WebGPU by default in version 113, and the API is progressing in Firefox and Safari. The specification was developed by the W3C GPU for the Web Working Group, and the API surface, while low-level, is entirely accessible from JavaScript.\n\nThe Compute Shader Difference\n\nThis is the critical distinction. WebGL provides only a rasterization pipeline: vertex shaders and fragment shaders designed for drawing pixels. Developers who needed GPGPU capabilities resorted to hacks, encoding tensor data as textures and running fragment shaders to process them. It works, but it's constrained, bandwidth-inefficient, and fundamentally not designed for the job.\n\nWebGPU introduces first-class compute pipelines. Compute shaders written in WGSL (WebGPU Shading Language) can perform arbitrary parallel computation on the GPU using storage buffers. Matrix multiplications, attention mechanisms, tensor operations: these map directly to compute workloads without any texture-encoding gymnastics.\n\nHere's what the basic API surface looks like:\n\nThat's standard JavaScript. No compilation step, no native dependencies. The entry point and call are what make ML inference possible in the browser.\n\nWebGL vs. WebGPU for AI Inference: The Performance Reality\n\nArchitecture Comparison\n\nBenchmark Breakdown\n\nThe performance gap between WebGL and WebGPU backends is significant for transformer-based models. ONNX Runtime Web, which supports both backends, consistently shows WebGPU outperforming WebGL on language model architectures. The gains are workload-dependent and device-dependent: transformer models with heavy matrix multiplication and attention operations benefit the most, while simpler vision models see smaller improvements.\n\nReported speedups in community benchmarks and library documentation range from roughly 3x to 5x for transformer models, though exact numbers vary by GPU vendor, model architecture, and browser version. First-run performance includes shader compilation overhead, which can be substantial. Subsequent inference calls are where WebGPU's compute pipeline architecture pays the biggest dividends.\n\nVision models like ResNet see less dramatic improvement because their operation profiles (convolutions, pooling) are less bottlenecked by WebGL's texture-encoding overhead than the dense matrix operations in transformers.\n\nThe JavaScript AI Ecosystem on WebGPU\n\nTransformers.js (Hugging Face)\n\nTransformers.js by Hugging Face is the most accessible entry point. The library provides a API that mirrors the Python Transformers library. It supports a broad range of architectures across tasks: text generation, sentiment analysis, translation, summarization, image segmentation, and embedding generation. Models are loaded from the Hugging Face Hub in ONNX format, typically quantized for efficient browser delivery.\n\nThe library's WebGPU backend allows these pipelines to execute on the GPU rather than falling back to WASM-based CPU inference.\n\nONNX Runtime Web\n\nMicrosoft's ONNX Runtime Web provides a cross-platform runtime with a WebGPU execution provider. Any model exported to ONNX format can target the browser, giving teams flexibility to bring their own models rather than relying on pre-packaged Hub assets. The WebGPU execution provider slots in alongside WebGL and WASM backends.\n\nMediaPipe and Beyond\n\nGoogle's MediaPipe for Web leverages WebGPU for on-device ML tasks, including its LLM Inference API that can run models like Gemma on-device. The Apache TVM web runtime (tvmjs) compiles models to WebGPU-optimized kernels. The WebLLM project demonstrates running Llama, Phi, and Mistral models entirely in-browser. The ecosystem is broad and growing.\n\nA practical note: running inference in a Web Worker keeps the main thread responsive. This is standard practice for any production deployment, regardless of which library you choose. Note that WebGPU access from Web Workers requires the API to be available in the Worker context, which is supported in Chromium-based browsers.\n\nTutorial: Run a Language Model in the Browser with WebGPU\n\nPrerequisites and Setup\n\nYou need Chrome or Edge version 113 or later, and a machine with a GPU (discrete or integrated). Start with a simple project:\n\nInstall with , then scaffold with Vite or use a vanilla ES module setup.\n\nLoading a Model with the WebGPU Backend\n\nWe'll use a quantized sentiment analysis model. The API handles model downloading, caching, and inference setup:\n\nIf WebGPU is unavailable, you should detect that and fall back:\n\nJavaScript developers can build AI features without Python, without dedicated servers, and without per-query inference costs.\n\nMeasuring Performance\n\nSeparating model load time from inference time is essential for understanding real-world behavior:\n\nExpect first-run load times of several seconds (model download plus shader compilation). Warm inference on classification tasks typically runs in tens of milliseconds, though exact numbers depend on your GPU and model size.\n\nConnecting to a UI\n\nWire the pipeline to a text input for an interactive demo. If you're building a responsive interface for this, CSS viewport units help ensure your AI-powered UI adapts cleanly across screen sizes.\n\nPractical Limitations You Should Know\n\nModel Size Constraints\n\nBrowser environments have real memory ceilings. Models must fit in available GPU VRAM, which practically caps at roughly 2 to 4 GB on most consumer integrated GPUs (discrete GPUs may offer more). Quantization is non-negotiable: INT4 and INT8 quantized models are the norm for browser deployment. A 7B parameter model at 4-bit precision lands at approximately 3.5 GB, which pushes the limits of what's feasible. The sweet spot today is sub-1B parameter models or task-specific fine-tuned small models.\n\nBrowser and Hardware Fragmentation\n\nWebGPU is stable in Chrome and Edge. Firefox has WebGPU available behind a flag (as of Firefox Nightly and beta channels), and Safari has shipped WebGPU support in Safari 18 on macOS Sequoia and later (with ongoing feature development). Your code needs a fallback chain: detect WebGPU, fall back to WebGL, fall back to WASM CPU inference. Mobile GPU support is inconsistent, with Android Chrome offering WebGPU availability on supported devices. Always feature-detect with before attempting to use the API.\n\nBe aware of GPU failure modes too. The promise can fire if the device becomes unavailable, and out-of-memory errors are possible with larger models.\n\nCold Start and Caching\n\nFirst load downloads the full model, which can range from tens to hundreds of megabytes. Use the Cache API or Origin Private File System for persistent model storage across sessions. Loading states in your UI are mandatory, not optional. Users will wait for the initial download, but subsequent visits should load from cache almost instantly.\n\nThe server-side GPU cost, the part that scales linearly with every user and every request, drops to nothing.\n\nWhen to Use Client-Side AI (And When Not To)\n\nIdeal Use Cases\n\nPrivacy-sensitive inference is the strongest argument. Medical text analysis, personal document processing, financial data: none of it leaves the user's device. Offline-capable PWAs gain AI features that work without connectivity. High-frequency, low-latency tasks like real-time text suggestions, image filters, or local code completion benefit from eliminating network round-trips. And cost-sensitive products where server GPU spend is prohibitive can serve AI features to millions of users with no per-query cost.\n\nHybrid architectures work well too: a small on-device model handles instant responses while a server-side model processes harder queries.\n\nStill Needs a Server\n\nLarge model inference at the GPT-4 class (hundreds of billions of parameters or large mixture-of-experts architectures) stays server-side. Training and fine-tuning require dedicated hardware. Multi-user aggregation, RAG over large knowledge bases, and any workflow requiring model weights to stay proprietary all need backend infrastructure. Note also that shipping model weights to the browser means users can extract them, which may trigger license distribution obligations.\n\nWhat's Coming Next\n\nThe WebGPU specification includes a features and limits mechanism for optional capabilities, and half-precision () support for faster inference is one such feature progressing through implementation. Broader browser adoption across Firefox and Safari should continue expanding the addressable user base. The Web Neural Network API (WebNN) is a complementary standard targeting hardware-specific ML accelerators like NPUs, which could work alongside WebGPU for even faster on-device inference. Meanwhile, model compression research is steadily making larger models viable for browser deployment.\n\nThe Bottom Line\n\nWebGPU transforms the browser from a rendering surface into a general-purpose compute platform. JavaScript developers can build AI features without Python, without dedicated servers, and without per-query inference costs. The ecosystem supports classification, embeddings, small generative models, and computer vision today.\n\nStart here: feature-detect WebGPU with , pick a quantized model under 500 MB, set up Cache API persistence, add a WASM fallback path, and measure both cold start and warm inference times.\n\nTransformers.js plus a quantized model from the Hugging Face Hub gets you from zero to working client-side AI in an afternoon. Ship something this week."
  },
  {
    "source": "IT News zu den Themen Künstliche Intelligenz, Roboter und Maschinelles Lernen - IT BOLTWISE® x Artificial Intelligence",
    "company": "Hugging Face",
    "title": "Android-Malware nutzt KI-Plattform fÃ¼r umfassende Angriffe",
    "date": "2026-02-14T22:53:55Z",
    "url": "https://www.it-boltwise.de/android-malware-nutzt-ki-plattform-fuer-umfassende-angriffe.html",
    "content": "LONDON (IT BOLTWISE) - Eine neue Android-Malware, die sich als Sicherheits-Update tarnt, sorgt für Aufsehen. Die Schadsoftware nutzt die KI-Plattform Hugging Face, um umfassende Angriffe auf Smartphones zu starten. Besonders gefährdet sind Nutzer, die Apps aus unsicheren Quellen installieren.\n\nDie Bedrohung durch Android-Malware hat eine neue Dimension erreicht, indem sie die KI-Plattform Hugging Face als Verbreitungsweg nutzt. Sicherheitsforscher von Bitdefender haben eine Schadsoftware entdeckt, die sich als legitimes Sicherheits-Update tarnt und so die Kontrolle über Android-Geräte erlangen kann. Diese Malware zielt darauf ab, sensible Daten wie Finanzinformationen und Login-Daten zu stehlen, indem sie die Accessibility Services von Android missbraucht.\n\nDie Malware operiert zunächst als Dropper, was bedeutet, dass die initiale App keinen Schadcode enthält. Stattdessen wird der Nutzer mit einer dringlichen Update-Meldung konfrontiert, die die Installation einer schadhaften APK erzwingen soll. Diese wird über die Plattform Hugging Face bereitgestellt, was den Anschein von Legitimität erweckt und die Erkennung durch Google Play Protect erschwert. Besonders problematisch ist, dass viele Android-Geräte nicht regelmäßig mit Sicherheitsupdates versorgt werden, was die Gefahr zusätzlich erhöht.\n\nEin zentrales Element des Angriffs ist der Missbrauch der Accessibility Services von Android. Diese mächtigen Berechtigungen ermöglichen es der Malware, Bildschirminhalte zu lesen, Eingaben zu simulieren und Tastatureingaben zu erfassen. Dadurch können PINs und Passwörter abgegriffen werden. Zudem nutzt die Malware Overlay-Angriffe, bei denen gefälschte Login-Fenster über echte Anwendungen gelegt werden, um Eingaben direkt an die Angreifer weiterzuleiten.\n\nDer Einsatz von Hugging Face als Distributionsweg markiert eine neue Stufe in der Malware-Verbreitung. Diese Plattform, bekannt für ihre Open-Source-KI-Entwicklung, wird hier missbraucht, um die Schadsoftware zu verbreiten. Für Nutzer in Deutschland und der EU bedeutet dies, dass sie bei Update-Hinweisen und bei der Installation von Apps aus nicht-offiziellen Quellen besonders wachsam sein müssen. Experten empfehlen, nur Apps aus offiziellen Stores zu installieren und die Berechtigungen von Apps kritisch zu prüfen, insbesondere die Accessibility Services.\n\nDie Kombination aus missbräuchlich genutzten KI-Plattformen und tiefgreifenden Berechtigungen stellt eine zentrale Herausforderung für die mobile Sicherheit dar. Angreifer könnten diese Methoden weiterentwickeln, um der Erkennung zu entgehen. Daher ist es wichtig, dass Nutzer ihre Geräte regelmäßig aktualisieren und Sicherheitsmaßnahmen wie Zwei-Faktor-Authentifizierung nutzen, um sich vor solchen Bedrohungen zu schützen."
  },
  {
    "source": "Ad Hoc News",
    "company": "Hugging Face",
    "title": "TrustBastion: Android-Malware nutzt KI-Plattform",
    "date": "2026-02-14T22:06:51Z",
    "url": "https://www.ad-hoc-news.de/boerse/ueberblick/trustbastion-android-malware-nutzt-ki-plattform/68581571",
    "content": "Sicherheitsforscher warnen vor Android-Malware, die sich als Update tarnt und über Hugging Face verbreitet wird. Die Schadsoftware nutzt Accessibility Services für umfassende Geräteübernahme und Datenlecks.\n\nTrustBastion nutzt KI-Plattform Hugging Face, um Android-Geräte zu kompromittieren.\n\nDie Bitdefender-Experten haben eine neue Android-Malware entdeckt, die sich als Sicherheits-App tarnt und über komplexe Tricks vollständigen Zugriff auf das Smartphone erlangen kann. Ziel ist es, Finanzdaten, Login-Daten und persönliche Informationen zu stehlen. Für Deutschland und die EU bedeutet dies: Mobile Nutzer sind verstärkt angreifbar, besonders wenn Apps aus unsicheren Quellen installiert werden.\n\nDie Malware operiert zunächst als Dropper: Die App enthält nicht den eigentlichen Schadcode, sondern zeigt dem Nutzer eine dringliche Update-Meldung, die eine Installation der schadhaften APK erzwingen soll. Das Update wird über Hugging Face gehostet - eine etablierte Open-Source-Plattform für KI-Entwicklung - und damit außerhalb herkömmlicherDownload-Pfade bereitgestellt. Google Play Protect schützt zwar vor bekannten Varianten, doch unbekannte Modifikationen entgehen der Erkennung.\n\nDer Kernangriff besteht darin, Android Accessibility Services missbräuchlich zu verwenden. Nach der Installation der schadhaften Update-Datei bittet die App den Nutzer dauerhaft um die Aktivierung dieser mächtigen Berechtigungen. Accessibility Services ermöglichen es, Bildschirminhalte zu lesen, Eingaben zu simulieren und Tastatureingaben zu erfassen - inklusive PINs und Passwörter.\n\nDarüber hinaus nutzt TrustBastion Overlay-Angriffe: Gefälschte Login-Fenster legen sich über echte Anwendungen, etwa Banking- oder Zahlungsapps. Eingaben landen dort direkt beim Angreifer. Die Malware kann zudem Screenshots erstellen und Inhalte von Messaging-Apps abfangen.\n\nDer Einsatz einer seriös klingenden Plattform wie Hugging Face als Distributionsweg markiert eine neue Stufe in der Malware-Verbreitung. Dadurch wirken Download-Alternativen auf Nutzer oft glaubwürdiger. Für Deutschland bedeutet das: Mehr Wachsamkeit bei Update-Hinweisen und bei Apps, die außerhalb offizieller Stores bezogen wurden. Die Branche verweist zudem darauf, dass viele Geräte nicht mehr regelmäßig Sicherheitsupdates erhalten und dadurch anfälliger sind.\n\nSicherheitsexperten empfehlen, darauf zu achten, welche Stellen Apps für ihre Berechtigungen nutzen. Die Kombination aus Accessibility Services und Overlay-Angriffen macht herkömmliche Schutzmechanismen anfällig für Täuschungen, auch wenn Play Protect gängige Bedrohungen blockiert.\n\nWer sich konkret vor solchen Android-Angriffen schützen möchte, findet im kostenlosen Sicherheitspaket fünf leicht umsetzbare Schutzmaßnahmen - von sicheren Installationsgewohnheiten bis zur richtigen Konfiguration von Berechtigungen wie Accessibility Services. Der Ratgeber erklärt praxisnah, wie Sie Overlay-Angriffe erkennen, Play Protect sinnvoll nutzen und Ihr Smartphone ohne teure Zusatz-Apps absichern. Gratis-Sicherheitspaket: Die 5 Schutzmaßnahmen für Ihr Android-Smartphone herunterladen\n\nSicherheitsexperten erwarten, dass Angreifer ähnliche Methoden weiterentwickeln, etwa durch Namensänderungen oder Code-Anpassungen, um der Erkennung zu entgehen. Die Kombination aus missbräuchlich genutzten KI-Plattformen und tiefgreifenden Berechtigungen bleibt eine zentrale Herausforderung für mobile Sicherheit in Deutschland und der EU. Bleiben Sie wachsam: Installationen aus unbekannten Quellen bleiben der Hauptrisikofaktor.\n\nHol dir den Wissensvorsprung der Profis. Seit 2005 liefert der Börsenbrief trading-notes verlässliche Trading-Empfehlungen - dreimal die Woche, direkt in dein Postfach. 100% kostenlos. 100% Expertenwissen. Trage einfach deine E-Mail Adresse ein und verpasse ab heute keine Top-Chance mehr.\n\nJetzt anmelden."
  },
  {
    "source": "Fox News",
    "company": "Hugging Face",
    "title": "Android malware hidden in fake antivirus app",
    "date": "2026-02-14T19:31:42Z",
    "url": "https://www.foxnews.com/tech/android-malware-hidden-fake-antivirus-app",
    "content": "Fox News Flash top headlines are here. Check out whats clicking on FoxNews.com.\n\nIf you use an Android phone, this deserves your attention. Right now, cybersecurity researchers warn that hackers are using Hugging Face, a popular platform for sharing artificial intelligence (AI) tools, to spread dangerous Android malware. At first, the threat appears harmless because it is disguised as a fake antivirus app. Then, once you install it, criminals gain direct access to your device. Because of this, the threat stands out as especially troubling. It combines two things people already trust: security apps and AI platforms.\n\nSign up for my FREE CyberGuy Report\n\nGet my best tech tips, urgent security alerts, and exclusive deals delivered straight to your inbox. Plus, you'll get instant access to my Ultimate Scam Survival Guide -- free when you join my CYBERGUY.COM newsletter.\n\nMALICIOUS GOOGLE CHROME EXTENSIONS HIJACK ACCOUNTS\n\nFor anyone unfamiliar, Hugging Face is an open platform where developers share AI, NLP and machine learning models. It is widely used by researchers and startups and has become a central hub for AI experimentation. That openness is also what attackers exploited. Because Hugging Face allows public repositories and supports many file types, criminals were able to host malicious code in plain sight.\n\nThe malware first appeared in an Android app called TrustBastion. On the surface, it looks like a helpful security tool. It promises virus protection, phishing defense and malware blocking. In reality, it does the opposite.\n\nOnce installed, TrustBastion immediately claims your phone is infected. It then pressures you to install an update. That update delivers the malicious code. This tactic is known as scareware. It relies on panic and urgency to push users into tapping before thinking.\n\nFAKE ERROR POPUPS ARE SPREADING MALWARE FAST\n\nAccording to Bitdefender, a global cybersecurity company, the campaign centers on a fake Android security app called TrustBastion. Victims were likely shown ads or warnings claiming their device was infected and were instructed to manually install the app.\n\nThe attackers hosted TrustBastion's APK files directly on Hugging Face, placing them inside public datasets that appeared legitimate at first glance. Once installed, the app immediately prompted users to install a required \"update,\" which delivered the actual malware.\n\nAfter researchers reported the malicious repository, it was taken down. However, Bitdefender observed that nearly identical repositories quickly reappeared, with small cosmetic changes but the same malicious behavior. That rapid re-creation made the campaign harder to fully shut down.\n\nThis Trojan is not minor or annoying. It is invasive. Bitdefender says the malware can:\n\nOnce collected, that data is sent to a third-party server. From there, attackers can move quickly to drain accounts or lock you out of your own phone.\n\nGoogle says users who stick to official app stores are protected. A Google spokesperson told CyberGuy, \"Based on our current detection, no apps containing this malware are found on Google Play.\"\n\nThe spokesperson added that \"Android users are automatically protected against known versions of this malware by Google Play Protect, which is on by default on Android devices with Google Play Services.\" They also noted that \"Google Play Protect can warn users or block apps known to exhibit malicious behavior, even when those apps come from sources outside of Play.\"\n\nBROWSER EXTENSION MALWARE INFECTED 8.8M USERS IN DARKSPECTRE ATTACK\n\nThis threat is a reminder that small choices matter. Here is what you should do right now:\n\nOnly download apps from reputable sources like Google Play Store or the Samsung Galaxy Store. These platforms have moderation and scanning in place.\n\nLook closely at ratings, download counts and recent comments. Fake security apps often have vague reviews or sudden rating spikes.\n\nEven careful users can have personal data exposed. A data removal service helps remove your phone number, email and other details from data broker sites that criminals rely on. That reduces follow-up scams, fake security alerts and account takeover attempts.\n\nWhile no service can guarantee the complete removal of your data from the internet, a data removal service is really a smart choice. They aren't cheap, and neither is your privacy. These services do all the work for you by actively monitoring and systematically erasing your personal information from hundreds of websites. It's what gives me peace of mind and has proven to be the most effective way to erase your personal data from the internet. By limiting the information available, you reduce the risk of scammers cross-referencing data from breaches with information they might find on the dark web, making it harder for them to target you.\n\nCheck out my top picks for data removal services and get a free scan to find out if your personal information is already out on the web by visiting Cyberguy.com\n\nGet a free scan to find out if your personal information is already out on the web: Cyberguy.com\n\nScan your device regularly with Play Protect and back it up with strong antivirus software for added protection. Google Play Protect, which is built-in malware protection for Android devices, automatically removes known malware. However, it is important to note that Google Play Protect may not be enough. Historically, it hasn't been 100% effective at removing all known malware from Android devices.\n\nThe best way to protect yourself against malicious links that install malware and potentially access your private information is to have strong antivirus software installed on all your devices. This protection can also help you detect phishing emails and ransomware, keeping your personal information and digital assets safe.\n\nGet my picks for the best 2026 antivirus protection winners for your Windows, Mac, Android & iOS devices at Cyberguy.com\n\nAvoid installing apps from websites outside the app store. These apps bypass security checks, so always verify the publisher name and URL.\n\nYour phone security depends on it. Enable two-step verification (2FA) first, then use a strong, unique password stored in a password manager to prevent account takeovers.\n\nNext, see if your email has been exposed in past breaches. Our #1 password manager (see Cyberguy.com) pick includes a built-in breach scanner that checks whether your email address or passwords have appeared in known leaks. If you discover a match, immediately change any reused passwords and secure those accounts with new, unique credentials.\n\nCheck out the best expert-reviewed password managers of 2026 at Cyberguy.com\n\nBe cautious with accessibility permissions. Malware often abuses them to take control of your device.\n\nMalware can hide inside fake updates. Be cautious of urgent fixes that push you outside the app store.\n\nThis attack shows how quickly trust can be weaponized. A platform designed to advance AI research was repurposed as a delivery system for malware. A fake antivirus app became the threat it claimed to stop. Staying safe no longer means avoiding sketchy-looking apps. It means questioning even those apps that appear helpful and professional.\n\nHave you seen something on your phone that made you question its security? Let us know your thoughts by writing to us at Cyberguy.com\n\nGet my best tech tips, urgent security alerts, and exclusive deals delivered straight to your inbox. Plus, you'll get instant access to my Ultimate Scam Survival Guide -- free when you join my CYBERGUY.COM newsletter."
  },
  {
    "source": "Chosun.com",
    "company": "Hugging Face",
    "title": "Chinese Open-Source AI Ecosystem Surpasses U.S. in Global Dominance",
    "date": "2026-02-08T09:46:17Z",
    "url": "https://www.chosun.com/english/industry-en/2026/02/08/EEOKEXA75ZAJLCCLISXOC3V2RA/",
    "content": "DeepSeek's Low-Cost Model Sparks Chinese Dominance in Open-Source AI, Influencing U.S. Startups\n\nLast year, on the AI model platform Hugging Face, the number of downloads for Chinese-made models surpassed those of U.S. models among newly created AI models. Following January of last year, when the Chinese AI startup DeepSeek shook the global market by implementing top-tier AI at low cost, the country appears to be building its own AI ecosystem using open-source as a weapon.\n\nAccording to industry sources on the 8th, Hugging Face posted a three-part report titled 'One Year After the DeepSeek Moment' detailing this content.\n\nDeepSeek led the spread of open-source. After DeepSeek, Chinese big tech companies one after another turned to open-source. For instance, while the Chinese IT company Baidu had never publicly released an AI model as open-source before, in the year following DeepSeek's release, it disclosed over 100 cases on Hugging Face as open-source. The Chinese AI company MoonShot also released the Kimi K2.\n\nAccording to a joint research report published late last year by the Massachusetts Institute of Technology (MIT) and Hugging Face, the share of open-source downloads for AI models was 17.1% for China, ahead of the U.S. at 15.8%.\n\nIn the number of derivative models based on AI models, Chinese open-source AI models also lead U.S. models. The Alibaba Group's Qwen model has 115,000 derivative AI models. This surpasses Google's 72,000, Meta's 46,000, and OpenAI's 11,000.\n\nIn the U.S. Silicon Valley, Chinese models are also exerting influence. According to the report, 80% of U.S. AI startups utilize Chinese open-source models in their product development processes.\n\nThe report stated, \"While OpenAI and Anthropic have maintained closed ecosystems, Chinese companies have released high-performance models as open-source, making the Chinese open-source ecosystem the global standard.\""
  },
  {
    "source": "Chosunbiz",
    "company": "Hugging Face",
    "title": "China leads AI open source as DeepSeek spurs global shift",
    "date": "2026-02-08T09:18:38Z",
    "url": "https://biz.chosun.com/en/en-it/2026/02/08/PHMHAGLWUZHGJA4PFFA4WIJTTU/",
    "content": "Last year, downloads of China-made models among newly created AI models on Hugging Face, an artificial intelligence (AI) model platform, surpassed those of U.S. models. In January last year, China AI startup DeepSeek shook the global market by delivering top-tier AI at low cost, and China has since been building its own AI ecosystem using open source as a weapon.\n\nOn the 8th, according to industry sources, Hugging Face posted a three-part report titled \"One year after the DeepSeek moment.\"\n\nDeepSeek led the spread of open source. After DeepSeek, China's big tech corporations switched to open source one after another. For example, China IT corporations Baidu had never released AI models as open source, but in the year since DeepSeek's launch it released more than 100 models on Hugging Face as open source. China AI corporations Moonshot also released Kimi K2.\n\nAccording to a research report jointly released late last year by the Massachusetts Institute of Technology (MIT) and Hugging Face, China leads the share of open-source downloads for AI models at 17.1%, ahead of the United States at 15.8%.\n\nChina's open-source AI models also lead U.S. models in the number of derivative models based on AI models. AI models derived from Alibaba corporations' Qwen model number 115,000. That outpaces Google at 72,000, Meta at 46,000, and OpenAI at 11,000.\n\nIn Silicon Valley in the United States, Chinese models are also exerting influence. According to the report, 80% of U.S. AI startups use Chinese open-source models in their product development process.\n\nThe report said, \"While OpenAI and Anthropic stuck to closed ecosystems, Chinese corporations released high-performance models as open source, making China's open-source ecosystem the global standard.\"\n\n※ This article has been translated by AI. Share your feedback here."
  },
  {
    "source": "Swiss IT Magazine",
    "company": "Hugging Face",
    "title": "Android-Trojaner nutzt Hugging Face als Tarnung",
    "date": "2026-02-06T05:16:10Z",
    "url": "https://www.itmagazine.ch/artikel/86486/Android-Trojaner_nutzt_Hugging_Face_als_Tarnung.html",
    "content": "Bitdefender hat eine Android-Kampagne entdeckt, die einen Remote-Access-Trojaner über Hugging Face ausliefert und Nutzer mit Fake-Updates täuscht. Die Angreifer erzeugen dabei ständig neue Varianten und missbrauchen Barrierefreiheitsfunktionen, um Kontrolle über Geräte zu bekommen. 6. Februar 2026 Eine neue Android-RAT-Kampagne verteilt Schadsoftware über Hugging Face und setzt auf Tricks, um Nutzer zum Installieren zu bewegen. Die Sicherheitsforscher von Bitdefender beschreiben eine zweistufige Infektionskette, bei der zuerst ein Dropper installiert wird, also eine unauffällige Vorstufe und danach die eigentliche Schad-App nachgeladen wird, und schreiben, dass Hugging Face zum Hosten der gefährlichen Dateien missbraucht werde.\n\nLaut dem Bericht beginnt der Angriff oft mit der App Trustbastion, die über Werbung oder Warnhinweise beworben wird, etwa mit der Behauptung, das Telefon sei infiziert. Nach der Installation zeige die App eine Update-Aufforderung, die wie ein offizielles Android- oder Google-Play-Update aussehe, um Nutzer zum Nachladen der nächsten Stufe zu bringen.\n\nDie endgültige APK, (Android-Installationsdateien) werde über die Website von Trustbastion angestossen und dann von Hugging-Face-Datensätzen heruntergeladen, so die Analyse. Die Angreifer hätten zudem etwa alle 15 Minuten neue Varianten erzeugt, um Erkennung über Hashwerte zu erschweren, also über digitale Fingerabdrücke von Dateien, und nach dem Entfernen eines Repositories, sei der Betrieb auf einen anderen Link umgezogen.\n\nDie nachgeladene Schadsoftware fordere laut den Forschern weitreichende Berechtigungen an, vor allem für Bedienungshilfen sowie Bildschirmfunktionen, um das Gerät zu überwachen und zu steuern. Genannt werden zudem gefälschte Login- und Finanzoberflächen zum Diebstahl von Zugangsdaten und eine Kommando- und Kontrollinfrastruktur, also Server, über die die Angreifer Befehle geben und Daten abziehen. (dow)"
  },
  {
    "source": "adalovelaceinstitute.org",
    "company": "Hugging Face",
    "title": "What is an AI transcription tool?",
    "date": "2026-02-05T15:46:37Z",
    "url": "https://www.adalovelaceinstitute.org/resource/what-is-an-ai-transcription-tool/",
    "content": "If you are a policymaker and you are interested in emerging insights about the adoption of AI across the public sector, read 'Current evaluations and assessments of AI transcription tools' and 'How are AI transcription tools used in the public sector?'.\n\nThere is growing opinion in policy and government circles that AI can make many aspects of the public sector more efficient. One example of a type of AI that is generating attention is AI transcription tools. These tools are designed and marketed to make administrative tasks, such as note-taking and writing summaries of meetings, faster and more efficient. They are increasingly built from the latest generation of foundation models.\n\nAI transcription tools have the potential to be highly effective when applied to tasks and professions requiring a high degree of information to be processed, recorded and transcribed - for example, in clinical contexts, courts and in social work. As such, AI transcription tools have captured policy attention[1] [2] and have been quickly adopted into public sector service delivery. It is estimated that around a third of social workers are using generative AI tools with transcription capabilities, such as Microsoft Copilot.[3]\n\nThe UK government has shown enthusiasm about the potential for AI transcription tools to ease the pressure on public services in the face of resourcing challenges. AI transcription tools feature heavily in the government's plans to roll out AI tools across the public sector. Ambitions for the adoption of AI transcription tools have been outlined in the 10 Year Health Plan for England[4] and the AI Action Plan for Justice.[5] Additionally, the government's AI Exemplars programme identifies AI transcription tools as one of the 'most promising' AI applications for public benefit.[6]\n\nThis explainer aims to support policymakers or public sector agencies recommending AI tool adoption and public sector workers using these tools. We combine technical detail about the underlying architecture of AI transcription tools (including their potential risks and benefits) with tailored insights into approaches for evaluating and assessing the efficacy of transcription tools. There is emphasis on public sector adoption throughout.\n\nFirst, by highlighting the core technical features of AI transcription tools, we provide a deeper understanding of their suitability for contexts where accuracy and efficiency are prioritised.\n\nWe show that as AI transcription tools are a type of generative AI system, they introduce the same potential risk of bias, hallucination and data protection risks.\n\nWe also pay attention to their wider application and systemic risks, such as how these tools may augment or challenge decision-making and accountability.\n\nSecond, by synthesising emerging evidence about AI transcription tool adoption and evaluation in the public sector, we aim to inform ongoing adoption efforts such as pilots. Ultimately, this explainer provides valuable evidence to ensure AI transcription tools are deployed and evaluated in the public interest.\n\nTranscription tools enable spoken audio to be reproduced as written text. They can be used to support note-taking and the documentation of conversations, meetings, interviews or recordings.\n\nWe use the term 'AI transcription tools' to refer to a new generation of transcription technologies that are powered by foundation models trained on extremely large datasets of recorded speech.[7] These tools are sometimes referred to as 'AI scribes', 'ambient scribes' or 'ambient voice technologies'.[8] Some general-purpose AI tools that are also powered by foundation models, such as ChatGPT, have transcription and summarisation capabilities.\n\nAI transcription tools can be used in a variety of contexts that require a transcript or a summary of an interaction, such as an interview or meeting. They can also support contexts where accurate record-keeping and official documentation are required as part of a best practice service, such as patient records or care assessments.\n\nSome AI transcription tools also include additional features to restructure transcribed text into different formats, including AI-generated summaries of conversations and meetings that underline key points or even provide recommended actions.\n\nThese features can be fine-tuned and iterated further for specific contexts, such as supporting healthcare staff to write a patient report that meets specific formatting requirements.\n\nAI transcription tools are powered by two types of foundation models: an automated speech recognition (ASR) model and a large language model (LLM). As set out in our explainer on the topic, foundation models are AI systems trained on large amounts of data for multimodal capabilities, meaning they can generate new text, images, video and voice materials.[9] Below, we describe how each model supports the function of an AI transcription tool.\n\nAutomated speech recognition (ASR) models are one of the two types of foundation models used to power AI transcription tools. ASR models are the newest generation of a family of models that can create a written transcript of a conversation.\n\nNew ASR models are trained on large datasets of recorded conversations that are paired with pre-transcribed text of what was said in each recording. The foundation models identify patterns between the sounds in the recordings and the words in the corresponding text.\n\nAI transcription tools use these patterns to transcribe speech from new audio files by estimating the sequence of written words that best fits the sounds in the audio files. Examples of ASR models that are commonly used in AI transcription tools include OpenAI's Whisper and Deepgram.\n\nDevelopers of AI transcription tools can fine-tune ASR models to improve their performance in specific, specialised contexts. Fine-tuning is the process of using context-specific audio recordings to further train a model to recognise specific words and phrases, such as medical terminology or other context-specific terms.\n\nLarge language models (LLMs) are the second kind of foundation model used to power AI transcription tools.\n\nLLMs are a subset of foundation models created for natural-language capabilities. They are trained on large amounts of written text and underpin a range of AI tools, such as customer service chatbots. In AI transcription tools, LLMs summarise the transcripts made by ASR models into key points and formats that can be included in official documentation. LLMs in AI transcription tools can be fine-tuned to introduce additional or context-specific safeguards from those of the underlying foundation model, to better enable faithful transcription. For example, it is possible that a conversation taking place in a social care setting may mention child sexual abuse material (CSAM), but commercial foundation models are designed to filter and block this content from appearing in outputs by default.\n\nThe use of ASR models and LLMs within the same tool means that transcription tools can also use written text as an input to accompany audio recordings. LLMs have an interface for users to prompt the tool to alter the final outputs of AI transcription tools, and some can also use photographs of handwritten notes as inputs. This text will be incorporated into the final output generated by the transcription tool. As a result, users can adapt the outputs produced by AI transcription tools to add more detail or, if they spot inaccuracies, by prompting the tool to make desired changes.\n\nFor example, a doctor could combine notes from previous interactions with a patient with an audio recording of a more recent interaction, to update their notes about the patient. The doctor can then prompt the AI transcription tool to make changes to the notes after they have reviewed them.\n\nFigure 1: The architecture of an AI transcription tool\n\nThis diagram shows the workings of a generic AI transcription tool and how it uses an ASR model and LLM to produce a summary of a conversation. The boxes show the different components of the AI transcription tool and the arrows show how information is passed between them. The dotted arrows show how the LLM may access additional sources of information, such as user prompts or other relevant documents, to generate summaries that are more suited to the user's needs.\n\nFigure 1 is one approximation of what an AI transcription tool may look like. Although different AI transcription tools will use roughly the same components, their specific design may differ.\n\nOlder transcription tools used automated speech recognition (ASR) models, without foundation models and large language models (LLMs).\n\nThese older models converted audio recordings to transcriptions using traditional statistical methods that consisted of algorithms designed to complete narrowly defined tasks, such as identifying specific sounds within speech. However, the narrowness of the underlying models made these older transcription tools inflexible and inaccurate.\n\nThe availability of large amounts of training data and computational power means that ASR models can now be developed as foundation models. These new ASR models match sounds to written text directly, using the associations between sounds and written text in their training data.\n\nAs a result, new ASR models are often described as having 'end-to-end' architectures. This is because information from the audio recordings is not processed through the multiple statistical models found in previous transcription technologies.[10]\n\nThis technological advancement means end-to-end ASR models perform better than older ASR models, because a higher percentage of words from the audio recording is faithfully transcribed into text. This metric is referred to as the 'word error rate'.[11]\n\nPopular examples of AI transcription tools on the market include Otter.ai and Rev, but other generative AI tools marketed for more general use may contain transcription capabilities, such as Microsoft's Copilot.\n\nA defining feature of foundation models is their need for large quantities of training data.[12] However, building foundation models requires developers to make decisions about what kinds of data to include, and how this data is cleaned and categorised. These decisions can significantly impact how foundation models function.[13]\n\nMany developers of foundation models do not publicly share information about the data their models are trained on. Without a clear understanding of the data used to build a foundation model, it is difficult to understand why the model generates certain outputs.[14]\n\nResearch shows that these data considerations are important for foundation models that power AI transcription tools. ASR models could inaccurately transcribe conversations if the words and phrases in the audio recordings used for training data do not reflect the kinds of conversations that they transcribe in practice.[15] Similarly, LLMs may reproduce harmful content from their training data in the new text that they generate, such as hate speech and slurs.[16]\n\nAI transcription tools may perform differently depending on their underlying foundation models, but even tools built from the same foundation models should be evaluated independently. For example, two different AI transcription tools could both be built from GPT-5, but one fine-tuned for use in healthcare will be trained to retrieve information from a smaller dataset, such as clinical records, which may mean it scores better on accuracy evaluations.[17] Organisations that want to deploy AI transcription tools should consider the trade-offs between different tools on the market and rigorously assess whether they function as intended for the desired context.\n\nPrevious Ada research shows that organisations should establish clear success criteria when deploying new technologies, as well as methods for regular testing and evaluation.[18] All of these processes should include the perspectives of people impacted by new technologies. This will ensure they work more effectively in the public interest.\n\nRegular testing is also important because the use of new technologies to automate tasks can create new responsibilities, such as reviewing and adapting the outputs of AI tools, that counteract anticipated efficiencies.[19]\n\nFor example, one evaluation of a pilot of AI transcription tools for generating witness statements found that AI transcription did not reduce the amount of time that police officers spent taking statements, because the time saved was replaced by the additional time required for amending poor quality statements.[20]\n\nOrganisations may have a number of incentives to adopt AI transcription tools. There are a range of potential benefits from the use of AI transcription tools. Many of these benefits stem from the ability of transcription tools to reduce the time that people spend documenting interactions.[21] However, AI transcription tools could also provide benefits beyond time savings.\n\nThe tables below summarise the possible benefits of AI transcription tools in three categories:\n\nLike all AI systems, AI transcription tools present multiple risks that stem from the technical aspects of their design and their integration into social structures. We summarise the risks of AI transcription tools using three categories:\n\nAll AI transcription tools are prone to bias and hallucination (producing fabricated content) because they are powered by foundation models.[27]\n\nThe potential for bias is present across all data-driven systems. A technical definition of bias is the statistical deviation from a desired or 'true' result, which can lead to real-world bias when the output of a system results in individuals or groups experiencing a different, unfair outcome.\n\nHallucination is a risk specific to foundation models and generative AI systems built on top of foundation models. The term refers to the phenomenon of these systems producing convincing but factually incorrect information, presented as if it were 'the truth'.[28]\n\nAI transcription tools compound the risks of bias and hallucination because they pass information through two separate foundation models and therefore create multiple opportunities for bias and hallucinations to occur in their outputs. First, during the transcription of audio recordings into text using an ASR model, and second, when the transcribed text is summarised for system usability using an LLM.\n\nIn ASR models, hallucination refers to generated text that has no relation to the conversation in the original audio recording. ASR models have been found to generate random text that promotes harmful perceptions of individuals included in the recording, including portrayals of violence, demographic stereotypes and false relationships between speakers.[29]\n\nASR models are also less accurate for people with characteristics that are underrepresented in the data that the models are trained on. Differences in the way that people with different characteristics speak, such as their accents, mean that transcription tools can create inaccurate outputs even if these characteristics are not mentioned in the recorded conversation.\n\nFor example, foundation models have been found to transcribe speech with varying accuracy between different groups, performing more poorly for non-native English speakers than for native English speakers.[30] Other research has shown that hallucinations disproportionately occur for people who have conditions that affect their ability to coordinate language. For example, aphasia can create delays, repetitions or missing words in peoples' speech.[31]\n\nLLMs also hallucinate and perpetuate biases, as shown in our evidence review of foundation models in the public sector.[32] These inaccuracies can occur even when LLMs perform a seemingly neutral function like summarising a written document.\n\nResearch has shown that summaries of care records produced by LLMs can differ significantly according to the characteristics of the people that the document relates to, such as their gender.[33]\n\nBecause of these potential inaccuracies, using AI transcription tools to support decision-making can put people at risk. For example, it is possible that inaccuracies from AI transcription tools used to write police reports could be used as evidence in court cases and increase the likelihood of injustices in legal proceedings.[34] Similarly, evidence suggests that transcription inaccuracies can produce incorrect diagnoses for patients when AI transcription tools are used in medical contexts.[35]\n\nAnother critical risk from the technical features of AI transcription tools arises from their underlying foundation models often being hosted externally by foundation model developers or third-party cloud providers.[36] This decreases an organisation's control over the data that is transcribed and summarised with AI transcription tools.\n\nAs a result, organisations may not be able to prevent sensitive information being used in unexpected ways or stored insecurely.[37] The foundation model developers may be based in different countries, increasing the number of cross-border transfers of information and further reducing the organisations' control over their information. Organisations should conduct robust Data Protection Impact Assessments to identify privacy risks introduced by AI transcription tools.[38]\n\nOrganisations will also need to regularly assess AI transcription tools, compare alternative tools and consider whether the risk of bias and hallucination is acceptable for the intended use case.\n\nAI transcription tools can create risks even if they perform perfectly well at a technical level. These risks stem from how people react to new technologies unpredictably and the challenges of implementing new technologies in sensitive contexts.\n\nThe fact that AI transcription tools generate outputs in compelling professional language could lead to users' over-reliance on the legitimacy of their outputs in ways that are detrimental.­ For example, frontline workers may defer to an AI-generated transcript in critical decision-making contexts, underplaying their own professional judgement.[39] This tendency is often known as 'automation bias'.\n\nPeople may also modify their behaviour to accommodate AI transcription. For example, it is possible that people who use frontline public services will feel uncomfortable with the use of AI transcription tools in certain sensitive contexts, such as clinical settings, which could cause them to 'self-censor' when faced with a transcription tool.[40] This could lead to people withholding important information from professionals that could improve the quality of care or support that they receive.\n\nAI transcription tools may not wholly reflect the 'true' context of a meeting or an interaction. For example, AI transcription tools use auditory information, not visual or sensory cues. In some cases, it could be important for frontline workers to record additional observations about people's body language or the environment that they live in, even if these factors are not spoken about directly. Moreover, poor quality audio, caused by use in noisy environments, could also mean that important information is missing. This can distort the accuracy of the final summary that is produced.\n\nSimilarly, official records based on summaries of interactions often require specific kinds of language and narrative styles that AI transcription tools may not replicate fully. One study of an AI transcription tool used for writing domestic abuse witness statements found that AI-generated statements used a more academic style that did not include a strong narrative of events and therefore did not accurately represent the witnesses' accounts in court.[41]\n\nIn sensitive contexts like social care, it is often more appropriate to use language that centres the perspectives of people and avoids complex, technical language. AI-generated technical language could obscure aspects of people's experiences in official records, impacting the decisions made based on these records and people's ability to understand significant events in their lives.[42]\n\nAdditionally, the AI-generated transcript and summaries may not reference that it was produced by AI, which may have implications for independent reviews of the documentation.[43]\n\nApplication risks and impacts underscore the importance of research, testing and piloting AI transcription tools with the intended user population. This should be done in context to create rich grounded evidence about the interface between the technology and affected groups.\n\nThe introduction of new technologies to established processes can change how organisations, industries or economies function. Such knock-on effects can change how people work with these institutions and the experiences of people who rely on them for support. We have identified two areas where AI transcription tools can potentially change processes and systems in a way that creates systemic impact.\n\nFirst, AI transcription tools can complicate processes for allocating accountability in critical decision pathways. The introduction of AI - wholly or partially - into official documentation such as witness statements may introduce challenges around the 'human in the loop' and the extent to which people have oversight and authority over work and decision-making processes supported by AI.\n\nAs a result, it could be difficult to identify who is responsible for official records when humans do not have control over the whole process. Due to the black box nature of foundation models, it is also impossible to explain why AI transcription tools prioritise, ignore or reframe different pieces of information when generating summaries of written texts.\n\nThis means there will be random variation in the outputs of AI transcription tools that will require oversight and, where necessary, correction, but may sometimes be difficult to catch. [44] This could create challenges for organisations or processes where accurate records are critical and could have implications for people seeking redress when mistakes are made.\n\nSecond, AI transcription tools could cause deskilling in some professions where skills are traditionally learned through the documentation processes. When tasks are automated, it can reduce the opportunities for people to develop skills and knowledge through repetition.[45]\n\nFor example, social workers may learn to exercise their professional judgement through the process of writing assessments of people's care needs. In the context of AI transcription, professionals may lose opportunities to develop sector-specific knowledge and reasoning skills that they would otherwise learn by manually recording and summarising interactions with people.\n\nThe widespread automation of significant parts of people's work could also lead to broader economic consequences. For example, automation could reduce the quality of people's work, if their work is reallocated to unfulfilling tasks, or exacerbate economic inequalities, if automation reduces demand for paid workers.[46]\n\nSystemic risks highlight how AI transcription tools exist alongside other AI technologies and demonstrate the need to consider whether the impacts of these technologies are justly and equitably distributed.\n\nAs we set out above, the latest AI transcription tools are powered by foundation models and incorporate LLMs into their architecture. This technical context presents a few challenges when evaluating a transcription tool's efficacy.\n\nFirst, any fine-tuning to an AI transcription tool (such as training it on a dataset of clinical records so it is more effective for adoption in a healthcare context) may override or degrade the safety guarantees of the foundation model. The LLM component of an AI transcription tool will therefore require specific, iterative evaluation different to that undertaken by the foundation model developer, to account for capability changes or potential biases created by the fine-tuning.\n\nSecond, LLMs - as a form of generative AI - are highly dynamic, meaning the same system prompt may generate different outputs. They also create risk of distributed impacts that are hard to measure at a system level. This type of system makes standardising testing and evaluations challenging and requires an iterative approach, to address any updates in the underlying model as well as to ensure the system is effective and safe for the deployment context.\n\nDeployers of AI transcription tools may face challenges and trade-offs identifying and operationalising appropriate approaches and metrics for evaluation.\n\nResearchers and practitioners in machine learning use evaluations to refer to assessments of technical capabilities of an AI model (including foundation models). For example, how well it performs against a chosen benchmark like bias, or how easily a model exhibits certain types of behaviours, like deception or sycophancy.\n\nThere is no consensus around a defined scope for a model evaluation. Previous Ada research finds that model evaluations are often targeted narrowly, presenting models with a variety of inputs and checking the outputs correspond with ethics or safety goals. Some evaluations are broader and consider how users interact with the application on top of the foundation model (see 'Human interaction evaluations' below).[47]\n\nTypical model evaluation methods for foundation models include benchmarking, which involves generating a score or metric from testing the model's performance on a range of tasks, like legal comprehension or medical reasoning.\n\nOther types of technical testing like red-teaming - an activity that involves the probing of a system in an adversarial way to identify potential harmful outputs - are a form of evaluation that is specifically safety focused. Different actors are involved with model evaluations and other forms of technical testing across the AI development and deployment lifecycle. In the context of AI transcription tools, evaluations may be conducted upstream by model developers using standardised assessments for safety or bias, and downstream by deployers after fine-tuning, to check for specific performance on the task of transcription.\n\nIf a model evaluation is a narrow form of evaluation purely focused on the model level, there are other evaluation approaches that focus on assessing an AI system in its real-world application. These are sometimes referred to as human interaction evaluations.[48]\n\nAn example evaluation of this kind might involve gathering evidence on an LLM being used in a school to understand how it changes pedagogical practices of teachers.\n\nOutside of AI, 'evaluation' can refer to any kind of experimental or descriptive investigation into a system, process or intervention. These kinds of evaluations might be used to deliver evidence of efficacy or value for money, which might be a requirement for public sector projects.\n\nThe UK government's guidance for central government agencies on how to conduct evaluations - the Magenta Book - defines evaluations as 'systematic assessment of the design, implementation and outcomes of an intervention. It involves understanding how an intervention is being, or has been, implemented and what effects it has, for whom and why', with a focus on impacts and cost-effectiveness.\n\nSome evaluations may include experimental or quasi-experimental design. Experimental methods include randomised controlled-trials (RCTs), which are often used in social science research, and are used for robust evidence-gathering to establish a causal relationship between two (or more) factors.\n\nClinical research, for example, uses methods like non-inferiority testing to ensure that a new medical treatment does not perform any worse than an existing treatment.[49]\n\nAI evaluations may be adopted alongside other resources that are more tailored towards organisational processes and day-to-day responsible management of AI systems. Examples of such assessments include the UK government's AI Management Essentials tool and Careful Industries' Careful Consequence Check.[50]\n\nWe have existing evidence about AI transcription tools' capabilities from studies assessing their accuracy and their pitfalls, such as demonstrating gender bias, and from their application into critical decision-making contexts that require complete and accurate information, such as medical settings.[51]\n\nAs we show above, there are several existing approaches for AI evaluations that draw from different disciplines. For meaningful evaluations of AI transcription tools that incorporate risks, capabilities and user interactions, a range of metrics will likely be required to gain a holistic picture of the system in practice.\n\nBelow we offer some example approaches and metrics for evaluating AI transcription tools that are drawn from the field of natural language processing (NLP). The approaches demonstrate how multiple different metrics could be used to assess a particular target of evaluation, e.g. model performance. We focus on approaches for human-led evaluations, as opposed to automated evaluations that use large language models (LLMs) to assess target metrics (a method known as 'LLM-as-a-judge').[53]\n\nIn the following section, we synthesise some recent evaluations of AI transcription tools in the UK public sector. While not an exhaustive list, these three studies provide useful evidence of emerging AI evaluation metrics in practice. They showcase recent efforts towards evidence gathering on the effectiveness of consequential applications of AI in the public sector, such as transcription tools.\n\nIn 2024, Kingston Council supported the piloting of a prominent AI transcription tool used in the public sector, Beam's Magic Notes. Magic Notes is a transcription tool specifically tailored for social work contexts, supporting transcriptions of meetings and the preparation of case notes and assessments. It is estimated that over 80 councils in the UK are currently using Magic Notes in social work contexts.[56]\n\nKingston Council worked with Beam in a user research capacity, testing and evaluating the transcription tool to adapt the existing product for use by social workers on mobile phones. The approach involved training Magic Notes with anonymised data from templates and assessments from council social workers and checking the outputs met the required standards for the council, including populating the correct fields within the case management system.[57]\n\nThe pilot also explored the time-saving ability and value for money of Magic Notes, finding an average time saving of over 50 per cent and an accuracy of 96 per cent in the transcripts.\n\nAs an early pilot of the real-world capabilities of an emerging AI transcription tool in the public sector, the study and its publicly available evidence may have informed wider uptake of Magic Notes.\n\nThe UK's Department for Business and Trade (DBT) and Department for Work and Pensions (DWP) have both conducted multi-month pilots and evaluations of Microsoft Copilot, a generative AI tool with wide-ranging capabilities, including transcription and summarisation.\n\nBoth evaluations collect data via self-reporting surveys. The DBT survey aimed to measure on use cases, satisfaction, accuracy and time savings. One thousand Copilot licences were allocated to volunteer and randomly selected participants from across the department, with a control group for comparison.[58] The DWP evaluation adopted a quasi-experimental design, using a control group and complementing the survey with qualitative interviews to measure staff experiences, perceived usefulness and the impact on task efficiency, job satisfaction and work quality.[59] The DWP also used an econometric model called Seemingly Unrelated Regression (SUR) to capture the interdependency of the outcomes of task efficiency, job satisfaction and work quality.\n\nIn the DBT evaluation, 'Transcribing or summarising a meeting' was the most popular use of Copilot among the use cases identified by participants, with 89 per cent of participants 'satisfied' with this capability. The evaluation also found that 'transcribing or summarising a meeting' using Copilot saved on average 0.7 hours per task, one of the higher time savings identified in the study. However, the study also noted that the link between time savings and increased productivity among participants was not conclusive. This suggests multistage or multifactor evaluations may be required to make robust empirical claims that time savings lead to improved productivity, which may include more longitudinal data gathering.\n\nIn the DWP evaluation, using Copilot for 'Transcribing or summarising meetings' was found to save on average nine minutes, and 'Summarising information or research' saved 24 minutes. The qualitative interviews revealed that time savings were often redirected towards 'higher value' work, including 'project delivery, planning, and mentoring'. The statistically significant effects on all three outcome measures are a positive result for this study, though the authors highlight the importance of human-in-the-loop for tasks involving 'nuance, sensitive content, or stakeholder-facing outputs'.\n\nBoth studies are limited by self-selection bias among their participant base. The use of self-reporting for measures like job satisfaction can introduce biases such as social desirability bias, where responses are tailored according to societal expectations.[60]\n\nThe Centre for Policing Research and Learning at the Open University conducted an evaluation of Hertfordshire Constabulary's 'ADA' (Anathem Digital Assistant) transcription tool, designed to transcribe and produce witness statement documentation. The purpose of the evaluation was to assess whether the tool led to increased productivity and quality of statements.[61]\n\nThe evaluation adopted a mixed-methods approach, combining interviews with police officer using the tool and various metrics from linguistic theory to determine the quality of AI-produced transcripts. These included the Flesch readability score,[62] which is a metric for evaluating a text for how easy it is to understand.\n\nThe researchers in this study highlight how assessing the transcripts through a broader lens enabled them to look beyond accuracies or inaccuracies, and to judge whether the completed transcript reflected the overall language and tone of the witness's original statement.\n\nThey found that the use of ADA did result in time savings, but they could not evidence any 'realisable claims' around productivity, recommending that future productivity assessments in this domain would need to operate 'end to end' to provide a more well-rounded picture of the impact of the AI transcription tool on work and practice. The team concluded that the study did not indicate sufficient proof of concept that ADA would increase productivity, but indicated that a forthcoming second version of the tool 'may improve significantly in terms of performance'.[63]\n\nAs highlighted in the UK government's guidance for conducting impact evaluations of AI, organisations should approach evaluations with good faith. A major incentive to conduct evaluations is to gain helpful insights and to learn about the tool's impact. However, the findings of the above evaluations offer several important lessons for informing ongoing evaluations of AI transcription tools, as detailed below.\n\nThe three studies above all show evidence that the transcription and summarisation functions of AI transcription tools are reducing time spent on drafting, which is promising for further adoption of these tools. However, two of the studies do not show evidence that this straightforwardly translates to improved productivity. Additional requirements may be needed within organisations, such as training and longer-term evidence gathering or the introduction of an alternative tool, to ensure that productivity benefits are fully realised.\n\nMore importantly, further research that specifically measures the costs and gains of AI transcription tools in a range of organisational contexts will be needed to understand the degree to which time savings empirically contribute to increased productivity.\n\nFor contexts where a holistic interpretation of a conversation is important, such as a witness statement, additional metrics may be required to measure details that may be omitted by transcription or summarisation capabilities.\n\nAdditional human safeguards to thoroughly check transcripts - for example, for erroneous detail inserted superfluously - will likely be required. There are also likely to be scenarios where details in transcripts are faithful to the conversation but may contain tonal or linguistic shifts that have an impact on the interpretation of meaning. This type of impact will be much harder to track at a single user level, and may require evaluations against non-AI generated transcripts, or comparisons with different AI transcription tools.\n\nOverall, the evidence from these three studies underscores that AI transcription tools will require an iterative, sociotechnical-informed approach to evaluation that prioritises grounded evidence from real-world impacts, beyond just efficiency and time savings.\n\nUK public services are facing significant financial challenges and the implementation of AI tools, such as transcription tools, is part of a broader government strategy to improve the efficiency and quality of public services, as outlined in the government's AI Opportunities Action Plan.[64]\n\nThe UK government has demonstrated sustained interest in the potential for AI transcription tools to create impactful transformation. Multiple AI transcription tools were included in the government's AI Exemplars Programme in August 2025.[65] The potential for transcription tools is focused on frontline public services, where it is important to maintain a written record of interactions between service providers and service users.\n\nOfficial documentation helps ensure that public services provide people with the support they need, that correct procedures are followed and that people experience consistent care over time. Effective documentation can also help individuals who draw on public services understand how frontline workers made decisions and act as a resource for understanding interactions with public services that impacted their lives, as well as supporting redress when things go wrong.[66]\n\nHowever, producing official documentation can be time consuming for frontline workers who often have a significant backlog of people who need to draw on their support. AI transcription tools could potentially allow frontline workers to provide essential services more efficiently by reducing the amount of time they spend documenting interactions.\n\nIn this context, AI transcription tools are being adopted or recommended for several public services, notably social care, healthcare and the justice system. All three sectors are experiencing acute resourcing challenges. In 2023-2024, only 31 per cent of requests for local authority-funded adult social care resulted in support being given.[67] In healthcare, the median waiting time for patients in England to start treatment was 13.4 weeks in June 2025, and192,000 people had waited over a year for treatment.[68] In the justice sector, backlogs have grown significantly in recent years, to the point where the Crown Court of England and Wales had around 75,000 outstanding cases at the end of 2024.[69]\n\nSocial workers spend most of their working time completing documentation processes and recording interactions with people who draw on their care.[70] As a result, staff in both adult and children's social care have begun to use AI transcription tools to record conversations and automatically generate official documentation such as care plans (a personalised document outlining the specific support a person or family requires for their wellbeing).\n\nAI transcription tools have been adopted rapidly in social care because the sector faces significant resource challenges. Increasing demand for social care, coupled with local authority budget cuts, means that people who need care are often unable to receive it from their local authority.[71] Resource shortages and growing workloads are also impacting the wellbeing of social workers themselves, with social workers reporting high levels of stress and concern about their ability to adequately support people who draw on their care.[72]\n\nMultiple local authorities are adopting AI transcription tools for social care contexts, but there is no nationally representative data to understand how many social workers are using this technology across the UK. Local authorities can procure AI transcription tools from independent developers or develop their own AI transcription tools. Incumbent providers of social care data management platforms have also started to develop their own AI transcription tools that they provide to their existing customers.\n\nThe Ada Lovelace Institute is focusing on the application of AI transcription tools in social care contexts through our 'Transcribing trust' research project, which aims to understand how these tools impact social care practice.\n\nResearchers have estimated that over 100 companies provide AI transcription tools for healthcare applications worldwide.[73] The UK government is beginning to recommend the use of transcription tools in meetings between healthcare professionals and patients to reduce backlogs and cost pressures and allow frontline staff to spend more time with patients.[74]\n\nThe UK's leading digital healthcare platform launched its own AI transcription tool in April 2025 and provides this as part of its platform services to 98 per cent of GP practices in England, with plans to extend the rollout to hospital and mental health settings.[75]\n\nGiven the range of transcription tools available for healthcare practitioners, NHS England has published a set of standards to support clinicians in using these tools safely, detailing recommended technical features and the ongoing monitoring required.[76] NHS England's current position is that 'ambient scribing products' with a summarisation capability should be registered as a medical device under the Medicines and Healthcare products Regulatory Authority (MHRA) Class I device status.[77]\n\nLiability for using an AI transcription tool that does not comply with guidance and regulation rests with individual users or the organisations (such as general practitioners or NHS trusts) deploying the tool.[78]\n\nSimilarly, the UK government is trialling the use of AI transcription tools in the justice system, to reduce the amount of time that frontline staff spend documenting interactions. This is part of a broader strategy to integrate AI into the justice system, as outlined in the AI Action Plan for Justice.[79]\n\nAt the time of writing, AI transcription tools have been piloted in multiple parts of the justice system. The government is conducting pilots of AI transcription tools in probation services in Kent, Surrey, Sussex and Wales.[80] The government plans to roll out its Justice Transcribe tool to all 12,000 probation officers after it has been piloted.[81]\n\nPolice forces have also used AI transcription tools. Hertfordshire Constabulary piloted a tool for automatically generating witness statements in domestic abuse cases based on interviews conducted by police officers.[82] Outside of the UK, AI transcription tools have also been used to generate police reports based on audio recordings from police body cameras.[83]\n\nThe use of AI transcription tools in public sector contexts creates additional challenges. While we do not examine these comprehensively in this explainer, we have highlighted two significant challenges below.\n\nPublic sector bodies may lack sufficient expertise and resources to adequately evaluate AI tools. Our previous research has shown that technological uncertainty about how AI works and the outsized market power of AI developers are impacting how well local authorities are able to procure and evaluate AI tools.[84] This is further exacerbated by the fact that specialist evaluators and engineers can command higher salaries in the private sector and might therefore be unwilling to work for the public sector.[85]\n\nFurthermore, the guidance and legislation surrounding the procurement and evaluation of AI tools in the public sector is difficult to implement in practice. There are multiple definitions and measures of social benefit that public bodies use when evaluating AI tools, but current guidance lacks specificity about where particular definitions are more appropriate and how to operationalise them.[86]\n\nOfficial guidance also uses multiple definitions of AI and key AI-related concepts like 'fairness' and 'transparency'.[87] The resulting lack of clarity often forces public bodies that lack expertise in AI to fill in the gaps on their own.[88]\n\nThese ecosystem-level difficulties in conducting effective evaluations pose a significant challenge to the fair and transparent use of AI transcription tools in the public sector.\n\nThe public sector is subject to higher standards of scrutiny than the private sector. Public officials have a commitment to accountability and fairness through legislation, such as the Public Sector Equality Duty[89] and the guiding values of public service, such as the Seven Principles of Public Life.[90]\n\nHowever, AI tools present challenges to these expectations because they can significantly influence how official documents reflect the experiences of the people that they pertain to. The summarisation of people's experiences to create official records requires professionals to sort, select and prioritise information about a person for the context in question, from children in social care to victims of crime.[91] When AI transcription tools make these decisions, it reduces the control of professionals over the official records they create.\n\nThis raises challenges relating to accountability if frontline professionals are held responsible for decisions over which they had limited control. It will also make it challenging for people who use public services to seek redress when things go wrong.\n\nAdditionally, the way AI transcription tools present information will often be biased (see the previous section 'What are AI transcription tools?'). Biases in official documentation processes can perpetuate inequalities in people's experience of public services in significant ways. For example, one study of the use of LLMs to summarise long-term care records from an English local authority found that some models consistently downplayed women's health issues and needs compared to those of men.[92]\n\nThis explainer aims to provide policymakers and those working in the public sector with a practical understanding of AI transcription tools by describing their technical features, examining their use in frontline public services and synthesising the current approaches to evaluation.\n\nWe have identified that AI transcription tools could be useful for public sector workers, in the face of resource challenges and demanding workloads, by reducing the burden of administrative work that they must complete. However, these tools pose the same risks as any other technology powered by foundation models.\n\nCurrently, there is not enough evidence on how these potential risks and benefits are materialising in the diverse contexts in which AI transcription tools are being deployed. As a result, the UK public sector could benefit from additional evaluations of the use of AI transcription tools, through pilots or other interventions. Metrics like accuracy and time savings are popular in evaluations but should be supported with wider investigations to understand whether anticipated benefits of transcription tools are being realised and that potential harms are avoided.\n\nMoreover, public sector agencies would benefit from the collation and synthesis of empirical evidence from the deployment of AI transcription tools in multiple contexts over longer periods of time. Siloed evaluations over short time frames risk missing the systemic impacts of AI transcription tools that could negatively impact the experiences of people who draw on public services. Such interventions will better enable AI transcription tools to create meaningful value in public sector contexts.\n\nThis explainer was co-authored by Oliver Bruff and Lara Groves, with substantive contributions from Mavis Machirori and Catherine Gregory.\n\n[1] Tommaso Spinelli, Senior Artificial Intelligence Change Manager, and GDS, 'Launching the Artificial Intelligence Playbook for the UK Government - Government Digital Service' (10 February 2025) <https://gds.blog.gov.uk/2025/02/10/launching-the-artificial-intelligence-playbook-for-the-uk-govern... accessed 29 September 2025.\n\n[2] 'AI Opportunities Action Plan' (GOV.UK) <https://www.gov.uk/government/publications/ai-opportunities-action-plan/ai-opportunities-action-pla... accessed 19 August 2025.\n\n[3] 'Use of AI Rising among Social Workers, Poll Finds' (Community Care, June 2025) <https://www.communitycare.co.uk/2025/06/12/use-of-ai-rising-among-social-workers-poll-finds/> accessed 29 September 2025.\n\n[4] '10 Year Health Plan for England: Fit for the Future' (GOV.UK, 30 July 2025) <https://www.gov.uk/government/publications/10-year-health-plan-for-england-fit-for-the-future> accessed 29 September 2025.\n\n[5] 'AI Action Plan for Justice' (GOV.UK) <https://www.gov.uk/government/publications/ai-action-plan-for-justice> accessed 29 September 2025.\n\n[6] 'AI Exemplars Programme' (GOV.UK) <https://www.gov.uk/guidance/ai-exemplars-programme> accessed 29 September 2025.\n\n[7] Elliot Jones, 'What Is a Foundation Model?' (Ada Lovelace Institute, 2023) <https://www.adalovelaceinstitute.org/resource/foundation-models-explainer/> accessed 31 July 2025.\n\n[8] 'AI-Enabled Ambient Scribing Products in Health and Care Settings' (NHS England) <https://www.england.nhs.uk/long-read/ai-enabled-ambient-scribing-products-in-health-and-care-settings/> accessed 29 September 2025.\n\n[9] Elliot Jones, 'What Is a Foundation Model?' (Ada Lovelace Institute, 2023) <https://www.adalovelaceinstitute.org/resource/foundation-models-explainer/> accessed 31 July 2025.\n\n[10] Rohit Prabhavalkar and others, 'End-to-End Speech Recognition: A Survey' (arXiv, 3 March 2023) <http://arxiv.org/abs/2303.03329> accessed 10 July 2025.\n\n[11] Hanin Atwany and others, 'Lost in Transcription, Found in Distribution Shift: Demystifying Hallucination in Speech Foundation Models' (arXiv, 18 February 2025) <http://arxiv.org/abs/2502.12414> accessed 24 April 2025.\n\n[12] Elliot Jones, 'What Is a Foundation Model?' (Ada Lovelace Institute, July 2023) <https://www.adalovelaceinstitute.org/resource/foundation-models-explainer/> accessed 31 July 2025.\n\n[13] Laura Weidinger and others, 'Ethical and Social Risks of Harm from Language Models' (arXiv, 8 December 2021) <http://arxiv.org/abs/2112.04359> accessed 13 August 2025.\n\n[14] Merlin Stein and Connor Dunlop, 'Safe before Sale' (Ada Lovelace Institute<https://www.adalovelaceinstitute.org/report/safe-before-sale/> accessed 4 September 2025.\n\n[15] Allison Koenecke and others, 'Careless Whisper: Speech-to-Text Hallucination Harms' (The 2024 ACM Conference on Fairness, Accountability, and Transparency, 2024) <http://arxiv.org/abs/2402.08021> accessed 24 April 2025.\n\n[16] Laura Weidinger and others, 'Ethical and Social Risks of Harm from Language Models' (arXiv, 8 December 2021) <http://arxiv.org/abs/2112.04359> accessed 13 August 2025.\n\n[17] Jiarui Li, Ye Yuan and Zehua Zhang, 'Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases' (arXiv, 15 March 2024) <http://arxiv.org/abs/2403.10446> accessed 28 January 2026.\n\n[18] Laura Carter and others, 'Critical Analytics? Learning from the Early Adoption of Data Analytics for Local Authority Service Delivery' (Ada Lovelace Institute, 2023) <https://www.adalovelaceinstitute.org/report/local-authority-data-analytics/> accessed 10 September 2025.\n\n[19] Ida Lindgren, 'Ironies of Automation and Their Implications for Public Service Automation' (2024) 41 Government Information Quarterly 101974 <https://www.sciencedirect.com/science/article/pii/S0740624X24000662> accessed 13 August 2025.\n\n[20] Dr Paul Walley and Dr Helen Glasspoole-Bird, 'An Evaluation of the Pilot Application of Artificial Intelligence to Witness Statement and Report Generation at Hertfordshire Constabulary' (2025) <https://university.open.ac.uk/centres/policing/research/digitally-enabled-policing/345-herts-ai-test-and-learn> accessed 3 September 2025.\n\n[21] 'AI Knowledge Hub' (GOV.UK) <https://ai.gov.uk/knowledge-hub/> accessed 21 August 2025.\n\n[22] Simon Guerrier, '\"Magic Notes\" AI Tool Saves Social Workers Time on Admin' (Social Care Today, 2024) <https://socialcare.today/2024/09/26/magic-notes-ai-tool-saves-social-workers-time-on-admin/> accessed 21 August 2025.\n\n[23] 'Fit for the Future: 10 Year Health Plan for England' (GOV.UK) <https://assets.publishing.service.gov.uk/media/6888a0996478525675738f3a/fit-for-the-future-10-year-health-plan-for-england-executive-summary.pdf> accessed 19 August 2025.\n\n[24] 'AI Action Plan for Justice' (GOV.UK) <https://www.gov.uk/government/publications/ai-action-plan-for-justice/ai-action-plan-for-justice> accessed 19 August 2025.\n\n[25] 'BASW Statement on Social Work and Generative Artificial' (British Association of Social Workers, 21 March 2025) <https://basw.co.uk/policy-and-practice/resources/basw-statement-social-work-and-generative-artificial-intelligence> accessed 19 August 2025.\n\n[26] 'Neurodiversity at Work: The Power of AI-Driven (The Access Group) <https://www.theaccessgroup.com/en-gb/blog/neurodiversity-at-work-the-power-of-ai-driven-support/> accessed 20 August 2025.\n\n[27] Elliot Jones, 'What Is a Foundation Model?' (Ada Lovelace Institute, 2023) <https://www.adalovelaceinstitute.org/resource/foundation-models-explainer/> accessed 31 July 2025\n\n[28] Ziwei Xu, Sanjay Jain and Mohan Kankanhalli, 'Hallucination Is Inevitable: An Innate Limitation of Large Language Models' (arXiv, 13 February 2025) <http://arxiv.org/abs/2401.11817> accessed 29 September 2025.\n\n[29] Allison Koenecke and others, 'Careless Whisper: Speech-to-Text Hallucination Harms', The 2024 ACM Conference on Fairness, Accountability, and Transparency (2024) <http://arxiv.org/abs/2402.08021> accessed 24 April 2025.\n\n[30] Ajinkya Kulkarni and others, 'Unveiling Biases While Embracing Sustainability: Assessing the Dual Challenges of Automatic Speech Recognition Systems', Interspeech 2024 (ISCA 2024) <https://www.isca-archive.org/interspeech_2024/kulkarni24_interspeech.html> accessed 6 February 2025; Li-Fang Lai and Nicole Holliday, 'Exploring Sources of Racial Bias in Automatic Speech Recognition through the Lens of Rhythmic Variation', INTERSPEECH 2023 (ISCA 2023) <https://www.isca-archive.org/interspeech_2023/lai23_interspeech.html> accessed 11 June 2025.\n\n[31] Allison Koenecke and others, 'Careless Whisper: Speech-to-Text Hallucination Harms', The 2024 ACM Conference on Fairness, Accountability, and Transparency (2024) <http://arxiv.org/abs/2402.08021> accessed 24 April 2025.\n\n[32] Elliot Jones, 'Foundation Models in the Public Sector' (Ada Lovelace Institute 2023) <https://www.adalovelaceinstitute.org/evidence-review/foundation-models-public-sector/> accessed 13 August 2025.\n\n[33] Sam Rickman, 'Evaluating Gender Bias in Large Language Models in Long-Term Care' (2025) 25 BMC Medical Informatics and Decision Making 274 <https://doi.org/10.1186/s12911-025-03118-0> accessed 21 August 2025.\n\n[34] 'ACLU White Paper on Police Departments' Use of AI to Draft Police Reports' (American Civil Liberties Union) <https://www.aclu.org/documents/aclu-on-police-departments-use-of-ai-to-draft-police-reports> accessed 21 July 2025.\n\n[35] Beatrice Nolan, 'UK Health Service AI Tool Generated a Set of False Diagnoses for a Patient' (Fortune) <https://fortune.com/2025/07/20/uk-health-service-ai-tool-false-diagnoses-patient-screening-nhs-anima-health-annie/> accessed 24 July 2025.\n\n[36] Allison Koenecke and others, 'Careless Whisper: Speech-to-Text Hallucination Harms', The 2024 ACM Conference on Fairness, Accountability, and Transparency (2024) <http://arxiv.org/abs/2402.08021> accessed 24 April 2025\n\n[37] Isabel Barberá, 'AI Privacy Risks & Mitigations Large Language Models (LLMs)' (EDPB, 2025) <https://www.edpb.europa.eu/system/files/2025-04/ai-privacy-risks-and-mitigations-in-llms.pdf> accessed 3 September 2025.\n\n[38] Elliot Jones, 'Foundation Models in the Public Sector' (Ada Lovelace Institute 2023) <https://www.adalovelaceinstitute.org/evidence-review/foundation-models-public-sector/> accessed 13 August 2025.\n\n[39] Thomas Dratsch and others, 'Automation Bias in Mammography: The Impact of Artificial Intelligence BI-RADS Suggestions on Reader Performance' (2023) 307 Radiology e222176 <https://pubs.rsna.org/doi/10.1148/radiol.222176> accessed 29 September 2025.\n\n[40] 'Guidance on the Use of AI-Enabled Ambient Scribing Products in Health and Care Settings' (NHS England) <https://www.england.nhs.uk/long-read/guidance-on-the-use-of-ai-enabled-ambient-scribing-products-in-health-and-care-settings/> accessed 28 April 2025.\n\n[41] Dr Paul Walley and Dr Helen Glasspoole-Bird, 'An Evaluation of the Pilot Application of Artificial Intelligence to Witness Statement and Report Generation at Hertfordshire Constabulary' (2025) <https://university.open.ac.uk/centres/policing/research/digitally-enabled-policing/345-herts-ai-test-and-learn> accessed 3 September 2025..\n\n[42] Rebecca O'Keefe, 'Case Recording in Child Protection: An Exploration of the Evidence Base and Good Practice' (2024) 33 Child Abuse Review e2894 <https://onlinelibrary.wiley.com/doi/abs/10.1002/car.2894> accessed 3 September 2025.\n\n[43] Tekendra Parmar, 'Government Documents Show Police Disabling AI Oversight Tools' (Mother Jones) <https://www.motherjones.com/criminal-justice/2025/08/axon-police-ai-draft-one-foia/> accessed 29 September 2025.\n\n[44] Sam Rickman, 'Evaluating Gender Bias in Large Language Models in Long-Term Care' (2025) 25 BMC Medical Informatics and Decision Making 274 <https://doi.org/10.1186/s12911-025-03118-0> accessed 21 August 2025.\n\n[45] Sofia Morandini and others 'The Impact of Artificial Intelligence on Workers' Skills: Upskilling and Reskilling in Organisations' (2023) 26 Informing Science: The International Journal of an Emerging Transdiscipline <https://www.informingscience.org/Publications/5078> accessed 5 September 2025 .\n\n[46] Sofia Morandini and others 'The Impact of Artificial Intelligence on Workers' Skills: Upskilling and Reskilling in Organisations' (2023) 26 Informing Science: The International Journal of an Emerging Transdiscipline <https://www.informingscience.org/Publications/5078> accessed 5 September 2025.\n\n[47] Elliot Jones, Mahi Hardalupas and William Agnew, 'Under the Radar?' (Ada Lovelace Institute 2024) <https://www.adalovelaceinstitute.org/report/under-the-radar/> accessed 4 March 2025\n\n[48] Evani Radiya-Dixit, 'Adopting More Holistic Approaches to Assess the Impacts of AI Systems' (Center for Democracy and Technology, 2025) <https://cdt.org/insights/adopting-more-holistic-approaches-to-assess-the-impacts-of-ai-systems/> accessed 29 September 2025.\n\n[49] 'Exploring Different Objectives in Non-Inferiority <https://www.bmj.com/content/385/bmj-2023-078000> accessed 29 September 2025.\n\n[50] Introducing the Careful Consequence Check' (Careful Industries, 2025) <https://www.careful.industries/blog/2025-11-introducing-the-careful-consequence-check> accessed 25 November 2025.\n\n[51] Katelyn Xiaoying Mei and others, 'Addressing Pitfalls in Auditing Practices of Automatic Speech Recognition Technologies: A Case Study of People with Aphasia' (arXiv, 11 July 2025) <http://arxiv.org/abs/2506.08846> accessed 16 July 2025.\n\n[52] Anna Smerdiagina, 'Lost in Transcription: Experimental Findings on Ethnic and Age Biases in AI Systems' (2024) 9 Junior Management Science (JUMS) 1591 <https://www.econstor.eu/handle/10419/305308> accessed 29 September 2025.\n\n[53] Aymeric Roucher, 'Using LLM-as-a-Judge🧑⚖️ for an Automated and Versatile Evaluation: Hugging Face Open-Source AI Cookbook' (Hugging Face) <https://huggingface.co/learn/cookbook/en/llm_judge> accessed 8 January 2026.\n\n[54] Hugging Face, 'WER: A Hugging Face Space by Evaluate-Metric' <https://huggingface.co/spaces/evaluate-metric/wer> accessed 29 September 2025.\n\n[55] Petrus te Braak and others, 'Data Quality and Recall Bias in Time-Diary Research: The Effects of Prolonged Recall Periods in Self-Administered Online Time-Use Surveys' (2023) 53 Sociological Methodology 115 <https://doi.org/10.1177/00811750221126499> accessed 28 January 2026.\n\n[56] Anastasia Koutsounia, 'AI Tool Improves Direct Work in Adult Social Care despite Accuracy Concerns, Practitioners Report (Community Care, 2025) <https://www.communitycare.co.uk/2025/02/10/ai-tool-adult-social-care-accuracy-issues-practitioners-report/> accessed 29 September 2025.\n\n[57] 'Kingston Council: Using AI in Adult Social Care Administration' (Local Government Association) <https://www.local.gov.uk/case-studies/kingston-council-using-ai-adult-social-care-administration> accessed 29 September 2025.\n\n[58] 'The Evaluation of the M365 Copilot Pilot in the Department for Business and Trade' (GOV.UK) <https://assets.publishing.service.gov.uk/media/68adbe409e1cebdd2c96a19d/dbt-microsoft-365-copilot-evaluation.pdf> accessed 21 October 2025\n\n[59] 'An Evaluation of DWP's Microsoft 365 Copilot Trial' (GOV.UK) <https://www.gov.uk/government/publications/an-evaluation-of-dwps-microsoft-copilot-365-trial/an-evaluation-of-dwps-microsoft-365-copilot-trial> accessed 3 February 2026.\n\n[60] The Decision Lab - Behavioral Science, Applied.' (The Decision Lab) <https://thedecisionlab.com/reference-guide/psychology/social-desirability-bias> accessed 3 February 2026.\n\n[61] Dr Paul Walley and Dr Helen Glasspoole-Bird, 'An Evaluation of the Pilot Application of Artificial Intelligence to Witness Statement and Report Generation at Hertfordshire Constabulary' (2025) <https://university.open.ac.uk/centres/policing/research/digitally-enabled-policing/345-herts-ai-test-and-learn> accessed 3 September 2025.\n\n[62] Pranay Jindal and Joy C MacDermid, 'Assessing Reading Levels of Health Information: Uses and Limitations of Flesch Formula' (2017) 30 Education for Health (Abingdon, England) 84.\n\n[63] Dr Paul Walley and Dr Helen Glasspoole-Bird, 'An Evaluation of the Pilot Application of Artificial Intelligence to Witness Statement and Report Generation at Hertfordshire Constabulary' (The Open University, 2025) <https://university.open.ac.uk/centres/policing/research/digitally-enabled-policing/345-herts-ai-test-and-learn> accessed 3 September 2025.\n\n[64]'AI Opportunities Action Plan' (GOV.UK) <https://www.gov.uk/government/publications/ai-opportunities-action-plan/ai-opportunities-action-plan> accessed 19 August 2025\n\n[65] 'AI Exemplars Programme' (GOV.UK) <https://www.gov.uk/guidance/ai-exemplars-programme> accessed 29 September 2025.\n\n[66] Victoria Hoyle and others, 'Child Social-Care Recording and the Information Rights of Care-Experienced People: A Recordkeeping Perspective' (2019) 49 The British Journal of Social Work 1856 <https://doi.org/10.1093/bjsw/bcy115> accessed 21 August 2025.\n\n[67] 'Key facts and figures about adult social care' (The King's Fund) <https://www.kingsfund.org.uk/insight-and-analysis/data-and-charts/key-facts-figures-adult-social-care> accessed 21 October 2025\n\n[68] 'NHS Backlog Data Analysis' (The British Medical Association) <https://www.bma.org.uk/advice-and-support/nhs-delivery-and-workforce/pressures/nhs-backlog-data-analysis> accessed 1 September 2025.\n\n[69] Magdalena Domínguez, Joe Tomlinson and Ben Zaranko, 'Crown Court Backlog Exacerbated by Post-Pandemic Productivity Slump' (Institute for Fiscal Studies, 6 June 2025) <https://ifs.org.uk/news/crown-court-backlog-exacerbated-post-pandemic-productivity-slump> accessed 4 September 2025.\n\n[70] '80-20 Campaign: How much 'direct' time do social workers spend with children and families?' (British Association of Social Workers) <https://basw.co.uk/sites/default/files/2023-08/FINAL%2080-20%20report.pdf> accessed 21 October 2025\n\n[71] 'Adult Social Care: Key Facts And Figures' (The King's Fund) <https://www.kingsfund.org.uk/insight-and-analysis/data-and-charts/key-facts-figures-adult-social-care> accessed 1 September 2025.\n\n[72] UNISON, 'Social Work and the Impact of the Covid Pandemic' (2022) <https://www.unison.org.uk/content/uploads/2022/06/26799-social-work-survey-FULL-final.pdf> accessed 1 September 2025.\n\n[73] Abi Eccles and others, 'Unintended Consequences of Using Ambient Scribes in General Practice' (2025) 390 BMJ e085754 <https://www.bmj.com/content/390/bmj-2025-085754> accessed 28 August 2025.\n\n[74] 'Fit for the Future: 10 Year Health Plan for England' (GOV.UK) <https://assets.publishing.service.gov.uk/media/6888a0996478525675738f3a/fit-for-the-future-10-year-health-plan-for-england-executive-summary.pdf> accessed 19 August 2025.\n\n[75] Jordan Sollof, 'Accurx and Tandem Health Roll Out AI Scribing Tool across the NHS' (Digital Health, 29 April 2025) <https://www.digitalhealth.net/2025/04/accurx-and-tandem-health-partner-to-roll-out-ai-scribing-in-the-nhs/> accessed 28 August 2025.\n\n[76] 'AI-Enabled Ambient Scribing Products in Health and Care Settings' (NHS England, 2025) <https://www.england.nhs.uk/long-read/ai-enabled-ambient-scribing-products-in-health-and-care-settings/> accessed 21 October 2025.\n\n[77] Russell Brown, 'AI Ambient Scribing - NHS England Warning and Provisional LMC Guidance' (Surrey and Sussex LMCS, 24 September 2025) <https://www.sslmcs.co.uk/news/ai-ambient-scribing/> accessed 29 September 2025.\n\n[78] ETHOS, 'New AI Voice Technology Rules: Is Your NHS Organisation Compliant?' (ETHOS, 27 June 2025) <https://ethos.co.im/new-ai-voice-technology-rules-is-your-nhs-organisation-compliant/> accessed 29 September 2025.\n\n[79] 'AI Action Plan for Justice' (GOV.UK) <https://www.gov.uk/government/publications/ai-action-plan-for-justice/ai-action-plan-for-justice> accessed 19 August 2022.\n\n[80] 'AI Action Plan for Justice' (GOV.UK) <https://www.gov.uk/government/publications/ai-action-plan-for-justice/ai-action-plan-for-justice> accessed 19 August 2022.\n\n[81] 'AI to Cut Paperwork to Free up Doctors' Time for Patients' (GOV.UK) <https://www.gov.uk/government/news/ai-to-cut-paperwork-to-free-up-doctors-time-for-patients> accessed 3 September 2025.\n\n[82] Dr Paul Walley and Dr Helen Glasspoole-Bird, 'An Evaluation of the Pilot Application of Artificial Intelligence to Witness Statement and Report Generation at Hertfordshire Constabulary' (The Open University, 2025) <https://university.open.ac.uk/centres/policing/research/digitally-enabled-policing/345-herts-ai-test-and-learn> accessed 3 September 2025.\n\n[83] 'ACLU White Paper on Police Departments' Use of AI to Draft Police Reports' (American Civil Liberties Union) <https://www.aclu.org/documents/aclu-on-police-departments-use-of-ai-to-draft-police-reports> accessed 21 July 2025.\n\n[84] Mavis Machirori and Anna Studman, 'Spending Wisely' (Ada Lovelace Institute, 2024) <https://www.adalovelaceinstitute.org/report/spending-wisely-procurement/> accessed 4 March 2025.\n\n[85] Elliot Jones and others, 'Under the Radar?' <https://www.adalovelaceinstitute.org/report/under-the-radar/> accessed 28 April 2025.\n\n[86] Mavis Machirori and Anna Studman, 'Buying AI - Is the Public Sector Equipped to Procure Technology in the Public Interest?' (Ada Lovelace Institute, 2024) <https://www.adalovelaceinstitute.org/wp-content/uploads/2024/09/Ada-Lovelace-Institute-Buying-AI.pdf> accessed 21 October 2025.\n\n[87] Mavis Machirori and Anna Studman, 'Buying AI - Is the Public Sector Equipped to Procure Technology in the Public Interest?' (Ada Lovelace Institute, 2024) <https://www.adalovelaceinstitute.org/wp-content/uploads/2024/09/Ada-Lovelace-Institute-Buying-AI.pdf> accessed 21 October 2025.\n\n[88] Mavis Machirori and Anna Studman, 'Spending Wisely' (Ada Lovelace Institute, 2024) <https://www.adalovelaceinstitute.org/report/spending-wisely-procurement/> accessed 4 March 2025.\n\n[89] 'Public Sector Equality Duty' (Equality and Human Rights Commission) <https://www.equalityhumanrights.com/en/advice-and-guidance/public-sector-equality-duty> accessed 2 September 2025.\n\n[90] 'The Seven Principles of Public Life' (GOV.UK) <https://www.gov.uk/government/publications/the-7-principles-of-public-life/the-7-principles-of-public-life-2> accessed 2 September 2025.\n\n[91] Stella M Sieling-Monas and Marie L Meilvang, 'The Practice of Social Work Documentation in an Age of Automatization: A Case from a Danish Municipal Job Centre' (2025) 55 The British Journal of Social Work 2482 <https://academic.oup.com/bjsw/article/55/5/2482/8104249> accessed 1 September 2025.\n\n[92] Sam Rickman, 'Evaluating Gender Bias in Large Language Models in Long-Term Care' (2025) 25 BMC Medical Informatics and Decision Making 274 <https://doi.org/10.1186/s12911-025-03118-0> accessed 21 August 2025."
  },
  {
    "source": "ICTjournal - Le magazine suisse des technologies de l’information pour l’entreprise",
    "company": "Hugging Face",
    "title": "Des cybercriminels diffusent un malware Android via Hugging Face",
    "date": "2026-02-04T15:36:46Z",
    "url": "https://www.ictjournal.ch/news/2026-02-04/des-cybercriminels-diffusent-un-malware-android-via-hugging-face",
    "content": "Hugging Face, plateforme de partage de modèles de langage, de données d'entraînement et d'applications d'intelligence artificielle accessibles au téléchargement et au développement collaboratif, a récemment été exploitée par des acteurs malveillants pour diffuser des applications Android infectées, rapporte le site spécialisé Bleeping Computer, citant une analyse de l'éditeur de cybersécurité Bitdefender.\n\nAfin de dissimuler leurs activités le plus longtemps possible, les pirates utilisent une stratégie en plusieurs étapes. Ils commencent par proposer à leurs futures victimes un prétendu outil de sécurité. Pour inciter au téléchargement, ils diffusent de faux avertissements faisant état de failles de sécurité inventées et affirment que leur logiciel permet d'y remédier.\n\nDans un premier temps, l'application ne contient aucun logiciel malveillant connu. Elle n'est donc pas détectée comme dangereuse par les solutions de sécurité légitimes.\n\nMais la situation évolue rapidement. Selon l'analyse, l'application, toujours présentée comme un outil de sécurité, obtient d'abord des droits d'accès étendus sur l'appareil Android ciblé, avant de signaler qu'une mise à jour serait nécessaire.\n\nDès que l'utilisateur lance la prétendue mise à jour, Hugging Face entre en jeu. L'application télécharge alors les composants malveillants qui manquaient lors du téléchargement initial. Pour éviter une détection trop rapide par les solutions de sécurité, les cybercriminels font en sorte que la charge utile varie légèrement toutes les 15 minutes, souligne Bitdefender. Les attaquants cherchent également à dissimuler l'origine du code malveillant. L'application initiale ne mentionne jamais la plateforme: elle contacte d'abord un serveur intermédiaire, qui relaie ensuite la requête vers Hugging Face.\n\nBitdefender précise que la campagne de malware observée a disparu pendant l'analyse, avant de réapparaître peu après sous un autre nom et avec une nouvelle apparence, mais en conservant le même code malveillant.\n\nBleeping Computer rappelle enfin que Hugging Face reste, de manière générale, une plateforme digne de confiance. Elle a toutefois déjà été exploitée par le passé à des fins malveillantes: des cybercriminels avaient alors réussi à y introduire des modèles d'IA infectés par des malwares."
  },
  {
    "source": "PhonAndroid",
    "company": "Hugging Face",
    "title": "Les hackers changent de QG : votre plateforme collaborative préférée est-elle devenue leur nouveau repaire ?",
    "date": "2026-02-02T12:50:11Z",
    "url": "https://www.phonandroid.com/les-hackers-changent-de-qg-votre-plateforme-collaborative-preferee-est-elle-devenue-leur-nouveau-repaire.html",
    "content": "Les hackers ne se contentent plus de diffuser leurs malwares via des applications hébergées sur des boutiques légitimes - comme le Google Play Store. Aujourd'hui, même votre plateforme collaborative préférée peut être le théâtre d'une nouvelle campagne malveillante. Voici le cas Hugging Face - et comment se protéger.\n\nConstamment, les pirates peaufinent leur stratégie de nuisance - et ils la musclent, notamment grâce à l'IA. Ils s'adaptent aux périodes charnières de l'année : essor des arnaques au colis pendant les fêtes de fin d'année, recrudescence des arnaques aux faux recrutements en début d'année... Mais il semble surtout que la confiance et le sentiment d'urgence soient désormais deux de leurs piliers.\n\nAujourd'hui, les utilisateurs ne sont plus à l'abri : se tenir loin des sites obscurs ne suffit plus à se protéger des attaques. En effet, les magasins d'applications légitimes ne sont plus gage de sécurité, les géants de la tech ne parvenant pas à suivre le rythme effréné auquel se développent les applications et extensions malveillantes. Mais ce n'est pas tout, les attaquants détournent également des plateformes de confiance.\n\nLire aussi - Vous avez dit \" Truman Show \" ? Cette arnaque redoutable dopée à l'IA crée une réalité factice pour vous dépouiller\n\nC'est notamment le cas de Hugging Face, selon un nouveau rapport réalisé par des chercheurs en cybersécurité relayé par le site SecureReading. Pour rappel, Hugging Face est une plateforme collaborative spécialisée dans l'IA qui jouit d'une bonne réputation. C'est pour cette raison qu'elle est la cible d'une nouvelle campagne de malware Android, qui a pour vocation le vol d'identifiants financiers. Voici le mode opératoire :\n\nVoici ses capacités d'espionnage : réalisation de captures d'écran en continu, interception des identifiants, vol des codes PIN, blocage des tentatives de désinstallation, affichage d'écrans de phishing imitant des applications financières, maintien d'une communication avec son serveur de commande et de contrôle (C2).\n\nCette campagne est sophistiquée et dangereuse à plusieurs égards. D'abord, les hackers optent pour une technique d'évasion avancée : le polymorphisme côté serveur. Icône, nom, charge malveillante sont constamment renouvelés pour éviter la détection - c'est la base de code malveillante qui demeure majoritairement la même.\n\nDe plus, l'attaque exploite les services d'accessibilité d'Android, permet l'exfiltration de données en temps réel, tout en maintenant l'illusion de légitimité de TrustBastion grâce à la diffusion de faux contenu. Enfin, les pirates trompent leurs cibles de diverses manières : ils jouent sur le sentiment de panique (annonces scareware), qu'ils renforcent en détournant une plateforme réputée (Hugging Face) - ce qui permet, de fait, de réduire la méfiance des victimes et de contourner certains contrôles de sécurité.\n\nC'est une nouvelle tendance qui se dessine : le recours, par les pirates, au détournement de nouvelles plateformes de confiance pour mener à bien leurs campagnes malveillantes - ce qui élargit, de fait, le champ des attaques. Mais il existe des moyens pour s'en prémunir :"
  },
  {
    "source": "m.163.com",
    "company": "Hugging Face",
    "title": "姚顺雨之后，又一95后天才加入腾讯/苹果AI团队再现离职潮/Clawdbot再改名，已破10万星_手机网易网",
    "date": "2026-02-01T18:51:41Z",
    "url": "https://m.163.com/dy/article/KKJE8M0F0511CSAO.html",
    "content": "姚顺雨之后，又一95后天才加入腾讯/苹果AI团队再现离职潮/Clawdbot再改名，已破10万星\n\n曝马斯克评估 SpaceX IPO 前与特斯拉或 xAI 合并\n\n苹果 AI 团队再现离职潮，多名研究员与 Siri 高层投向 Meta 与 Google\n\n庞天宇加盟腾讯混元，负责多模态强化学习方向\n\n♂️\n\n曝理想新设软件本体与人形机器人团队\n\nClawdbot 再改名：OpenClaw 确立开源与社区驱动定位\n\nGDC 报告：52% 游戏开发者认为生成式 AI 对行业有害\n\nKimi 海外收入首次超越国内，K2.5 推动全球用户激增\n\nAI 视频热潮消退，Sora App 下载量连续两月下滑\n\n千问 C 端应用团队四篇论文入选 ICLR 2026\n\n音乐出版商再诉 Anthropic：非法下载 2 万首歌曲\n\n宇树王兴兴：通用机器人十年可期，大模型是行业「诺奖级」突破\n\n生数科技推出 Vidu Q3，视频生成进入「一键成片」阶段\n\n商汤开源 SenseNova‑MARS，多模态推理性能超越 Gemini‑3‑Pro\n\n三星 Galaxy S26 系列发布时间曝光：2 月 25 日亮相\n\n周末也值得一看的新闻 苹果发布 2026 新春影片《碰见你》\n\n昨天，苹果 2026 新春影片《碰见你》正式上线。\n\n影片由平遥国际电影展获奖影片《过春天》导演白雪执导，以 iPhone 17 Pro 拍摄完成，并结合定格动画等混合媒介形式，呈现苹果在移动影像上的最新能力。\n\n《碰见你》讲述了一只在春节期间迷路、渴望回家的小狗，与一路给予帮助的女主角林微相伴前行的故事。影片以更贴近年轻人的叙事方式，聚焦人与动物之间的情感连接，并围绕「家」的意义展开。\n\n苹果 CEO 库克在微博表示：「《碰见你》是一段关于在意想不到的朋友陪伴下，重新找回自己的故事。这部 Apple 新春大片由 iPhone 17 Pro 拍摄，捕捉了许多温暖动人的瞬间，希望大家喜欢。」\n\n作为苹果首次采用混合媒介的新春影片，《碰见你》在影像上大量使用 iPhone 17 Pro 的新特性，包括等效 200mm 的 8 倍光学品质变焦、全系 4800 万像素融合式摄像头，以及 Center Stage 前置摄像头等功能。\n\n影片摄影团队表示，iPhone 的轻巧设计与高倍率变焦让拍摄更具灵活性，也为以动物视角取景提供了可能。\n\n自 2018 年起，苹果已连续九年在农历新年推出由当代旗舰 iPhone 拍摄的短片，包括《三分钟》《一个桶》《女儿》《阿年》《卷土重来》《过五关》《小蒜头》以及去年由 iPhone 16 Pro 拍摄的《想和你一起听听歌》。\n\n曝马斯克评估 SpaceX IPO 前与特斯拉或 xAI 合并\n\n据彭博社报道，马斯克正评估合并 SpaceX、特斯拉与 xAI 的可能性，相关讨论仍处于早期阶段，但已引发资本市场与行业的高度关注。\n\n多名知情人士称，SpaceX 正在研究两种主要路径：其一是与特斯拉合并，其二是在今年 IPO 前与 xAI 进行股份置换式合并。\n\n部分投资者正推动 SpaceX 与特斯拉的整合，而 SpaceX 与 xAI 的合并则被视为另一种可行方案。报道指出，若交易推进，可能吸引基础设施基金及中东主权财富基金参与，并需要配套大规模融资。\n\n报道称，从战略层面看，合并设想与马斯克的长期规划密切相关。\n\nSpaceX 正探索将数据中心部署至太空，为 AI 提供高密度算力；若工程可行，xAI 将直接受益于轨道算力布局，而特斯拉在能源存储系统方面的制造能力，也可能为太空数据中心提供太阳能供电支持。\n\n此前，马斯克还讨论过利用 SpaceX 星舰将特斯拉 Optimus 机器人运往月球或火星。\n\n市场方面，受合并传闻影响，特斯拉股价在周四盘后交易中一度上涨 4.5%。\n\n彭博社此前报道称，SpaceX 计划在今年 6 月左右上市，估值或达 1.5 万亿美元，融资规模最高可达 500 亿美元，或成为史上规模最大的 IPO。\n\n内华达州企业登记信息显示，本月 21 日已设立两家名称含有「merger sub」的法律实体，登记高管包括 SpaceX 首席财务官 Bret Johnsen，被视为潜在交易准备动作之一。\n\n路透社援引消息人士称，若 SpaceX 与 xAI 合并，部分 xAI 高管可能选择以现金而非 SpaceX 股票作为对价。\n\nSpaceX 去年已向 xAI 投资 20 亿美元，特斯拉本周也宣布向 xAI 投资约 20 亿美元，显示马斯克旗下企业间的资本流动正在加速。\n\n苹果 AI 团队再现离职潮，多名研究员与 Siri 高层投向 Meta 与 Google\n\n据彭博社报道，苹果近期再度出现 AI 人才流失，多名核心研究人员及一名负责 Siri 的高层主管在过去数周内相继离职，去向包括 Meta 与 Google DeepMind 等竞争对手。\n\n报道指出，最新离职的研究人员包括 Yinfei Yang、Haoxuan You、Bailin Wang 与 Zirui Wang。\n\n其中，Yang 创办了新公司，You 与 Bailin Wang 加入 Meta，分别投身该公司的 Superintelligence 研究部门与推荐系统团队；Zirui Wang 则转投 Google DeepMind。\n\n与此同时，曾负责 Siri 关键能力重建的高层主管 Stuart Bowers 也已离开苹果，加入 Google DeepMind。\n\n报道指出，这些离职发生在苹果去年大规模调整 AI 组织架构之后。\n\n当时，CEO Tim Cook 将长期负责 AI 的 John Giannandrea 调离核心岗位，由软件工程高级副总裁 Craig Federighi 接手整体 AI 战略，并引入前 Google 与微软高管 Amar Subramanya 负责部分组织。\n\n报道提到，苹果的 Foundation Models（AFM）团队在过去半年已流失十余名研究人员，该团队负责支撑 Apple Intelligence 的底层模型，并因 Siri 推出延期与功能反响平平而承受内部压力。\n\n值得注意的是，苹果今年计划推出两版 Siri：一版将利用个人数据提供更具上下文的回答；另一版则是更彻底的改造，采用类似聊天机器人的交互方式。\n\n两者均基于由 Google 团队协助开发的新架构模型。库克在财报电话会上回应为何选择 Google 时表示，这将为苹果提供「最强大的基础」，并强调双方合作有助于解锁更多体验。\n\n尽管苹果在最新财报中公布了超过 850 亿美元的 iPhone 销售额，但缺乏亮眼的 AI 进展与持续的人才流失，仍被视为影响其今年股价表现的重要因素。\n\n庞天宇加盟腾讯混元，负责多模态强化学习方向\n\n据凤凰网科技报道，清华大学计算机系博士、可信机器学习与生成式模型领域研究者庞天宇昨天确认加入腾讯混元团队，担任 Multimodal RL 方向的 Tech Lead，并将于 2 月 4 日正式入职。\n\n他将主要负责多模态模型的强化学习研究，前期重点聚焦生成模型方向，并在腾讯混元多模态部 Exploration Center 开展前沿算法探索。\n\n公开资料显示，庞天宇 1995 年出生，清华直博期间在机器学习鲁棒性、深度学习等方向取得多项成果，以第一作者（含共同一作）身份在 ICML、NeurIPS、ICLR 等顶会发表多篇论文，并多次入选 Oral 或 Spotlight。\n\n他曾获得微软学者奖学金、英伟达学术先锋奖等荣誉，毕业后在新加坡 Sea AI Lab 担任高级研究科学家。\n\n此次加入腾讯，被业内视为混元团队持续吸引原生 AI 人才的又一信号。今年以来，腾讯在大模型架构、组织结构和人才体系上持续调整。\n\n此前，前 OpenAI 高级研究员姚顺雨加入腾讯并担任首席 AI 科学家，负责 AI Infra 部及大语言模型部，推动混元体系的深度重构。\n\n马化腾在今年年会上也提到，混元团队在过去一年经历了「深度重构」，并加速了人才引入与内部协同。\n\n在产品层面，腾讯 AI 助手「元宝」从去年起持续迭代，今年春节档上线「派」功能，并宣布派发 10 亿红包。\n\n1 月 28 日，混元团队开源混元图像 3.0 图生图版本，并同步接入元宝。在最新 LMArena 图像编辑榜单中，该模型进入第一梯队。\n\n与此同时，MLNLP 社区也发布了庞天宇团队的招聘信息，面向 26 届、27 届校招及毕业三年内社招人才，重点寻找具备生成模型、diffusion models、RL infra、VLM agent 等方向经验的候选人。\n\n实习岗位则面向在读硕博生，要求具备较强工程能力或理论基础，并有顶会论文经历者优先。\n\n从更早的公开活动可见，庞天宇长期深耕可信机器学习方向，其在 2022 年的 TechBeat Talk 中曾提出「合理定义的鲁棒性与准确率之间不存在矛盾」，并提出自洽鲁棒性 SCORE 方法，为对抗训练与模型泛化提供新的解释框架。\n\n随着庞天宇正式加入，腾讯混元在多模态强化学习方向的研发力量进一步增强，也为其在开源模型、生成式 AI 应用及基础设施层面的竞争提供更多技术储备。\n\n曝理想新设软件本体与人形机器人团队\n\n据晚点 Auto 报道，理想汽车正在进行一轮覆盖软件、自动驾驶与前沿智能体方向的重大组织架构调整，进一步强化其在 AI 战略上的投入，并以更贴近先进 AI 公司的方式重组研发体系。\n\n理想已新设两个关键部门：软件本体团队与人形机器人团队。智能空间副总裁勾晓菲出任软件本体负责人；自动驾驶研发高级副总裁郎咸朋转任人形机器人负责人。\n\n与此同时，多数自动驾驶部门员工被划入由詹锟负责的基座模型团队，形成更集中、更模型化的研发结构。\n\n在本周一举行的全员线上战略宣贯会上，理想 CEO 李想强调，公司将「参照最先进 AI 公司的运作模式」，以协作构建硅基生命的方式重新划分技术团队，并明确提出将大幅调整研发架构。这次组织变动正是该战略的直接落地。\n\n勾晓菲长期负责理想智能座舱架构，深度参与理想 ONE 与 L 系列平台的系统设计；\n\n郎咸朋则是理想自动驾驶团队的第一号员工，主导从 NOA、端到端到 VLA 的多代技术演进，并推动 2024 年「端到端 + VLM」方案量产、2025 年 VLA 全量推送以及今年 OTA 8.2 强化版 VLA 司机大模型上线。\n\n詹锟自 2021 年加入理想后，专注自动驾驶架构与三代技术栈部署，去年起负责「VLA 模型」部门，如今将带领基座模型团队，承接更多自动驾驶员工与模型化研发任务。\n\n此外，原自动驾驶部门的 AI 评测与运营负责人湛逸飞、数据标注负责人凌琳均随郎咸朋转入人形机器人团队，两人均为理想自动驾驶早期核心成员，分别在地图、系统架构、数据标注等方向承担关键角色。\n\n报道认为，此次调整意味着理想正将自动驾驶、软件本体与人形机器人纳入统一的 AI 战略框架，通过模型化、体系化的组织方式加速技术迭代，并为未来的智能体产品线奠定基础。\n\nClawdbot 再改名：OpenClaw 确立开源与社区驱动定位\n\n据「AI 寒武纪」报道，昨天，开源个人 AI 助手项目 Clawdbot 正式宣布其最终名称为「OpenClaw」。团队表示，新名称旨在强化开源、开放与社区驱动的定位，同时继续保留龙虾作为项目吉祥物。\n\n项目最初于 2025 年 11 月以「Clawd」立项，随后因商标与法务因素短暂更名为「Moltbot」。在完成商标检索、域名购买与代码迁移后，团队最终确定使用「OpenClaw」作为正式名称。\n\nOpenClaw 运行在用户本地设备，通过 WhatsApp、Telegram、Discord、Slack、Teams 等常用聊天应用提供 AI 助手能力。\n\n与传统 SaaS 助手不同，用户数据不上传至第三方服务器，所有密钥与数据均保留在用户自有设备或服务器上。\n\n此次更新中，OpenClaw 新增对 Twitch 与 Google Chat 的支持，并扩展模型兼容范围，包括 Kimi K2.5 与小米 MiMo-V2-Flash。\n\n同时，Web 端界面现已支持图片发送。团队还进行了 30 余项安全相关提交，并发布可机器检查的安全模型，强调提示注入仍是行业未解难题。\n\n项目维护者 Peter Steinberger 表示，随着项目规模迅速扩大，团队正引入更多维护者并建立流程，以应对大量 PR 与 Issue，并探索为核心贡献者提供全职报酬的可行方式。\n\n未来，OpenClaw 将继续强化网关可靠性、扩展模型支持，并提升整体功能完善度。\n\nGDC 报告：52% 游戏开发者认为生成式 AI 对行业有害\n\n据 PC Gamer 报道，游戏开发者大会（GDC）近日发布的《2026 游戏行业现状》调查显示，超过半数的游戏开发者认为生成式 AI 正在对行业造成负面影响，这一比例在近两年内出现显著上升。\n\n调查覆盖了超过 2300 名游戏行业从业者。报告指出，今年有 52% 的受访者认为生成式 AI 对行业发展不利，而在去年这一比例为 30%，前年仅为 18%。与此同时，认为生成式 AI 有益的比例从去年的 13% 下滑至今年的 7%。\n\n尽管态度趋于负面，但开发者对生成式 AI 的使用比例并未出现明显变化。\n\n今年有 33% 的受访者表示在工作中使用生成式 AI，与 2021 年的 31% 基本持平。\n\n不过，52% 的受访者称其所在工作室正在使用生成式 AI 技术，主要集中在研讨灵感（81%）、办公文书（47%）、协助编程（47%）和原型设计（35%）等环节，只有 5% 的人表示会将其用于面向玩家的功能开发。\n\n从岗位分布来看，美术、设计、编剧和程序人员对生成式 AI 的接受度最低；而商务和管理人员则是最频繁的使用者。\n\n报告显示，58% 的商务人员会使用 AI 工具，47% 的公司高管也在使用，而基层员工的比例为 29%。PC Gamer 指出，这与管理层更依赖 AI 工具的行业观察一致。\n\n在匿名评论中，支持者认为外界对生成式 AI 的批评属于「道德恐慌」，甚至有人声称正在开发「让所有游戏开发者失业的平台」。\n\n反对者则强调生成式 AI「建立在盗窃与抄袭之上」，产出内容是「已有素材的混合」。一位英国游戏设计主管直言「我宁愿离开这个行业，也不使用生成式 AI」。\n\n报告总结称，游戏从业者对生成式 AI 了解越深入，越倾向于对其持保留甚至反对态度；但在实际工作中，许多人仍不得不使用相关工具，以避免在竞争中落后。\n\nKimi 海外收入首次超越国内，K2.5 推动全球用户激增\n\n据智能涌现报道，Kimi 在发布新模型 K2.5 后，海外市场表现出现显著跃升，公司的海外收入已超过国内收入，全球付费用户数在模型更新后的短短几天内实现 4 倍增长。\n\n与此同时，K2.5 在 Openrouter 上的热度快速攀升，排名已升至第三位，仅次于 Claude Sonnet 4.5 和 Gemini 3 Flash。\n\nKimi 的商业化进程始于去年 10 月。根据公司在 2025 年末的内部信披露，自 2025 年 11 月以来，海外 API 收入增长 4 倍，海内外付费用户月度环比增速均超过 170%。\n\nK2.5 是 Kimi 迄今最智能的模型，采用原生多模态架构，覆盖视觉理解、代码生成、Agent 集群、思考与非思考模式等能力。\n\n在 HLE、BrowseComp、SWE‑Bench Verified 等基准测试中，K2.5 达到开源 SOTA，部分指标超过 GPT‑5.2、Claude Opus 4.5 等闭源模型。\n\n在模型路线方面，Kimi 正逐渐靠近「Anthropic + Manus」的组合路径：\n\n一方面，通过开源模型权重与工具链，强化技术影响力；\n\n另一方面，在产品端明确定位为生产力工具，并持续强化 Agent 能力。\n\nK2.5 能调度多达 100 个 Agent，并行处理约 1500 个步骤，在大规模信息收集等场景中效率提升 3 至 10 倍。\n\nKimi 创始人杨植麟在 1 月 29 日的 Reddit AMA 中表示，高质量数据增长速度已赶不上算力增长，传统「预测下一个 token」的扩展方式边际效应下降。\n\n因此，团队选择通过 Agent Swarm 的方式扩展模型能力，将并行代理数量视为新的扩展维度。\n\n在产品层面，Kimi 继续强化 C 端生产力定位，并将此前内测的 OK Computer 更名为 Kimi Agent，强调统一品牌与通用能力。\n\n团队在复杂场景的可编辑性上投入较多，例如生成 PPT 后自动拆分元素编辑、从录屏中识别 UI 逻辑并生成前端代码、自动完成 Word 批注、Excel 建模、PPT 生成、PDF 翻译编辑等任务。\n\nKimi 总裁张予彤曾在去年 12 月表示，公司会刻意控制业务边界，专注大模型层、逻辑层、Agent 层，以及研究、PPT、数据分析、网站开发等偏复杂的生产力链路。\n\n随着行业普遍押注 Coding、Office 等刚需场景，Kimi 也在争取在基础模型保持第一梯队的同时，打造具有独特用户心智的 C 端产品。\n\nAI 视频热潮消退，Sora App 下载量连续两月下滑\n\n据 TechCrunch 报道，OpenAI 的 Sora 应用在去年 10 月登顶 App Store 后，近期出现增长乏力迹象。\n\n根据 Appfigures 的最新数据，Sora 在去年 12 月的下载量环比下降 32%，今年 1 月继续下滑 45%，仅录得 120 万次安装。同期消费者支出也下降 32%，从去年 12 月的 54 万美元降至今年 1 月的 36.7 万美元。\n\nSora 由 OpenAI 的视频生成模型 Sora 2 驱动，最初以 iOS 独占、邀请制形式上线，首日安装量突破 10 万次，并迅速登上美国 App Store 总榜第一，累计下载量已达 960 万次，累计消费支出约 140 万美元，其中美国贡献 110 万美元。\n\n不过，自移动端上线以来，应用热度持续回落。目前 Sora 在美国 App Store 总榜排名已跌至第 101 位，在 Google Play 的表现更弱，仅排在免费榜第 181 位。\n\n报道认为，多重因素导致了这波下滑：\n\nGoogle Gemini（尤其是 Nano Banana 模型）带来强劲竞争；\n\nMeta AI 推出的 Vibes 视频功能在去年 10 月吸引大量用户；\n\nSora 在版权管理上持续承压，早期允许用户生成包含热门角色的视频推动了增长，但随后从「默认允许」转向「默认禁止」商业 IP 使用，限制了内容创作空间；\n\n尽管 OpenAI 上月宣布与迪士尼达成合作，允许使用其角色生成视频，但目前尚未带来明显增长。\n\n此外，Sora 的社交属性也面临挑战。应用允许用户将自己或朋友「出演」视频，但许多用户并不愿意让他人使用自己的肖像生成内容；在缺乏熟悉面孔与热门 IP 的情况下，用户兴趣明显下降。\n\n千问 C 端应用团队四篇论文入选 ICLR 2026\n\n昨天，千问 C 端应用团队宣布，其四篇人工智能研究论文入选今年的国际学习表征会议（ICLR 2026）。\n\n论文覆盖扩散模型训练、多轮对话决策、信息验证机制及价值观对齐等关键方向，部分成果已在实际产品中落地，旨在提升 AI 助手在复杂场景下的稳定性、可靠性与实用性。\n\n据介绍，ICLR 与 NeurIPS、ICML 并列为机器学习领域三大顶级国际会议。本届投稿量接近 19000 篇，接收率为近年来新低，竞争激烈。\n\n团队在扩散语言模型（Diffusion Models）研究中，将 dLLM 掩码训练不稳定性拆解为三类噪声来源，并提出帕累托最优的无偏训练算法，以显著降低训练波动并提升图文生成质量。\n\n这意味着在内容创作、图文生成等应用中，AI 输出将更稳定。\n\n在医疗多轮对话推理方面，团队提出自适应树策略优化（ATPO）方法，使模型能够根据对话不确定性动态调整推理路径。\n\n当信息不足时主动追问关键问题，当线索明确时快速给出判断，帮助 AI 在医疗咨询等专业场景具备类似经验医生的「主动问诊」能力，减少无效往返。\n\n在信息检索与验证方向，团队构建了「提问 -- 解答 -- 验证」的自博弈强化学习框架，使模型无需人工标注即可持续自我验证与迭代，提升在知识密集型任务中的检索与核验能力。\n\n在价值观对齐研究中，团队引入信息论偏见消除方法，引导奖励模型关注真正与人类偏好相关的有效信号，减少冗长但信息密度不高的输出，使模型在训练中更聚焦用户核心需求，降低「表面迎合」式回答的出现。\n\n值得注意的是，本次入选的四篇论文相关代码均已开源，为行业在提升 AI 可用性与可靠性方面提供了可复用的技术参考。\n\n音乐出版商再诉 Anthropic：非法下载 2 万首歌曲\n\n据 TechCrunch 报道，多家音乐出版商近日对 Anthropic 提起新一轮诉讼，指控其在未经授权的情况下非法下载超过 2 万首受版权保护的歌曲、乐谱、歌词及音乐作品。\n\n索赔金额或将超过 30 亿美元，成为美国历史上规模最大的非集体版权诉讼之一。\n\n出版商方面表示，他们在去年 Bartz v. Anthropic 案件的取证过程中发现，Anthropic 的侵权范围远超此前掌握的约 500 首作品，实际涉及的盗版下载数量达到数万首。\n\n此前，Bartz 案中法官 William Alsup 曾裁定，AI 模型可以在版权内容上进行训练，但获取内容的方式不得违法；而 Anthropic 在该案中因侵权被判赔 15 亿美元，约 50 万名受影响作者每人获得约 3000 美元补偿。\n\n在本次诉讼中，出版商指出，法院去年 10 月曾驳回他们在原诉讼中追加盗版指控的请求，理由是出版商未能更早提出相关调查，因此他们选择单独提起新案。\n\n诉讼同时将 Anthropic CEO Dario Amodei 与联合创始人 Benjamin Mann 列为被告。\n\n出版商在声明中称，Anthropic 一边将自己包装为「AI 安全与研究公司」，一边却通过大规模盗版构建商业帝国，相关行为与其公开形象严重不符。\n\n宇树王兴兴：通用机器人十年可期，大模型是行业「诺奖级」突破\n\n近日，《扬声》栏目上线了宇树科技创始人兼 CEO 王兴兴的预告访谈，王兴兴在访谈中系统阐述了对通用型机器人、具身智能商业化路径及行业未来的判断。\n\n他认为，当下正处在智能升级的临界点，通用型机器人在「我们这代人」身上完全可实现，最快在未来十年内将带来「翻天覆地的变化」。\n\n王兴兴表示，谁能率先打造真正可用于机器人的大模型，谁就将成为全球最强的AI公司与机器人公司，「完全足够拿诺贝尔奖」。\n\n在他看来，智能不受物理规律限制，因此 AGI 与通用机器人在理论上必然可实现，其对社会的改变将超过电与蒸汽机时代。\n\n在谈及宇树的发展时，王兴兴回顾了从 2009 年自制人形机器人、到 2013 -- 2016 年以极低成本打造早期机器狗的经历。\n\n他强调，成本控制与软硬件一体化能力是宇树的核心竞争力。公司目前多款机构产品出货量位居全球前列，其中小型机构年出货量达数万台，占据约 60% 市场份额。\n\n对于行业热度，他认为 2025 年以来的爆发并非偶然，而是动力系统、关节架构与端到端 AI 控制等关键技术成熟的结果。宇树 2025 年人形机器人实际出货量超过 5500 台，本体量产下线超 6500 台。\n\n在商业化方面，王兴兴强调营收与利润是硬件公司的生命线，创业不能依赖融资驱动。\n\n他指出，行业中许多失败案例源于产品定义、招人、商业化与运营等环节的失误，而优秀公司的共性在于严谨、执行力强、持续学习与持续迭代。\n\n对于未来落地场景，他认为短期内机器人将在会议室、商场等 2B 场景承担基础任务；家庭场景因安全要求更高，将更晚普及。他强调，宇树的终极目标是让机器人真正「去干活」，为社会创造实际价值。\n\n王兴兴也谈到个人成长经历。他自述从小动手能力强、性格敏感且内向，这种敏感后来反而帮助他在合同、风险判断与团队管理中保持谨慎。\n\n他坦言，创业初期极度焦虑，2016 年辞职创业时资金未到位、团队仅三四人，但坚持技术突破与产品落地让公司逐步走上正轨。\n\n生数科技推出 Vidu Q3，视频生成进入「一键成片」阶段\n\n昨天，生数科技 Vidu 宣布推出全新的 Vidu Q3 视频生成模型，主打「为剧而生」，强调其在多镜头叙事、声画同步和多语言渲染上的全面升级。\n\nVidu Q3 是全球首个支持 16 s 音视频直出的模型，能够在单次生成中完成画面、声音、镜头调度与文本渲染，显著提升短剧、漫剧、影视内容的生产效率。\n\n官方称，该模型可实现「一镜到底」式的叙事生成，支持多角色对话、情绪节奏控制以及复杂转场，生成内容可直接作为成片交付。\n\n在功能层面，Vidu Q3 具备以下能力：\n\n支持最长 16 s 视频生成，声画同步输出；\n\n多镜头自由切换，可根据剧情张力自动调整景别；\n\n支持中、英、日多语种文字自然嵌入画面；\n\n适配漫剧、短剧、影视剧等多行业场景，强调工业化生产能力。\n\n生数科技表示，Vidu Q3 在国际权威 AI 基准测试机构 Artificial Analysis 最新榜单中排名中国第一、全球第二，位列马斯克 xAI Grok、Runway Gen-4.5、Google Veo 3.1 和 OpenAI Sora 2 之前。\n\n商汤开源 SenseNova‑MARS，多模态推理性能超越 Gemini‑3‑Pro\n\n昨天，商汤科技宣布正式开源多模态自主推理模型 SenseNova‑MARS，核心定位为 Agentic VLM（视觉语言模型），强调「自主规划步骤 + 多工具协作」的执行能力，提供 8B 与 32B 两个版本。\n\n据悉，该模型在多模态搜索与推理的核心基准测试中取得 69.74 分的综合成绩，超过 Gemini‑3‑Pro 与 GPT‑5.2 等闭源模型，并在多个开源与闭源模型对比中位列前列。\n\n模型通过强化学习（RL）训练，使其能够在推理过程中动态调用图像裁剪、文本搜索、图像搜索等工具，实现跨模态、多步骤的复杂任务处理。\n\n在性能方面，SenseNova‑MARS‑32B 在 MMSearch、HR‑MMSearch 等搜索导向评测中分别取得 74.27 分与 54.43 分，均领先于 Gemini‑3‑Pro 与 GPT‑5.2。\n\n在实际应用场景中，SenseNova‑MARS 能够处理「细节识别 + 信息检索 + 逻辑推理」的复合任务，例如识别赛车服上的微小 Logo、查询企业背景、比对人物信息等，并可在无人工干预的情况下完成完整推理链路。\n\n训练方法方面，模型采用「自动化数据合成 + 强化学习」的双阶段机制：\n\n第一阶段通过多模态智能体生成高复杂度、多跳推理数据，并加入闭环一致性校验以减少幻觉；\n\n第二阶段使用 BN‑GSPO 算法提升 RL 训练稳定性，使模型在多工具调用场景中保持一致表现。\n\n目前，模型、代码与数据集均已在 GitHub 与 Hugging Face 全面开源。\n\n论文: https://arxiv.org/abs/2512.24330\n\nGitHub: https://github.com/OpenSenseNova/SenseNova-MARS\n\nHugging Face: https://huggingface.co/sensenova/SenseNova-MARS-8B\n\n蚂蚁灵波开源 LingBot-VA：让世界模型直接驱动机器人行动\n\n昨天，蚂蚁灵波科技宣布开源具身世界模型 LingBot-VA。\n\n该模型提出自回归视频-动作世界建模框架，将大规模视频生成模型与机器人控制深度融合，可在生成「下一步世界状态」的同时直接推演并输出动作序列，使机器人能够像人类一样「边推演、边行动」。\n\n在真机评测中，LingBot-VA 展现出对复杂物理交互的高适应性，覆盖长时序、高精度及柔性物体操控三大类六项高难度任务，仅需 30~50 条真机演示数据即可完成适配，任务成功率相较业界强基线 Pi0.5 平均提升 20%。\n\n在仿真评测中，模型在双臂协同操作基准 RoboTwin 2.0 上首次将成功率提升至超过 90%，在长时序终身学习基准 LIBERO 上达到 98.5% 平均成功率，均刷新行业纪录。\n\nLingBot-VA 采用 Mixture-of-Transformers（MoT）架构，实现视频处理与动作控制的跨模态融合。其闭环推演机制会在每一步生成时纳入真实世界反馈，确保推演过程不偏离物理现实。\n\n为解决大规模视频世界模型在机器人端侧落地的计算瓶颈，模型引入异步推理管线、持久化记忆缓存与噪声历史增强策略，使其兼具大模型的理解深度与低延迟控制能力。\n\nLingBot-VA 亦与此前开源的 LingBot-World（模拟环境）、LingBot-VLA（智能基座）与 LingBot-Depth（空间感知）形成体系化能力。\n\n蚂蚁灵波表示，将依托 InclusionAI 社区持续推进开源开放，与行业共建具身智能基础能力，加速构建面向真实产业场景的 AGI 生态。目前，LingBot-VA 的模型权重与推理代码已全面开源。\n\nHugging Face: https://huggingface.co/robbyant/lingbot-va-base\n\n百度开源 PaddleOCR-VL-1.5，文档解析精度超越 DeepSeek-OCR2\n\n昨天，百度正式发布并开源新一代文档解析模型 PaddleOCR-VL-1.5。\n\n据介绍，该模型在全球权威文档解析评测榜单 OmniDocBench V1.5 中取得综合性能第一，整体精度达到 94.5%，超过 DeepSeek-OCR2、Gemini-3-Pro、Qwen3-VL-235B-A22B 以及 GPT-5.2 等主流模型。\n\nPaddleOCR-VL-1.5 采用仅 0.9B 参数的轻量化架构，但在多项关键指标上实现领先。其中，表格结构理解得分为 92.8 分，阅读顺序预测得分为 95.8 分，均位列榜单第一。\n\n百度表示，在文档阅读顺序预测任务中，该模型的版面逻辑解析错误率仅为同类模型的一半左右，在合同、财报等高复杂度文档场景中具备更高稳定性和可用性。\n\n此次发布的另一核心突破在于「异形框定位」能力。百度称，这是全球首次在 OCR 模型中实现对倾斜、弯折、拍照畸变等非规则文档形态的稳定识别，使移动拍照、扫描件变形及复杂光照条件下的文档解析成功率显著提升。\n\n该能力可应用于金融票据处理、档案数字化以及政务文档流转等真实业务场景。\n\n在功能层面，PaddleOCR-VL-1.5 在上一代基础上进一步集成印章识别、文本检测与识别能力，并针对生僻字、古籍文献、多语种表格、下划线和复选框等复杂结构进行优化。\n\n同时，模型新增对藏语、孟加拉语等语种的支持，并支持跨页表格自动合并与跨页段落标题识别，以缓解长文档解析中的结构断裂问题。\n\nGitHub: https://github.com/PaddlePaddle/PaddleOCR\n\nHugging Face: https://huggingface.co/PaddlePaddle/PaddleOCR-VL-1.5\n\nModelScope: https://modelscope.cn/models/PaddlePaddle/PaddleOCR-VL-1.5\n\n三星 Galaxy S26 系列发布时间曝光：2 月 25 日亮相\n\n据 Android Authority 报道，三星下一场 Galaxy Unpacked 发布会的日期已被提前曝光。\n\n知名爆料人 Evan Blass 分享了一张疑似官方海报，显示 Galaxy S26 系列将于 2 月 25 日正式亮相。\n\n相比去年 S25 系列在 1 月发布，今年发布时间延后约一个月，报道指出原因与三星对产品线的临时调整及 Edge 机型的取消有关。\n\n海报信息与此前多轮爆料一致。报道提到，三星虽然早前确认今年上半年会推出新旗舰，但至今尚未启动正式宣传。除 Ultra 机型已公布的「Privacy Display」隐私显示功能预告外，官方尚未披露更多细节。\n\n根据泄露内容，Galaxy S26、S26 Plus 与 S26 Ultra 将在发布会后立即开启预购，公开销售预计从 3 月 11 日开始。\n\n此外，三星也将在同场发布 Galaxy Buds 4 系列。该产品此前已多次曝光，最新爆料显示其定价将与 Buds 3 系列保持一致。\n\n三只松鼠发布 2025 年业绩预告，净利或创近年新低\n\n据界面新闻报道，三只松鼠近日披露 2025 年度业绩预告，公司预计全年归属于上市公司股东的净利润为 1.35 亿元至 1.75 亿元，同比下降 57.08%-66.89%；\n\n扣除非经常性损益后的净利润预计为 4500 万元至 6500 万元，同比降幅进一步扩大至 79.64%-85.91%。相关数据尚未经审计，最终结果将在今年年度报告中披露。\n\n业绩预告发布当日，公司股价短线反弹，1 月 29 日收盘报 24.88 元，上涨 4.85%。但在随后一个交易日开盘后股价再度走弱，截至发稿报 24.24 元，总市值约 97.36 亿元。\n\n三只松鼠将利润大幅下滑归因于多重因素：\n\n其一，销售旺季出现结构性错档，叠加坚果原料价格大幅上涨，公司主动调整销售结构，导致利润空间被压缩；\n\n其二，全球坚果主产区受气候影响减产，进一步推高原料成本；\n\n此外，公司在社区零售新赛道的战略投入也对阶段性利润造成影响。\n\n从历史表现看，公司业绩在 2021 年达到阶段性高点，归母净利润为 4.11 亿元。随后受行业竞争加剧、电商红利消退等因素影响，利润出现波动，2022 年和 2023 年分别降至 1.29 亿元和 2.2 亿元，2024 年回升至 4.08 亿元。\n\n今年前三季度，公司营收同比增长 8.22% 至 77.59 亿元，但归母净利润同比下降 52.91% 至 1.61 亿元，扣非净利润降幅更达到 78.57%。\n\n为缓解成本压力，三只松鼠已在去年 10 月上调 35 款核心产品供货价，涨幅在 0.2 元至 10 元之间，部分产品涨幅超过 20%；今年 1 月，公司再次启动线下分销坚果礼出厂价调整，但提价仍未完全覆盖原料上涨带来的成本压力。\n\n目前，坚果品类仍占公司收入约 50%，烘焙与综合零食两大新兴品类合计占比不足 40%，且竞争激烈。\n\n线上渠道方面，抖音、天猫、京东三大平台贡献了 78.42% 的收入，但抖音渠道增速已从 2024 年的 81.73% 放缓至今年上半年的 20.75%，流量红利减弱趋势明显。\n\n业内人士认为，三只松鼠的业绩压力也是当前休闲零食行业整体竞争加剧的缩影。随着流量红利见顶，企业在短期业绩与长期能力建设之间面临更艰难的平衡；\n\n线下市场中，以零食很忙、赵一鸣零食为代表的量贩零食品牌凭借高性价比和快速扩张，对传统品牌形成明显冲击。\n\n好想来回应门店称重多收费：公司称因新员工操作失误\n\n据界面新闻报道，量贩零食品牌「好想来」近日因门店称重被指多收费引发关注。\n\n事件源于一名消费者在中国浙江慈溪市的门店购买 23 件零食后，回家自行称重发现多款商品的实际重量低于门店称重结果，累计差值约 129 克，按其计算多付了 6.41 元。\n\n好想来方面表示，公司在关注到相关内容后已进行内部核查，确认问题源于新员工操作失误，相关员工已重新培训。企业称已主动联系消费者致歉并协商赔偿。\n\n消费者此前发布的视频显示，多款商品存在不同程度的重量差异，例如笋尖鸡肉肠门店称重为 76 克，自行称重为 62.2 克；无穷盐焗大鸡翅门店称重 216 克，自行称重 202 克；\n\n其他如三角芝士、麦烧、乳酪包、海苔、小糖果、面包、鸡腿等均存在 12 克左右的差值。\n\n该消费者还称，好想来曾向平台投诉要求下架视频，理由为「个人摆拍、真实性不确定」。\n\n慈溪市市场监管局工作人员表示，监管部门已于 1 月 27 日对涉事门店的秤具进行检查，未发现异常。好想来方面强调，所有门店计量器具均经检定合格，并执行每日校验制度，如发现误差会立即停用并维修或更换。\n\n公开资料显示，好想来为万辰集团旗下品牌，于 2023 年通过整合多个零食品牌形成统一运营体系，总部位于江苏南京，SKU 数量近 2000 个。\n\n按 2024 年商品交易总额计算，其以 426 亿元 GMV 位居中国零食饮料零售榜首。万辰集团去年已向港交所递表，2025 年上半年营收达 225.83 亿元，同比增长 106.89%，其中量贩零食业务贡献近 99%。\n\n《庇护之地》全球同步上映\n\n昨天，《庇护之地》在全球同步公映，并释出「绝命搏杀」正片片段及上映日海报。\n\n影片讲述特工迈克尔 · 梅森隐居孤岛，却因营救少女杰茜被迫重返战场，在黑鸢组织的追杀下揭开阴谋并展开生死突围。\n\n作为「开年一部杰森」的延续，《庇护之地》成为动作片爱好者的热门选择。映前活动中，影迷以角色造型打卡、还原名场面，映后更有观众表示将二刷支持。\n\n✨ 是周末啊！ One Fun Thing｜70 岁「萌新」成龙官宣入驻小红书，首日吸粉超 8 万\n\n昨天，演员成龙于正式入驻小红书，账号简介显示「70 岁小红书萌新报道！用最快乐的方式，做认真的事。」目前已吸引约 8.2 万粉丝关注。\n\n成龙发布的首条视频为自我介绍，并表示希望能与每一位「momo」成为朋友，未来会常来平台互动。\n\n成龙还在评论区发表语音，称自己知道小红书是个「特别好玩特别有梗的地方」，但刚加入还不太熟悉，希望用户能成为他的「小红书导游」，并向网友提问「小红书最火的是什么呢」。\n\n此次入驻引发平台用户热烈讨论，相关互动持续攀升。\n\n周末看什么｜经典心理惊悚片《闪灵》回归院线\n\n经典心理惊悚片《闪灵》昨天起正式在大陆院线以 4K 修复版重映。\n\n影片改编自斯蒂芬 · 金同名小说，1980 年上映后长期被视为恐怖类型的标杆作品。剧情围绕作家杰克·托兰斯展开。他在冬季受雇看管远望酒店，并携妻子温蒂与儿子丹尼入住。\n\n大雪封山后，酒店只剩三人。丹尼逐渐出现「闪灵」力，看到酒店中诡异的幻象；温蒂则察觉丈夫情绪失控；杰克在封闭环境与心理压力下逐步崩溃，最终走向暴力失控。\n\n影片以对称构图、长镜头与斯坦尼康技术构建压迫感，是库布里克美学与类型创新的代表案例。豆瓣评分为 8.3 分，超过 58 万人评价，位列恐怖片高分区间前列。\n\n买书不读指南｜《温柔的讲述者》\n\n奥尔加 · 托卡尔丘克获得诺贝尔文学奖后的首部作品《温柔的讲述者》收录 12 篇散文、演讲与思想札记，主题跨越文学、心理学、哲学、神话学与生物学等多个维度。\n\n她在书中持续追问「如何讲述世界」：反对自 19 世纪以来的「全知叙述者」传统，主张以容纳矛盾、保留暧昧的方式理解现实。\n\n在社交媒体时代，她提出「罗得之妻综合征」这一隐喻，指向人们在算法驱动下不断回头张望、沉溺碎片化信息的认知困境。\n\n「拒绝洞察意味着成为恶的同谋，而语言本身塑造我们理解他者的方式。」她呼吁作家发明独立的「个人语言」，像指纹一样独特，能够抵抗集体语言的浑浊，并为世界重新命名。\n\n游戏推荐｜生存攀岩游戏《孤山独影》\n\n《孤山独影》是一款由 The Game Bakers 开发的生存攀岩游戏，以高度写实的攀登机制与叙事驱动的冒险流程为核心。\n\n玩家在游戏中扮演专业登山者艾瓦，目标是登顶被称为「神之山」的险峻山峰。\n\n游戏采用自由攀登设计，玩家可在岩壁上寻找支点、调整姿势与平衡，并在不同地形中解决攀登难题。\n\n若操作失误，角色会从高处坠落，形成极强的紧张感。开发团队将每一面岩壁视作「BOSS 战」，强调体力、节奏与策略的综合挑战。\n\n剧情部分围绕艾瓦的个人追求展开，玩家在攀登途中会遇到其他登山者，倾听他们的故事，并逐步揭开山脉的历史背景。\n\n叙事重点在于「梦想的代价」与「极限挑战的意义」，呈现出兼具孤独感与探索感的旅程。\n\n资源管理是另一核心机制，包括岩钉、防滑粉、缠手带、食物、水与药品等。玩家需在山体中布置营地、收集补给，以维持长线攀登的生存能力。\n\n价格方面，《孤山独影》Steam 国区定价为 108 元，首发 9 折后价格为 97.2 元。Steam 当前共收录 780 条玩家评测，整体评价为「特别好评」。\n\n制糖工厂发布 2026 款硬糖 C³，代际升级不一般\n\n制糖工厂重磅新品 2026 款硬糖 C³，12 月 23 日正式开启预售。这是一次跨代升级，集极致便携、智能调度与高性能输出及全面协议于一体，将三口充电器做到极致。麻烦研究透，才能给你最好的。\n\n它精致玲珑，虽为三 C 口设计，却出乎意料的小，随身携带无烦忧，是差旅充电的最优解决方案；\n\n它是优雅的实力派，打破市面上常见的「鸡血头」模式，三口满载持续强，真正做到以一抵三，搞定数码全家桶；\n\n它是当下少有的支持 AVS 低温动态快充的多口充电器，不仅如此，还是 AVS+PPS 双核双持，苹果安卓都不掉链；\n\n它搭载着制糖工厂自研的 TimiC 时序调度 2.0 充电方案，打破常规智能，毫秒级响应，全程时序调度，60s 动态优化，实现更智能高效的充电体验；\n\n它也是继硬糖 A 充后，第二台可通过制糖工厂原创 OTW 持续进化的硬糖充电器，可定制多设备专属模式，亦可持续升级 UFCS 2.0 等多种协议。\n\n欢迎加入 APPSO AI 社群，一起畅聊 AI 产品，获取，解锁更多 AI 新知\n\n✉️ 邮件标题「姓名+岗位名称」（请随简历附上项目/作品或相关链接）"
  },
  {
    "source": "Ad Hoc News",
    "company": "Hugging Face",
    "title": "TrustBastion: Android-Malware nutzt KI-Plattform Hugging Face für komplette Geräteübernahme",
    "date": "2026-02-01T12:05:17Z",
    "url": "https://www.ad-hoc-news.de/boerse/ueberblick/trustbastion-android-malware-nutzt-ki-plattform-hugging-face-fuer/68540733",
    "content": "Eine neue Android-Schadsoftware tarnt sich als System-Update und nutzt die KI-Plattform Hugging Face, um Sicherheitsfilter zu umgehen. Sie erlangt totale Kontrolle über Smartphones.\n\nEine raffinierte Android-Schadsoftware tarnt sich als System-Update und kapert Smartphones. Das Besondere: Sie nutzt die populäre KI-Plattform Hugging Face für ihre Angriffe und entgeht so klassischen Sicherheitsfiltern. Forscher von Bitdefender haben die als TrustBastion getaufte Bedrohung entdeckt.\n\nDie Infektion beginnt mit einem perfiden Trick, der die Sicherheitsängste der Nutzer ausnutzt. Die Malware tarnt sich zunächst als legitime Sicherheits-App namens \"TrustBastion\". Nutzer werden über aggressive Werbung oder Pop-up-Warnungen darauf aufmerksam gemacht, die behaupten, ihr Gerät sei bereits kompromittiert.\n\nNach der Installation startet die zweite, kritische Phase. Die App zeigt einen Pop-up an, der optisch kaum von echten Android- oder Google-Play-Systemupdate-Dialogen zu unterscheiden ist. Diese gefälschte Benachrichtigung fordert ein \"dringendes Update\". Stimmt das Opfer zu, lädt die App jedoch keinen Patch von Google herunter, sondern einen bösartigen Remote Access Trojaner (RAT).\n\nMobile Angriffe wie TrustBastion zeigen, wie perfide Social-Engineering geworden ist und wie Angreifer vertrauenswürdige Infrastruktur missbrauchen. Ein kostenloses E-Book fasst aktuelle Cyber-Security-Trends zusammen, erklärt, wie polymorphe Loader funktionieren und liefert konkrete Schutzmaßnahmen für Smartphones, Netzwerke und Administratoren. Mit praxisnahen Checklisten zur Erkennung bösartiger Updates, Sofortmaßnahmen und Empfehlungen für die Absicherung von Android-Apps - ideal für IT-Verantwortliche und technikaffine Leser. Gratis Cyber-Security-E-Book jetzt herunterladen\n\nDiese Methode ist so effektiv, weil sie das Vertrauen in systemeigene Meldungen ausnutzt. Die Angreifer kopieren das Design echter Android-Hinweise täuschend echt und tricksen die Nutzer so aus, die Installation der Spionage-Software selbst zu autorisieren - im Glauben, ihr Gerät zu schützen.\n\nDie eigentliche Innovation dieser Kampagne ist der Missbrauch vertrauenswürdiger Cloud-Infrastruktur. Die Bitdefender-Forscher Alecsandru Cătălin Daj und Silviu Stahie berichten, dass die Betreiber ihre schädlichen Dateien auf Hugging Face hosten. Diese Plattform ist eigentlich für den Austausch von KI-Modellen und Datensätzen gedacht.\n\nDurch das Hosten auf der vertrauten Domain () umgehen die Angreifer viele Standard-Netzwerksicherheitsfilter, die Traffic zu bekannten bösartigen Servern blockieren würden. Die Forscher beobachteten zudem den Einsatz von \"Server-seitigem Polymorphismus\". Dabei wird die Malware-Datei etwa alle 15 Minuten leicht verändert, was Tausende einzigartige Dateivarianten erzeugt. Für automatisierte Antiviren-Systeme, die auf statische Dateisignaturen angewiesen sind, wird die Erkennung so extrem schwierig.\n\nDen Berichten zufolge luden die Angreifer in weniger als einem Monat über 6.000 einzigartige Android-Dateien auf die Plattform hoch. Dieser hochvolumige, automatisierte Ansatz deutet auf eine gut ausgestattete Operation hin, die auf langfristige Persistenz auf den Geräten der Opfer abzielt.\n\nSobald das gefälschte Update installiert ist, strebt die Malware aggressiv nach weitreichenden Berechtigungen. Die Spyware fordert Zugriff auf Android Accessibility Services an - einen mächtigen Funktionssatz, der eigentlich Nutzern mit Behinderungen helfen soll. Die Malware tarnt diese Anfrage als notwendigen Schritt für die \"Sicherheits-App\".\n\nSicherheitsexperten warnen: Die Erteilung dieser Berechtigungen an bösartige Apps gleicht der Übergabe des Generalschlüssels für das Gerät. Mit diesen Rechten kann TrustBastion alle Nutzeraktivitäten überwachen, Bildschirminhalte erfassen und sensible Informationen abfangen. Die Malware kann zudem Screen Overlays einblenden - gefälschte Login-Masken, die über legitimen Apps liegen, um Zugangsdaten zu stehlen. Die Bitdefender-Analyse hob besonders die Fähigkeit hervor, Finanz-, Banking- und Messaging-Apps wie WeChat und Alipay nachzuahmen.\n\nDarüber hinaus kann der RAT seine eigene Deinstallation verhindern. Indem er die Bildschirmkontrolle übernimmt, kann er Versuche des Nutzers, die App zu entfernen oder ihre Berechtigungen zu widerrufen, automatisch abweisen oder blockieren. Die Malware schließt sich so effektiv im Gerät ein.\n\nDiese Entwicklung unterstreicht einen wachsenden Trend in der Cyber-Bedrohungslandschaft für 2026: die Weaponisierung vertrauenswürdiger Cloud- und KI-Plattformen. Erst wenige Tage vor der Enthüllung von TrustBastion waren Berichte über \"Moltbot\" aufgetaucht, eine weitere Malware-Kampagne, die sich als KI-Assistent tarnt.\n\nDer Missbrauch von Plattformen wie Hugging Face stellt Netzwerkverteidiger vor eine große Herausforderung. Sie können diese Domains nicht einfach blockieren, ohne legitime Geschäfts- und Forschungs-aktivitäten zu stören. Branchenanalysten gehen davon aus, dass das \"Dropper\"-Modell - bei dem eine harmlos aussehende App die eigentliche Malware später nachlädt - zum Standard für Android-Angriffe wird. Diese Methode ermöglicht es bösartigen Apps, die initialen Sicherheitsprüfungen der App-Stores zu umgehen, da der schädliche Code zum Zeitpunkt des Downloads noch nicht vorhanden ist.\n\nDie Verwendung gefälschter Systemupdates ist zwar ein wiederkehrendes Thema, doch die täuschende Echtheit der Nachahmung in der TrustBastion-Kampagne schlägt neue Alarmglocken. Da mobile Betriebssysteme immer sicherer werden, setzen Angreifer zunehmend auf Social Engineering, um Nutzer zu täuschen, diese Schutzmaßnahmen manuell zu umgehen.\n\nSicherheitsanbieter werden ihre Erkennungsmechanismen anpassen, um die spezifischen Verhaltensweisen des TrustBastion-Loaders besser zu identifizieren. Doch die schnelle Polymorphie der Malware bedeutet, dass signaturbasierte Erkennung allein wahrscheinlich unzureichend bleiben wird.\n\nExperten erwarten, dass Google und andere Plattformbetreiber in künftigen Android-Versionen strengere Einschränkungen für die Nutzung von Accessibility Services einführen werden. Bis dahin sollten Nutzer extrem skeptisch sein, wenn eine App unmittelbar nach dem Öffnen ein sofortiges Update verlangt - besonders dann, wenn die App nicht aus dem offiziellen Google Play Store stammt.\n\nDer Vorfall erinnert daran, dass die Aufforderung zu einem \"dringenden Update\" eine der wirksamsten Waffen im Arsenal von Cyberkriminellen ist. Die Grenze zwischen legitimen App-Verhalten und bösartiger Täuschung verschwimmt zunehmend und erfordert von Nutzern höchste Wachsamkeit.\n\nPS: Wenn Sie sich gegen solche Angriffe wappnen wollen, ist dieser kostenlose Leitfaden einen Blick wert. Er erklärt verständlich, welche organisatorischen und technischen Maßnahmen sofort helfen - von der Absicherung mobiler Endgeräte über die Erkennung polymorpher Malware bis zu Schulungsmaßnahmen gegen Social Engineering. Außerdem werden neue gesetzliche Anforderungen und KI-Risiken eingeordnet. Holen Sie sich die praktischen Checklisten und Sofortmaßnahmen. Jetzt kostenlosen Cyber-Security-Leitfaden sichern"
  },
  {
    "source": "SC Media",
    "company": "Hugging Face",
    "title": "Android malware campaign abuses Hugging Face for credential theft",
    "date": "2026-01-30T22:59:30Z",
    "url": "https://www.scworld.com/brief/android-malware-campaign-abuses-hugging-face-for-credential-theft",
    "content": "A new Android malware campaign is using the Hugging Face platform as a repository for thousands of variations of an APK payload that collects credentials for popular financial and payment services, according to a recent report by Bleeping Computer.\n\nThe campaign begins with users being tricked into installing a dropper app named TrustBastion, which masquerades as a security tool. This app then prompts users for a mandatory update, which redirects to a Hugging Face dataset repository hosting the malicious APK. The final payload is downloaded via Hugging Face's content distribution network. To evade detection, the threat actor employs server-side polymorphism, generating new payload variants every 15 minutes. The malware exploits Android's Accessibility Services to capture screenshots, steal credentials through fake login interfaces for services like Alipay and WeChat, and exfiltrate data to its command-and-control server. It also attempts to steal lock screen codes and block uninstallation.\n\nThis incident highlights the evolving tactics of threat actors who leverage trusted platforms like Hugging Face for malicious purposes. It underscores the importance of user vigilance in downloading applications and reviewing requested permissions."
  },
  {
    "source": "Tom's Guide",
    "company": "Hugging Face",
    "title": "Hugging Face AI platform used to deliver Android malware via fake apps: don't fall for this",
    "date": "2026-01-30T19:59:23Z",
    "url": "https://www.tomsguide.com/computing/malware-adware/hugging-face-ai-platform-used-to-deliver-android-malware-via-fake-apps-dont-fall-for-this",
    "content": "False antivirus software could help steal sensitive information\n\nHackers are reportedly using the popular Hugging Face AI platform to release Android malware that can take over your device. The malware is delivered via a fake app.\n\nFor the unfamiliar, Hugging Face is an open platform that hosts AI tools and machine learning bots. Users and creators can distribute and download AL, NLP and ML models. Unfortunately, sometimes it can be used to release bad models as well.\n\nResearchers at the cybersecurity firm Bitdefender found that this new malware first appeared in an app called TrustBastion. Hugging Face \"doesn't seem to have meaningful filters that govern what people can upload,\" the researchers said.\n\nApparently, TrustBastion pretends to be an Android antivirus program by \"offering\" virus protection, phishing defense and malware blocking. In reality, this app is \"scareware\": once you install it, it claims your device is infected and demands an update. Once you update the app, it installs the malicious code.\n\nBitdefender says TrustBastion connects to a third-party server, which then redirects to a Hugging Face repository with 6,000 commits. Despite being reported, Bitdefender says a new repository almost immediately appeared with a new name and icons, but the same malicious code.\n\nThis Trojan malware is quite powerful. According to Bitdefender, it can take screenshots, display fake login interfaces for financial serives and capture your lock screen pin. That information is then sent to a third-party server.\n\nThe simplest thing you can do is download Android apps only from reputable sources with some form of moderation and security filtering, such as the Google Play Store or the Samsung Galaxy Store. Even in those places, be sure to scour the reviews and note the overall downloads and rating.\n\nAvoid sideloading APKs outside of the store. If you are triple-checking that the publisher and URL are correct before you download. Be wary of any apps that ask for accessibility permissions.\n\nYou should periodically scan your Android device with Play Protect and bolster your security with some of the best Android antivirus apps."
  },
  {
    "source": "TechRadar",
    "company": "Hugging Face",
    "title": "Hugging Face platform hijacked to send out Android malware - here's what we know so far",
    "date": "2026-01-30T16:07:01Z",
    "url": "https://www.techradar.com/pro/security/hugging-face-platform-hijacked-to-send-out-android-malware-heres-what-we-know-so-far",
    "content": "Campaign persisted with new repositories despite takedown, highlighting risks of unverified app source\n\nHackers are abusing the Hugging Face platform to deliver Android malware which can entirely take over compromised endpoints, experts have warned.\n\nHugging Face is an open platform for AI tools and machine learning, where users can host and distribute AL, NLP, or ML models - but it seems it also sometimes used as a launchpad for poisoned models too.\n\nIn this case, the crooks used it to deliver Android malware, cybersecurity researchers at Bitdefender noted, starting with a dropper app called TrustBastion.\n\nThis app acts like an Android antivirus solution - it offers virus protection, defense against phishing, malware, and fraudulent SMS messages. However, TrustBastion engages in scareware - as soon as the victim installs it, it says the device is infected with malware. Then, it demands the user update the app, which is when the malicious code is actually installed.\n\nTo deliver the malware, TrustBastion connects to a third-party server, which redirects to a Hugging Face repository where the malicious APK is hosted. From there, the malware is downloaded and delivered via Hugging Face's CDN.\n\nWhile these types of campaigns are rather common, unfortunately this one was also successful. In less than a month of activity, it accumulated more than 6,000 commits, Bitdefender said. To make matters worse, as soon as the campaign was spotted and terminated, a new repository popped up, named 'Premium Club', using new icons, but retaining the same malicious code.\n\nThe malware itself is rather powerful. It can grab screenshots, display fake login interfaces for popular payment services, and steal the lock screen code. Everything is then exfiltrated to a third-party server.\n\nThe best way to defend against this type of malware is to only download Android apps from reputable sources, such as the Google Play Store, or the Galaxy Store. Also, make sure to read through the reviews, and be mindful of the number of downloads and overall rating.\n\nVia BleepingComputer"
  },
  {
    "source": "newsletter.pricingsaas.com",
    "company": "Hugging Face",
    "title": "5 ways to reframe your pricing and packaging",
    "date": "2026-01-30T13:28:16Z",
    "url": "https://newsletter.pricingsaas.com/p/5-ways-to-reframe-your-pricing-and",
    "content": "Plus: Updates from Klaviyo, Lindy, Eden AI, DeepBrain, and Hugging Face.\n\nEach week, we break down real pricing and packaging moves from SaaS leaders and extract the ideas worth stealing. This week, we observed five ways to reframe your pricing and packaging.\n\nTLDR:\n\nKeep reading to go deeper on each idea, with concrete examples and commentary.\n\nPS. We also released our latest Trends Report. We tracked 500 pricing pages for two years, and unpacked our biggest findings. Get the full report →\n\nNeed a sounding board? We've helped hundreds of teams figure out their pricing strategy and stack. Book a free Pricing Review →\n\nKlaviyo updated the pricing page to deprioritize the pricing calculator, and highlight the free plan. This suggests they're making a push to drive acquisition, and use a PLG/SLG combo to help free customers get the most out of the product, before expanding the account.\n\nWhat you can steal: This goes both ways, whether you're focused on adoption or monetization, the layout of your pricing page can support either. Previously, the free plan was an afterthought for Klaviyo. That clearly changed, and this update should push more visitors to get started for free.\n\nLindy launched a personalized AI Assistant, and quickly made it the centerpiece of the Enterprise plan. They did this by adding the AI Assistant as the first Enterprise feature in the main pricing menu, and by updating the banner on the Enterprise plan from \"Unlimited Phone Calls\" to \"AI Assistant for Teams.\"\n\nWhat you can steal: Volume has always been a key differentiator for Enterprise plans, but \"work completed\" is an even more compelling value prop. This reframe creates a new value ladder for Lindy where the more you pay, the more work the product does for you, shifting the value for Enterprise customers to outcomes instead of more inputs.\n\nEden AI previously positioned its higher tier strictly as Professional Services. They've since reframed it as an Advanced AI Platform, with Professional Services being an important ingredient.\n\nWhat you can Steal: The lines between services and software are blurring. Eden was already building and maintaining a more complex environment for these customers, clearly going beyond pure services. Historically, similar services have been billed as a one-time implementation or onboarding fee. Because of that, I wonder if Eden ran into friction for recurring professional services payments. This reframe is a way to position a true combo of SaaS + Services, drive recurring revenue, and ensure they can charge a premium for any human-intensive solutions they're providing.\n\nShare\n\nWe've been talking about credits a lot lately, and its clear buyers are starting to get tired of credit complexity. That complexity can include vague definitions, unclear expectations on how credits are drawn down, and ops around rollovers and top-ups.\n\nWhat you can steal: Making Annual the default does a few things. It anchors visitors to the annual credit allotment (a bigger number), it sets the expectation that annual is the most common approach, and it gives the sales team a talk track around credit flexibility by granting all annual credits upfront. If you're shifting to a credit model, keep this in mind.\n\nHugging Face prioritized the placement of the Data Storage add-on just below Hugging Face Hub, making it clear that it's a focus. This makes sense, as Data Storage can be a bottleneck for AI/ML workflows. Adding Data Storage also makes Hugging Face stickier, and gives them another recurring revenue layer."
  }
]